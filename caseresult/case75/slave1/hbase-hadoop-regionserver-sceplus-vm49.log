Fri Jul 11 00:51:49 PDT 2014 Starting regionserver on sceplus-vm49
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 128203
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 32768
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 128203
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
2014-07-11 00:51:49,700 INFO  [main] util.VersionInfo: HBase 0.98.3-hadoop1
2014-07-11 00:51:49,701 INFO  [main] util.VersionInfo: Subversion git://acer/usr/src/Hadoop/hbase -r d5e65a9144e315bb0a964e7730871af32f5018d5
2014-07-11 00:51:49,701 INFO  [main] util.VersionInfo: Compiled by apurtell on Sat May 31 19:34:57 PDT 2014
2014-07-11 00:51:49,939 INFO  [main] util.ServerCommandLine: env:JAVA_HOME=/usr/lib/jvm/java-1.6.0-openjdk-amd64/
2014-07-11 00:51:49,939 INFO  [main] util.ServerCommandLine: env:SHLVL=3
2014-07-11 00:51:49,939 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_DIR=/home/hadoop/hbase/bin/../logs
2014-07-11 00:51:49,939 INFO  [main] util.ServerCommandLine: env:HBASE_HOME=/home/hadoop/hbase/bin/..
2014-07-11 00:51:49,939 INFO  [main] util.ServerCommandLine: env:HBASE_OPTS=-XX:+UseConcMarkSweepGC   -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm49.log -Dhbase.home.dir=/home/hadoop/hbase/bin/.. -Dhbase.id.str=hadoop -Dhbase.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64 -Dhbase.security.logger=INFO,RFAS
2014-07-11 00:51:49,939 INFO  [main] util.ServerCommandLine: env:HBASE_ENV_INIT=true
2014-07-11 00:51:49,939 INFO  [main] util.ServerCommandLine: env:SSH_CLIENT=9.1.143.58 44892 22
2014-07-11 00:51:49,940 INFO  [main] util.ServerCommandLine: env:HBASE_HEAPSIZE=10240
2014-07-11 00:51:49,940 INFO  [main] util.ServerCommandLine: env:MAIL=/var/mail/hadoop
2014-07-11 00:51:49,940 INFO  [main] util.ServerCommandLine: env:HBASE_ZNODE_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.znode
2014-07-11 00:51:49,940 INFO  [main] util.ServerCommandLine: env:PWD=/home/hadoop/hbase
2014-07-11 00:51:49,940 INFO  [main] util.ServerCommandLine: env:LOGNAME=hadoop
2014-07-11 00:51:49,940 INFO  [main] util.ServerCommandLine: env:HBASE_MANAGES_ZK=true
2014-07-11 00:51:49,940 INFO  [main] util.ServerCommandLine: env:HBASE_NICENESS=0
2014-07-11 00:51:49,940 INFO  [main] util.ServerCommandLine: env:HBASE_REST_OPTS=
2014-07-11 00:51:49,940 INFO  [main] util.ServerCommandLine: env:LD_LIBRARY_PATH=/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64/server:/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64:/usr/lib/jvm/java-6-openjdk-amd64/jre/../lib/amd64::/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-07-11 00:51:49,940 INFO  [main] util.ServerCommandLine: env:MALLOC_ARENA_MAX=4
2014-07-11 00:51:49,941 INFO  [main] util.ServerCommandLine: env:SSH_CONNECTION=9.1.143.58 44892 9.1.143.59 22
2014-07-11 00:51:49,941 INFO  [main] util.ServerCommandLine: env:SHELL=/bin/bash
2014-07-11 00:51:49,941 INFO  [main] util.ServerCommandLine: env:HBASE_PID_DIR=/var/hadoop/pids
2014-07-11 00:51:49,941 INFO  [main] util.ServerCommandLine: env:HBASE_ROOT_LOGGER=INFO,RFA
2014-07-11 00:51:49,943 INFO  [main] util.ServerCommandLine: env:CLASSPATH=/home/hadoop/hbase/bin/../conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-07-11 00:51:49,943 INFO  [main] util.ServerCommandLine: env:HBASE_THRIFT_OPTS=
2014-07-11 00:51:49,943 INFO  [main] util.ServerCommandLine: env:PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games
2014-07-11 00:51:49,943 INFO  [main] util.ServerCommandLine: env:USER=hadoop
2014-07-11 00:51:49,943 INFO  [main] util.ServerCommandLine: env:HBASE_SECURITY_LOGGER=INFO,RFAS
2014-07-11 00:51:49,943 INFO  [main] util.ServerCommandLine: env:HOME=/home/hadoop
2014-07-11 00:51:49,944 INFO  [main] util.ServerCommandLine: env:HBASE_LIBRARY_PATH=/home/hadoop/hbase/lib/native/Linux-amd64-64
2014-07-11 00:51:49,944 INFO  [main] util.ServerCommandLine: env:HBASE_START_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.autorestart
2014-07-11 00:51:49,944 INFO  [main] util.ServerCommandLine: env:XDG_SESSION_ID=219
2014-07-11 00:51:49,944 INFO  [main] util.ServerCommandLine: env:HBASE_LOGFILE=hbase-hadoop-regionserver-sceplus-vm49.log
2014-07-11 00:51:49,944 INFO  [main] util.ServerCommandLine: env:XDG_RUNTIME_DIR=/run/user/1001
2014-07-11 00:51:49,944 INFO  [main] util.ServerCommandLine: env:LANG=en_US.UTF-8
2014-07-11 00:51:49,944 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_PREFIX=hbase-hadoop-regionserver-sceplus-vm49
2014-07-11 00:51:49,944 INFO  [main] util.ServerCommandLine: env:HBASE_IDENT_STRING=hadoop
2014-07-11 00:51:49,946 INFO  [main] util.ServerCommandLine: vmName=OpenJDK 64-Bit Server VM, vmVendor=Sun Microsystems Inc., vmVersion=23.25-b01
2014-07-11 00:51:49,947 INFO  [main] util.ServerCommandLine: vmInputArguments=[-Dproc_regionserver, -XX:OnOutOfMemoryError=kill -9 %p, -Xmx10240m, -XX:+UseConcMarkSweepGC, -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs, -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm49.log, -Dhbase.home.dir=/home/hadoop/hbase/bin/.., -Dhbase.id.str=hadoop, -Dhbase.root.logger=INFO,RFA, -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64, -Dhbase.security.logger=INFO,RFAS]
2014-07-11 00:51:50,180 DEBUG [main] regionserver.HRegionServer: regionserver/sceplus-vm49.almaden.ibm.com/9.1.143.59:60020 HConnection server-to-server retries=350
2014-07-11 00:51:50,575 INFO  [main] ipc.RpcServer: regionserver/sceplus-vm49.almaden.ibm.com/9.1.143.59:60020: started 10 reader(s).
2014-07-11 00:51:50,677 INFO  [main] impl.MetricsConfig: loaded properties from hadoop-metrics2-hbase.properties
2014-07-11 00:51:50,691 INFO  [main] impl.MetricsSinkAdapter: Sink file-all started
2014-07-11 00:51:50,754 INFO  [main] impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.
2014-07-11 00:51:50,755 INFO  [main] impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2014-07-11 00:51:50,756 INFO  [main] impl.MetricsSystemImpl: HBase metrics system started
2014-07-11 00:51:50,761 INFO  [main] impl.MetricsSourceAdapter: MBean for source jvm registered.
2014-07-11 00:51:50,766 INFO  [main] impl.MetricsSourceAdapter: MBean for source IPC,sub=IPC registered.
2014-07-11 00:51:50,852 INFO  [main] impl.MetricsSourceAdapter: MBean for source ugi registered.
2014-07-11 00:51:50,852 WARN  [main] impl.MetricsSystemImpl: Source name ugi already exists!
2014-07-11 00:51:50,856 DEBUG [main] util.DirectMemoryUtils: Failed to retrieve nio.BufferPool direct MemoryUsed attribute.
javax.management.InstanceNotFoundException: java.nio:type=BufferPool,name=direct
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1117)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:678)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:682)
	at org.apache.hadoop.hbase.util.DirectMemoryUtils.<clinit>(DirectMemoryUtils.java:72)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.instantiateBlockCache(CacheConfig.java:396)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.<init>(CacheConfig.java:179)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:621)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:534)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2393)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:61)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:85)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2410)
2014-07-11 00:51:50,859 INFO  [main] hfile.CacheConfig: Allocating LruBlockCache with maximum size 4.0g
2014-07-11 00:51:50,933 INFO  [main] mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2014-07-11 00:51:50,990 INFO  [main] http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
2014-07-11 00:51:51,001 INFO  [main] http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 60030
2014-07-11 00:51:51,003 INFO  [main] http.HttpServer: listener.getLocalPort() returned 60030 webServer.getConnectors()[0].getLocalPort() returned 60030
2014-07-11 00:51:51,003 INFO  [main] http.HttpServer: Jetty bound to port 60030
2014-07-11 00:51:51,003 INFO  [main] mortbay.log: jetty-6.1.26
2014-07-11 00:51:51,349 INFO  [main] mortbay.log: Started SelectChannelConnector@0.0.0.0:60030
2014-07-11 00:51:51,398 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2014-07-11 00:51:51,399 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:host.name=sceplus-vm49.almaden.ibm.com
2014-07-11 00:51:51,399 INFO  [main] regionserver.ShutdownHook: Installed shutdown hook thread: Shutdownhook:regionserver60020
2014-07-11 00:51:51,399 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.version=1.6.0_31
2014-07-11 00:51:51,399 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.vendor=Sun Microsystems Inc.
2014-07-11 00:51:51,399 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-6-openjdk-amd64/jre
2014-07-11 00:51:51,399 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.class.path=/home/hadoop/hbase/bin/../conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-07-11 00:51:51,399 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-07-11 00:51:51,399 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
2014-07-11 00:51:51,399 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
2014-07-11 00:51:51,399 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.name=Linux
2014-07-11 00:51:51,399 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.arch=amd64
2014-07-11 00:51:51,399 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.version=3.13.0-24-generic
2014-07-11 00:51:51,399 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.name=hadoop
2014-07-11 00:51:51,399 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.home=/home/hadoop
2014-07-11 00:51:51,399 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.dir=/home/hadoop/hbase-0.98.3-hadoop1
2014-07-11 00:51:51,400 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=regionserver:60020, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-11 00:51:51,424 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=regionserver:60020 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-11 00:51:51,428 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-11 00:51:51,432 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-11 00:51:51,447 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, sessionid = 0x1472468205c0000, negotiated timeout = 90000
2014-07-11 00:52:22,162 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x5ff3bbc4, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-11 00:52:22,164 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x5ff3bbc4 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-11 00:52:22,164 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-11 00:52:22,165 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-11 00:52:22,178 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, sessionid = 0x1472468205c0003, negotiated timeout = 90000
2014-07-11 00:52:22,446 DEBUG [regionserver60020] catalog.CatalogTracker: Starting catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@3c646cd6
2014-07-11 00:52:22,450 INFO  [regionserver60020] regionserver.HRegionServer: ClusterId : 3b61b992-e8ee-43f8-b0c6-14cd23a8afbe
2014-07-11 00:52:22,456 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initializing
2014-07-11 00:52:22,471 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Node /hbase/online-snapshot/acquired already exists and this is not a retry
2014-07-11 00:52:22,507 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initialized
2014-07-11 00:52:22,514 INFO  [regionserver60020] regionserver.MemStoreFlusher: globalMemStoreLimit=4.0g, globalMemStoreLimitLowMark=3.8g, maxHeap=9.9g
2014-07-11 00:52:22,528 INFO  [regionserver60020] regionserver.HRegionServer: CompactionChecker runs every 10sec
2014-07-11 00:52:22,546 INFO  [regionserver60020] regionserver.HRegionServer: reportForDuty to master=sceplus-vm48.almaden.ibm.com,60000,1405065109226 with port=60020, startcode=1405065110777
2014-07-11 00:52:22,931 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: hbase.rootdir=hdfs://master:54310/hbase
2014-07-11 00:52:22,931 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: fs.default.name=hdfs://master:54310
2014-07-11 00:52:22,931 INFO  [regionserver60020] regionserver.HRegionServer: Master passed us a different hostname to use; was=sceplus-vm49.almaden.ibm.com, but now=slave1
2014-07-11 00:52:22,957 INFO  [regionserver60020] fs.HFileSystem: Added intercepting call to namenode#getBlockLocations so can do block reordering using class class org.apache.hadoop.hbase.fs.HFileSystem$ReorderWALBlocks
2014-07-11 00:52:22,966 DEBUG [regionserver60020] regionserver.HRegionServer: logdir=hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777
2014-07-11 00:52:23,021 DEBUG [regionserver60020] regionserver.Replication: ReplicationStatisticsThread 300
2014-07-11 00:52:23,037 INFO  [regionserver60020] wal.FSHLog: WAL/HLog configuration: blocksize=64 MB, rollsize=60.8 MB, enabled=true
2014-07-11 00:52:23,150 INFO  [regionserver60020] wal.FSHLog: New WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065143043
2014-07-11 00:52:23,161 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=WAL registered.
2014-07-11 00:52:23,166 INFO  [regionserver60020] regionserver.MetricsRegionServerWrapperImpl: Computing regionserver metrics every 5000 milliseconds
2014-07-11 00:52:23,170 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Server registered.
2014-07-11 00:52:23,175 INFO  [regionserver60020] trace.SpanReceiverHost: SpanReceiver org.cloudera.htrace.impl.LocalFileSpanReceiver was loaded successfully.
2014-07-11 00:52:23,177 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_REGION-slave1:60020, corePoolSize=3, maxPoolSize=3
2014-07-11 00:52:23,178 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_META-slave1:60020, corePoolSize=1, maxPoolSize=1
2014-07-11 00:52:23,178 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_REGION-slave1:60020, corePoolSize=3, maxPoolSize=3
2014-07-11 00:52:23,178 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_META-slave1:60020, corePoolSize=1, maxPoolSize=1
2014-07-11 00:52:23,178 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_LOG_REPLAY_OPS-slave1:60020, corePoolSize=2, maxPoolSize=2
2014-07-11 00:52:23,188 INFO  [regionserver60020] regionserver.ReplicationSourceManager: Current list of replicators: [slave1,60020,1405065110777, sceplus-vm48.almaden.ibm.com,60020,1405065111161] other RSs: [slave1,60020,1405065110777, sceplus-vm48.almaden.ibm.com,60020,1405065111161]
2014-07-11 00:52:23,211 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Replication registered.
2014-07-11 00:52:23,213 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x772a025f, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-11 00:52:23,214 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x772a025f connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-11 00:52:23,214 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-11 00:52:23,215 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-11 00:52:23,218 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, sessionid = 0x1472468205c0005, negotiated timeout = 90000
2014-07-11 00:52:23,223 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: starting
2014-07-11 00:52:23,223 INFO  [RpcServer.listener,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: starting
2014-07-11 00:52:23,265 INFO  [regionserver60020] regionserver.HRegionServer: Serving as slave1,60020,1405065110777, RpcServer on sceplus-vm49.almaden.ibm.com/9.1.143.59:60020, sessionid=0x1472468205c0000
2014-07-11 00:52:23,265 INFO  [SplitLogWorker-slave1,60020,1405065110777] regionserver.SplitLogWorker: SplitLogWorker slave1,60020,1405065110777 starting
2014-07-11 00:52:23,266 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is starting
2014-07-11 00:52:23,266 DEBUG [regionserver60020] snapshot.RegionServerSnapshotManager: Start Snapshot Manager slave1,60020,1405065110777
2014-07-11 00:52:23,266 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Starting procedure member 'slave1,60020,1405065110777'
2014-07-11 00:52:23,266 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Checking for aborted procedures on node: '/hbase/online-snapshot/abort'
2014-07-11 00:52:23,268 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Looking for new procedures under znode:'/hbase/online-snapshot/acquired'
2014-07-11 00:52:23,270 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is started
2014-07-11 00:52:27,964 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user2,1405064330981.32792a07034f41faaccd662f993e2b43.
2014-07-11 00:52:28,133 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 32792a07034f41faaccd662f993e2b43 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:52:28,134 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user7,1405064330982.f786fa5220833e0d9abad8d116fd675f.
2014-07-11 00:52:28,134 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user1,1405064330981.ce66150b8920e89cf4a5b997f24aec20.
2014-07-11 00:52:28,135 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning f786fa5220833e0d9abad8d116fd675f from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:52:28,136 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-11 00:52:28,137 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning ce66150b8920e89cf4a5b997f24aec20 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:52:28,155 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node f786fa5220833e0d9abad8d116fd675f from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:52:28,155 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node ce66150b8920e89cf4a5b997f24aec20 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:52:28,156 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 32792a07034f41faaccd662f993e2b43 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:52:28,157 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user8,1405064330982.6104404231110482fd5b48c58320e753.
2014-07-11 00:52:28,157 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,,1405064330981.bc95a01b489d14ad77746282c8629bef.
2014-07-11 00:52:28,183 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => ce66150b8920e89cf4a5b997f24aec20, NAME => 'usertable,user1,1405064330981.ce66150b8920e89cf4a5b997f24aec20.', STARTKEY => 'user1', ENDKEY => 'user2'}
2014-07-11 00:52:28,183 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => f786fa5220833e0d9abad8d116fd675f, NAME => 'usertable,user7,1405064330982.f786fa5220833e0d9abad8d116fd675f.', STARTKEY => 'user7', ENDKEY => 'user8'}
2014-07-11 00:52:28,184 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => 32792a07034f41faaccd662f993e2b43, NAME => 'usertable,user2,1405064330981.32792a07034f41faaccd662f993e2b43.', STARTKEY => 'user2', ENDKEY => 'user3'}
2014-07-11 00:52:28,213 INFO  [RS_OPEN_REGION-slave1:60020-0] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Regions registered.
2014-07-11 00:52:28,214 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 32792a07034f41faaccd662f993e2b43
2014-07-11 00:52:28,214 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable f786fa5220833e0d9abad8d116fd675f
2014-07-11 00:52:28,214 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable ce66150b8920e89cf4a5b997f24aec20
2014-07-11 00:52:28,215 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,user2,1405064330981.32792a07034f41faaccd662f993e2b43.
2014-07-11 00:52:28,215 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user7,1405064330982.f786fa5220833e0d9abad8d116fd675f.
2014-07-11 00:52:28,215 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user1,1405064330981.ce66150b8920e89cf4a5b997f24aec20.
2014-07-11 00:52:28,227 INFO  [RS_OPEN_REGION-slave1:60020-2] util.NativeCodeLoader: Loaded the native-hadoop library
2014-07-11 00:52:28,231 INFO  [RS_OPEN_REGION-slave1:60020-2] zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
2014-07-11 00:52:28,233 INFO  [RS_OPEN_REGION-slave1:60020-1] compress.CodecPool: Got brand-new compressor
2014-07-11 00:52:28,233 INFO  [RS_OPEN_REGION-slave1:60020-2] compress.CodecPool: Got brand-new compressor
2014-07-11 00:52:28,233 INFO  [RS_OPEN_REGION-slave1:60020-0] compress.CodecPool: Got brand-new compressor
2014-07-11 00:52:28,320 INFO  [StoreOpener-ce66150b8920e89cf4a5b997f24aec20-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-11 00:52:28,320 INFO  [StoreOpener-32792a07034f41faaccd662f993e2b43-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-11 00:52:28,327 INFO  [StoreOpener-f786fa5220833e0d9abad8d116fd675f-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-11 00:52:28,372 INFO  [StoreFileOpenerThread-family-1] util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc32
2014-07-11 00:52:28,414 INFO  [StoreFileOpenerThread-family-1] compress.CodecPool: Got brand-new decompressor
2014-07-11 00:52:28,414 INFO  [StoreFileOpenerThread-family-1] compress.CodecPool: Got brand-new decompressor
2014-07-11 00:52:28,428 DEBUG [StoreOpener-32792a07034f41faaccd662f993e2b43-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/32792a07034f41faaccd662f993e2b43/family/5570402dc066420c9671199e3a6e7fde, isReference=false, isBulkLoadResult=false, seqid=3797, majorCompaction=false
2014-07-11 00:52:28,428 DEBUG [StoreOpener-ce66150b8920e89cf4a5b997f24aec20-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ce66150b8920e89cf4a5b997f24aec20/family/1397836bae944a3da401e10cbe7df3d6, isReference=false, isBulkLoadResult=false, seqid=4516, majorCompaction=false
2014-07-11 00:52:28,451 DEBUG [StoreOpener-f786fa5220833e0d9abad8d116fd675f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f786fa5220833e0d9abad8d116fd675f/family/359f9612ec264d259dfb884033b1bdb5, isReference=false, isBulkLoadResult=false, seqid=4072, majorCompaction=false
2014-07-11 00:52:28,459 DEBUG [StoreOpener-32792a07034f41faaccd662f993e2b43-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/32792a07034f41faaccd662f993e2b43/family/80318a23d05d43998d516793e58dd38d, isReference=false, isBulkLoadResult=false, seqid=4431, majorCompaction=false
2014-07-11 00:52:28,466 DEBUG [StoreOpener-ce66150b8920e89cf4a5b997f24aec20-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ce66150b8920e89cf4a5b997f24aec20/family/1904d03ac7654a23873ab39ce9913246, isReference=false, isBulkLoadResult=false, seqid=3310, majorCompaction=false
2014-07-11 00:52:28,475 DEBUG [StoreOpener-32792a07034f41faaccd662f993e2b43-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/32792a07034f41faaccd662f993e2b43/family/85bf715213f14dfa8afeb3a171fd08aa, isReference=false, isBulkLoadResult=false, seqid=4560, majorCompaction=false
2014-07-11 00:52:28,492 DEBUG [StoreOpener-ce66150b8920e89cf4a5b997f24aec20-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ce66150b8920e89cf4a5b997f24aec20/family/1ae9ef86d58940c896f9764c920eedf7, isReference=false, isBulkLoadResult=false, seqid=4566, majorCompaction=false
2014-07-11 00:52:28,501 DEBUG [StoreOpener-32792a07034f41faaccd662f993e2b43-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/32792a07034f41faaccd662f993e2b43/family/a1d8f7b2dc684a87a68a991681801353, isReference=false, isBulkLoadResult=false, seqid=3506, majorCompaction=true
2014-07-11 00:52:28,503 DEBUG [StoreOpener-f786fa5220833e0d9abad8d116fd675f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f786fa5220833e0d9abad8d116fd675f/family/4a1d84ceea114d7e921ba95434c6ce27, isReference=false, isBulkLoadResult=false, seqid=2764, majorCompaction=false
2014-07-11 00:52:28,554 DEBUG [StoreOpener-ce66150b8920e89cf4a5b997f24aec20-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ce66150b8920e89cf4a5b997f24aec20/family/47f829d82365423d8dbc208d548d9024, isReference=false, isBulkLoadResult=false, seqid=2346, majorCompaction=false
2014-07-11 00:52:28,555 DEBUG [StoreOpener-f786fa5220833e0d9abad8d116fd675f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f786fa5220833e0d9abad8d116fd675f/family/60fd803a56ef495286ba05f68ec70a8c, isReference=false, isBulkLoadResult=false, seqid=3506, majorCompaction=false
2014-07-11 00:52:28,559 DEBUG [StoreOpener-32792a07034f41faaccd662f993e2b43-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/32792a07034f41faaccd662f993e2b43/family/c1224449c3854492a7a843652b0ae366, isReference=false, isBulkLoadResult=false, seqid=4283, majorCompaction=false
2014-07-11 00:52:28,568 DEBUG [StoreOpener-f786fa5220833e0d9abad8d116fd675f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f786fa5220833e0d9abad8d116fd675f/family/6594d69fa3f447bea5bb9dc8c9372cb0, isReference=false, isBulkLoadResult=false, seqid=1591, majorCompaction=false
2014-07-11 00:52:28,585 DEBUG [StoreOpener-ce66150b8920e89cf4a5b997f24aec20-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ce66150b8920e89cf4a5b997f24aec20/family/5313deac44ac4942b9983b18f6c95ac3, isReference=false, isBulkLoadResult=false, seqid=3623, majorCompaction=false
2014-07-11 00:52:28,591 DEBUG [StoreOpener-32792a07034f41faaccd662f993e2b43-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/32792a07034f41faaccd662f993e2b43/family/c2d6c5f886ed4539a07b9ac5797ee970, isReference=false, isBulkLoadResult=false, seqid=4065, majorCompaction=false
2014-07-11 00:52:28,599 DEBUG [StoreOpener-f786fa5220833e0d9abad8d116fd675f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f786fa5220833e0d9abad8d116fd675f/family/9197ee10d2ad4567b2cf5e3f76ed0ae5, isReference=false, isBulkLoadResult=false, seqid=3815, majorCompaction=false
2014-07-11 00:52:28,609 DEBUG [StoreOpener-32792a07034f41faaccd662f993e2b43-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/32792a07034f41faaccd662f993e2b43/family/d71c5c3c8d29422b845f4d921ec968fd, isReference=false, isBulkLoadResult=false, seqid=4572, majorCompaction=false
2014-07-11 00:52:28,622 DEBUG [StoreOpener-ce66150b8920e89cf4a5b997f24aec20-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ce66150b8920e89cf4a5b997f24aec20/family/68ebd076840b408ebb985e55c8a834d6, isReference=false, isBulkLoadResult=false, seqid=4388, majorCompaction=false
2014-07-11 00:52:28,642 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/32792a07034f41faaccd662f993e2b43
2014-07-11 00:52:28,645 DEBUG [StoreOpener-f786fa5220833e0d9abad8d116fd675f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f786fa5220833e0d9abad8d116fd675f/family/a1d3922c47a844f0a012a030460f2304, isReference=false, isBulkLoadResult=false, seqid=2438, majorCompaction=false
2014-07-11 00:52:28,652 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined 32792a07034f41faaccd662f993e2b43; next sequenceid=4573
2014-07-11 00:52:28,653 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 32792a07034f41faaccd662f993e2b43
2014-07-11 00:52:28,657 INFO  [PostOpenDeployTasks:32792a07034f41faaccd662f993e2b43] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user2,1405064330981.32792a07034f41faaccd662f993e2b43.
2014-07-11 00:52:28,661 DEBUG [PostOpenDeployTasks:32792a07034f41faaccd662f993e2b43] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:52:28,663 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-11 00:52:28,665 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 6 files of size 407595028 starting at candidate #1 after considering 15 permutations with 10 in ratio
2014-07-11 00:52:28,666 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.HStore: 32792a07034f41faaccd662f993e2b43 - family: Initiating minor compaction
2014-07-11 00:52:28,666 INFO  [regionserver60020-smallCompactions-1405065148661] regionserver.HRegion: Starting compaction on family in region usertable,user2,1405064330981.32792a07034f41faaccd662f993e2b43.
2014-07-11 00:52:28,667 INFO  [regionserver60020-smallCompactions-1405065148661] regionserver.HStore: Starting compaction of 6 file(s) in family of usertable,user2,1405064330981.32792a07034f41faaccd662f993e2b43. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/32792a07034f41faaccd662f993e2b43/.tmp, totalSize=388.7m
2014-07-11 00:52:28,668 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/32792a07034f41faaccd662f993e2b43/family/5570402dc066420c9671199e3a6e7fde, keycount=101583, bloomtype=ROW, size=72.4m, encoding=NONE, seqNum=3797
2014-07-11 00:52:28,668 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/32792a07034f41faaccd662f993e2b43/family/c2d6c5f886ed4539a07b9ac5797ee970, keycount=104227, bloomtype=ROW, size=74.2m, encoding=NONE, seqNum=4065
2014-07-11 00:52:28,668 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/32792a07034f41faaccd662f993e2b43/family/c1224449c3854492a7a843652b0ae366, keycount=133190, bloomtype=ROW, size=94.9m, encoding=NONE, seqNum=4283
2014-07-11 00:52:28,668 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/32792a07034f41faaccd662f993e2b43/family/80318a23d05d43998d516793e58dd38d, keycount=103476, bloomtype=ROW, size=73.7m, encoding=NONE, seqNum=4431
2014-07-11 00:52:28,668 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/32792a07034f41faaccd662f993e2b43/family/85bf715213f14dfa8afeb3a171fd08aa, keycount=93418, bloomtype=ROW, size=66.6m, encoding=NONE, seqNum=4560
2014-07-11 00:52:28,669 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/32792a07034f41faaccd662f993e2b43/family/d71c5c3c8d29422b845f4d921ec968fd, keycount=9780, bloomtype=ROW, size=7.0m, encoding=NONE, seqNum=4572
2014-07-11 00:52:28,677 DEBUG [StoreOpener-ce66150b8920e89cf4a5b997f24aec20-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ce66150b8920e89cf4a5b997f24aec20/family/7a2bc453f99a49fc83911219aa3eae91, isReference=false, isBulkLoadResult=false, seqid=3977, majorCompaction=false
2014-07-11 00:52:28,706 DEBUG [StoreOpener-f786fa5220833e0d9abad8d116fd675f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f786fa5220833e0d9abad8d116fd675f/family/bb4bf5f664f34a27a2abdacced42b750, isReference=false, isBulkLoadResult=false, seqid=2076, majorCompaction=false
2014-07-11 00:52:28,735 DEBUG [StoreOpener-ce66150b8920e89cf4a5b997f24aec20-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ce66150b8920e89cf4a5b997f24aec20/family/ca5f6977b5534c9596950ea706eabf21, isReference=false, isBulkLoadResult=false, seqid=4206, majorCompaction=false
2014-07-11 00:52:28,745 DEBUG [StoreOpener-f786fa5220833e0d9abad8d116fd675f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f786fa5220833e0d9abad8d116fd675f/family/dd9bf35d90194b63a6bd0a58f2a0064f, isReference=false, isBulkLoadResult=false, seqid=4258, majorCompaction=false
2014-07-11 00:52:28,767 DEBUG [StoreOpener-ce66150b8920e89cf4a5b997f24aec20-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ce66150b8920e89cf4a5b997f24aec20/family/e6e6b3df8032471f8540f7f8f170e4b1, isReference=false, isBulkLoadResult=false, seqid=2136, majorCompaction=true
2014-07-11 00:52:28,774 DEBUG [StoreOpener-f786fa5220833e0d9abad8d116fd675f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f786fa5220833e0d9abad8d116fd675f/family/f02031a83e9e47e1ab6d40a0fb912b64, isReference=false, isBulkLoadResult=false, seqid=1821, majorCompaction=false
2014-07-11 00:52:28,790 DEBUG [StoreOpener-ce66150b8920e89cf4a5b997f24aec20-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ce66150b8920e89cf4a5b997f24aec20/family/e78322c1415f48618d4571f8095211c0, isReference=false, isBulkLoadResult=false, seqid=2791, majorCompaction=false
2014-07-11 00:52:28,795 DEBUG [regionserver60020-smallCompactions-1405065148661] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:52:28,796 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/ce66150b8920e89cf4a5b997f24aec20
2014-07-11 00:52:28,797 DEBUG [StoreOpener-f786fa5220833e0d9abad8d116fd675f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f786fa5220833e0d9abad8d116fd675f/family/f76a521c786d4cbe900c704d845c5bdd, isReference=false, isBulkLoadResult=false, seqid=3152, majorCompaction=false
2014-07-11 00:52:28,800 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined ce66150b8920e89cf4a5b997f24aec20; next sequenceid=4567
2014-07-11 00:52:28,800 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node ce66150b8920e89cf4a5b997f24aec20
2014-07-11 00:52:28,802 INFO  [PostOpenDeployTasks:ce66150b8920e89cf4a5b997f24aec20] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user1,1405064330981.ce66150b8920e89cf4a5b997f24aec20.
2014-07-11 00:52:28,803 DEBUG [PostOpenDeployTasks:ce66150b8920e89cf4a5b997f24aec20] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-11 00:52:28,812 DEBUG [StoreOpener-f786fa5220833e0d9abad8d116fd675f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f786fa5220833e0d9abad8d116fd675f/family/f9228f2eb20548b1b21a11986ce45b53, isReference=false, isBulkLoadResult=false, seqid=1412, majorCompaction=true
2014-07-11 00:52:28,816 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/f786fa5220833e0d9abad8d116fd675f
2014-07-11 00:52:28,819 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined f786fa5220833e0d9abad8d116fd675f; next sequenceid=4259
2014-07-11 00:52:28,819 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node f786fa5220833e0d9abad8d116fd675f
2014-07-11 00:52:28,821 INFO  [PostOpenDeployTasks:f786fa5220833e0d9abad8d116fd675f] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user7,1405064330982.f786fa5220833e0d9abad8d116fd675f.
2014-07-11 00:52:28,821 DEBUG [PostOpenDeployTasks:f786fa5220833e0d9abad8d116fd675f] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:2), split_queue=0, merge_queue=0
2014-07-11 00:52:28,840 INFO  [PostOpenDeployTasks:f786fa5220833e0d9abad8d116fd675f] catalog.MetaEditor: Updated row usertable,user7,1405064330982.f786fa5220833e0d9abad8d116fd675f. with server=slave1,60020,1405065110777
2014-07-11 00:52:28,840 INFO  [PostOpenDeployTasks:ce66150b8920e89cf4a5b997f24aec20] catalog.MetaEditor: Updated row usertable,user1,1405064330981.ce66150b8920e89cf4a5b997f24aec20. with server=slave1,60020,1405065110777
2014-07-11 00:52:28,840 INFO  [PostOpenDeployTasks:f786fa5220833e0d9abad8d116fd675f] regionserver.HRegionServer: Finished post open deploy task for usertable,user7,1405064330982.f786fa5220833e0d9abad8d116fd675f.
2014-07-11 00:52:28,840 INFO  [PostOpenDeployTasks:ce66150b8920e89cf4a5b997f24aec20] regionserver.HRegionServer: Finished post open deploy task for usertable,user1,1405064330981.ce66150b8920e89cf4a5b997f24aec20.
2014-07-11 00:52:28,840 INFO  [PostOpenDeployTasks:32792a07034f41faaccd662f993e2b43] catalog.MetaEditor: Updated row usertable,user2,1405064330981.32792a07034f41faaccd662f993e2b43. with server=slave1,60020,1405065110777
2014-07-11 00:52:28,842 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning f786fa5220833e0d9abad8d116fd675f from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:52:28,842 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning ce66150b8920e89cf4a5b997f24aec20 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:52:28,842 INFO  [PostOpenDeployTasks:32792a07034f41faaccd662f993e2b43] regionserver.HRegionServer: Finished post open deploy task for usertable,user2,1405064330981.32792a07034f41faaccd662f993e2b43.
2014-07-11 00:52:28,843 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 32792a07034f41faaccd662f993e2b43 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:52:28,847 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node f786fa5220833e0d9abad8d116fd675f from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:52:28,847 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned f786fa5220833e0d9abad8d116fd675f to OPENED in zk on slave1,60020,1405065110777
2014-07-11 00:52:28,848 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user7,1405064330982.f786fa5220833e0d9abad8d116fd675f. on slave1,60020,1405065110777
2014-07-11 00:52:28,848 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node ce66150b8920e89cf4a5b997f24aec20 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:52:28,848 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 32792a07034f41faaccd662f993e2b43 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:52:28,849 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e5ee55a21ff19d69490518939b0887e0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:52:28,848 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned ce66150b8920e89cf4a5b997f24aec20 to OPENED in zk on slave1,60020,1405065110777
2014-07-11 00:52:28,849 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned 32792a07034f41faaccd662f993e2b43 to OPENED in zk on slave1,60020,1405065110777
2014-07-11 00:52:28,849 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user1,1405064330981.ce66150b8920e89cf4a5b997f24aec20. on slave1,60020,1405065110777
2014-07-11 00:52:28,849 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,user2,1405064330981.32792a07034f41faaccd662f993e2b43. on slave1,60020,1405065110777
2014-07-11 00:52:28,850 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 6104404231110482fd5b48c58320e753 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:52:28,850 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning bc95a01b489d14ad77746282c8629bef from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:52:28,855 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e5ee55a21ff19d69490518939b0887e0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:52:28,856 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => e5ee55a21ff19d69490518939b0887e0, NAME => 'hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.', STARTKEY => '', ENDKEY => ''}
2014-07-11 00:52:28,856 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 6104404231110482fd5b48c58320e753 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:52:28,856 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node bc95a01b489d14ad77746282c8629bef from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:52:28,856 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table namespace e5ee55a21ff19d69490518939b0887e0
2014-07-11 00:52:28,856 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => 6104404231110482fd5b48c58320e753, NAME => 'usertable,user8,1405064330982.6104404231110482fd5b48c58320e753.', STARTKEY => 'user8', ENDKEY => 'user9'}
2014-07-11 00:52:28,856 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => bc95a01b489d14ad77746282c8629bef, NAME => 'usertable,,1405064330981.bc95a01b489d14ad77746282c8629bef.', STARTKEY => '', ENDKEY => 'user1'}
2014-07-11 00:52:28,856 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-11 00:52:28,857 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable bc95a01b489d14ad77746282c8629bef
2014-07-11 00:52:28,857 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,,1405064330981.bc95a01b489d14ad77746282c8629bef.
2014-07-11 00:52:28,857 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 6104404231110482fd5b48c58320e753
2014-07-11 00:52:28,857 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user8,1405064330982.6104404231110482fd5b48c58320e753.
2014-07-11 00:52:28,864 INFO  [StoreOpener-e5ee55a21ff19d69490518939b0887e0-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 604800000, major jitter 0.500000
2014-07-11 00:52:28,865 INFO  [StoreOpener-bc95a01b489d14ad77746282c8629bef-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-11 00:52:28,867 INFO  [StoreOpener-6104404231110482fd5b48c58320e753-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-11 00:52:28,874 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/bc95a01b489d14ad77746282c8629bef
2014-07-11 00:52:28,877 DEBUG [StoreOpener-e5ee55a21ff19d69490518939b0887e0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/hbase/namespace/e5ee55a21ff19d69490518939b0887e0/info/5b0102065d284f308d4c0a8d64d9fab5, isReference=false, isBulkLoadResult=false, seqid=4, majorCompaction=false
2014-07-11 00:52:28,879 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined bc95a01b489d14ad77746282c8629bef; next sequenceid=1
2014-07-11 00:52:28,880 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bc95a01b489d14ad77746282c8629bef
2014-07-11 00:52:28,880 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/hbase/namespace/e5ee55a21ff19d69490518939b0887e0
2014-07-11 00:52:28,881 INFO  [PostOpenDeployTasks:bc95a01b489d14ad77746282c8629bef] regionserver.HRegionServer: Post open deploy tasks for region=usertable,,1405064330981.bc95a01b489d14ad77746282c8629bef.
2014-07-11 00:52:28,886 DEBUG [StoreOpener-6104404231110482fd5b48c58320e753-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/6104404231110482fd5b48c58320e753/family/1ac6349f01874acfbae8045adc03081e, isReference=false, isBulkLoadResult=false, seqid=4255, majorCompaction=false
2014-07-11 00:52:28,888 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined e5ee55a21ff19d69490518939b0887e0; next sequenceid=5
2014-07-11 00:52:28,888 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node e5ee55a21ff19d69490518939b0887e0
2014-07-11 00:52:28,888 INFO  [PostOpenDeployTasks:bc95a01b489d14ad77746282c8629bef] catalog.MetaEditor: Updated row usertable,,1405064330981.bc95a01b489d14ad77746282c8629bef. with server=slave1,60020,1405065110777
2014-07-11 00:52:28,889 INFO  [PostOpenDeployTasks:bc95a01b489d14ad77746282c8629bef] regionserver.HRegionServer: Finished post open deploy task for usertable,,1405064330981.bc95a01b489d14ad77746282c8629bef.
2014-07-11 00:52:28,889 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning bc95a01b489d14ad77746282c8629bef from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:52:28,890 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] regionserver.HRegionServer: Post open deploy tasks for region=hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-11 00:52:28,896 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] catalog.MetaEditor: Updated row hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0. with server=slave1,60020,1405065110777
2014-07-11 00:52:28,897 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] regionserver.HRegionServer: Finished post open deploy task for hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-11 00:52:28,898 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e5ee55a21ff19d69490518939b0887e0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:52:28,900 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node bc95a01b489d14ad77746282c8629bef from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:52:28,900 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned bc95a01b489d14ad77746282c8629bef to OPENED in zk on slave1,60020,1405065110777
2014-07-11 00:52:28,900 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,,1405064330981.bc95a01b489d14ad77746282c8629bef. on slave1,60020,1405065110777
2014-07-11 00:52:28,904 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e5ee55a21ff19d69490518939b0887e0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:52:28,904 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned e5ee55a21ff19d69490518939b0887e0 to OPENED in zk on slave1,60020,1405065110777
2014-07-11 00:52:28,904 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0. on slave1,60020,1405065110777
2014-07-11 00:52:28,936 DEBUG [StoreOpener-6104404231110482fd5b48c58320e753-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/6104404231110482fd5b48c58320e753/family/3a80e5108ebe4737a19c0572a00eccc6, isReference=false, isBulkLoadResult=false, seqid=1729, majorCompaction=false
2014-07-11 00:52:28,958 DEBUG [StoreOpener-6104404231110482fd5b48c58320e753-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/6104404231110482fd5b48c58320e753/family/59f593114b5c44a389050065f4ae924d, isReference=false, isBulkLoadResult=false, seqid=2300, majorCompaction=false
2014-07-11 00:52:28,967 DEBUG [StoreOpener-6104404231110482fd5b48c58320e753-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/6104404231110482fd5b48c58320e753/family/7a143e936a544670a88c43743b79b4d1, isReference=false, isBulkLoadResult=false, seqid=1132, majorCompaction=true
2014-07-11 00:52:28,976 DEBUG [StoreOpener-6104404231110482fd5b48c58320e753-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/6104404231110482fd5b48c58320e753/family/92d109f741a44046bb90018f02e2f84d, isReference=false, isBulkLoadResult=false, seqid=3350, majorCompaction=false
2014-07-11 00:52:28,996 DEBUG [StoreOpener-6104404231110482fd5b48c58320e753-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/6104404231110482fd5b48c58320e753/family/951fd7ce22fe4d659f479104f02d575f, isReference=false, isBulkLoadResult=false, seqid=2591, majorCompaction=false
2014-07-11 00:52:29,009 DEBUG [StoreOpener-6104404231110482fd5b48c58320e753-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/6104404231110482fd5b48c58320e753/family/a6bf6e977fdc4b46bef23d9ac40c7fc7, isReference=false, isBulkLoadResult=false, seqid=3707, majorCompaction=false
2014-07-11 00:52:29,021 DEBUG [StoreOpener-6104404231110482fd5b48c58320e753-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/6104404231110482fd5b48c58320e753/family/b1b10398c6d341feb19c2cc203012ce4, isReference=false, isBulkLoadResult=false, seqid=1535, majorCompaction=false
2014-07-11 00:52:29,037 DEBUG [StoreOpener-6104404231110482fd5b48c58320e753-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/6104404231110482fd5b48c58320e753/family/bb4ca852e72341ad9d9c0759c9a0af03, isReference=false, isBulkLoadResult=false, seqid=4002, majorCompaction=false
2014-07-11 00:52:29,046 DEBUG [StoreOpener-6104404231110482fd5b48c58320e753-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/6104404231110482fd5b48c58320e753/family/dc3e9ebf3dab4a33a1618844a662db70, isReference=false, isBulkLoadResult=false, seqid=2962, majorCompaction=false
2014-07-11 00:52:29,066 DEBUG [StoreOpener-6104404231110482fd5b48c58320e753-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/6104404231110482fd5b48c58320e753/family/f047413262484bec92e199f3fc3fd01c, isReference=false, isBulkLoadResult=false, seqid=1323, majorCompaction=false
2014-07-11 00:52:29,079 DEBUG [StoreOpener-6104404231110482fd5b48c58320e753-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/6104404231110482fd5b48c58320e753/family/ff97695415d44e8f8a06456dea8ef6b1, isReference=false, isBulkLoadResult=false, seqid=1936, majorCompaction=false
2014-07-11 00:52:29,083 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/6104404231110482fd5b48c58320e753
2014-07-11 00:52:29,086 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined 6104404231110482fd5b48c58320e753; next sequenceid=4256
2014-07-11 00:52:29,087 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 6104404231110482fd5b48c58320e753
2014-07-11 00:52:29,090 INFO  [PostOpenDeployTasks:6104404231110482fd5b48c58320e753] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user8,1405064330982.6104404231110482fd5b48c58320e753.
2014-07-11 00:52:29,090 DEBUG [PostOpenDeployTasks:6104404231110482fd5b48c58320e753] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:3), split_queue=0, merge_queue=0
2014-07-11 00:52:29,099 INFO  [PostOpenDeployTasks:6104404231110482fd5b48c58320e753] catalog.MetaEditor: Updated row usertable,user8,1405064330982.6104404231110482fd5b48c58320e753. with server=slave1,60020,1405065110777
2014-07-11 00:52:29,099 INFO  [PostOpenDeployTasks:6104404231110482fd5b48c58320e753] regionserver.HRegionServer: Finished post open deploy task for usertable,user8,1405064330982.6104404231110482fd5b48c58320e753.
2014-07-11 00:52:29,100 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 6104404231110482fd5b48c58320e753 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:52:29,104 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 6104404231110482fd5b48c58320e753 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:52:29,104 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned 6104404231110482fd5b48c58320e753 to OPENED in zk on slave1,60020,1405065110777
2014-07-11 00:52:29,104 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user8,1405064330982.6104404231110482fd5b48c58320e753. on slave1,60020,1405065110777
2014-07-11 00:52:33,181 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:4), split_queue=0, merge_queue=0
2014-07-11 00:52:33,182 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:5), split_queue=0, merge_queue=0
2014-07-11 00:52:33,182 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:6), split_queue=0, merge_queue=0
2014-07-11 00:52:58,669 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Close 32792a07034f41faaccd662f993e2b43, via zk=yes, znode version=0, on null
2014-07-11 00:52:58,672 INFO  [Priority.RpcServer.handler=2,port=60020] regionserver.HRegionServer: Close 6104404231110482fd5b48c58320e753, via zk=yes, znode version=0, on null
2014-07-11 00:52:58,672 INFO  [Priority.RpcServer.handler=3,port=60020] regionserver.HRegionServer: Close f786fa5220833e0d9abad8d116fd675f, via zk=yes, znode version=0, on null
2014-07-11 00:52:58,672 INFO  [Priority.RpcServer.handler=4,port=60020] regionserver.HRegionServer: Close bc95a01b489d14ad77746282c8629bef, via zk=yes, znode version=0, on null
2014-07-11 00:52:58,672 INFO  [Priority.RpcServer.handler=5,port=60020] regionserver.HRegionServer: Close ce66150b8920e89cf4a5b997f24aec20, via zk=yes, znode version=0, on null
2014-07-11 00:52:58,675 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Processing close of usertable,user2,1405064330981.32792a07034f41faaccd662f993e2b43.
2014-07-11 00:52:58,676 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Processing close of usertable,user1,1405064330981.ce66150b8920e89cf4a5b997f24aec20.
2014-07-11 00:52:58,677 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Processing close of usertable,,1405064330981.bc95a01b489d14ad77746282c8629bef.
2014-07-11 00:52:58,679 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closing usertable,user1,1405064330981.ce66150b8920e89cf4a5b997f24aec20.: disabling compactions & flushes
2014-07-11 00:52:58,679 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closing usertable,user2,1405064330981.32792a07034f41faaccd662f993e2b43.: disabling compactions & flushes
2014-07-11 00:52:58,679 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: waiting for 1 compactions to complete for region usertable,user2,1405064330981.32792a07034f41faaccd662f993e2b43.
2014-07-11 00:52:58,680 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closing usertable,,1405064330981.bc95a01b489d14ad77746282c8629bef.: disabling compactions & flushes
2014-07-11 00:52:58,680 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Updates disabled for region usertable,user1,1405064330981.ce66150b8920e89cf4a5b997f24aec20.
2014-07-11 00:52:58,680 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Updates disabled for region usertable,,1405064330981.bc95a01b489d14ad77746282c8629bef.
2014-07-11 00:52:58,682 INFO  [StoreCloserThread-usertable,,1405064330981.bc95a01b489d14ad77746282c8629bef.-1] regionserver.HStore: Closed family
2014-07-11 00:52:58,686 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closed usertable,,1405064330981.bc95a01b489d14ad77746282c8629bef.
2014-07-11 00:52:58,686 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning bc95a01b489d14ad77746282c8629bef from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-11 00:52:58,692 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node bc95a01b489d14ad77746282c8629bef from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-11 00:52:58,692 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Set closed state in zk for usertable,,1405064330981.bc95a01b489d14ad77746282c8629bef. on slave1,60020,1405065110777
2014-07-11 00:52:58,692 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Closed usertable,,1405064330981.bc95a01b489d14ad77746282c8629bef.
2014-07-11 00:52:58,692 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Processing close of usertable,user8,1405064330982.6104404231110482fd5b48c58320e753.
2014-07-11 00:52:58,693 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closing usertable,user8,1405064330982.6104404231110482fd5b48c58320e753.: disabling compactions & flushes
2014-07-11 00:52:58,694 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Updates disabled for region usertable,user8,1405064330982.6104404231110482fd5b48c58320e753.
2014-07-11 00:52:58,728 INFO  [StoreCloserThread-usertable,user1,1405064330981.ce66150b8920e89cf4a5b997f24aec20.-1] regionserver.HStore: Closed family
2014-07-11 00:52:58,728 INFO  [StoreCloserThread-usertable,user8,1405064330982.6104404231110482fd5b48c58320e753.-1] regionserver.HStore: Closed family
2014-07-11 00:52:58,729 INFO  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closed usertable,user1,1405064330981.ce66150b8920e89cf4a5b997f24aec20.
2014-07-11 00:52:58,729 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning ce66150b8920e89cf4a5b997f24aec20 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-11 00:52:58,729 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closed usertable,user8,1405064330982.6104404231110482fd5b48c58320e753.
2014-07-11 00:52:58,730 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 6104404231110482fd5b48c58320e753 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-11 00:52:58,734 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 6104404231110482fd5b48c58320e753 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-11 00:52:58,734 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Set closed state in zk for usertable,user8,1405064330982.6104404231110482fd5b48c58320e753. on slave1,60020,1405065110777
2014-07-11 00:52:58,734 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Closed usertable,user8,1405064330982.6104404231110482fd5b48c58320e753.
2014-07-11 00:52:58,734 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Processing close of usertable,user7,1405064330982.f786fa5220833e0d9abad8d116fd675f.
2014-07-11 00:52:58,734 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node ce66150b8920e89cf4a5b997f24aec20 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-11 00:52:58,734 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Set closed state in zk for usertable,user1,1405064330981.ce66150b8920e89cf4a5b997f24aec20. on slave1,60020,1405065110777
2014-07-11 00:52:58,734 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Closed usertable,user1,1405064330981.ce66150b8920e89cf4a5b997f24aec20.
2014-07-11 00:52:58,735 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closing usertable,user7,1405064330982.f786fa5220833e0d9abad8d116fd675f.: disabling compactions & flushes
2014-07-11 00:52:58,736 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Updates disabled for region usertable,user7,1405064330982.f786fa5220833e0d9abad8d116fd675f.
2014-07-11 00:52:58,739 INFO  [StoreCloserThread-usertable,user7,1405064330982.f786fa5220833e0d9abad8d116fd675f.-1] regionserver.HStore: Closed family
2014-07-11 00:52:58,739 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closed usertable,user7,1405064330982.f786fa5220833e0d9abad8d116fd675f.
2014-07-11 00:52:58,739 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning f786fa5220833e0d9abad8d116fd675f from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-11 00:52:58,746 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node f786fa5220833e0d9abad8d116fd675f from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-11 00:52:58,746 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Set closed state in zk for usertable,user7,1405064330982.f786fa5220833e0d9abad8d116fd675f. on slave1,60020,1405065110777
2014-07-11 00:52:58,746 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Closed usertable,user7,1405064330982.f786fa5220833e0d9abad8d116fd675f.
2014-07-11 00:52:58,753 INFO  [regionserver60020-smallCompactions-1405065148661] regionserver.HRegion: compaction interrupted
java.io.InterruptedIOException: Aborting compaction of store family in region usertable,user2,1405064330981.32792a07034f41faaccd662f993e2b43. because it was interrupted.
	at org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.compact(DefaultCompactor.java:81)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext.compact(DefaultStoreEngine.java:109)
	at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:1086)
	at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1481)
	at org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.run(CompactSplitThread.java:475)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-11 00:52:58,753 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Updates disabled for region usertable,user2,1405064330981.32792a07034f41faaccd662f993e2b43.
2014-07-11 00:52:58,757 INFO  [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Aborted compaction: Request = regionName=usertable,user2,1405064330981.32792a07034f41faaccd662f993e2b43., storeName=family, fileCount=6, fileSize=388.7m (66.6m, 7.0m), priority=13, time=19056093335078; duration=30sec
2014-07-11 00:52:58,757 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:6), split_queue=0, merge_queue=0
2014-07-11 00:52:58,758 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user1,1405064330981.ce66150b8920e89cf4a5b997f24aec20. because compaction request was cancelled
2014-07-11 00:52:58,758 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user1,1405064330981.ce66150b8920e89cf4a5b997f24aec20. because compaction request was cancelled
2014-07-11 00:52:58,758 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user7,1405064330982.f786fa5220833e0d9abad8d116fd675f. because compaction request was cancelled
2014-07-11 00:52:58,758 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user8,1405064330982.6104404231110482fd5b48c58320e753. because compaction request was cancelled
2014-07-11 00:52:58,758 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user8,1405064330982.6104404231110482fd5b48c58320e753. because compaction request was cancelled
2014-07-11 00:52:58,758 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user7,1405064330982.f786fa5220833e0d9abad8d116fd675f. because compaction request was cancelled
2014-07-11 00:52:58,759 INFO  [StoreCloserThread-usertable,user2,1405064330981.32792a07034f41faaccd662f993e2b43.-1] regionserver.HStore: Closed family
2014-07-11 00:52:58,760 INFO  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closed usertable,user2,1405064330981.32792a07034f41faaccd662f993e2b43.
2014-07-11 00:52:58,760 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 32792a07034f41faaccd662f993e2b43 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-11 00:52:58,765 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 32792a07034f41faaccd662f993e2b43 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-11 00:52:58,765 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Set closed state in zk for usertable,user2,1405064330981.32792a07034f41faaccd662f993e2b43. on slave1,60020,1405065110777
2014-07-11 00:52:58,766 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Closed usertable,user2,1405064330981.32792a07034f41faaccd662f993e2b43.
2014-07-11 00:56:50,869 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=9304, hits=1684, hitRatio=18.09%, , cachingAccesses=1688, cachingHits=1684, cachingHitsRatio=99.76%, evictions=0, evicted=2, evictedPerRun=Infinity
2014-07-11 00:58:09,047 INFO  [Priority.RpcServer.handler=6,port=60020] regionserver.HRegionServer: Open usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224.
2014-07-11 00:58:09,059 INFO  [Priority.RpcServer.handler=6,port=60020] regionserver.HRegionServer: Open usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2.
2014-07-11 00:58:09,059 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 2c9ec51d492f963529797ef469f19224 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:58:09,059 INFO  [Priority.RpcServer.handler=6,port=60020] regionserver.HRegionServer: Open usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355.
2014-07-11 00:58:09,060 INFO  [Priority.RpcServer.handler=6,port=60020] regionserver.HRegionServer: Open usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2.
2014-07-11 00:58:09,060 INFO  [Priority.RpcServer.handler=6,port=60020] regionserver.HRegionServer: Open usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29.
2014-07-11 00:58:09,061 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning bc1bf2a12fb05f36e29f0cabc7669ca2 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:58:09,062 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e468587ab0e54014075f4520dcd9c355 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:58:09,074 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 2c9ec51d492f963529797ef469f19224 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:58:09,075 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node bc1bf2a12fb05f36e29f0cabc7669ca2 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:58:09,075 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => 2c9ec51d492f963529797ef469f19224, NAME => 'usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224.', STARTKEY => 'user5', ENDKEY => 'user6'}
2014-07-11 00:58:09,075 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e468587ab0e54014075f4520dcd9c355 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:58:09,075 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => bc1bf2a12fb05f36e29f0cabc7669ca2, NAME => 'usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2.', STARTKEY => 'user6', ENDKEY => 'user7'}
2014-07-11 00:58:09,075 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 2c9ec51d492f963529797ef469f19224
2014-07-11 00:58:09,076 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => e468587ab0e54014075f4520dcd9c355, NAME => 'usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355.', STARTKEY => 'user3', ENDKEY => 'user4'}
2014-07-11 00:58:09,076 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224.
2014-07-11 00:58:09,077 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable bc1bf2a12fb05f36e29f0cabc7669ca2
2014-07-11 00:58:09,077 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable e468587ab0e54014075f4520dcd9c355
2014-07-11 00:58:09,078 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2.
2014-07-11 00:58:09,078 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355.
2014-07-11 00:58:09,084 INFO  [StoreOpener-2c9ec51d492f963529797ef469f19224-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-11 00:58:09,086 INFO  [StoreOpener-bc1bf2a12fb05f36e29f0cabc7669ca2-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-11 00:58:09,087 INFO  [StoreOpener-e468587ab0e54014075f4520dcd9c355-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-11 00:58:09,090 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224
2014-07-11 00:58:09,091 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2
2014-07-11 00:58:09,091 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355
2014-07-11 00:58:09,093 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined 2c9ec51d492f963529797ef469f19224; next sequenceid=1
2014-07-11 00:58:09,093 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 2c9ec51d492f963529797ef469f19224
2014-07-11 00:58:09,094 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined bc1bf2a12fb05f36e29f0cabc7669ca2; next sequenceid=1
2014-07-11 00:58:09,094 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bc1bf2a12fb05f36e29f0cabc7669ca2
2014-07-11 00:58:09,095 INFO  [PostOpenDeployTasks:2c9ec51d492f963529797ef469f19224] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224.
2014-07-11 00:58:09,096 INFO  [PostOpenDeployTasks:bc1bf2a12fb05f36e29f0cabc7669ca2] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2.
2014-07-11 00:58:09,106 INFO  [PostOpenDeployTasks:2c9ec51d492f963529797ef469f19224] catalog.MetaEditor: Updated row usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. with server=slave1,60020,1405065110777
2014-07-11 00:58:09,107 INFO  [PostOpenDeployTasks:2c9ec51d492f963529797ef469f19224] regionserver.HRegionServer: Finished post open deploy task for usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224.
2014-07-11 00:58:09,107 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 2c9ec51d492f963529797ef469f19224 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:58:09,108 INFO  [PostOpenDeployTasks:bc1bf2a12fb05f36e29f0cabc7669ca2] catalog.MetaEditor: Updated row usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. with server=slave1,60020,1405065110777
2014-07-11 00:58:09,108 INFO  [PostOpenDeployTasks:bc1bf2a12fb05f36e29f0cabc7669ca2] regionserver.HRegionServer: Finished post open deploy task for usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2.
2014-07-11 00:58:09,109 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning bc1bf2a12fb05f36e29f0cabc7669ca2 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:58:09,112 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 2c9ec51d492f963529797ef469f19224 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:58:09,112 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned 2c9ec51d492f963529797ef469f19224 to OPENED in zk on slave1,60020,1405065110777
2014-07-11 00:58:09,112 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. on slave1,60020,1405065110777
2014-07-11 00:58:09,113 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e40f437ea2cd04f5e80b9966a93834b2 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:58:09,115 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node bc1bf2a12fb05f36e29f0cabc7669ca2 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:58:09,115 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned bc1bf2a12fb05f36e29f0cabc7669ca2 to OPENED in zk on slave1,60020,1405065110777
2014-07-11 00:58:09,115 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. on slave1,60020,1405065110777
2014-07-11 00:58:09,115 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning f5085e3c4a405ca169db6dcb31b38b29 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:58:09,121 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e40f437ea2cd04f5e80b9966a93834b2 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:58:09,121 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => e40f437ea2cd04f5e80b9966a93834b2, NAME => 'usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2.', STARTKEY => 'user7', ENDKEY => 'user8'}
2014-07-11 00:58:09,122 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable e40f437ea2cd04f5e80b9966a93834b2
2014-07-11 00:58:09,122 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2.
2014-07-11 00:58:09,122 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node f5085e3c4a405ca169db6dcb31b38b29 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:58:09,122 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => f5085e3c4a405ca169db6dcb31b38b29, NAME => 'usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29.', STARTKEY => 'user8', ENDKEY => 'user9'}
2014-07-11 00:58:09,123 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable f5085e3c4a405ca169db6dcb31b38b29
2014-07-11 00:58:09,123 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29.
2014-07-11 00:58:09,123 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined e468587ab0e54014075f4520dcd9c355; next sequenceid=1
2014-07-11 00:58:09,123 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node e468587ab0e54014075f4520dcd9c355
2014-07-11 00:58:09,124 INFO  [PostOpenDeployTasks:e468587ab0e54014075f4520dcd9c355] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355.
2014-07-11 00:58:09,139 INFO  [PostOpenDeployTasks:e468587ab0e54014075f4520dcd9c355] catalog.MetaEditor: Updated row usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. with server=slave1,60020,1405065110777
2014-07-11 00:58:09,139 INFO  [PostOpenDeployTasks:e468587ab0e54014075f4520dcd9c355] regionserver.HRegionServer: Finished post open deploy task for usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355.
2014-07-11 00:58:09,139 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e468587ab0e54014075f4520dcd9c355 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:58:09,143 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e468587ab0e54014075f4520dcd9c355 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:58:09,143 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned e468587ab0e54014075f4520dcd9c355 to OPENED in zk on slave1,60020,1405065110777
2014-07-11 00:58:09,143 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. on slave1,60020,1405065110777
2014-07-11 00:58:09,144 INFO  [StoreOpener-e40f437ea2cd04f5e80b9966a93834b2-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-11 00:58:09,146 INFO  [StoreOpener-f5085e3c4a405ca169db6dcb31b38b29-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-11 00:58:09,152 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2
2014-07-11 00:58:09,153 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29
2014-07-11 00:58:09,156 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined e40f437ea2cd04f5e80b9966a93834b2; next sequenceid=1
2014-07-11 00:58:09,156 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node e40f437ea2cd04f5e80b9966a93834b2
2014-07-11 00:58:09,157 INFO  [PostOpenDeployTasks:e40f437ea2cd04f5e80b9966a93834b2] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2.
2014-07-11 00:58:09,164 INFO  [PostOpenDeployTasks:e40f437ea2cd04f5e80b9966a93834b2] catalog.MetaEditor: Updated row usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. with server=slave1,60020,1405065110777
2014-07-11 00:58:09,164 INFO  [PostOpenDeployTasks:e40f437ea2cd04f5e80b9966a93834b2] regionserver.HRegionServer: Finished post open deploy task for usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2.
2014-07-11 00:58:09,166 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e40f437ea2cd04f5e80b9966a93834b2 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:58:09,169 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e40f437ea2cd04f5e80b9966a93834b2 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:58:09,170 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned e40f437ea2cd04f5e80b9966a93834b2 to OPENED in zk on slave1,60020,1405065110777
2014-07-11 00:58:09,170 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. on slave1,60020,1405065110777
2014-07-11 00:58:09,193 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined f5085e3c4a405ca169db6dcb31b38b29; next sequenceid=1
2014-07-11 00:58:09,193 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node f5085e3c4a405ca169db6dcb31b38b29
2014-07-11 00:58:09,195 INFO  [PostOpenDeployTasks:f5085e3c4a405ca169db6dcb31b38b29] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29.
2014-07-11 00:58:09,200 INFO  [PostOpenDeployTasks:f5085e3c4a405ca169db6dcb31b38b29] catalog.MetaEditor: Updated row usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. with server=slave1,60020,1405065110777
2014-07-11 00:58:09,200 INFO  [PostOpenDeployTasks:f5085e3c4a405ca169db6dcb31b38b29] regionserver.HRegionServer: Finished post open deploy task for usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29.
2014-07-11 00:58:09,201 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning f5085e3c4a405ca169db6dcb31b38b29 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:58:09,205 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472468205c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node f5085e3c4a405ca169db6dcb31b38b29 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:58:09,205 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned f5085e3c4a405ca169db6dcb31b38b29 to OPENED in zk on slave1,60020,1405065110777
2014-07-11 00:58:09,205 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. on slave1,60020,1405065110777
2014-07-11 00:58:28,241 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:58:28,317 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 95 synced till here 82
2014-07-11 00:58:28,577 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065143043 with entries=95, filesize=81.3m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065508241
2014-07-11 00:58:30,592 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:58:30,750 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 188 synced till here 175
2014-07-11 00:58:30,996 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065508241 with entries=93, filesize=79.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065510593
2014-07-11 00:58:32,720 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:58:33,065 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 280 synced till here 259
2014-07-11 00:58:33,576 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065510593 with entries=92, filesize=78.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065512721
2014-07-11 00:58:35,813 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:58:35,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 393 synced till here 372
2014-07-11 00:58:36,586 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065512721 with entries=113, filesize=96.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065515814
2014-07-11 00:58:39,795 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:58:39,971 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 513 synced till here 493
2014-07-11 00:58:40,691 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065515814 with entries=120, filesize=102.0m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065519796
2014-07-11 00:58:42,545 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:58:42,846 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 597 synced till here 587
2014-07-11 00:58:43,312 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065519796 with entries=84, filesize=72.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065522546
2014-07-11 00:58:46,044 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:58:46,295 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 718 synced till here 697
2014-07-11 00:58:46,618 DEBUG [RpcServer.handler=13,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355.
2014-07-11 00:58:46,621 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355., current region memstore size 262.1m
2014-07-11 00:58:46,771 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065522546 with entries=121, filesize=103.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065526045
2014-07-11 00:58:47,750 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224.
2014-07-11 00:58:47,750 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224., current region memstore size 263.1m
2014-07-11 00:58:47,862 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:58:48,073 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:58:48,341 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 824 synced till here 795
2014-07-11 00:58:48,615 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2.
2014-07-11 00:58:48,848 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:58:48,863 INFO  [MemStoreFlusher.0] compress.CodecPool: Got brand-new compressor
2014-07-11 00:58:48,914 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065526045 with entries=106, filesize=88.2m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065528074
2014-07-11 00:58:49,552 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2.
2014-07-11 00:58:50,942 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:58:51,169 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 932 synced till here 921
2014-07-11 00:58:51,733 DEBUG [RpcServer.handler=19,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29.
2014-07-11 00:58:51,873 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065528074 with entries=108, filesize=92.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065530943
2014-07-11 00:58:53,055 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:58:53,146 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1033 synced till here 1007
2014-07-11 00:58:53,399 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065530943 with entries=101, filesize=86.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065533055
2014-07-11 00:58:55,041 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:58:55,256 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1137 synced till here 1108
2014-07-11 00:58:55,669 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065533055 with entries=104, filesize=88.9m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065535041
2014-07-11 00:58:56,106 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=174, memsize=54.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/a869186487784315ab6c07a87adfa70a
2014-07-11 00:58:56,124 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/a869186487784315ab6c07a87adfa70a as hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/a869186487784315ab6c07a87adfa70a
2014-07-11 00:58:56,137 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/a869186487784315ab6c07a87adfa70a, entries=197560, sequenceid=174, filesize=14.1m
2014-07-11 00:58:56,139 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~266.7m/279608080, currentsize=127.0m/133156320 for region usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. in 9518ms, sequenceid=174, compaction requested=false
2014-07-11 00:58:56,147 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2., current region memstore size 347.6m
2014-07-11 00:58:56,494 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=174, memsize=54.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/3b27813f49074338ad1306be6aa6e994
2014-07-11 00:58:56,513 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/3b27813f49074338ad1306be6aa6e994 as hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/3b27813f49074338ad1306be6aa6e994
2014-07-11 00:58:56,548 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/3b27813f49074338ad1306be6aa6e994, entries=198430, sequenceid=174, filesize=14.1m
2014-07-11 00:58:56,548 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~267.9m/280920400, currentsize=90.3m/94684560 for region usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. in 8798ms, sequenceid=174, compaction requested=false
2014-07-11 00:58:56,548 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2., current region memstore size 346.9m
2014-07-11 00:58:57,866 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:58:59,581 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:58:59,653 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1342 synced till here 1315
2014-07-11 00:58:59,654 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:58:59,992 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065535041 with entries=205, filesize=175.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065537866
2014-07-11 00:59:02,584 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:59:02,605 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1419 synced till here 1415
2014-07-11 00:59:02,764 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065537866 with entries=77, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065542585
2014-07-11 00:59:04,866 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:59:04,933 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065542585 with entries=72, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065544867
2014-07-11 00:59:05,193 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=233, memsize=54.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/be76d47b1acc41f399780e76cf96e9c4
2014-07-11 00:59:05,212 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/be76d47b1acc41f399780e76cf96e9c4 as hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/be76d47b1acc41f399780e76cf96e9c4
2014-07-11 00:59:05,224 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/be76d47b1acc41f399780e76cf96e9c4, entries=197280, sequenceid=233, filesize=14.1m
2014-07-11 00:59:05,225 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~353.1m/370304320, currentsize=109.7m/115067840 for region usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. in 8677ms, sequenceid=233, compaction requested=false
2014-07-11 00:59:05,225 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29., current region memstore size 465.6m
2014-07-11 00:59:05,855 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=232, memsize=54.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/227e748340f34aaba554648b03de9cc9
2014-07-11 00:59:05,874 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/227e748340f34aaba554648b03de9cc9 as hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/227e748340f34aaba554648b03de9cc9
2014-07-11 00:59:05,892 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/227e748340f34aaba554648b03de9cc9, entries=197750, sequenceid=232, filesize=14.1m
2014-07-11 00:59:05,892 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~355.3m/372513680, currentsize=114.8m/120330320 for region usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. in 9745ms, sequenceid=232, compaction requested=false
2014-07-11 00:59:06,100 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:59:07,144 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:59:07,162 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1566 synced till here 1562
2014-07-11 00:59:07,364 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065544867 with entries=75, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065547144
2014-07-11 00:59:09,269 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:59:09,290 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1649 synced till here 1645
2014-07-11 00:59:09,493 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065547144 with entries=83, filesize=71.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065549270
2014-07-11 00:59:09,661 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355.
2014-07-11 00:59:09,662 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355., current region memstore size 257.4m
2014-07-11 00:59:10,001 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224.
2014-07-11 00:59:10,710 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:59:11,295 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:59:11,326 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1732 synced till here 1721
2014-07-11 00:59:11,609 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065549270 with entries=83, filesize=71.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065551295
2014-07-11 00:59:13,256 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:59:13,376 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1807 synced till here 1805
2014-07-11 00:59:13,416 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065551295 with entries=75, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065553350
2014-07-11 00:59:15,429 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:59:15,449 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1880 synced till here 1879
2014-07-11 00:59:15,473 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065553350 with entries=73, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065555430
2014-07-11 00:59:18,060 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:59:18,083 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1953 synced till here 1951
2014-07-11 00:59:18,124 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065555430 with entries=73, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065558061
2014-07-11 00:59:18,284 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=301, memsize=66.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/6a94313357ef48548f45c228112a2d7d
2014-07-11 00:59:18,301 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/6a94313357ef48548f45c228112a2d7d as hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/6a94313357ef48548f45c228112a2d7d
2014-07-11 00:59:18,319 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/6a94313357ef48548f45c228112a2d7d, entries=243290, sequenceid=301, filesize=17.3m
2014-07-11 00:59:18,320 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~465.6m/488231600, currentsize=139.8m/146614960 for region usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. in 13095ms, sequenceid=301, compaction requested=false
2014-07-11 00:59:18,320 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224., current region memstore size 342.7m
2014-07-11 00:59:19,142 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2.
2014-07-11 00:59:19,166 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2.
2014-07-11 00:59:19,626 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:59:20,463 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=343, memsize=102.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/c01f60e81b2c4dd09cee6e5085880c7e
2014-07-11 00:59:20,480 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/c01f60e81b2c4dd09cee6e5085880c7e as hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/c01f60e81b2c4dd09cee6e5085880c7e
2014-07-11 00:59:20,503 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/c01f60e81b2c4dd09cee6e5085880c7e, entries=372910, sequenceid=343, filesize=26.6m
2014-07-11 00:59:20,504 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~260.6m/273246880, currentsize=101.2m/106156240 for region usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. in 10842ms, sequenceid=343, compaction requested=false
2014-07-11 00:59:20,504 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2., current region memstore size 263.8m
2014-07-11 00:59:21,153 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:59:21,308 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2045 synced till here 2034
2014-07-11 00:59:21,647 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065558061 with entries=92, filesize=78.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065561153
2014-07-11 00:59:21,648 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065143043
2014-07-11 00:59:21,648 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065508241
2014-07-11 00:59:21,648 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065510593
2014-07-11 00:59:21,648 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065512721
2014-07-11 00:59:21,648 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065515814
2014-07-11 00:59:21,648 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065519796
2014-07-11 00:59:21,648 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065522546
2014-07-11 00:59:32,809 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:59:32,815 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 10646ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=11072ms
2014-07-11 00:59:33,021 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13591,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44923","starttimems":1405065559359,"queuetimems":1,"class":"HRegionServer","responsesize":19882,"method":"Multi"}
2014-07-11 00:59:33,021 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13385,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44923","starttimems":1405065559565,"queuetimems":1,"class":"HRegionServer","responsesize":19332,"method":"Multi"}
2014-07-11 00:59:33,021 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13629,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44923","starttimems":1405065559321,"queuetimems":0,"class":"HRegionServer","responsesize":19431,"method":"Multi"}
2014-07-11 00:59:33,022 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 626 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44923: output error
2014-07-11 00:59:33,023 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:59:33,023 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 622 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44923: output error
2014-07-11 00:59:33,023 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:59:33,024 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 628 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44923: output error
2014-07-11 00:59:33,024 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:59:49,692 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x1472468205c0005, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-11 00:59:49,692 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x1472468205c0000, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-11 00:59:49,692 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x1472468205c0003, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-11 00:59:49,860 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Opening socket connection to server master/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-11 00:59:49,862 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Socket connection established to master/9.1.143.58:2181, initiating session
2014-07-11 00:59:49,883 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Opening socket connection to server master/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-11 00:59:49,889 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Socket connection established to master/9.1.143.58:2181, initiating session
2014-07-11 00:59:50,122 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x1472468205c0005, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-11 00:59:50,123 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x1472468205c0000, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-11 00:59:50,441 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Opening socket connection to server master/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-11 00:59:50,442 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Socket connection established to master/9.1.143.58:2181, initiating session
2014-07-11 00:59:50,444 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x1472468205c0003, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-11 00:59:51,553 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-11 00:59:51,554 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-11 00:59:51,554 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x1472468205c0000, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-11 00:59:51,572 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-11 00:59:51,572 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-11 00:59:51,572 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x1472468205c0005, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-11 00:59:51,752 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Opening socket connection to server master/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-11 00:59:51,752 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Socket connection established to master/9.1.143.58:2181, initiating session
2014-07-11 00:59:51,754 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x1472468205c0000, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-11 00:59:52,155 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-11 00:59:52,155 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-11 00:59:52,161 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, sessionid = 0x1472468205c0003, negotiated timeout = 90000
2014-07-11 00:59:52,293 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Opening socket connection to server master/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-11 00:59:52,294 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Socket connection established to master/9.1.143.58:2181, initiating session
2014-07-11 00:59:52,297 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Session establishment complete on server master/9.1.143.58:2181, sessionid = 0x1472468205c0005, negotiated timeout = 90000
2014-07-11 00:59:53,787 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-11 00:59:53,788 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-11 00:59:53,790 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, sessionid = 0x1472468205c0000, negotiated timeout = 90000
2014-07-11 01:00:00,897 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":41283,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44923","starttimems":1405065559611,"queuetimems":0,"class":"HRegionServer","responsesize":20678,"method":"Multi"}
2014-07-11 01:00:00,897 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 630 service: ClientService methodName: Multi size: 3.6m connection: 9.1.143.53:44923: output error
2014-07-11 01:00:00,897 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:09,527 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":49694,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44923","starttimems":1405065559832,"queuetimems":1,"class":"HRegionServer","responsesize":19803,"method":"Multi"}
2014-07-11 01:00:09,529 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 632 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44923: output error
2014-07-11 01:00:09,529 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:09,771 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:00:09,816 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2149 synced till here 2123
2014-07-11 01:00:10,097 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":49891,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44923","starttimems":1405065560205,"queuetimems":123,"class":"HRegionServer","responsesize":19567,"method":"Multi"}
2014-07-11 01:00:10,098 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 638 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44923: output error
2014-07-11 01:00:10,098 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:10,098 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":49854,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44923","starttimems":1405065560243,"queuetimems":0,"class":"HRegionServer","responsesize":18969,"method":"Multi"}
2014-07-11 01:00:10,098 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 639 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:44923: output error
2014-07-11 01:00:10,098 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:10,099 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":49775,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44923","starttimems":1405065560323,"queuetimems":1,"class":"HRegionServer","responsesize":19643,"method":"Multi"}
2014-07-11 01:00:10,099 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":49814,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44923","starttimems":1405065560284,"queuetimems":1,"class":"HRegionServer","responsesize":19817,"method":"Multi"}
2014-07-11 01:00:10,100 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":50059,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44923","starttimems":1405065560041,"queuetimems":1,"class":"HRegionServer","responsesize":19122,"method":"Multi"}
2014-07-11 01:00:10,102 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":50232,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44923","starttimems":1405065559869,"queuetimems":0,"class":"HRegionServer","responsesize":19729,"method":"Multi"}
2014-07-11 01:00:10,097 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":49602,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44923","starttimems":1405065560494,"queuetimems":0,"class":"HRegionServer","responsesize":19729,"method":"Multi"}
2014-07-11 01:00:10,099 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 647 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44923: output error
2014-07-11 01:00:10,106 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:10,106 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065561153 with entries=104, filesize=88.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065609772
2014-07-11 01:00:10,106 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 646 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44923: output error
2014-07-11 01:00:10,106 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:10,109 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 641 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44923: output error
2014-07-11 01:00:10,109 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:10,114 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 634 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44923: output error
2014-07-11 01:00:10,114 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:10,114 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 636 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:44923: output error
2014-07-11 01:00:10,115 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:10,363 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":49487,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44923","starttimems":1405065560876,"queuetimems":1,"class":"HRegionServer","responsesize":19750,"method":"Multi"}
2014-07-11 01:00:10,363 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":49301,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44923","starttimems":1405065561062,"queuetimems":1,"class":"HRegionServer","responsesize":19789,"method":"Multi"}
2014-07-11 01:00:10,363 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":49824,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44923","starttimems":1405065560539,"queuetimems":0,"class":"HRegionServer","responsesize":19936,"method":"Multi"}
2014-07-11 01:00:10,363 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":49447,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44923","starttimems":1405065560916,"queuetimems":1,"class":"HRegionServer","responsesize":19732,"method":"Multi"}
2014-07-11 01:00:10,364 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 643 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44923: output error
2014-07-11 01:00:10,364 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:10,364 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 660 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44923: output error
2014-07-11 01:00:10,364 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":49669,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44923","starttimems":1405065560694,"queuetimems":1,"class":"HRegionServer","responsesize":19952,"method":"Multi"}
2014-07-11 01:00:10,364 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:10,364 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 645 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44923: output error
2014-07-11 01:00:10,365 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:10,365 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 644 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44923: output error
2014-07-11 01:00:10,365 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:10,369 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":49269,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44923","starttimems":1405065561100,"queuetimems":0,"class":"HRegionServer","responsesize":19959,"method":"Multi"}
2014-07-11 01:00:10,370 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 656 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44923: output error
2014-07-11 01:00:10,370 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:10,372 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 642 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44923: output error
2014-07-11 01:00:10,373 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:11,970 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:00:11,977 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":50697,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44923","starttimems":1405065561279,"queuetimems":0,"class":"HRegionServer","responsesize":19410,"method":"Multi"}
2014-07-11 01:00:11,978 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 655 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44923: output error
2014-07-11 01:00:11,979 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:12,026 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2250 synced till here 2229
2014-07-11 01:00:12,387 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065609772 with entries=101, filesize=85.9m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065611972
2014-07-11 01:00:12,387 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":39576,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44923","starttimems":1405065572811,"queuetimems":0,"class":"HRegionServer","responsesize":19760,"method":"Multi"}
2014-07-11 01:00:12,388 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 651 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44923: output error
2014-07-11 01:00:12,388 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:12,421 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":39534,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44925","starttimems":1405065572886,"queuetimems":7,"class":"HRegionServer","responsesize":19766,"method":"Multi"}
2014-07-11 01:00:12,422 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 748 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44925: output error
2014-07-11 01:00:12,446 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:12,611 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":39765,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44923","starttimems":1405065572845,"queuetimems":1,"class":"HRegionServer","responsesize":19547,"method":"Multi"}
2014-07-11 01:00:12,611 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 650 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44923: output error
2014-07-11 01:00:12,611 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:14,014 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":41090,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44925","starttimems":1405065572924,"queuetimems":2,"class":"HRegionServer","responsesize":20678,"method":"Multi"}
2014-07-11 01:00:14,015 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 762 service: ClientService methodName: Multi size: 3.6m connection: 9.1.143.53:44925: output error
2014-07-11 01:00:14,015 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:14,015 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":52328,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44923","starttimems":1405065561686,"queuetimems":1,"class":"HRegionServer","responsesize":19745,"method":"Multi"}
2014-07-11 01:00:14,015 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 652 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44923: output error
2014-07-11 01:00:14,015 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:14,021 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":52568,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44923","starttimems":1405065561453,"queuetimems":1,"class":"HRegionServer","responsesize":19536,"method":"Multi"}
2014-07-11 01:00:14,022 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":40677,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44925","starttimems":1405065573344,"queuetimems":6,"class":"HRegionServer","responsesize":18969,"method":"Multi"}
2014-07-11 01:00:14,022 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 654 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44923: output error
2014-07-11 01:00:14,022 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:14,022 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 753 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:44925: output error
2014-07-11 01:00:14,022 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:14,022 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":52517,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44923","starttimems":1405065561505,"queuetimems":1,"class":"HRegionServer","responsesize":19791,"method":"Multi"}
2014-07-11 01:00:14,023 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 653 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44923: output error
2014-07-11 01:00:14,023 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:14,028 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":40803,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44925","starttimems":1405065573225,"queuetimems":1,"class":"HRegionServer","responsesize":20173,"method":"Multi"}
2014-07-11 01:00:14,028 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 756 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44925: output error
2014-07-11 01:00:14,028 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:14,042 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":40983,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44925","starttimems":1405065573058,"queuetimems":13,"class":"HRegionServer","responsesize":19567,"method":"Multi"}
2014-07-11 01:00:14,042 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 760 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44925: output error
2014-07-11 01:00:14,042 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:14,045 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":40935,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44925","starttimems":1405065573110,"queuetimems":1,"class":"HRegionServer","responsesize":19309,"method":"Multi"}
2014-07-11 01:00:14,046 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 759 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44925: output error
2014-07-11 01:00:14,046 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:14,054 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14293,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44929","starttimems":1405065599760,"queuetimems":1,"class":"HRegionServer","responsesize":19332,"method":"Multi"}
2014-07-11 01:00:14,054 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":40527,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44925","starttimems":1405065573526,"queuetimems":0,"class":"HRegionServer","responsesize":19874,"method":"Multi"}
2014-07-11 01:00:14,054 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 765 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:14,054 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 750 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44925: output error
2014-07-11 01:00:14,054 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:14,055 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":40471,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44925","starttimems":1405065573583,"queuetimems":9,"class":"HRegionServer","responsesize":19782,"method":"Multi"}
2014-07-11 01:00:14,055 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 749 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44925: output error
2014-07-11 01:00:14,055 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:14,054 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":40789,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44925","starttimems":1405065573264,"queuetimems":1,"class":"HRegionServer","responsesize":19803,"method":"Multi"}
2014-07-11 01:00:14,061 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 755 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44925: output error
2014-07-11 01:00:14,061 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:14,054 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":41053,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44925","starttimems":1405065573000,"queuetimems":1,"class":"HRegionServer","responsesize":19882,"method":"Multi"}
2014-07-11 01:00:14,055 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14246,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44929","starttimems":1405065599808,"queuetimems":6,"class":"HRegionServer","responsesize":19745,"method":"Multi"}
2014-07-11 01:00:14,065 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 761 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44925: output error
2014-07-11 01:00:14,065 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:14,072 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:14,072 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 795 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:14,072 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:14,073 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":40887,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44925","starttimems":1405065573186,"queuetimems":1,"class":"HRegionServer","responsesize":19789,"method":"Multi"}
2014-07-11 01:00:14,074 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 757 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44925: output error
2014-07-11 01:00:14,074 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:14,162 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":40859,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44925","starttimems":1405065573302,"queuetimems":1,"class":"HRegionServer","responsesize":19122,"method":"Multi"}
2014-07-11 01:00:14,162 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":40694,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44925","starttimems":1405065573467,"queuetimems":12,"class":"HRegionServer","responsesize":19431,"method":"Multi"}
2014-07-11 01:00:14,162 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 754 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:44925: output error
2014-07-11 01:00:14,162 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:14,162 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 751 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44925: output error
2014-07-11 01:00:14,162 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:14,162 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":40767,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44925","starttimems":1405065573394,"queuetimems":17,"class":"HRegionServer","responsesize":19729,"method":"Multi"}
2014-07-11 01:00:14,165 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 752 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44925: output error
2014-07-11 01:00:14,165 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:14,565 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:00:14,627 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2355 synced till here 2326
2014-07-11 01:00:14,996 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 768 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:14,996 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:14,996 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14540,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44929","starttimems":1405065600455,"queuetimems":0,"class":"HRegionServer","responsesize":19791,"method":"Multi"}
2014-07-11 01:00:14,996 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14579,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44929","starttimems":1405065600416,"queuetimems":0,"class":"HRegionServer","responsesize":19936,"method":"Multi"}
2014-07-11 01:00:14,997 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14701,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44929","starttimems":1405065600295,"queuetimems":0,"class":"HRegionServer","responsesize":19952,"method":"Multi"}
2014-07-11 01:00:14,997 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 784 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:14,997 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:14,997 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 785 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:14,997 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:15,005 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14625,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44929","starttimems":1405065600380,"queuetimems":5,"class":"HRegionServer","responsesize":19122,"method":"Multi"}
2014-07-11 01:00:15,005 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15119,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44929","starttimems":1405065599886,"queuetimems":5,"class":"HRegionServer","responsesize":19817,"method":"Multi"}
2014-07-11 01:00:15,006 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 783 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:15,006 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:15,009 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15077,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44929","starttimems":1405065599932,"queuetimems":0,"class":"HRegionServer","responsesize":19750,"method":"Multi"}
2014-07-11 01:00:15,009 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14672,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44929","starttimems":1405065600337,"queuetimems":1,"class":"HRegionServer","responsesize":19760,"method":"Multi"}
2014-07-11 01:00:15,009 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15037,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44929","starttimems":1405065599972,"queuetimems":1,"class":"HRegionServer","responsesize":19536,"method":"Multi"}
2014-07-11 01:00:15,009 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 792 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:15,019 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:15,019 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065611972 with entries=105, filesize=89.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065614566
2014-07-11 01:00:15,022 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14486,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44929","starttimems":1405065600536,"queuetimems":0,"class":"HRegionServer","responsesize":19729,"method":"Multi"}
2014-07-11 01:00:15,022 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":41867,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44925","starttimems":1405065573155,"queuetimems":9,"class":"HRegionServer","responsesize":19332,"method":"Multi"}
2014-07-11 01:00:15,022 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14766,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44929","starttimems":1405065600256,"queuetimems":1,"class":"HRegionServer","responsesize":19547,"method":"Multi"}
2014-07-11 01:00:15,022 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14125,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44929","starttimems":1405065600897,"queuetimems":322,"class":"HRegionServer","responsesize":19732,"method":"Multi"}
2014-07-11 01:00:15,023 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 758 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44925: output error
2014-07-11 01:00:15,023 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:15,022 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15177,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44929","starttimems":1405065599845,"queuetimems":4,"class":"HRegionServer","responsesize":19959,"method":"Multi"}
2014-07-11 01:00:15,023 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14527,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44929","starttimems":1405065600495,"queuetimems":0,"class":"HRegionServer","responsesize":19431,"method":"Multi"}
2014-07-11 01:00:15,055 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 789 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:15,055 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:15,055 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 770 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:15,055 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:15,056 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 794 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:15,056 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:15,056 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 767 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:15,056 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:15,056 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 787 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:15,056 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:15,058 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 766 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:15,059 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:15,059 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 790 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:15,059 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:15,059 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 788 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:15,059 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:15,059 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 791 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:15,059 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:15,059 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 782 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:15,059 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:15,060 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 793 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:15,060 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:15,194 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 786 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:15,194 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:15,195 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 771 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:15,195 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:15,573 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 773 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:15,573 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:15,573 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 772 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:15,573 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:15,574 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 769 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:15,574 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:15,584 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 779 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:15,584 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:15,647 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=406, memsize=163.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/9103e46591c4423d8c3befca84b558c3
2014-07-11 01:00:15,672 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/9103e46591c4423d8c3befca84b558c3 as hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/9103e46591c4423d8c3befca84b558c3
2014-07-11 01:00:15,674 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 777 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:15,674 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:15,699 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/9103e46591c4423d8c3befca84b558c3, entries=593330, sequenceid=406, filesize=42.3m
2014-07-11 01:00:15,700 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~266.9m/279884080, currentsize=110.9m/116329840 for region usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. in 55195ms, sequenceid=406, compaction requested=false
2014-07-11 01:00:15,700 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2., current region memstore size 377.2m
2014-07-11 01:00:16,373 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29.
2014-07-11 01:00:16,375 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 774 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:16,375 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:16,437 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 781 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:16,437 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:16,441 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 780 service: ClientService methodName: Multi size: 3.6m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:16,442 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:16,548 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:00:16,549 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 775 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:44929: output error
2014-07-11 01:00:16,549 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:00:16,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2434 synced till here 2426
2014-07-11 01:00:16,662 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065614566 with entries=79, filesize=67.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065616548
2014-07-11 01:00:16,983 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:00:22,865 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:00:22,934 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065616548 with entries=73, filesize=62.2m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065622865
2014-07-11 01:00:23,746 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=495, memsize=179.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/d212f3a5c5ee4ea0bf5149ae90faf1be
2014-07-11 01:00:23,766 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/d212f3a5c5ee4ea0bf5149ae90faf1be as hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/d212f3a5c5ee4ea0bf5149ae90faf1be
2014-07-11 01:00:23,783 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/d212f3a5c5ee4ea0bf5149ae90faf1be, entries=655100, sequenceid=495, filesize=46.7m
2014-07-11 01:00:23,783 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~404.9m/424600800, currentsize=19.0m/19900240 for region usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. in 8083ms, sequenceid=495, compaction requested=false
2014-07-11 01:00:23,784 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29., current region memstore size 313.9m
2014-07-11 01:00:24,019 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:00:24,065 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355.
2014-07-11 01:00:25,824 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=396, memsize=158.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/3804991da7994caea4ebffc11a9a6ca7
2014-07-11 01:00:25,868 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/3804991da7994caea4ebffc11a9a6ca7 as hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/3804991da7994caea4ebffc11a9a6ca7
2014-07-11 01:00:25,902 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/3804991da7994caea4ebffc11a9a6ca7, entries=578680, sequenceid=396, filesize=41.3m
2014-07-11 01:00:25,902 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~344.2m/360934480, currentsize=185.4m/194441280 for region usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. in 67582ms, sequenceid=396, compaction requested=false
2014-07-11 01:00:25,903 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355., current region memstore size 266.9m
2014-07-11 01:00:26,499 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:00:26,499 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:00:26,533 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2586 synced till here 2578
2014-07-11 01:00:26,621 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065622865 with entries=79, filesize=67.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065626499
2014-07-11 01:00:26,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065526045
2014-07-11 01:00:26,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065528074
2014-07-11 01:00:26,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065530943
2014-07-11 01:00:26,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065533055
2014-07-11 01:00:26,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065535041
2014-07-11 01:00:26,622 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065537866
2014-07-11 01:00:26,622 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065542585
2014-07-11 01:00:29,191 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:00:29,246 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2664 synced till here 2658
2014-07-11 01:00:29,588 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065626499 with entries=78, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065629191
2014-07-11 01:00:30,749 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=505, memsize=155.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/ff0c838fb7d44519b113bb7dbbdd94eb
2014-07-11 01:00:30,786 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/ff0c838fb7d44519b113bb7dbbdd94eb as hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/ff0c838fb7d44519b113bb7dbbdd94eb
2014-07-11 01:00:30,807 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/ff0c838fb7d44519b113bb7dbbdd94eb, entries=564230, sequenceid=505, filesize=40.3m
2014-07-11 01:00:30,808 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~313.9m/329130240, currentsize=57.0m/59807920 for region usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. in 7024ms, sequenceid=505, compaction requested=false
2014-07-11 01:00:32,248 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=517, memsize=135.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/d91bfa9eece94d6e888be80e5a793d1a
2014-07-11 01:00:32,262 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/d91bfa9eece94d6e888be80e5a793d1a as hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/d91bfa9eece94d6e888be80e5a793d1a
2014-07-11 01:00:32,276 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/d91bfa9eece94d6e888be80e5a793d1a, entries=491700, sequenceid=517, filesize=35.1m
2014-07-11 01:00:32,276 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~268.4m/281483840, currentsize=50.4m/52837840 for region usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. in 6373ms, sequenceid=517, compaction requested=true
2014-07-11 01:00:32,277 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:00:32,277 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-11 01:00:32,278 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-11 01:00:32,278 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:00:32,278 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:00:32,278 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. because compaction request was cancelled
2014-07-11 01:00:33,941 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:00:34,007 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2739 synced till here 2736
2014-07-11 01:00:34,286 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065629191 with entries=75, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065633941
2014-07-11 01:00:34,286 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065544867
2014-07-11 01:00:34,286 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065547144
2014-07-11 01:00:34,286 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065549270
2014-07-11 01:00:34,286 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065551295
2014-07-11 01:00:34,286 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065553350
2014-07-11 01:00:34,286 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065555430
2014-07-11 01:00:35,687 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224.
2014-07-11 01:00:35,688 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224., current region memstore size 260.4m
2014-07-11 01:00:35,997 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:00:36,080 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:00:36,103 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2818 synced till here 2812
2014-07-11 01:00:36,917 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065633941 with entries=79, filesize=67.5m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065636081
2014-07-11 01:00:38,049 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2.
2014-07-11 01:00:38,051 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2., current region memstore size 256.3m
2014-07-11 01:00:38,333 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:00:38,864 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:00:38,922 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2895 synced till here 2891
2014-07-11 01:00:38,994 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065636081 with entries=77, filesize=65.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065638865
2014-07-11 01:00:41,414 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=566, memsize=115.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/33061bff92db4eaeae0227dd8079b21b
2014-07-11 01:00:41,432 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/33061bff92db4eaeae0227dd8079b21b as hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/33061bff92db4eaeae0227dd8079b21b
2014-07-11 01:00:41,451 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/33061bff92db4eaeae0227dd8079b21b, entries=419190, sequenceid=566, filesize=29.9m
2014-07-11 01:00:41,452 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~263.5m/276277200, currentsize=44.5m/46623440 for region usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. in 5764ms, sequenceid=566, compaction requested=true
2014-07-11 01:00:41,453 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:00:41,453 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-11 01:00:41,453 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-11 01:00:41,453 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:00:41,453 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:00:41,453 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. because compaction request was cancelled
2014-07-11 01:00:41,782 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:00:41,814 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2970 synced till here 2967
2014-07-11 01:00:41,947 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065638865 with entries=75, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065641782
2014-07-11 01:00:43,762 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:00:43,798 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3045 synced till here 3042
2014-07-11 01:00:43,868 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065641782 with entries=75, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065643763
2014-07-11 01:00:43,927 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=578, memsize=120.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/2b5884a8cca14a92b9cdb0f22820fb3a
2014-07-11 01:00:43,949 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/2b5884a8cca14a92b9cdb0f22820fb3a as hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/2b5884a8cca14a92b9cdb0f22820fb3a
2014-07-11 01:00:43,961 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/2b5884a8cca14a92b9cdb0f22820fb3a, entries=438910, sequenceid=578, filesize=31.3m
2014-07-11 01:00:43,962 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~264.1m/276881920, currentsize=55.9m/58632640 for region usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. in 5911ms, sequenceid=578, compaction requested=true
2014-07-11 01:00:43,962 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:00:43,962 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-11 01:00:43,962 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-11 01:00:43,963 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:00:43,963 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:00:43,963 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. because compaction request was cancelled
2014-07-11 01:00:47,398 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:00:47,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3132 synced till here 3125
2014-07-11 01:00:48,086 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065643763 with entries=87, filesize=74.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065647399
2014-07-11 01:00:48,086 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065558061
2014-07-11 01:00:48,086 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065561153
2014-07-11 01:00:48,086 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065609772
2014-07-11 01:00:48,087 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065611972
2014-07-11 01:00:50,551 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:00:50,591 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3205 synced till here 3204
2014-07-11 01:00:50,608 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065647399 with entries=73, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065650552
2014-07-11 01:00:53,837 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:00:53,883 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065650552 with entries=71, filesize=60.9m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065653838
2014-07-11 01:00:53,965 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2.
2014-07-11 01:00:53,966 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2., current region memstore size 257.1m
2014-07-11 01:00:54,171 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:00:58,147 DEBUG [RpcServer.handler=39,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29.
2014-07-11 01:00:58,148 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29., current region memstore size 257.1m
2014-07-11 01:00:58,176 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:00:58,227 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3351 synced till here 3350
2014-07-11 01:00:58,261 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065653838 with entries=75, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065658177
2014-07-11 01:00:58,315 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:00:59,296 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355.
2014-07-11 01:00:59,824 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:00:59,852 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3425 synced till here 3422
2014-07-11 01:00:59,886 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065658177 with entries=74, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065659824
2014-07-11 01:01:01,615 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=661, memsize=212.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/7d85ab3ee66348fe801cf8718de83fb7
2014-07-11 01:01:01,654 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/7d85ab3ee66348fe801cf8718de83fb7 as hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/7d85ab3ee66348fe801cf8718de83fb7
2014-07-11 01:01:01,670 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/7d85ab3ee66348fe801cf8718de83fb7, entries=775300, sequenceid=661, filesize=55.2m
2014-07-11 01:01:01,671 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~257.1m/269552640, currentsize=57.2m/59989440 for region usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. in 7704ms, sequenceid=661, compaction requested=true
2014-07-11 01:01:01,671 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:01:01,671 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355., current region memstore size 279.3m
2014-07-11 01:01:01,678 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-11 01:01:01,678 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-11 01:01:01,678 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:01:01,679 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:01:01,679 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. because compaction request was cancelled
2014-07-11 01:01:01,880 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:01:02,504 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:01:02,530 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3497 synced till here 3496
2014-07-11 01:01:02,551 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065659824 with entries=72, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065662505
2014-07-11 01:01:02,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065614566
2014-07-11 01:01:02,552 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065616548
2014-07-11 01:01:06,527 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=672, memsize=221.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/02bef540e445426788c2c47e00d28058
2014-07-11 01:01:06,561 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/02bef540e445426788c2c47e00d28058 as hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/02bef540e445426788c2c47e00d28058
2014-07-11 01:01:06,582 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/02bef540e445426788c2c47e00d28058, entries=807760, sequenceid=672, filesize=57.5m
2014-07-11 01:01:06,583 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~257.1m/269562000, currentsize=65.5m/68691200 for region usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. in 8435ms, sequenceid=672, compaction requested=true
2014-07-11 01:01:06,583 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:01:06,583 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-11 01:01:06,583 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-11 01:01:06,584 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:01:06,584 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:01:06,584 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. because compaction request was cancelled
2014-07-11 01:01:09,314 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=698, memsize=231.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/274605514a244f809dd5ae7f822a0586
2014-07-11 01:01:09,331 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/274605514a244f809dd5ae7f822a0586 as hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/274605514a244f809dd5ae7f822a0586
2014-07-11 01:01:09,366 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/274605514a244f809dd5ae7f822a0586, entries=842910, sequenceid=698, filesize=60.0m
2014-07-11 01:01:09,366 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~279.3m/292857280, currentsize=28.4m/29760960 for region usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. in 7695ms, sequenceid=698, compaction requested=true
2014-07-11 01:01:09,366 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:01:09,367 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-11 01:01:09,367 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-11 01:01:09,367 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:01:09,367 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:01:09,367 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. because compaction request was cancelled
2014-07-11 01:01:09,909 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:01:09,942 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3571 synced till here 3569
2014-07-11 01:01:09,964 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065662505 with entries=74, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065669909
2014-07-11 01:01:09,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065622865
2014-07-11 01:01:09,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065626499
2014-07-11 01:01:09,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065629191
2014-07-11 01:01:11,331 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224.
2014-07-11 01:01:11,333 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224., current region memstore size 261.6m
2014-07-11 01:01:11,551 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:01:11,639 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3658 synced till here 3644
2014-07-11 01:01:11,657 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:01:11,993 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065669909 with entries=87, filesize=74.5m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065671551
2014-07-11 01:01:14,394 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2.
2014-07-11 01:01:14,401 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2., current region memstore size 256.0m
2014-07-11 01:01:15,091 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:01:16,622 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3772 synced till here 3751
2014-07-11 01:01:16,728 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:01:16,982 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065671551 with entries=114, filesize=98.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065675092
2014-07-11 01:01:18,806 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:01:18,834 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3853 synced till here 3846
2014-07-11 01:01:18,920 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065675092 with entries=81, filesize=69.3m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065678806
2014-07-11 01:01:21,072 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1356ms
GC pool 'ParNew' had collection(s): count=1 time=1370ms
2014-07-11 01:01:21,466 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:01:21,522 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3962 synced till here 3938
2014-07-11 01:01:21,796 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065678806 with entries=109, filesize=93.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065681466
2014-07-11 01:01:23,224 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1149ms
GC pool 'ParNew' had collection(s): count=1 time=1270ms
2014-07-11 01:01:24,173 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=738, memsize=228.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/54c8b9e84ae847d0b6fd6ed2411ca26c
2014-07-11 01:01:24,269 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/54c8b9e84ae847d0b6fd6ed2411ca26c as hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/54c8b9e84ae847d0b6fd6ed2411ca26c
2014-07-11 01:01:24,287 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/54c8b9e84ae847d0b6fd6ed2411ca26c, entries=830000, sequenceid=738, filesize=59.1m
2014-07-11 01:01:24,289 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~264.9m/277728880, currentsize=119.7m/125468720 for region usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. in 12956ms, sequenceid=738, compaction requested=true
2014-07-11 01:01:24,290 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-11 01:01:24,290 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-11 01:01:24,290 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:01:24,290 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:01:24,290 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. because compaction request was cancelled
2014-07-11 01:01:24,290 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:01:25,889 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1164ms
GC pool 'ParNew' had collection(s): count=1 time=1473ms
2014-07-11 01:01:26,369 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:01:26,400 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4077 synced till here 4067
2014-07-11 01:01:26,583 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065681466 with entries=115, filesize=98.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065686369
2014-07-11 01:01:26,583 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065633941
2014-07-11 01:01:28,424 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2.
2014-07-11 01:01:28,429 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2., current region memstore size 265.6m
2014-07-11 01:01:28,664 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:01:29,525 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:01:30,763 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355.
2014-07-11 01:01:30,803 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4205 synced till here 4203
2014-07-11 01:01:30,911 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065686369 with entries=128, filesize=109.9m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065689525
2014-07-11 01:01:32,692 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1015ms
GC pool 'ParNew' had collection(s): count=2 time=1077ms
2014-07-11 01:01:32,901 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=747, memsize=229.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/f644ec93987c4a7c9d038aec9a7bf102
2014-07-11 01:01:32,920 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/f644ec93987c4a7c9d038aec9a7bf102 as hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/f644ec93987c4a7c9d038aec9a7bf102
2014-07-11 01:01:32,942 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:01:33,365 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29.
2014-07-11 01:01:33,605 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4363 synced till here 4322
2014-07-11 01:01:33,615 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/f644ec93987c4a7c9d038aec9a7bf102, entries=836580, sequenceid=747, filesize=59.6m
2014-07-11 01:01:33,616 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~260.6m/273279200, currentsize=172.3m/180655120 for region usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. in 19215ms, sequenceid=747, compaction requested=true
2014-07-11 01:01:33,616 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:01:33,617 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-11 01:01:33,617 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-11 01:01:33,617 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355., current region memstore size 288.0m
2014-07-11 01:01:33,617 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:01:33,617 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:01:33,617 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. because compaction request was cancelled
2014-07-11 01:01:34,947 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065689525 with entries=158, filesize=135.9m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065692943
2014-07-11 01:01:34,947 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065636081
2014-07-11 01:01:34,947 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065638865
2014-07-11 01:01:34,947 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065641782
2014-07-11 01:01:34,948 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065643763
2014-07-11 01:01:34,948 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065647399
2014-07-11 01:01:34,948 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065650552
2014-07-11 01:01:35,706 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:01:35,710 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:01:35,775 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4478 synced till here 4447
2014-07-11 01:01:36,017 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224.
2014-07-11 01:01:36,970 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065692943 with entries=115, filesize=97.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065695710
2014-07-11 01:01:38,004 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:01:38,017 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4582 synced till here 4550
2014-07-11 01:01:39,430 DEBUG [RpcServer.handler=24,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2.
2014-07-11 01:01:39,539 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065695710 with entries=104, filesize=89.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065698004
2014-07-11 01:01:39,962 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=834, memsize=161.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/ae46de3c322f423fa9c6baa67c3d1b82
2014-07-11 01:01:39,979 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/ae46de3c322f423fa9c6baa67c3d1b82 as hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/ae46de3c322f423fa9c6baa67c3d1b82
2014-07-11 01:01:39,998 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/ae46de3c322f423fa9c6baa67c3d1b82, entries=586960, sequenceid=834, filesize=41.9m
2014-07-11 01:01:39,998 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~267.2m/280189600, currentsize=145.7m/152813280 for region usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. in 11569ms, sequenceid=834, compaction requested=true
2014-07-11 01:01:39,998 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:01:39,999 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-11 01:01:39,999 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-11 01:01:39,999 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:01:39,999 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29., current region memstore size 358.8m
2014-07-11 01:01:39,999 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:01:39,999 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. because compaction request was cancelled
2014-07-11 01:01:41,397 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1074ms
GC pool 'ParNew' had collection(s): count=1 time=1124ms
2014-07-11 01:01:41,650 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:01:41,674 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:01:41,719 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4698 synced till here 4671
2014-07-11 01:01:42,003 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065698004 with entries=116, filesize=99.2m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065701650
2014-07-11 01:01:42,003 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065653838
2014-07-11 01:01:43,590 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:01:43,641 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4817 synced till here 4782
2014-07-11 01:01:44,053 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065701650 with entries=119, filesize=102.2m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065703591
2014-07-11 01:01:44,255 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=901, memsize=103.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/b2d4171b22664cf38160849689b91217
2014-07-11 01:01:44,268 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/b2d4171b22664cf38160849689b91217 as hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/b2d4171b22664cf38160849689b91217
2014-07-11 01:01:44,297 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/b2d4171b22664cf38160849689b91217, entries=374880, sequenceid=901, filesize=26.7m
2014-07-11 01:01:44,297 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~314.5m/329829200, currentsize=127.4m/133639040 for region usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. in 10680ms, sequenceid=901, compaction requested=true
2014-07-11 01:01:44,297 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:01:44,297 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-11 01:01:44,297 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-11 01:01:44,298 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:01:44,298 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:01:44,298 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224., current region memstore size 372.5m
2014-07-11 01:01:44,298 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. because compaction request was cancelled
2014-07-11 01:01:56,168 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=9304, hits=1684, hitRatio=18.09%, , cachingAccesses=1688, cachingHits=1684, cachingHitsRatio=99.76%, evictions=0, evicted=2, evictedPerRun=Infinity
2014-07-11 01:01:56,170 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 11237ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=11434ms
2014-07-11 01:01:56,308 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:01:56,475 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:01:56,476 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16393,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065700083,"queuetimems":3224,"class":"HRegionServer","responsesize":20112,"method":"Multi"}
2014-07-11 01:01:56,477 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16297,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065700179,"queuetimems":3283,"class":"HRegionServer","responsesize":19640,"method":"Multi"}
2014-07-11 01:01:56,477 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16403,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065700073,"queuetimems":4846,"class":"HRegionServer","responsesize":19784,"method":"Multi"}
2014-07-11 01:01:56,477 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1562 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:56,477 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16416,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065700061,"queuetimems":4907,"class":"HRegionServer","responsesize":19779,"method":"Multi"}
2014-07-11 01:01:56,478 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:56,481 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16404,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065700077,"queuetimems":4776,"class":"HRegionServer","responsesize":19658,"method":"Multi"}
2014-07-11 01:01:56,478 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1552 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:56,484 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:56,485 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14811,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065701674,"queuetimems":4685,"class":"HRegionServer","responsesize":19620,"method":"Multi"}
2014-07-11 01:01:56,485 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14805,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065701680,"queuetimems":4652,"class":"HRegionServer","responsesize":19793,"method":"Multi"}
2014-07-11 01:01:56,485 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15081,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065701404,"queuetimems":4464,"class":"HRegionServer","responsesize":19876,"method":"Multi"}
2014-07-11 01:01:56,485 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1566 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:56,486 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:56,486 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1565 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:56,486 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:56,486 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1568 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:56,486 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:56,486 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1567 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:56,486 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:56,487 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1550 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:56,487 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:56,488 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1554 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:56,488 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:56,489 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14579,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065701909,"queuetimems":4842,"class":"HRegionServer","responsesize":20006,"method":"Multi"}
2014-07-11 01:01:56,489 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1564 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:56,489 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:56,497 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16443,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065700053,"queuetimems":4973,"class":"HRegionServer","responsesize":19653,"method":"Multi"}
2014-07-11 01:01:56,497 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1556 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:56,497 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:56,512 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4933 synced till here 4916
2014-07-11 01:01:56,682 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065703591 with entries=116, filesize=99.3m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065716476
2014-07-11 01:01:57,047 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15108,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065701938,"queuetimems":4835,"class":"HRegionServer","responsesize":19594,"method":"Multi"}
2014-07-11 01:01:57,047 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14912,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065702134,"queuetimems":4662,"class":"HRegionServer","responsesize":19730,"method":"Multi"}
2014-07-11 01:01:57,047 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15088,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065701958,"queuetimems":4560,"class":"HRegionServer","responsesize":20012,"method":"Multi"}
2014-07-11 01:01:57,047 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15106,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065701940,"queuetimems":4798,"class":"HRegionServer","responsesize":19909,"method":"Multi"}
2014-07-11 01:01:57,047 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1563 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:57,047 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:57,047 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1572 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:57,047 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:57,048 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15073,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065701974,"queuetimems":4539,"class":"HRegionServer","responsesize":19486,"method":"Multi"}
2014-07-11 01:01:57,048 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1573 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:57,048 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:57,048 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1580 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:57,048 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:57,048 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1571 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:57,049 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:57,069 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2.
2014-07-11 01:01:57,144 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13552,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065703592,"queuetimems":6003,"class":"HRegionServer","responsesize":19484,"method":"Multi"}
2014-07-11 01:01:57,144 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14318,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065702826,"queuetimems":5273,"class":"HRegionServer","responsesize":19620,"method":"Multi"}
2014-07-11 01:01:57,145 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1577 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:57,145 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:57,145 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1578 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:57,145 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:57,145 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13552,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065703593,"queuetimems":5928,"class":"HRegionServer","responsesize":20108,"method":"Multi"}
2014-07-11 01:01:57,146 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14319,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065702826,"queuetimems":5314,"class":"HRegionServer","responsesize":19869,"method":"Multi"}
2014-07-11 01:01:57,146 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1575 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:57,146 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:57,146 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1579 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:57,146 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:57,959 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:01:57,960 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14050,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065703910,"queuetimems":4273,"class":"HRegionServer","responsesize":19674,"method":"Multi"}
2014-07-11 01:01:57,961 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1587 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:57,961 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:57,961 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13938,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065704023,"queuetimems":4293,"class":"HRegionServer","responsesize":19979,"method":"Multi"}
2014-07-11 01:01:57,962 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1585 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:57,962 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:57,965 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14370,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065703594,"queuetimems":4252,"class":"HRegionServer","responsesize":19671,"method":"Multi"}
2014-07-11 01:01:57,965 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14371,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065703593,"queuetimems":5891,"class":"HRegionServer","responsesize":19750,"method":"Multi"}
2014-07-11 01:01:57,965 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14367,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065703597,"queuetimems":4018,"class":"HRegionServer","responsesize":19750,"method":"Multi"}
2014-07-11 01:01:57,965 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1583 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:57,965 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:57,965 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1574 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:57,965 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:57,965 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14373,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065703592,"queuetimems":5965,"class":"HRegionServer","responsesize":19783,"method":"Multi"}
2014-07-11 01:01:57,965 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1588 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:57,966 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:57,966 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1576 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:57,966 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:57,965 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14371,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065703594,"queuetimems":4184,"class":"HRegionServer","responsesize":19617,"method":"Multi"}
2014-07-11 01:01:57,973 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13982,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065703991,"queuetimems":4300,"class":"HRegionServer","responsesize":19721,"method":"Multi"}
2014-07-11 01:01:57,974 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1590 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:57,974 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:57,974 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1586 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:57,974 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:57,974 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14380,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065703594,"queuetimems":4106,"class":"HRegionServer","responsesize":19461,"method":"Multi"}
2014-07-11 01:01:57,975 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1589 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:57,975 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:57,999 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5023 synced till here 5013
2014-07-11 01:01:58,039 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065716476 with entries=90, filesize=76.3m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065717959
2014-07-11 01:01:58,073 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13687,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065704386,"queuetimems":4575,"class":"HRegionServer","responsesize":19784,"method":"Multi"}
2014-07-11 01:01:58,074 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1593 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:58,074 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:58,180 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13796,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065704383,"queuetimems":4615,"class":"HRegionServer","responsesize":19943,"method":"Multi"}
2014-07-11 01:01:58,180 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1584 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:58,180 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:58,181 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13795,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065704386,"queuetimems":4535,"class":"HRegionServer","responsesize":19289,"method":"Multi"}
2014-07-11 01:01:58,181 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13648,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065704533,"queuetimems":4489,"class":"HRegionServer","responsesize":19599,"method":"Multi"}
2014-07-11 01:01:58,182 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1604 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:58,182 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:58,182 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1600 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:58,182 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:58,271 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13709,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065704561,"queuetimems":1011,"class":"HRegionServer","responsesize":19640,"method":"Multi"}
2014-07-11 01:01:58,271 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13709,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065704561,"queuetimems":1048,"class":"HRegionServer","responsesize":20006,"method":"Multi"}
2014-07-11 01:01:58,271 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1613 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:58,271 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:58,271 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1614 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:58,271 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:58,272 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13885,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065704386,"queuetimems":4496,"class":"HRegionServer","responsesize":19658,"method":"Multi"}
2014-07-11 01:01:58,272 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13725,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065704546,"queuetimems":3107,"class":"HRegionServer","responsesize":19869,"method":"Multi"}
2014-07-11 01:01:58,272 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1603 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:58,272 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:58,272 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1595 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:58,272 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:58,271 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13884,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065704386,"queuetimems":4436,"class":"HRegionServer","responsesize":19804,"method":"Multi"}
2014-07-11 01:01:58,272 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13878,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065704393,"queuetimems":4401,"class":"HRegionServer","responsesize":19859,"method":"Multi"}
2014-07-11 01:01:58,273 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1602 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:58,273 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:58,273 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1601 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:58,273 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:58,375 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13832,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065704542,"queuetimems":4415,"class":"HRegionServer","responsesize":19653,"method":"Multi"}
2014-07-11 01:01:58,376 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1598 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:58,376 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:58,438 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13868,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065704570,"queuetimems":903,"class":"HRegionServer","responsesize":19793,"method":"Multi"}
2014-07-11 01:01:58,438 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13879,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065704559,"queuetimems":1087,"class":"HRegionServer","responsesize":19620,"method":"Multi"}
2014-07-11 01:01:58,439 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13893,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065704545,"queuetimems":3124,"class":"HRegionServer","responsesize":19779,"method":"Multi"}
2014-07-11 01:01:58,439 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13891,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065704547,"queuetimems":3076,"class":"HRegionServer","responsesize":19395,"method":"Multi"}
2014-07-11 01:01:58,438 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13876,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065704562,"queuetimems":933,"class":"HRegionServer","responsesize":19876,"method":"Multi"}
2014-07-11 01:01:58,438 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13884,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065704553,"queuetimems":3038,"class":"HRegionServer","responsesize":19909,"method":"Multi"}
2014-07-11 01:01:58,439 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13711,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065704727,"queuetimems":1022,"class":"HRegionServer","responsesize":20112,"method":"Multi"}
2014-07-11 01:01:58,439 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1610 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:58,439 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:58,440 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1609 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:58,440 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:58,440 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1605 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:58,440 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:58,440 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1611 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:58,440 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:58,440 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1594 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:58,440 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:58,440 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1596 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:58,440 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:58,440 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1608 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:58,441 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:58,468 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13923,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065704544,"queuetimems":4369,"class":"HRegionServer","responsesize":19780,"method":"Multi"}
2014-07-11 01:01:58,468 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13928,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065704539,"queuetimems":4454,"class":"HRegionServer","responsesize":19603,"method":"Multi"}
2014-07-11 01:01:58,468 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13907,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44931","starttimems":1405065704561,"queuetimems":975,"class":"HRegionServer","responsesize":19594,"method":"Multi"}
2014-07-11 01:01:58,468 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1597 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:58,469 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:58,469 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1599 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:58,469 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:58,469 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1612 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:58,469 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:58,578 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1617 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44931: output error
2014-07-11 01:01:58,578 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 01:01:59,234 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:02:01,560 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5223 synced till here 5210
2014-07-11 01:02:01,732 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=983, memsize=63.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/858c92c5e8684b418740613cae467b5c
2014-07-11 01:02:01,732 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=919, memsize=141.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/518213efd9a7463f90b1610cffd15a92
2014-07-11 01:02:01,778 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065717959 with entries=200, filesize=171.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065719234
2014-07-11 01:02:01,810 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/518213efd9a7463f90b1610cffd15a92 as hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/518213efd9a7463f90b1610cffd15a92
2014-07-11 01:02:01,810 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/858c92c5e8684b418740613cae467b5c as hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/858c92c5e8684b418740613cae467b5c
2014-07-11 01:02:01,841 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/518213efd9a7463f90b1610cffd15a92, entries=514950, sequenceid=919, filesize=36.7m
2014-07-11 01:02:01,841 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~381.9m/400413920, currentsize=193.6m/203006800 for region usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. in 21842ms, sequenceid=919, compaction requested=true
2014-07-11 01:02:01,841 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:02:01,842 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-11 01:02:01,842 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-11 01:02:01,842 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:02:01,842 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2., current region memstore size 468.3m
2014-07-11 01:02:01,842 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:02:01,842 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. because compaction request was cancelled
2014-07-11 01:02:01,887 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/858c92c5e8684b418740613cae467b5c, entries=231520, sequenceid=983, filesize=16.5m
2014-07-11 01:02:01,887 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~378.7m/397144560, currentsize=103.9m/108915120 for region usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. in 17589ms, sequenceid=983, compaction requested=true
2014-07-11 01:02:01,888 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-11 01:02:01,888 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:02:01,888 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-11 01:02:01,888 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:02:01,888 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2., current region memstore size 337.4m
2014-07-11 01:02:01,888 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:02:01,889 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. because compaction request was cancelled
2014-07-11 01:02:02,146 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:02:02,247 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:02:02,653 DEBUG [RpcServer.handler=24,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355.
2014-07-11 01:02:02,820 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:02:03,364 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5309 synced till here 5304
2014-07-11 01:02:03,431 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065719234 with entries=86, filesize=73.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065722821
2014-07-11 01:02:03,431 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065658177
2014-07-11 01:02:03,431 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065659824
2014-07-11 01:02:03,431 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065662505
2014-07-11 01:02:03,431 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065669909
2014-07-11 01:02:04,515 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:02:04,572 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5387 synced till here 5382
2014-07-11 01:02:04,664 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065722821 with entries=78, filesize=67.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065724516
2014-07-11 01:02:05,594 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29.
2014-07-11 01:02:05,906 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1053, memsize=68.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/7f05d132a28b4d62b1c85f28965b746d
2014-07-11 01:02:05,922 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/7f05d132a28b4d62b1c85f28965b746d as hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/7f05d132a28b4d62b1c85f28965b746d
2014-07-11 01:02:05,937 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/7f05d132a28b4d62b1c85f28965b746d, entries=249190, sequenceid=1053, filesize=17.7m
2014-07-11 01:02:05,937 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~337.4m/353835920, currentsize=68.3m/71574240 for region usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. in 4049ms, sequenceid=1053, compaction requested=true
2014-07-11 01:02:05,937 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:02:05,938 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-11 01:02:05,938 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-11 01:02:05,938 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:02:05,938 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355., current region memstore size 307.1m
2014-07-11 01:02:05,938 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:02:05,938 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. because compaction request was cancelled
2014-07-11 01:02:06,099 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:02:06,156 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5475 synced till here 5458
2014-07-11 01:02:07,253 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065724516 with entries=88, filesize=75.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065726099
2014-07-11 01:02:07,412 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:02:07,457 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1053, memsize=70.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/ce998fc98c78436d94c9874b4f4bcc45
2014-07-11 01:02:07,476 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/ce998fc98c78436d94c9874b4f4bcc45 as hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/ce998fc98c78436d94c9874b4f4bcc45
2014-07-11 01:02:07,488 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/ce998fc98c78436d94c9874b4f4bcc45, entries=254850, sequenceid=1053, filesize=18.2m
2014-07-11 01:02:07,489 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~474.7m/497765600, currentsize=71.9m/75419520 for region usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. in 5647ms, sequenceid=1053, compaction requested=true
2014-07-11 01:02:07,489 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:02:07,489 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-11 01:02:07,489 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-11 01:02:07,490 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29., current region memstore size 272.1m
2014-07-11 01:02:07,490 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:02:07,490 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:02:07,490 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. because compaction request was cancelled
2014-07-11 01:02:07,737 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:02:10,006 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:02:10,067 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5579 synced till here 5558
2014-07-11 01:02:10,273 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065726099 with entries=104, filesize=89.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065730007
2014-07-11 01:02:10,273 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065671551
2014-07-11 01:02:10,273 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065675092
2014-07-11 01:02:10,273 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065678806
2014-07-11 01:02:10,273 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065681466
2014-07-11 01:02:10,273 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065686369
2014-07-11 01:02:10,273 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065689525
2014-07-11 01:02:12,142 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:02:12,173 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5679 synced till here 5655
2014-07-11 01:02:12,484 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065730007 with entries=100, filesize=86.5m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065732142
2014-07-11 01:02:12,638 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224.
2014-07-11 01:02:14,025 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:02:15,156 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065732142 with entries=97, filesize=83.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065734025
2014-07-11 01:02:15,278 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1105, memsize=136.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/09144c8b4aa844c8bd968f14d3aa45a5
2014-07-11 01:02:15,278 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1096, memsize=126.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/4df57675f1d84d40858434c6941e788e
2014-07-11 01:02:15,314 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/09144c8b4aa844c8bd968f14d3aa45a5 as hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/09144c8b4aa844c8bd968f14d3aa45a5
2014-07-11 01:02:15,321 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/4df57675f1d84d40858434c6941e788e as hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/4df57675f1d84d40858434c6941e788e
2014-07-11 01:02:15,337 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/09144c8b4aa844c8bd968f14d3aa45a5, entries=496500, sequenceid=1105, filesize=35.4m
2014-07-11 01:02:15,337 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~313.4m/328614320, currentsize=108.5m/113785200 for region usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. in 9399ms, sequenceid=1105, compaction requested=true
2014-07-11 01:02:15,338 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-11 01:02:15,338 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-11 01:02:15,338 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:02:15,338 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:02:15,338 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. because compaction request was cancelled
2014-07-11 01:02:15,338 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:02:15,339 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224., current region memstore size 295.1m
2014-07-11 01:02:15,344 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/4df57675f1d84d40858434c6941e788e, entries=459820, sequenceid=1096, filesize=32.8m
2014-07-11 01:02:15,344 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~273.6m/286933120, currentsize=89.1m/93409280 for region usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. in 7854ms, sequenceid=1096, compaction requested=true
2014-07-11 01:02:15,344 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-11 01:02:15,345 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-11 01:02:15,345 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:02:15,345 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:02:15,345 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. because compaction request was cancelled
2014-07-11 01:02:15,345 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:02:15,552 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:02:16,232 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:02:16,252 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5848 synced till here 5847
2014-07-11 01:02:16,270 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065734025 with entries=72, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065736232
2014-07-11 01:02:16,270 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065692943
2014-07-11 01:02:16,270 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065695710
2014-07-11 01:02:16,270 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065698004
2014-07-11 01:02:16,270 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065701650
2014-07-11 01:02:18,334 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:02:18,376 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5941 synced till here 5923
2014-07-11 01:02:18,524 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065736232 with entries=93, filesize=80.0m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065738334
2014-07-11 01:02:20,196 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:02:20,219 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6018 synced till here 6013
2014-07-11 01:02:20,268 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065738334 with entries=77, filesize=66.2m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065740196
2014-07-11 01:02:20,496 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2.
2014-07-11 01:02:20,497 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2., current region memstore size 258.1m
2014-07-11 01:02:20,982 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2.
2014-07-11 01:02:21,170 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:02:21,337 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:02:22,163 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6108 synced till here 6092
2014-07-11 01:02:22,448 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065740196 with entries=90, filesize=77.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065741337
2014-07-11 01:02:23,186 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1174, memsize=152.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/10475c1277954cfbbbf8e626dd4b8215
2014-07-11 01:02:23,201 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/10475c1277954cfbbbf8e626dd4b8215 as hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/10475c1277954cfbbbf8e626dd4b8215
2014-07-11 01:02:23,217 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/10475c1277954cfbbbf8e626dd4b8215, entries=553460, sequenceid=1174, filesize=39.5m
2014-07-11 01:02:23,217 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~295.1m/309385200, currentsize=104.5m/109584160 for region usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. in 7878ms, sequenceid=1174, compaction requested=true
2014-07-11 01:02:23,218 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:02:23,218 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-11 01:02:23,218 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-11 01:02:23,218 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2., current region memstore size 285.1m
2014-07-11 01:02:23,218 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:02:23,218 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:02:23,218 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. because compaction request was cancelled
2014-07-11 01:02:23,416 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:02:23,535 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:02:23,569 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6187 synced till here 6181
2014-07-11 01:02:24,473 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065741337 with entries=79, filesize=67.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065743535
2014-07-11 01:02:24,473 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065703591
2014-07-11 01:02:24,473 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065716476
2014-07-11 01:02:24,473 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065717959
2014-07-11 01:02:27,352 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:02:27,405 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6276 synced till here 6261
2014-07-11 01:02:27,491 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1221, memsize=117.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/0f1fad51ab9240c8a7f6ad668ed12543
2014-07-11 01:02:27,517 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/0f1fad51ab9240c8a7f6ad668ed12543 as hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/0f1fad51ab9240c8a7f6ad668ed12543
2014-07-11 01:02:27,541 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/0f1fad51ab9240c8a7f6ad668ed12543, entries=426660, sequenceid=1221, filesize=30.4m
2014-07-11 01:02:27,542 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~262.9m/275719360, currentsize=57.8m/60613280 for region usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. in 7045ms, sequenceid=1221, compaction requested=true
2014-07-11 01:02:27,542 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:02:27,542 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-11 01:02:27,542 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-11 01:02:27,543 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:02:27,543 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:02:27,543 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. because compaction request was cancelled
2014-07-11 01:02:27,617 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355.
2014-07-11 01:02:27,617 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355., current region memstore size 257.0m
2014-07-11 01:02:27,666 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065743535 with entries=89, filesize=76.3m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065747352
2014-07-11 01:02:28,147 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:02:29,749 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:02:29,807 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6379 synced till here 6355
2014-07-11 01:02:29,963 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29.
2014-07-11 01:02:30,075 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065747352 with entries=103, filesize=88.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065749750
2014-07-11 01:02:30,215 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1236, memsize=128.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/2aef356290954069b13a8edf31798d5e
2014-07-11 01:02:30,231 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/2aef356290954069b13a8edf31798d5e as hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/2aef356290954069b13a8edf31798d5e
2014-07-11 01:02:30,241 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/2aef356290954069b13a8edf31798d5e, entries=466490, sequenceid=1236, filesize=33.2m
2014-07-11 01:02:30,242 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~285.1m/298958480, currentsize=65.3m/68421440 for region usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. in 7023ms, sequenceid=1236, compaction requested=true
2014-07-11 01:02:30,242 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:02:30,242 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-11 01:02:30,242 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29., current region memstore size 262.7m
2014-07-11 01:02:30,242 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-11 01:02:30,242 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:02:30,242 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:02:30,242 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. because compaction request was cancelled
2014-07-11 01:02:30,606 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:02:31,315 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:02:31,333 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6457 synced till here 6452
2014-07-11 01:02:31,382 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065749750 with entries=78, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065751315
2014-07-11 01:02:31,382 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065719234
2014-07-11 01:02:31,382 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065722821
2014-07-11 01:02:32,441 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1277, memsize=78.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/e05c4cfba45049029326c185a8023a16
2014-07-11 01:02:32,456 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/e05c4cfba45049029326c185a8023a16 as hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/e05c4cfba45049029326c185a8023a16
2014-07-11 01:02:32,471 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/e05c4cfba45049029326c185a8023a16, entries=286430, sequenceid=1277, filesize=20.4m
2014-07-11 01:02:32,471 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~264.9m/277734800, currentsize=65.1m/68279760 for region usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. in 4854ms, sequenceid=1277, compaction requested=true
2014-07-11 01:02:32,472 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:02:32,473 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-11 01:02:32,473 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-11 01:02:32,473 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:02:32,473 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:02:32,473 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. because compaction request was cancelled
2014-07-11 01:02:32,547 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:02:32,571 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6536 synced till here 6528
2014-07-11 01:02:32,666 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065751315 with entries=79, filesize=68.0m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065752548
2014-07-11 01:02:32,667 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065724516
2014-07-11 01:02:34,320 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:02:34,364 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6619 synced till here 6612
2014-07-11 01:02:34,411 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065752548 with entries=83, filesize=70.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065754320
2014-07-11 01:02:34,594 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1275, memsize=79.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/63072c4080d84175b066adc585ca4085
2014-07-11 01:02:34,611 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/63072c4080d84175b066adc585ca4085 as hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/63072c4080d84175b066adc585ca4085
2014-07-11 01:02:34,634 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/63072c4080d84175b066adc585ca4085, entries=289080, sequenceid=1275, filesize=20.6m
2014-07-11 01:02:34,635 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~276.9m/290380400, currentsize=86.6m/90763680 for region usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. in 4392ms, sequenceid=1275, compaction requested=true
2014-07-11 01:02:34,635 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:02:34,635 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-11 01:02:34,635 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-11 01:02:34,635 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:02:34,635 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:02:34,635 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. because compaction request was cancelled
2014-07-11 01:02:35,395 DEBUG [RpcServer.handler=46,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224.
2014-07-11 01:02:35,396 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224., current region memstore size 256.6m
2014-07-11 01:02:35,641 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:02:35,878 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:02:36,522 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6718 synced till here 6708
2014-07-11 01:02:37,016 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065754320 with entries=99, filesize=85.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065755878
2014-07-11 01:02:37,017 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065726099
2014-07-11 01:02:37,017 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065730007
2014-07-11 01:02:37,017 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065732142
2014-07-11 01:02:38,614 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:02:38,633 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6795 synced till here 6792
2014-07-11 01:02:38,690 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065755878 with entries=77, filesize=66.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065758615
2014-07-11 01:02:41,030 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:02:41,055 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6875 synced till here 6870
2014-07-11 01:02:41,084 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2.
2014-07-11 01:02:41,084 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2., current region memstore size 256.3m
2014-07-11 01:02:41,091 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1342, memsize=122.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/7ca367f53a734fabaf508c49fd32fba7
2014-07-11 01:02:41,094 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065758615 with entries=80, filesize=68.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065761031
2014-07-11 01:02:41,107 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/7ca367f53a734fabaf508c49fd32fba7 as hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/7ca367f53a734fabaf508c49fd32fba7
2014-07-11 01:02:41,122 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/7ca367f53a734fabaf508c49fd32fba7, entries=447460, sequenceid=1342, filesize=31.9m
2014-07-11 01:02:41,122 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~259.8m/272397120, currentsize=68.2m/71502480 for region usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. in 5726ms, sequenceid=1342, compaction requested=true
2014-07-11 01:02:41,123 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:02:41,123 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-11 01:02:41,123 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-11 01:02:41,123 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:02:41,123 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:02:41,123 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. because compaction request was cancelled
2014-07-11 01:02:41,419 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:02:43,157 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:02:43,249 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6968 synced till here 6951
2014-07-11 01:02:43,551 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065761031 with entries=93, filesize=79.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065763157
2014-07-11 01:02:43,552 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065734025
2014-07-11 01:02:43,552 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065736232
2014-07-11 01:02:43,552 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065738334
2014-07-11 01:02:44,155 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2.
2014-07-11 01:02:44,155 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2., current region memstore size 256.6m
2014-07-11 01:02:45,775 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:02:48,164 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:02:48,203 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1387, memsize=137.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/994c1e983bdc470184d2005cef95981f
2014-07-11 01:02:48,235 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7078 synced till here 7050
2014-07-11 01:02:48,269 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/994c1e983bdc470184d2005cef95981f as hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/994c1e983bdc470184d2005cef95981f
2014-07-11 01:02:48,287 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/994c1e983bdc470184d2005cef95981f, entries=498750, sequenceid=1387, filesize=35.6m
2014-07-11 01:02:48,289 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~257.8m/270348000, currentsize=43.7m/45795760 for region usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. in 7205ms, sequenceid=1387, compaction requested=true
2014-07-11 01:02:48,289 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-11 01:02:48,289 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:02:48,289 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-11 01:02:48,290 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:02:48,290 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:02:48,290 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. because compaction request was cancelled
2014-07-11 01:02:48,382 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355.
2014-07-11 01:02:48,383 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355., current region memstore size 258.1m
2014-07-11 01:02:48,417 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065763157 with entries=110, filesize=94.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065768164
2014-07-11 01:02:48,418 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065740196
2014-07-11 01:02:49,811 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:02:50,499 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:02:50,573 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7194 synced till here 7157
2014-07-11 01:02:51,391 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065768164 with entries=116, filesize=99.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065770499
2014-07-11 01:02:53,364 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1406, memsize=147.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/1ac9b438b33748bb8b6dc21f0fa0a93f
2014-07-11 01:02:53,390 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/1ac9b438b33748bb8b6dc21f0fa0a93f as hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/1ac9b438b33748bb8b6dc21f0fa0a93f
2014-07-11 01:02:53,403 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/1ac9b438b33748bb8b6dc21f0fa0a93f, entries=536940, sequenceid=1406, filesize=38.3m
2014-07-11 01:02:53,403 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~262.9m/275637760, currentsize=64.8m/67936640 for region usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. in 9248ms, sequenceid=1406, compaction requested=true
2014-07-11 01:02:53,404 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:02:53,404 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-11 01:02:53,404 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-11 01:02:53,404 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:02:53,404 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:02:53,404 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. because compaction request was cancelled
2014-07-11 01:02:54,194 DEBUG [RpcServer.handler=30,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29.
2014-07-11 01:02:54,197 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:02:54,215 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29., current region memstore size 256.2m
2014-07-11 01:02:55,567 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7347 synced till here 7303
2014-07-11 01:02:55,633 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:02:56,006 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065770499 with entries=153, filesize=130.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065774197
2014-07-11 01:02:56,007 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065741337
2014-07-11 01:02:56,007 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065743535
2014-07-11 01:02:57,637 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:02:57,661 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7451 synced till here 7420
2014-07-11 01:02:58,027 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224.
2014-07-11 01:02:58,042 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065774197 with entries=104, filesize=89.2m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065777638
2014-07-11 01:02:59,294 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1449, memsize=175.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/87a210ada6c449ec8bef21c736a28dac
2014-07-11 01:02:59,310 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/87a210ada6c449ec8bef21c736a28dac as hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/87a210ada6c449ec8bef21c736a28dac
2014-07-11 01:02:59,325 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/87a210ada6c449ec8bef21c736a28dac, entries=640130, sequenceid=1449, filesize=45.6m
2014-07-11 01:02:59,325 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~266.1m/279012560, currentsize=109.2m/114465200 for region usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. in 10942ms, sequenceid=1449, compaction requested=true
2014-07-11 01:02:59,326 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:02:59,326 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 20 blocking
2014-07-11 01:02:59,326 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-11 01:02:59,326 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:02:59,326 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:02:59,326 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224., current region memstore size 269.2m
2014-07-11 01:02:59,326 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. because compaction request was cancelled
2014-07-11 01:02:59,660 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:03:01,228 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:01,326 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7574 synced till here 7551
2014-07-11 01:03:01,436 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065777638 with entries=123, filesize=105.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065781228
2014-07-11 01:03:01,436 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065747352
2014-07-11 01:03:02,156 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:02,188 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7670 synced till here 7646
2014-07-11 01:03:02,940 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065781228 with entries=96, filesize=82.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065782157
2014-07-11 01:03:03,146 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2.
2014-07-11 01:03:05,202 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:05,228 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7777 synced till here 7749
2014-07-11 01:03:05,587 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065782157 with entries=107, filesize=89.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065785203
2014-07-11 01:03:06,549 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:06,583 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7868 synced till here 7861
2014-07-11 01:03:06,633 DEBUG [RpcServer.handler=39,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2.
2014-07-11 01:03:06,659 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065785203 with entries=91, filesize=78.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065786549
2014-07-11 01:03:07,877 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:07,999 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1447, memsize=177.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/5444e33562be44968d1d90f668515fb2
2014-07-11 01:03:08,013 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/5444e33562be44968d1d90f668515fb2 as hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/5444e33562be44968d1d90f668515fb2
2014-07-11 01:03:08,036 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1519, memsize=110.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/592389c954694fb898dab027f4385622
2014-07-11 01:03:08,049 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7963 synced till here 7954
2014-07-11 01:03:08,057 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/5444e33562be44968d1d90f668515fb2, entries=645160, sequenceid=1447, filesize=46.0m
2014-07-11 01:03:08,057 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~265.6m/278479520, currentsize=226.4m/237374960 for region usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. in 13842ms, sequenceid=1447, compaction requested=true
2014-07-11 01:03:08,057 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:03:08,057 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-11 01:03:08,058 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-11 01:03:08,058 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:03:08,058 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2., current region memstore size 327.0m
2014-07-11 01:03:08,058 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:03:08,058 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. because compaction request was cancelled
2014-07-11 01:03:08,100 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065786549 with entries=95, filesize=81.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065787877
2014-07-11 01:03:08,101 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065749750
2014-07-11 01:03:08,102 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065751315
2014-07-11 01:03:08,102 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065752548
2014-07-11 01:03:08,103 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/592389c954694fb898dab027f4385622 as hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/592389c954694fb898dab027f4385622
2014-07-11 01:03:08,136 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/592389c954694fb898dab027f4385622, entries=402020, sequenceid=1519, filesize=28.7m
2014-07-11 01:03:08,137 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~272.4m/285612080, currentsize=134.3m/140859200 for region usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. in 8810ms, sequenceid=1519, compaction requested=true
2014-07-11 01:03:08,137 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:03:08,137 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 20 blocking
2014-07-11 01:03:08,137 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-11 01:03:08,137 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2., current region memstore size 293.5m
2014-07-11 01:03:08,137 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:03:08,137 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:03:08,137 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. because compaction request was cancelled
2014-07-11 01:03:09,160 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:03:09,202 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:03:09,429 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355.
2014-07-11 01:03:09,837 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29.
2014-07-11 01:03:09,938 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:09,987 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8044 synced till here 8042
2014-07-11 01:03:10,016 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065787877 with entries=81, filesize=69.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065789939
2014-07-11 01:03:10,017 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065754320
2014-07-11 01:03:10,017 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065755878
2014-07-11 01:03:10,017 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065758615
2014-07-11 01:03:11,656 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:12,023 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8143 synced till here 8132
2014-07-11 01:03:12,131 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065789939 with entries=99, filesize=84.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065791657
2014-07-11 01:03:12,958 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1601, memsize=55.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/941a67989a794296b7c1297c31476ca9
2014-07-11 01:03:12,989 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/941a67989a794296b7c1297c31476ca9 as hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/941a67989a794296b7c1297c31476ca9
2014-07-11 01:03:13,181 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/941a67989a794296b7c1297c31476ca9, entries=202550, sequenceid=1601, filesize=14.4m
2014-07-11 01:03:13,181 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~298.2m/312649520, currentsize=72.7m/76278800 for region usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. in 5044ms, sequenceid=1601, compaction requested=true
2014-07-11 01:03:13,182 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:03:13,182 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 20 blocking
2014-07-11 01:03:13,182 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-11 01:03:13,182 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355., current region memstore size 312.2m
2014-07-11 01:03:13,182 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:03:13,182 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:03:13,183 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. because compaction request was cancelled
2014-07-11 01:03:13,496 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1604, memsize=70.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/890dfe28afdf4bd1bb0cfa469b91b46a
2014-07-11 01:03:13,517 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/890dfe28afdf4bd1bb0cfa469b91b46a as hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/890dfe28afdf4bd1bb0cfa469b91b46a
2014-07-11 01:03:13,553 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/890dfe28afdf4bd1bb0cfa469b91b46a, entries=255030, sequenceid=1604, filesize=18.2m
2014-07-11 01:03:13,553 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~336.2m/352534880, currentsize=68.5m/71833600 for region usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. in 5495ms, sequenceid=1604, compaction requested=true
2014-07-11 01:03:13,554 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:03:13,554 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 20 blocking
2014-07-11 01:03:13,554 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-11 01:03:13,554 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:03:13,554 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29., current region memstore size 304.2m
2014-07-11 01:03:13,554 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:03:13,554 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. because compaction request was cancelled
2014-07-11 01:03:13,775 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:03:13,885 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:13,894 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:03:13,908 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8224 synced till here 8223
2014-07-11 01:03:13,931 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065791657 with entries=81, filesize=69.3m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065793885
2014-07-11 01:03:13,932 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065761031
2014-07-11 01:03:13,932 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065763157
2014-07-11 01:03:15,282 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:15,320 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8307 synced till here 8297
2014-07-11 01:03:15,420 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065793885 with entries=83, filesize=71.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065795282
2014-07-11 01:03:16,478 DEBUG [RpcServer.handler=38,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224.
2014-07-11 01:03:16,520 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:16,537 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8383 synced till here 8381
2014-07-11 01:03:16,588 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065795282 with entries=76, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065796520
2014-07-11 01:03:17,729 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1646, memsize=79.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/b72ef0c60b5f4444b8b448098c9f9d25
2014-07-11 01:03:17,751 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/b72ef0c60b5f4444b8b448098c9f9d25 as hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/b72ef0c60b5f4444b8b448098c9f9d25
2014-07-11 01:03:17,768 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/b72ef0c60b5f4444b8b448098c9f9d25, entries=289260, sequenceid=1646, filesize=20.6m
2014-07-11 01:03:17,769 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~305.8m/320603840, currentsize=71.5m/74944000 for region usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. in 4215ms, sequenceid=1646, compaction requested=true
2014-07-11 01:03:17,769 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:03:17,770 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 20 blocking
2014-07-11 01:03:17,770 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-11 01:03:17,770 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224., current region memstore size 274.7m
2014-07-11 01:03:17,770 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:03:17,770 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:03:17,770 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. because compaction request was cancelled
2014-07-11 01:03:18,065 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:03:18,077 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1654, memsize=87.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/30b4fcaa8ff24e9da6e50e5d556b26a7
2014-07-11 01:03:18,112 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/30b4fcaa8ff24e9da6e50e5d556b26a7 as hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/30b4fcaa8ff24e9da6e50e5d556b26a7
2014-07-11 01:03:18,127 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/30b4fcaa8ff24e9da6e50e5d556b26a7, entries=317170, sequenceid=1654, filesize=22.6m
2014-07-11 01:03:18,127 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~318.2m/333607120, currentsize=77.7m/81484800 for region usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. in 4945ms, sequenceid=1654, compaction requested=true
2014-07-11 01:03:18,128 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:03:18,128 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 20 blocking
2014-07-11 01:03:18,128 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-11 01:03:18,128 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:03:18,128 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:03:18,128 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. because compaction request was cancelled
2014-07-11 01:03:18,194 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:18,219 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8463 synced till here 8457
2014-07-11 01:03:18,333 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065796520 with entries=80, filesize=68.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065798195
2014-07-11 01:03:18,333 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065768164
2014-07-11 01:03:18,333 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065770499
2014-07-11 01:03:18,335 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065774197
2014-07-11 01:03:19,675 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:19,699 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8537 synced till here 8536
2014-07-11 01:03:19,717 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065798195 with entries=74, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065799675
2014-07-11 01:03:20,983 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:21,289 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8614 synced till here 8610
2014-07-11 01:03:21,381 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065799675 with entries=77, filesize=66.0m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065800984
2014-07-11 01:03:22,605 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:23,210 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8699 synced till here 8691
2014-07-11 01:03:23,247 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065800984 with entries=85, filesize=72.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065802605
2014-07-11 01:03:24,554 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:24,805 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2.
2014-07-11 01:03:24,806 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2., current region memstore size 256.5m
2014-07-11 01:03:24,882 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2.
2014-07-11 01:03:25,259 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:03:25,454 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1700, memsize=154.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/a1193d48f2cc46638ee60fc2c4815fe1
2014-07-11 01:03:25,505 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8825 synced till here 8824
2014-07-11 01:03:25,522 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065802605 with entries=126, filesize=108.0m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065804555
2014-07-11 01:03:25,561 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/a1193d48f2cc46638ee60fc2c4815fe1 as hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/a1193d48f2cc46638ee60fc2c4815fe1
2014-07-11 01:03:25,574 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/a1193d48f2cc46638ee60fc2c4815fe1, entries=561510, sequenceid=1700, filesize=40.0m
2014-07-11 01:03:25,574 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~280.8m/294388400, currentsize=116.8m/122485600 for region usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. in 7804ms, sequenceid=1700, compaction requested=true
2014-07-11 01:03:25,575 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:03:25,575 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 20 blocking
2014-07-11 01:03:25,576 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2., current region memstore size 267.2m
2014-07-11 01:03:25,576 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-11 01:03:25,576 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:03:25,576 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:03:25,576 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. because compaction request was cancelled
2014-07-11 01:03:25,730 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:03:26,990 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:27,009 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8904 synced till here 8899
2014-07-11 01:03:27,063 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065804555 with entries=79, filesize=67.5m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065806991
2014-07-11 01:03:27,063 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065777638
2014-07-11 01:03:27,063 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065781228
2014-07-11 01:03:27,063 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065782157
2014-07-11 01:03:27,063 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065785203
2014-07-11 01:03:27,063 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065786549
2014-07-11 01:03:27,947 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:27,979 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065806991 with entries=73, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065807947
2014-07-11 01:03:29,172 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29.
2014-07-11 01:03:29,172 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355.
2014-07-11 01:03:29,387 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:29,407 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9058 synced till here 9053
2014-07-11 01:03:29,900 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065807947 with entries=81, filesize=69.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065809388
2014-07-11 01:03:30,825 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:30,852 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9131 synced till here 9130
2014-07-11 01:03:30,864 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065809388 with entries=73, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065810827
2014-07-11 01:03:31,991 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:32,013 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9208 synced till here 9204
2014-07-11 01:03:32,062 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065810827 with entries=77, filesize=65.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065811991
2014-07-11 01:03:33,811 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1768, memsize=228.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/b51081b397cf48499cdbb26fd3391df2
2014-07-11 01:03:33,825 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/b51081b397cf48499cdbb26fd3391df2 as hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/b51081b397cf48499cdbb26fd3391df2
2014-07-11 01:03:33,841 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/b51081b397cf48499cdbb26fd3391df2, entries=833130, sequenceid=1768, filesize=59.4m
2014-07-11 01:03:33,842 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~256.5m/268935040, currentsize=141.4m/148262800 for region usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. in 9036ms, sequenceid=1768, compaction requested=true
2014-07-11 01:03:33,842 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:03:33,842 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 20 blocking
2014-07-11 01:03:33,842 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-11 01:03:33,842 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:03:33,842 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29., current region memstore size 324.5m
2014-07-11 01:03:33,842 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:03:33,843 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. because compaction request was cancelled
2014-07-11 01:03:33,915 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224.
2014-07-11 01:03:34,048 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:34,068 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9284 synced till here 9280
2014-07-11 01:03:34,110 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065811991 with entries=76, filesize=65.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065814048
2014-07-11 01:03:34,132 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:03:34,556 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1777, memsize=237.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/5749c3bb832c41f0aa135f3303acc9da
2014-07-11 01:03:34,577 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/5749c3bb832c41f0aa135f3303acc9da as hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/5749c3bb832c41f0aa135f3303acc9da
2014-07-11 01:03:34,594 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/5749c3bb832c41f0aa135f3303acc9da, entries=864760, sequenceid=1777, filesize=61.6m
2014-07-11 01:03:34,594 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~267.2m/280133680, currentsize=147.6m/154810000 for region usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. in 9019ms, sequenceid=1777, compaction requested=true
2014-07-11 01:03:34,595 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:03:34,595 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 20 blocking
2014-07-11 01:03:34,595 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-11 01:03:34,595 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355., current region memstore size 336.1m
2014-07-11 01:03:34,595 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:03:34,595 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:03:34,595 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. because compaction request was cancelled
2014-07-11 01:03:34,800 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:03:36,281 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:36,687 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9379 synced till here 9356
2014-07-11 01:03:37,727 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065814048 with entries=95, filesize=81.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065816281
2014-07-11 01:03:37,727 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065787877
2014-07-11 01:03:37,728 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065789939
2014-07-11 01:03:39,794 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:39,865 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9483 synced till here 9464
2014-07-11 01:03:40,102 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065816281 with entries=104, filesize=88.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065819795
2014-07-11 01:03:40,686 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:40,734 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9560 synced till here 9555
2014-07-11 01:03:41,535 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065819795 with entries=77, filesize=65.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065820686
2014-07-11 01:03:42,577 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2.
2014-07-11 01:03:42,727 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:42,735 DEBUG [RpcServer.handler=38,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2.
2014-07-11 01:03:42,751 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9649 synced till here 9643
2014-07-11 01:03:42,800 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065820686 with entries=89, filesize=76.5m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065822727
2014-07-11 01:03:44,213 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:44,325 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9746 synced till here 9724
2014-07-11 01:03:45,344 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065822727 with entries=97, filesize=82.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065824214
2014-07-11 01:03:45,931 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:46,211 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9836 synced till here 9833
2014-07-11 01:03:46,238 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065824214 with entries=90, filesize=77.2m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065825932
2014-07-11 01:03:49,031 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:49,099 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9930 synced till here 9921
2014-07-11 01:03:49,177 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065825932 with entries=94, filesize=80.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065829031
2014-07-11 01:03:50,087 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:51,022 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10028 synced till here 10004
2014-07-11 01:03:51,076 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1857, memsize=308.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/5ef3a95fffd04274ac2c3f6ce2cb6a1b
2014-07-11 01:03:51,106 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/5ef3a95fffd04274ac2c3f6ce2cb6a1b as hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/5ef3a95fffd04274ac2c3f6ce2cb6a1b
2014-07-11 01:03:51,126 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/5ef3a95fffd04274ac2c3f6ce2cb6a1b, entries=1124610, sequenceid=1857, filesize=80.1m
2014-07-11 01:03:51,126 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~326.1m/341922800, currentsize=223.6m/234439840 for region usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. in 17284ms, sequenceid=1857, compaction requested=true
2014-07-11 01:03:51,126 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:03:51,127 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 20 blocking
2014-07-11 01:03:51,127 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224., current region memstore size 493.2m
2014-07-11 01:03:51,127 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-11 01:03:51,127 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:03:51,127 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:03:51,127 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. because compaction request was cancelled
2014-07-11 01:03:51,186 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065829031 with entries=98, filesize=84.3m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065830087
2014-07-11 01:03:51,606 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:03:51,925 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:52,813 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10119 synced till here 10104
2014-07-11 01:03:52,881 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1871, memsize=318.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/cba959103dcb4cc2a1d919ad40cadfaf
2014-07-11 01:03:52,928 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/cba959103dcb4cc2a1d919ad40cadfaf as hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/cba959103dcb4cc2a1d919ad40cadfaf
2014-07-11 01:03:52,958 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/cba959103dcb4cc2a1d919ad40cadfaf, entries=1161040, sequenceid=1871, filesize=82.7m
2014-07-11 01:03:52,959 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~336.1m/352374000, currentsize=269.0m/282038080 for region usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. in 18364ms, sequenceid=1871, compaction requested=true
2014-07-11 01:03:52,959 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 20 blocking
2014-07-11 01:03:52,959 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-11 01:03:52,959 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:03:52,959 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:03:52,959 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. because compaction request was cancelled
2014-07-11 01:03:52,960 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:03:52,960 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2., current region memstore size 407.0m
2014-07-11 01:03:52,975 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065830087 with entries=91, filesize=77.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065831925
2014-07-11 01:03:52,975 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065791657
2014-07-11 01:03:52,975 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065793885
2014-07-11 01:03:52,975 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065795282
2014-07-11 01:03:53,273 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355.
2014-07-11 01:03:53,281 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29.
2014-07-11 01:03:53,679 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:03:53,746 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:53,772 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10194 synced till here 10191
2014-07-11 01:03:53,801 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065831925 with entries=75, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065833746
2014-07-11 01:03:55,298 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:55,331 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10271 synced till here 10268
2014-07-11 01:03:55,385 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065833746 with entries=77, filesize=66.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065835299
2014-07-11 01:03:56,865 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:57,023 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10377 synced till here 10366
2014-07-11 01:03:57,931 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065835299 with entries=106, filesize=91.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065836865
2014-07-11 01:03:58,776 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:03:59,807 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10474 synced till here 10451
2014-07-11 01:04:00,005 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065836865 with entries=97, filesize=82.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065838777
2014-07-11 01:04:01,510 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:01,569 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10568 synced till here 10551
2014-07-11 01:04:01,685 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065838777 with entries=94, filesize=80.5m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065841510
2014-07-11 01:04:03,494 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:03,575 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10673 synced till here 10644
2014-07-11 01:04:04,315 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065841510 with entries=105, filesize=90.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065843495
2014-07-11 01:04:06,028 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:06,057 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10778 synced till here 10753
2014-07-11 01:04:06,280 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065843495 with entries=105, filesize=89.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065846028
2014-07-11 01:04:07,967 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:08,044 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10885 synced till here 10857
2014-07-11 01:04:08,276 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065846028 with entries=107, filesize=90.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065847968
2014-07-11 01:04:09,985 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2037, memsize=227.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/18dab4721ca340b2ba3709577f4eb8a2
2014-07-11 01:04:10,017 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/18dab4721ca340b2ba3709577f4eb8a2 as hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/18dab4721ca340b2ba3709577f4eb8a2
2014-07-11 01:04:10,045 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/18dab4721ca340b2ba3709577f4eb8a2, entries=826940, sequenceid=2037, filesize=58.9m
2014-07-11 01:04:10,045 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~418.1m/438452160, currentsize=248.0m/260024560 for region usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. in 17085ms, sequenceid=2037, compaction requested=true
2014-07-11 01:04:10,046 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:04:10,046 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 20 blocking
2014-07-11 01:04:10,046 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-11 01:04:10,046 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:04:10,046 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:04:10,046 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2., current region memstore size 661.3m
2014-07-11 01:04:10,046 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. because compaction request was cancelled
2014-07-11 01:04:10,116 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:10,176 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10986 synced till here 10964
2014-07-11 01:04:10,329 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2.
2014-07-11 01:04:10,351 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065847968 with entries=101, filesize=86.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065850116
2014-07-11 01:04:11,736 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:04:12,129 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:12,175 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11084 synced till here 11066
2014-07-11 01:04:12,305 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065850116 with entries=98, filesize=83.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065852129
2014-07-11 01:04:12,628 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2022, memsize=316.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/02893e97616c4cf3b8a2cdf5dec80485
2014-07-11 01:04:12,668 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/02893e97616c4cf3b8a2cdf5dec80485 as hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/02893e97616c4cf3b8a2cdf5dec80485
2014-07-11 01:04:12,686 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/02893e97616c4cf3b8a2cdf5dec80485, entries=1152650, sequenceid=2022, filesize=82.1m
2014-07-11 01:04:12,686 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~496.3m/520460000, currentsize=334.3m/350578560 for region usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. in 21559ms, sequenceid=2022, compaction requested=true
2014-07-11 01:04:12,688 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:04:12,688 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355., current region memstore size 566.4m
2014-07-11 01:04:12,688 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 20 blocking
2014-07-11 01:04:12,688 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-11 01:04:12,688 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:04:12,688 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:04:12,689 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. because compaction request was cancelled
2014-07-11 01:04:13,469 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224.
2014-07-11 01:04:13,667 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:13,688 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11158 synced till here 11156
2014-07-11 01:04:13,699 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065852129 with entries=74, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065853668
2014-07-11 01:04:13,699 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065796520
2014-07-11 01:04:13,699 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065798195
2014-07-11 01:04:13,699 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065799675
2014-07-11 01:04:13,699 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065800984
2014-07-11 01:04:13,699 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065802605
2014-07-11 01:04:13,748 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:04:15,084 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:15,117 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11237 synced till here 11231
2014-07-11 01:04:15,183 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065853668 with entries=79, filesize=67.5m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065855084
2014-07-11 01:04:16,826 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:17,548 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11376 synced till here 11367
2014-07-11 01:04:17,620 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065855084 with entries=139, filesize=119.5m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065856826
2014-07-11 01:04:19,105 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:19,130 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065856826 with entries=74, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065859105
2014-07-11 01:04:20,715 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:20,733 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11531 synced till here 11523
2014-07-11 01:04:20,798 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065859105 with entries=81, filesize=69.5m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065860715
2014-07-11 01:04:22,610 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:22,644 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11613 synced till here 11604
2014-07-11 01:04:22,791 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065860715 with entries=82, filesize=70.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065862611
2014-07-11 01:04:24,584 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:24,628 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2238, memsize=136.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/d1559b188e24407e813df9800801d716
2014-07-11 01:04:24,642 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11706 synced till here 11690
2014-07-11 01:04:24,647 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/d1559b188e24407e813df9800801d716 as hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/d1559b188e24407e813df9800801d716
2014-07-11 01:04:24,661 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/d1559b188e24407e813df9800801d716, entries=497550, sequenceid=2238, filesize=35.4m
2014-07-11 01:04:24,662 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~566.4m/593896720, currentsize=183.3m/192167760 for region usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. in 11974ms, sequenceid=2238, compaction requested=true
2014-07-11 01:04:24,663 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:04:24,663 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 20 blocking
2014-07-11 01:04:24,663 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-11 01:04:24,663 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29., current region memstore size 743.7m
2014-07-11 01:04:24,663 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:04:24,663 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:04:24,663 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. because compaction request was cancelled
2014-07-11 01:04:24,737 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065862611 with entries=93, filesize=79.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065864585
2014-07-11 01:04:26,533 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:26,593 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11788 synced till here 11780
2014-07-11 01:04:26,655 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065864585 with entries=82, filesize=70.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065866534
2014-07-11 01:04:26,769 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:04:28,423 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:28,683 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11894 synced till here 11877
2014-07-11 01:04:28,847 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065866534 with entries=106, filesize=91.2m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065868424
2014-07-11 01:04:30,022 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355.
2014-07-11 01:04:30,228 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:30,250 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2209, memsize=265.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/7c2631e326b6407a81f843da94bd2440
2014-07-11 01:04:30,267 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11970 synced till here 11967
2014-07-11 01:04:30,274 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/7c2631e326b6407a81f843da94bd2440 as hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/7c2631e326b6407a81f843da94bd2440
2014-07-11 01:04:30,302 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065868424 with entries=76, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065870228
2014-07-11 01:04:30,312 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/7c2631e326b6407a81f843da94bd2440, entries=967880, sequenceid=2209, filesize=69.0m
2014-07-11 01:04:30,314 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~669.1m/701569040, currentsize=306.3m/321188000 for region usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. in 20268ms, sequenceid=2209, compaction requested=true
2014-07-11 01:04:30,315 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:04:30,316 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 20 blocking
2014-07-11 01:04:30,316 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-11 01:04:30,316 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2., current region memstore size 569.4m
2014-07-11 01:04:30,316 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:04:30,316 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:04:30,316 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. because compaction request was cancelled
2014-07-11 01:04:30,365 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2.
2014-07-11 01:04:30,974 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:04:32,569 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:32,596 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12051 synced till here 12042
2014-07-11 01:04:32,715 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065870228 with entries=81, filesize=69.3m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065872570
2014-07-11 01:04:32,716 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065804555
2014-07-11 01:04:32,716 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065806991
2014-07-11 01:04:32,716 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065807947
2014-07-11 01:04:32,716 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065809388
2014-07-11 01:04:32,716 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065810827
2014-07-11 01:04:34,645 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:36,251 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12229 synced till here 12223
2014-07-11 01:04:36,311 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065872570 with entries=178, filesize=152.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065874645
2014-07-11 01:04:37,000 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:37,018 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12305 synced till here 12302
2014-07-11 01:04:37,080 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065874645 with entries=76, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065877001
2014-07-11 01:04:38,445 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:38,500 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065877001 with entries=78, filesize=67.2m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065878446
2014-07-11 01:04:39,522 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:39,540 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12460 synced till here 12456
2014-07-11 01:04:39,597 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065878446 with entries=77, filesize=65.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065879522
2014-07-11 01:04:40,781 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:40,836 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12537 synced till here 12534
2014-07-11 01:04:40,891 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065879522 with entries=77, filesize=66.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065880781
2014-07-11 01:04:42,909 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:42,924 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2348, memsize=220.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/d2b1d53d906e4c75b164df0b7f90f494
2014-07-11 01:04:42,951 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12613 synced till here 12610
2014-07-11 01:04:42,952 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/d2b1d53d906e4c75b164df0b7f90f494 as hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/d2b1d53d906e4c75b164df0b7f90f494
2014-07-11 01:04:42,976 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/d2b1d53d906e4c75b164df0b7f90f494, entries=802900, sequenceid=2348, filesize=57.2m
2014-07-11 01:04:42,977 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~760.8m/797736240, currentsize=283.5m/297231920 for region usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. in 18314ms, sequenceid=2348, compaction requested=true
2014-07-11 01:04:42,978 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:04:42,978 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 20 blocking
2014-07-11 01:04:42,978 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-11 01:04:42,978 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:04:42,978 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:04:42,978 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. because compaction request was cancelled
2014-07-11 01:04:42,978 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224., current region memstore size 794.1m
2014-07-11 01:04:42,984 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29.
2014-07-11 01:04:42,991 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065880781 with entries=76, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065882910
2014-07-11 01:04:42,991 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065811991
2014-07-11 01:04:42,991 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065814048
2014-07-11 01:04:42,991 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065816281
2014-07-11 01:04:42,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065819795
2014-07-11 01:04:42,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065820686
2014-07-11 01:04:42,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065822727
2014-07-11 01:04:42,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065824214
2014-07-11 01:04:42,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065825932
2014-07-11 01:04:42,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065829031
2014-07-11 01:04:44,420 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:44,440 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12691 synced till here 12687
2014-07-11 01:04:44,486 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065882910 with entries=78, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065884420
2014-07-11 01:04:44,554 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:04:44,750 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2407, memsize=191.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/6cc2ae556a1e45f58071485a332743df
2014-07-11 01:04:44,782 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/6cc2ae556a1e45f58071485a332743df as hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/6cc2ae556a1e45f58071485a332743df
2014-07-11 01:04:44,794 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/6cc2ae556a1e45f58071485a332743df, entries=698450, sequenceid=2407, filesize=49.8m
2014-07-11 01:04:44,795 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~575.6m/603547680, currentsize=223.9m/234812640 for region usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. in 14479ms, sequenceid=2407, compaction requested=true
2014-07-11 01:04:44,795 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:04:44,795 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 20 blocking
2014-07-11 01:04:44,795 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-11 01:04:44,795 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355., current region memstore size 496.1m
2014-07-11 01:04:44,795 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:04:44,796 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:04:44,796 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. because compaction request was cancelled
2014-07-11 01:04:45,327 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:04:46,296 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:46,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12775 synced till here 12766
2014-07-11 01:04:46,370 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065884420 with entries=84, filesize=71.9m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065886297
2014-07-11 01:04:46,869 DEBUG [RpcServer.handler=19,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2.
2014-07-11 01:04:48,315 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1063ms
GC pool 'ParNew' had collection(s): count=1 time=1212ms
2014-07-11 01:04:48,317 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:48,342 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12859 synced till here 12848
2014-07-11 01:04:48,449 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065886297 with entries=84, filesize=72.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065888318
2014-07-11 01:04:50,196 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:50,275 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12963 synced till here 12941
2014-07-11 01:04:50,450 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065888318 with entries=104, filesize=88.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065890196
2014-07-11 01:04:51,920 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:51,939 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13044 synced till here 13036
2014-07-11 01:04:52,025 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065890196 with entries=81, filesize=69.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065891920
2014-07-11 01:04:52,804 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:52,840 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13122 synced till here 13115
2014-07-11 01:04:52,907 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065891920 with entries=78, filesize=66.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065892804
2014-07-11 01:04:54,742 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:54,780 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13212 synced till here 13201
2014-07-11 01:04:54,877 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065892804 with entries=90, filesize=77.0m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065894743
2014-07-11 01:04:56,599 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:04:56,622 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13293 synced till here 13285
2014-07-11 01:04:56,701 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065894743 with entries=81, filesize=69.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065896599
2014-07-11 01:04:58,793 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:05:00,906 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1755ms
GC pool 'ParNew' had collection(s): count=1 time=1793ms
2014-07-11 01:05:00,954 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13407 synced till here 13402
2014-07-11 01:05:01,017 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065896599 with entries=114, filesize=97.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065898793
2014-07-11 01:05:01,781 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:05:03,190 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1283ms
GC pool 'ParNew' had collection(s): count=1 time=1398ms
2014-07-11 01:05:03,212 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13495 synced till here 13485
2014-07-11 01:05:03,298 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065898793 with entries=88, filesize=75.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065901781
2014-07-11 01:05:03,967 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:05:03,987 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13575 synced till here 13567
2014-07-11 01:05:04,083 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065901781 with entries=80, filesize=68.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065903967
2014-07-11 01:05:05,742 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:05:05,770 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13657 synced till here 13646
2014-07-11 01:05:05,879 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065903967 with entries=82, filesize=70.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065905743
2014-07-11 01:05:07,249 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:05:07,279 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13735 synced till here 13730
2014-07-11 01:05:07,343 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065905743 with entries=78, filesize=66.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065907249
2014-07-11 01:05:08,871 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:08,872 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:08,881 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:08,918 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:08,966 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:08,966 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:08,967 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:08,968 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:08,968 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:08,972 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:08,972 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:08,972 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:08,972 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:08,973 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:08,973 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:08,973 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:08,974 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:08,978 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:09,015 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:09,023 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:09,023 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:09,023 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:09,024 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:09,024 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:09,024 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:09,052 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:09,089 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:09,118 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2562, memsize=304.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/569b55acd10b4e98b694345d60912ffa
2014-07-11 01:05:09,129 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/569b55acd10b4e98b694345d60912ffa as hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/569b55acd10b4e98b694345d60912ffa
2014-07-11 01:05:09,131 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:09,143 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/569b55acd10b4e98b694345d60912ffa, entries=1106730, sequenceid=2562, filesize=78.8m
2014-07-11 01:05:09,143 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~500.8m/525095200, currentsize=339.2m/355632080 for region usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. in 24348ms, sequenceid=2562, compaction requested=true
2014-07-11 01:05:09,144 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:05:09,144 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 20 blocking
2014-07-11 01:05:09,144 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13ms
2014-07-11 01:05:09,144 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-11 01:05:09,144 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2., current region memstore size 868.7m
2014-07-11 01:05:09,144 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:05:09,144 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:09,144 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:05:09,144 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 55ms
2014-07-11 01:05:09,145 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:09,145 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. because compaction request was cancelled
2014-07-11 01:05:09,145 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 93ms
2014-07-11 01:05:09,145 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:09,145 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 122ms
2014-07-11 01:05:09,145 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:09,145 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 122ms
2014-07-11 01:05:09,145 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:09,145 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 122ms
2014-07-11 01:05:09,145 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:09,146 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 123ms
2014-07-11 01:05:09,146 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:09,149 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 126ms
2014-07-11 01:05:09,150 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:09,150 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 127ms
2014-07-11 01:05:09,150 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:09,155 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 139ms
2014-07-11 01:05:09,155 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:09,155 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 177ms
2014-07-11 01:05:09,155 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:09,155 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 181ms
2014-07-11 01:05:09,155 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:09,155 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 182ms
2014-07-11 01:05:09,155 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:09,155 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 189ms
2014-07-11 01:05:09,155 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:09,157 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 185ms
2014-07-11 01:05:09,157 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:09,169 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 197ms
2014-07-11 01:05:09,169 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:09,169 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 197ms
2014-07-11 01:05:09,169 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:09,169 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 203ms
2014-07-11 01:05:09,169 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:09,169 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 197ms
2014-07-11 01:05:09,169 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:09,174 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 206ms
2014-07-11 01:05:09,174 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:09,174 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 206ms
2014-07-11 01:05:09,174 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:09,174 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 208ms
2014-07-11 01:05:09,174 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:09,177 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 211ms
2014-07-11 01:05:09,177 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:09,177 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 211ms
2014-07-11 01:05:09,177 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:09,189 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 271ms
2014-07-11 01:05:09,189 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:09,189 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 317ms
2014-07-11 01:05:09,189 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:09,189 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 318ms
2014-07-11 01:05:09,189 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:09,189 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 318ms
2014-07-11 01:05:09,189 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:09,323 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:05:09,381 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13833 synced till here 13808
2014-07-11 01:05:09,517 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355.
2014-07-11 01:05:09,605 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065907249 with entries=98, filesize=83.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065909323
2014-07-11 01:05:10,973 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:05:11,279 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:05:11,318 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13926 synced till here 13909
2014-07-11 01:05:11,466 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065909323 with entries=93, filesize=80.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065911280
2014-07-11 01:05:13,414 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:05:14,717 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2537, memsize=326.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/2ff03441c636489982b1dcbdb30f0924
2014-07-11 01:05:14,808 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/2ff03441c636489982b1dcbdb30f0924 as hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/2ff03441c636489982b1dcbdb30f0924
2014-07-11 01:05:15,044 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065911280 with entries=168, filesize=143.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065913415
2014-07-11 01:05:15,099 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/2ff03441c636489982b1dcbdb30f0924, entries=1188500, sequenceid=2537, filesize=84.6m
2014-07-11 01:05:15,105 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~798.9m/837681520, currentsize=466.7m/489367440 for region usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. in 32127ms, sequenceid=2537, compaction requested=true
2014-07-11 01:05:15,113 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 20 blocking
2014-07-11 01:05:15,113 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-11 01:05:15,113 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:05:15,113 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:05:15,113 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. because compaction request was cancelled
2014-07-11 01:05:15,114 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:05:15,114 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29., current region memstore size 728.3m
2014-07-11 01:05:15,323 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224.
2014-07-11 01:05:17,227 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:05:17,298 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14189 synced till here 14174
2014-07-11 01:05:17,396 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065913415 with entries=95, filesize=81.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065917227
2014-07-11 01:05:17,397 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065830087
2014-07-11 01:05:17,397 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065831925
2014-07-11 01:05:17,397 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065833746
2014-07-11 01:05:17,397 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065835299
2014-07-11 01:05:17,397 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065836865
2014-07-11 01:05:17,398 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065838777
2014-07-11 01:05:17,400 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065841510
2014-07-11 01:05:17,400 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065843495
2014-07-11 01:05:17,400 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065846028
2014-07-11 01:05:17,400 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065847968
2014-07-11 01:05:17,548 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:05:19,115 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:05:19,192 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14278 synced till here 14268
2014-07-11 01:05:19,262 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065917227 with entries=89, filesize=76.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065919116
2014-07-11 01:05:21,060 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:05:21,085 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14374 synced till here 14357
2014-07-11 01:05:21,283 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065919116 with entries=96, filesize=82.2m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065921060
2014-07-11 01:05:23,425 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:05:23,455 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14452 synced till here 14446
2014-07-11 01:05:23,558 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065921060 with entries=78, filesize=66.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065923426
2014-07-11 01:05:25,250 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:05:25,311 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14540 synced till here 14525
2014-07-11 01:05:25,448 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065923426 with entries=88, filesize=75.0m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065925250
2014-07-11 01:05:28,823 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:05:28,864 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14616 synced till here 14614
2014-07-11 01:05:28,929 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:28,929 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:28,937 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065925250 with entries=76, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065928824
2014-07-11 01:05:28,967 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:28,970 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:28,986 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:28,991 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:29,009 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:29,024 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:29,025 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:29,027 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:29,027 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:29,028 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:29,028 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:29,033 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:29,034 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:29,036 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:29,046 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:29,083 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:29,119 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:30,722 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:30,761 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:30,797 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:30,834 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:30,873 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:30,911 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:31,023 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:31,180 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:31,237 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:31,399 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:33,467 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:33,567 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:33,929 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 01:05:33,930 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 01:05:34,787 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5668ms
2014-07-11 01:05:34,787 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5759ms
2014-07-11 01:05:34,788 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5817ms
2014-07-11 01:05:34,788 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5822ms
2014-07-11 01:05:34,788 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5802ms
2014-07-11 01:05:34,788 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5798ms
2014-07-11 01:05:34,788 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5779ms
2014-07-11 01:05:34,788 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5764ms
2014-07-11 01:05:34,789 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5762ms
2014-07-11 01:05:34,789 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5762ms
2014-07-11 01:05:34,789 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5765ms
2014-07-11 01:05:34,790 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5762ms
2014-07-11 01:05:34,790 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5761ms
2014-07-11 01:05:34,791 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5756ms
2014-07-11 01:05:34,791 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5755ms
2014-07-11 01:05:34,791 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5745ms
2014-07-11 01:05:34,792 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5709ms
2014-07-11 01:05:35,247 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:35,289 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:35,325 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:35,362 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:35,400 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:35,437 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:35,473 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:35,512 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:35,549 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:35,588 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:35,624 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:35,662 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:35,700 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:35,723 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 01:05:35,736 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:35,762 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 01:05:35,774 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:35,798 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 01:05:35,811 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:35,835 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 01:05:35,847 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:35,874 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 01:05:35,883 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:05:35,912 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 01:05:36,024 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 01:05:36,181 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 01:05:36,237 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 01:05:36,400 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 01:05:37,685 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2827, memsize=335.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/138e6fda1f494fc6ac6f336f7435cc52
2014-07-11 01:05:37,701 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/138e6fda1f494fc6ac6f336f7435cc52 as hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/138e6fda1f494fc6ac6f336f7435cc52
2014-07-11 01:05:37,712 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/138e6fda1f494fc6ac6f336f7435cc52, entries=1222370, sequenceid=2827, filesize=87.1m
2014-07-11 01:05:37,712 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~744.3m/780458720, currentsize=163.0m/170956880 for region usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. in 22598ms, sequenceid=2827, compaction requested=true
2014-07-11 01:05:37,713 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:05:37,713 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 20 blocking
2014-07-11 01:05:37,713 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-11 01:05:37,713 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:05:37,713 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:05:37,713 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. because compaction request was cancelled
2014-07-11 01:05:37,713 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2., current region memstore size 818.4m
2014-07-11 01:05:37,713 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6314ms
2014-07-11 01:05:37,713 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,714 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6477ms
2014-07-11 01:05:37,714 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,717 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6537ms
2014-07-11 01:05:37,717 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,717 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6694ms
2014-07-11 01:05:37,717 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,717 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6806ms
2014-07-11 01:05:37,717 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,718 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1835ms
2014-07-11 01:05:37,718 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,719 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6845ms
2014-07-11 01:05:37,719 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,720 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1872ms
2014-07-11 01:05:37,720 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,733 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6899ms
2014-07-11 01:05:37,733 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,734 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1922ms
2014-07-11 01:05:37,734 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,735 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6938ms
2014-07-11 01:05:37,735 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,735 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1961ms
2014-07-11 01:05:37,735 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,736 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6974ms
2014-07-11 01:05:37,736 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,737 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2001ms
2014-07-11 01:05:37,737 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,737 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7015ms
2014-07-11 01:05:37,737 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,741 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2041ms
2014-07-11 01:05:37,741 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,742 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2079ms
2014-07-11 01:05:37,742 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,742 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2118ms
2014-07-11 01:05:37,742 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,742 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2154ms
2014-07-11 01:05:37,742 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,754 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2204ms
2014-07-11 01:05:37,754 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,754 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2242ms
2014-07-11 01:05:37,754 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,755 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2281ms
2014-07-11 01:05:37,756 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,760 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2322ms
2014-07-11 01:05:37,761 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,761 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2362ms
2014-07-11 01:05:37,761 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,773 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2411ms
2014-07-11 01:05:37,774 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,774 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2449ms
2014-07-11 01:05:37,774 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,777 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2488ms
2014-07-11 01:05:37,777 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,778 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2530ms
2014-07-11 01:05:37,778 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,779 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8695ms
2014-07-11 01:05:37,779 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,779 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8733ms
2014-07-11 01:05:37,779 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,779 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8743ms
2014-07-11 01:05:37,779 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,779 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8745ms
2014-07-11 01:05:37,779 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,780 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8751ms
2014-07-11 01:05:37,780 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,780 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8752ms
2014-07-11 01:05:37,780 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,780 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8756ms
2014-07-11 01:05:37,780 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,788 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8761ms
2014-07-11 01:05:37,789 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,801 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8774ms
2014-07-11 01:05:37,801 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,809 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8784ms
2014-07-11 01:05:37,809 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,809 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8800ms
2014-07-11 01:05:37,810 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,817 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8827ms
2014-07-11 01:05:37,817 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,822 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8836ms
2014-07-11 01:05:37,822 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,822 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8856ms
2014-07-11 01:05:37,822 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,822 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8852ms
2014-07-11 01:05:37,822 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,823 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8795ms
2014-07-11 01:05:37,823 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,829 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8710ms
2014-07-11 01:05:37,829 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,829 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8900ms
2014-07-11 01:05:37,829 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,829 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8900ms
2014-07-11 01:05:37,829 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,837 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4270ms
2014-07-11 01:05:37,837 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,838 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4370ms
2014-07-11 01:05:37,838 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:05:37,923 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2769, memsize=393.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/e185eee2416c47ca849ecbb084c88689
2014-07-11 01:05:37,946 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/e185eee2416c47ca849ecbb084c88689 as hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/e185eee2416c47ca849ecbb084c88689
2014-07-11 01:05:37,963 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/e185eee2416c47ca849ecbb084c88689, entries=1432230, sequenceid=2769, filesize=102.0m
2014-07-11 01:05:37,977 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~868.7m/910853760, currentsize=259.4m/271987360 for region usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. in 28833ms, sequenceid=2769, compaction requested=true
2014-07-11 01:05:37,978 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 20 blocking
2014-07-11 01:05:37,978 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-11 01:05:37,978 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:05:37,978 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:05:37,978 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. because compaction request was cancelled
2014-07-11 01:05:37,985 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:05:37,985 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355., current region memstore size 593.2m
2014-07-11 01:05:38,259 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11056,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405065927203,"queuetimems":0,"class":"HRegionServer","responsesize":19600,"method":"Multi"}
2014-07-11 01:05:38,261 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2.
2014-07-11 01:05:38,261 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10972,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405065927289,"queuetimems":1,"class":"HRegionServer","responsesize":19420,"method":"Multi"}
2014-07-11 01:05:38,270 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10871,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405065927399,"queuetimems":0,"class":"HRegionServer","responsesize":19676,"method":"Multi"}
2014-07-11 01:05:38,277 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10915,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405065927362,"queuetimems":0,"class":"HRegionServer","responsesize":19509,"method":"Multi"}
2014-07-11 01:05:38,277 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10951,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405065927326,"queuetimems":0,"class":"HRegionServer","responsesize":19906,"method":"Multi"}
2014-07-11 01:05:38,394 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:05:39,314 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10633,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405065928680,"queuetimems":557,"class":"HRegionServer","responsesize":19693,"method":"Multi"}
2014-07-11 01:05:39,316 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14701 synced till here 14695
2014-07-11 01:05:39,385 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065928824 with entries=85, filesize=70.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065938394
2014-07-11 01:05:39,385 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065850116
2014-07-11 01:05:39,385 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065852129
2014-07-11 01:05:39,385 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065853668
2014-07-11 01:05:39,385 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065855084
2014-07-11 01:05:39,385 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065856826
2014-07-11 01:05:39,385 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065859105
2014-07-11 01:05:39,386 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065860715
2014-07-11 01:05:39,386 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065862611
2014-07-11 01:05:39,386 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065864585
2014-07-11 01:05:39,386 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065866534
2014-07-11 01:05:39,386 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065868424
2014-07-11 01:05:39,611 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:05:39,613 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10890,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405065928723,"queuetimems":1,"class":"HRegionServer","responsesize":19688,"method":"Multi"}
2014-07-11 01:05:39,614 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12177,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405065927437,"queuetimems":0,"class":"HRegionServer","responsesize":19927,"method":"Multi"}
2014-07-11 01:05:39,615 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10916,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405065928698,"queuetimems":1,"class":"HRegionServer","responsesize":19746,"method":"Multi"}
2014-07-11 01:05:39,615 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11503,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405065928111,"queuetimems":1,"class":"HRegionServer","responsesize":19461,"method":"Multi"}
2014-07-11 01:05:39,797 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11030,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405065928767,"queuetimems":1,"class":"HRegionServer","responsesize":19944,"method":"Multi"}
2014-07-11 01:05:39,799 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10876,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405065928922,"queuetimems":1,"class":"HRegionServer","responsesize":19605,"method":"Multi"}
2014-07-11 01:05:39,853 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10991,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405065928861,"queuetimems":0,"class":"HRegionServer","responsesize":19373,"method":"Multi"}
2014-07-11 01:05:39,919 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:05:40,049 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:05:40,061 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11242,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405065928819,"queuetimems":1,"class":"HRegionServer","responsesize":19844,"method":"Multi"}
2014-07-11 01:05:40,932 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14832 synced till here 14830
2014-07-11 01:05:41,874 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065938394 with entries=131, filesize=110.9m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065940050
2014-07-11 01:05:42,024 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12905,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405065929117,"queuetimems":0,"class":"HRegionServer","responsesize":19650,"method":"Multi"}
2014-07-11 01:05:42,024 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11114,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405065930909,"queuetimems":0,"class":"HRegionServer","responsesize":19678,"method":"Multi"}
2014-07-11 01:05:42,024 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12942,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405065929081,"queuetimems":1,"class":"HRegionServer","responsesize":19837,"method":"Multi"}
2014-07-11 01:05:42,024 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13016,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405065929007,"queuetimems":0,"class":"HRegionServer","responsesize":19611,"method":"Multi"}
2014-07-11 01:05:42,024 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12980,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405065929044,"queuetimems":0,"class":"HRegionServer","responsesize":19433,"method":"Multi"}
2014-07-11 01:05:42,025 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13057,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405065928968,"queuetimems":0,"class":"HRegionServer","responsesize":19616,"method":"Multi"}
2014-07-11 01:05:42,025 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11305,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405065930720,"queuetimems":0,"class":"HRegionServer","responsesize":19742,"method":"Multi"}
2014-07-11 01:05:43,758 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29.
2014-07-11 01:05:43,820 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:05:43,843 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14904 synced till here 14903
2014-07-11 01:05:43,862 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065940050 with entries=72, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065943820
2014-07-11 01:05:45,798 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:05:45,837 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065943820 with entries=72, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065945799
2014-07-11 01:05:47,238 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:05:47,272 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065945799 with entries=74, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065947241
2014-07-11 01:05:48,636 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2977, memsize=142.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/57cff25fcf56423ebbd71785b5dfe490
2014-07-11 01:05:48,651 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/57cff25fcf56423ebbd71785b5dfe490 as hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/57cff25fcf56423ebbd71785b5dfe490
2014-07-11 01:05:48,664 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/57cff25fcf56423ebbd71785b5dfe490, entries=519700, sequenceid=2977, filesize=37.0m
2014-07-11 01:05:48,664 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~641.3m/672403680, currentsize=96.5m/101159680 for region usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. in 10679ms, sequenceid=2977, compaction requested=true
2014-07-11 01:05:48,665 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:05:48,665 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224., current region memstore size 772.8m
2014-07-11 01:05:48,665 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 20 blocking
2014-07-11 01:05:48,666 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-11 01:05:48,666 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:05:48,666 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:05:48,667 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. because compaction request was cancelled
2014-07-11 01:05:48,728 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:05:48,745 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15127 synced till here 15123
2014-07-11 01:05:48,821 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065947241 with entries=77, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065948729
2014-07-11 01:05:50,548 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:05:50,582 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15211 synced till here 15202
2014-07-11 01:05:50,689 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:05:50,700 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065948729 with entries=84, filesize=72.0m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065950548
2014-07-11 01:05:51,957 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:05:51,977 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15287 synced till here 15283
2014-07-11 01:05:52,015 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065950548 with entries=76, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065951957
2014-07-11 01:05:52,924 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:05:54,948 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15454 synced till here 15445
2014-07-11 01:05:55,039 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065951957 with entries=167, filesize=143.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065952924
2014-07-11 01:05:56,890 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:05:56,908 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15526 synced till here 15524
2014-07-11 01:05:56,979 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065952924 with entries=72, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065956890
2014-07-11 01:05:57,311 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2936, memsize=302.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/be580ca4a6df4fc2aa1d8a322895d0d6
2014-07-11 01:05:57,342 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/be580ca4a6df4fc2aa1d8a322895d0d6 as hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/be580ca4a6df4fc2aa1d8a322895d0d6
2014-07-11 01:05:57,403 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/be580ca4a6df4fc2aa1d8a322895d0d6, entries=1101920, sequenceid=2936, filesize=78.5m
2014-07-11 01:05:57,403 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~818.4m/858145840, currentsize=283.7m/297483440 for region usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. in 19690ms, sequenceid=2936, compaction requested=true
2014-07-11 01:05:57,404 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:05:57,404 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 20 blocking
2014-07-11 01:05:57,405 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-11 01:05:57,405 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2., current region memstore size 541.0m
2014-07-11 01:05:57,405 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:05:57,405 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:05:57,405 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. because compaction request was cancelled
2014-07-11 01:05:57,447 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2.
2014-07-11 01:05:58,316 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:05:59,020 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:05:59,068 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15603 synced till here 15599
2014-07-11 01:05:59,114 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065956890 with entries=77, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065959020
2014-07-11 01:05:59,114 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065870228
2014-07-11 01:05:59,115 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065872570
2014-07-11 01:05:59,116 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065874645
2014-07-11 01:05:59,116 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065877001
2014-07-11 01:05:59,116 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065878446
2014-07-11 01:05:59,116 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065879522
2014-07-11 01:05:59,116 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065880781
2014-07-11 01:06:00,303 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355.
2014-07-11 01:06:00,733 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:00,748 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15675 synced till here 15674
2014-07-11 01:06:00,767 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065959020 with entries=72, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065960733
2014-07-11 01:06:02,257 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:02,288 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15754 synced till here 15746
2014-07-11 01:06:02,367 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065960733 with entries=79, filesize=67.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065962258
2014-07-11 01:06:04,377 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:04,411 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15842 synced till here 15830
2014-07-11 01:06:04,580 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065962258 with entries=88, filesize=72.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065964377
2014-07-11 01:06:06,949 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:07,014 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15939 synced till here 15923
2014-07-11 01:06:07,037 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3039, memsize=237.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/98b45286ac2f43729a3c9d4ce791ffe1
2014-07-11 01:06:07,051 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/98b45286ac2f43729a3c9d4ce791ffe1 as hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/98b45286ac2f43729a3c9d4ce791ffe1
2014-07-11 01:06:07,065 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/98b45286ac2f43729a3c9d4ce791ffe1, entries=864200, sequenceid=3039, filesize=61.6m
2014-07-11 01:06:07,066 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~777.4m/815207760, currentsize=251.6m/263781520 for region usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. in 18401ms, sequenceid=3039, compaction requested=true
2014-07-11 01:06:07,066 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:06:07,066 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 20 blocking
2014-07-11 01:06:07,066 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-11 01:06:07,066 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29., current region memstore size 552.7m
2014-07-11 01:06:07,067 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:06:07,067 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:06:07,067 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. because compaction request was cancelled
2014-07-11 01:06:07,095 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065964377 with entries=97, filesize=80.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065966950
2014-07-11 01:06:07,095 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065882910
2014-07-11 01:06:07,095 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065884420
2014-07-11 01:06:07,095 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065886297
2014-07-11 01:06:07,095 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065888318
2014-07-11 01:06:07,095 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065890196
2014-07-11 01:06:07,095 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065891920
2014-07-11 01:06:07,095 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065892804
2014-07-11 01:06:07,096 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065894743
2014-07-11 01:06:07,096 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065896599
2014-07-11 01:06:07,096 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065898793
2014-07-11 01:06:07,096 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065901781
2014-07-11 01:06:07,096 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065903967
2014-07-11 01:06:07,096 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065905743
2014-07-11 01:06:07,306 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224.
2014-07-11 01:06:08,715 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:06:08,907 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:08,935 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16030 synced till here 16018
2014-07-11 01:06:09,008 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065966950 with entries=91, filesize=76.0m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065968907
2014-07-11 01:06:10,657 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:10,677 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16111 synced till here 16103
2014-07-11 01:06:10,803 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065968907 with entries=81, filesize=68.0m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065970657
2014-07-11 01:06:11,441 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:11,460 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16188 synced till here 16185
2014-07-11 01:06:12,054 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065970657 with entries=77, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065971441
2014-07-11 01:06:12,164 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3121, memsize=212.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/8d1c1f0a5e9444a3bd3cc66745d43384
2014-07-11 01:06:12,180 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/8d1c1f0a5e9444a3bd3cc66745d43384 as hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/8d1c1f0a5e9444a3bd3cc66745d43384
2014-07-11 01:06:12,227 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/8d1c1f0a5e9444a3bd3cc66745d43384, entries=773360, sequenceid=3121, filesize=55.1m
2014-07-11 01:06:12,228 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~542.5m/568901920, currentsize=202.0m/211840240 for region usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. in 14823ms, sequenceid=3121, compaction requested=true
2014-07-11 01:06:12,228 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:06:12,228 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 20 blocking
2014-07-11 01:06:12,229 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-11 01:06:12,229 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2., current region memstore size 485.9m
2014-07-11 01:06:12,229 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:06:12,229 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:06:12,229 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. because compaction request was cancelled
2014-07-11 01:06:12,728 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:06:12,869 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:12,889 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16262 synced till here 16261
2014-07-11 01:06:12,910 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065971441 with entries=74, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065972869
2014-07-11 01:06:12,910 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065907249
2014-07-11 01:06:12,910 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065909323
2014-07-11 01:06:12,911 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065911280
2014-07-11 01:06:14,015 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:14,032 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16338 synced till here 16337
2014-07-11 01:06:14,043 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065972869 with entries=76, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065974015
2014-07-11 01:06:15,499 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2.
2014-07-11 01:06:15,856 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:15,979 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16424 synced till here 16417
2014-07-11 01:06:16,033 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065974015 with entries=86, filesize=73.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065975856
2014-07-11 01:06:17,604 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:18,822 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16544 synced till here 16533
2014-07-11 01:06:18,928 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065975856 with entries=120, filesize=102.9m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065977605
2014-07-11 01:06:20,894 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:20,949 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16643 synced till here 16627
2014-07-11 01:06:21,092 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065977605 with entries=99, filesize=85.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065980895
2014-07-11 01:06:22,775 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:22,796 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16731 synced till here 16718
2014-07-11 01:06:22,965 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065980895 with entries=88, filesize=75.3m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065982776
2014-07-11 01:06:23,678 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:24,520 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16813 synced till here 16802
2014-07-11 01:06:24,634 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065982776 with entries=82, filesize=70.5m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065983679
2014-07-11 01:06:25,544 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:25,561 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16895 synced till here 16886
2014-07-11 01:06:25,645 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065983679 with entries=82, filesize=70.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065985545
2014-07-11 01:06:27,030 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3193, memsize=294.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/5709182fef5e4bd39bcc4a59f485b52d
2014-07-11 01:06:27,055 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/5709182fef5e4bd39bcc4a59f485b52d as hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/5709182fef5e4bd39bcc4a59f485b52d
2014-07-11 01:06:27,074 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/5709182fef5e4bd39bcc4a59f485b52d, entries=1072520, sequenceid=3193, filesize=76.4m
2014-07-11 01:06:27,075 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~566.0m/593443920, currentsize=307.0m/321950640 for region usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. in 20009ms, sequenceid=3193, compaction requested=true
2014-07-11 01:06:27,076 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:06:27,076 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 20 blocking
2014-07-11 01:06:27,076 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-11 01:06:27,076 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:06:27,076 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355., current region memstore size 658.5m
2014-07-11 01:06:27,076 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:06:27,076 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. because compaction request was cancelled
2014-07-11 01:06:27,100 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29.
2014-07-11 01:06:27,470 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:27,516 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16976 synced till here 16968
2014-07-11 01:06:27,604 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065985545 with entries=81, filesize=69.3m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065987470
2014-07-11 01:06:27,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065913415
2014-07-11 01:06:27,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065917227
2014-07-11 01:06:27,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065919116
2014-07-11 01:06:27,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065921060
2014-07-11 01:06:27,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065923426
2014-07-11 01:06:27,605 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065925250
2014-07-11 01:06:28,679 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:06:29,332 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:29,381 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17061 synced till here 17050
2014-07-11 01:06:29,488 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065987470 with entries=85, filesize=73.0m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065989332
2014-07-11 01:06:30,709 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1041ms
GC pool 'ParNew' had collection(s): count=1 time=1107ms
2014-07-11 01:06:31,235 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3255, memsize=291.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/4c8f34ceaa5a4046815f73d61a49eed8
2014-07-11 01:06:31,255 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/4c8f34ceaa5a4046815f73d61a49eed8 as hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/4c8f34ceaa5a4046815f73d61a49eed8
2014-07-11 01:06:31,399 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:31,482 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/4c8f34ceaa5a4046815f73d61a49eed8, entries=1059350, sequenceid=3255, filesize=75.5m
2014-07-11 01:06:31,482 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~489.0m/512783120, currentsize=280.2m/293801680 for region usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. in 19253ms, sequenceid=3255, compaction requested=true
2014-07-11 01:06:31,483 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:06:31,483 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 20 blocking
2014-07-11 01:06:31,483 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-11 01:06:31,483 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224., current region memstore size 625.6m
2014-07-11 01:06:31,483 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:06:31,483 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:06:31,483 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. because compaction request was cancelled
2014-07-11 01:06:31,802 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2.
2014-07-11 01:06:33,059 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17174 synced till here 17169
2014-07-11 01:06:33,129 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065989332 with entries=113, filesize=96.9m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065991399
2014-07-11 01:06:33,649 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:06:33,881 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:33,916 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17268 synced till here 17248
2014-07-11 01:06:35,043 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065991399 with entries=94, filesize=80.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065993881
2014-07-11 01:06:35,940 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:35,975 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17361 synced till here 17343
2014-07-11 01:06:36,864 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065993881 with entries=93, filesize=80.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065995941
2014-07-11 01:06:37,609 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:37,647 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17452 synced till here 17440
2014-07-11 01:06:37,704 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065995941 with entries=91, filesize=78.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065997609
2014-07-11 01:06:39,170 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:39,189 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17534 synced till here 17526
2014-07-11 01:06:39,265 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065997609 with entries=82, filesize=70.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065999170
2014-07-11 01:06:40,930 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:40,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17623 synced till here 17613
2014-07-11 01:06:41,017 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065999170 with entries=89, filesize=76.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066000930
2014-07-11 01:06:42,756 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:42,788 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17697 synced till here 17696
2014-07-11 01:06:42,819 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066000930 with entries=74, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066002756
2014-07-11 01:06:44,116 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:45,396 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17829 synced till here 17826
2014-07-11 01:06:45,446 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066002756 with entries=132, filesize=113.0m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066004117
2014-07-11 01:06:46,159 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:46,182 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17908 synced till here 17902
2014-07-11 01:06:46,999 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066004117 with entries=79, filesize=68.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066006159
2014-07-11 01:06:47,940 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:49,429 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1016ms
GC pool 'ParNew' had collection(s): count=1 time=1439ms
2014-07-11 01:06:49,441 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17996 synced till here 17981
2014-07-11 01:06:49,606 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066006159 with entries=88, filesize=75.3m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066007941
2014-07-11 01:06:51,515 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1085ms
GC pool 'ParNew' had collection(s): count=1 time=1185ms
2014-07-11 01:06:51,515 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=9304, hits=1684, hitRatio=18.09%, , cachingAccesses=1688, cachingHits=1684, cachingHitsRatio=99.76%, evictions=0, evicted=2, evictedPerRun=Infinity
2014-07-11 01:06:51,577 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:51,589 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18096 synced till here 18075
2014-07-11 01:06:51,787 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066007941 with entries=100, filesize=85.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066011577
2014-07-11 01:06:52,263 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:52,300 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:52,336 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:52,374 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:52,396 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:52,396 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:52,397 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:52,397 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:52,398 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:52,398 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:52,399 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:52,399 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:52,400 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:52,400 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:52,401 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:52,404 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:52,405 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:52,405 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:52,405 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:52,405 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:52,405 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:52,406 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:52,406 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:52,414 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:53,444 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:53,456 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:53,466 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:53,470 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:53,471 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:53,473 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:53,474 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:53,474 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:53,475 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:53,476 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:53,483 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:53,485 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18171 synced till here 18170
2014-07-11 01:06:53,492 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066011577 with entries=75, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066013466
2014-07-11 01:06:53,522 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:53,562 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:53,598 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:53,635 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:53,670 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:53,705 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:53,743 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:53,780 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:53,817 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:53,854 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:53,891 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:53,929 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:53,968 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:54,004 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:54,043 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:54,083 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:06:55,832 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3410, memsize=381.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/820d551387d344848bb0995db43b7a1e
2014-07-11 01:06:55,851 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/820d551387d344848bb0995db43b7a1e as hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/820d551387d344848bb0995db43b7a1e
2014-07-11 01:06:55,862 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/820d551387d344848bb0995db43b7a1e, entries=1389360, sequenceid=3410, filesize=99.0m
2014-07-11 01:06:55,863 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~661.6m/693723040, currentsize=392.4m/411430960 for region usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. in 28786ms, sequenceid=3410, compaction requested=true
2014-07-11 01:06:55,863 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:06:55,864 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 20 blocking
2014-07-11 01:06:55,864 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-11 01:06:55,864 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1781ms
2014-07-11 01:06:55,864 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:06:55,864 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,864 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:06:55,864 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2., current region memstore size 815.3m
2014-07-11 01:06:55,864 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. because compaction request was cancelled
2014-07-11 01:06:55,869 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1826ms
2014-07-11 01:06:55,869 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,869 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1865ms
2014-07-11 01:06:55,869 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,869 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1901ms
2014-07-11 01:06:55,870 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,870 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1941ms
2014-07-11 01:06:55,870 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,885 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1994ms
2014-07-11 01:06:55,885 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,885 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2031ms
2014-07-11 01:06:55,885 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,885 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2068ms
2014-07-11 01:06:55,886 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,886 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2106ms
2014-07-11 01:06:55,886 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,886 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2143ms
2014-07-11 01:06:55,886 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,886 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2181ms
2014-07-11 01:06:55,886 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,892 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2223ms
2014-07-11 01:06:55,892 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,899 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2264ms
2014-07-11 01:06:55,899 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,905 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2307ms
2014-07-11 01:06:55,905 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,905 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2343ms
2014-07-11 01:06:55,905 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,905 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2383ms
2014-07-11 01:06:55,906 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,909 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2426ms
2014-07-11 01:06:55,909 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,910 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2433ms
2014-07-11 01:06:55,910 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,911 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2435ms
2014-07-11 01:06:55,911 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,912 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2438ms
2014-07-11 01:06:55,912 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,913 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2439ms
2014-07-11 01:06:55,913 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,918 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2445ms
2014-07-11 01:06:55,918 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,918 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2447ms
2014-07-11 01:06:55,918 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,918 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2448ms
2014-07-11 01:06:55,918 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,919 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2462ms
2014-07-11 01:06:55,919 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,920 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2476ms
2014-07-11 01:06:55,920 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,921 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3507ms
2014-07-11 01:06:55,921 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,921 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3515ms
2014-07-11 01:06:55,921 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,921 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3515ms
2014-07-11 01:06:55,921 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,934 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3529ms
2014-07-11 01:06:55,934 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,934 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3529ms
2014-07-11 01:06:55,934 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,935 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3529ms
2014-07-11 01:06:55,935 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,935 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3531ms
2014-07-11 01:06:55,935 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,935 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3531ms
2014-07-11 01:06:55,935 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,941 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3537ms
2014-07-11 01:06:55,941 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,955 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3540ms
2014-07-11 01:06:55,956 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,969 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3569ms
2014-07-11 01:06:55,969 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,969 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3570ms
2014-07-11 01:06:55,969 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,970 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3570ms
2014-07-11 01:06:55,970 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,981 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3582ms
2014-07-11 01:06:55,981 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,981 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3583ms
2014-07-11 01:06:55,982 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,983 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3585ms
2014-07-11 01:06:55,984 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,997 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3600ms
2014-07-11 01:06:55,997 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:55,997 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3600ms
2014-07-11 01:06:55,997 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:56,049 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3653ms
2014-07-11 01:06:56,049 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:56,050 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3653ms
2014-07-11 01:06:56,050 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:56,051 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3677ms
2014-07-11 01:06:56,051 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:56,051 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3715ms
2014-07-11 01:06:56,051 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:56,061 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3761ms
2014-07-11 01:06:56,061 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:56,062 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3798ms
2014-07-11 01:06:56,062 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:06:57,794 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1335ms
GC pool 'ParNew' had collection(s): count=1 time=1596ms
2014-07-11 01:06:58,203 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3452, memsize=314.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/5fff10690d18445d8e0542045f728309
2014-07-11 01:06:58,214 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/5fff10690d18445d8e0542045f728309 as hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/5fff10690d18445d8e0542045f728309
2014-07-11 01:06:58,228 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/5fff10690d18445d8e0542045f728309, entries=1145820, sequenceid=3452, filesize=81.6m
2014-07-11 01:06:58,228 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~634.6m/665463600, currentsize=321.8m/337400400 for region usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. in 26745ms, sequenceid=3452, compaction requested=true
2014-07-11 01:06:58,228 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:06:58,228 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 20 blocking
2014-07-11 01:06:58,229 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-11 01:06:58,229 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:06:58,229 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29., current region memstore size 686.1m
2014-07-11 01:06:58,229 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:06:58,229 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. because compaction request was cancelled
2014-07-11 01:06:58,261 DEBUG [RpcServer.handler=5,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224.
2014-07-11 01:06:58,265 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355.
2014-07-11 01:06:58,574 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:06:58,639 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18287 synced till here 18252
2014-07-11 01:06:58,683 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:07:00,229 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066013466 with entries=116, filesize=99.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066018575
2014-07-11 01:07:00,229 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065928824
2014-07-11 01:07:00,229 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065938394
2014-07-11 01:07:00,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065940050
2014-07-11 01:07:00,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065943820
2014-07-11 01:07:00,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065945799
2014-07-11 01:07:00,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065947241
2014-07-11 01:07:00,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065948729
2014-07-11 01:07:00,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065950548
2014-07-11 01:07:00,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065951957
2014-07-11 01:07:00,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065952924
2014-07-11 01:07:00,474 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:07:01,110 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:07:02,132 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18397 synced till here 18386
2014-07-11 01:07:02,236 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066018575 with entries=110, filesize=94.5m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066021110
2014-07-11 01:07:02,593 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10222,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405066012371,"queuetimems":0,"class":"HRegionServer","responsesize":20218,"method":"Multi"}
2014-07-11 01:07:02,617 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10319,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405066012298,"queuetimems":0,"class":"HRegionServer","responsesize":20155,"method":"Multi"}
2014-07-11 01:07:02,621 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10287,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405066012334,"queuetimems":0,"class":"HRegionServer","responsesize":19757,"method":"Multi"}
2014-07-11 01:07:02,621 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10209,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405066012412,"queuetimems":1,"class":"HRegionServer","responsesize":20175,"method":"Multi"}
2014-07-11 01:07:02,625 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10365,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405066012260,"queuetimems":0,"class":"HRegionServer","responsesize":19591,"method":"Multi"}
2014-07-11 01:07:02,628 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10178,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405066012450,"queuetimems":0,"class":"HRegionServer","responsesize":19889,"method":"Multi"}
2014-07-11 01:07:02,923 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:07:04,404 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1299ms
GC pool 'ParNew' had collection(s): count=2 time=1413ms
2014-07-11 01:07:04,409 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18510 synced till here 18470
2014-07-11 01:07:04,739 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066021110 with entries=113, filesize=96.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066022924
2014-07-11 01:07:06,798 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:07:06,864 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18619 synced till here 18588
2014-07-11 01:07:07,140 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066022924 with entries=109, filesize=92.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066026798
2014-07-11 01:07:09,508 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:07:09,600 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18731 synced till here 18714
2014-07-11 01:07:09,780 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066026798 with entries=112, filesize=95.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066029509
2014-07-11 01:07:10,480 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:07:11,317 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18818 synced till here 18809
2014-07-11 01:07:11,425 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066029509 with entries=87, filesize=74.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066030480
2014-07-11 01:07:12,068 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:07:12,110 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18906 synced till here 18890
2014-07-11 01:07:12,273 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066030480 with entries=88, filesize=75.5m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066032068
2014-07-11 01:07:14,158 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:14,171 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:14,181 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:07:14,184 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:14,197 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:14,197 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:14,208 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066032068 with entries=73, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066034182
2014-07-11 01:07:14,251 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:14,251 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:14,257 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:14,274 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:14,308 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:16,403 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:17,189 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:17,231 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:17,281 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:17,331 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:17,371 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:17,410 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:17,447 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:17,676 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:17,714 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:18,573 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:18,595 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:19,158 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 01:07:19,171 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 01:07:19,185 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 01:07:19,198 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 01:07:19,198 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 01:07:19,251 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 01:07:19,251 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 01:07:19,257 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 01:07:19,274 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 01:07:19,309 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 01:07:19,607 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:20,510 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:21,064 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3642, memsize=261.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/50fc3fe988a748e59425d6594eca7358
2014-07-11 01:07:21,077 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/50fc3fe988a748e59425d6594eca7358 as hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/50fc3fe988a748e59425d6594eca7358
2014-07-11 01:07:21,087 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/50fc3fe988a748e59425d6594eca7358, entries=953000, sequenceid=3642, filesize=67.9m
2014-07-11 01:07:21,087 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~692.3m/725968800, currentsize=251.6m/263787840 for region usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. in 22858ms, sequenceid=3642, compaction requested=true
2014-07-11 01:07:21,087 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:07:21,088 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 20 blocking
2014-07-11 01:07:21,088 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 578ms
2014-07-11 01:07:21,088 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2., current region memstore size 866.8m
2014-07-11 01:07:21,088 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-11 01:07:21,088 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:21,088 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:07:21,088 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:07:21,088 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1481ms
2014-07-11 01:07:21,088 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:21,088 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. because compaction request was cancelled
2014-07-11 01:07:21,089 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6781ms
2014-07-11 01:07:21,089 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:21,089 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6816ms
2014-07-11 01:07:21,089 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:21,090 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6833ms
2014-07-11 01:07:21,090 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:21,090 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6839ms
2014-07-11 01:07:21,090 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:21,090 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6840ms
2014-07-11 01:07:21,090 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:21,093 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6896ms
2014-07-11 01:07:21,093 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:21,094 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6897ms
2014-07-11 01:07:21,094 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:21,095 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6910ms
2014-07-11 01:07:21,095 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:21,108 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6930ms
2014-07-11 01:07:21,108 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:21,108 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6950ms
2014-07-11 01:07:21,109 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:21,112 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2516ms
2014-07-11 01:07:21,112 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:21,112 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2539ms
2014-07-11 01:07:21,113 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:21,113 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3399ms
2014-07-11 01:07:21,114 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:21,114 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3438ms
2014-07-11 01:07:21,114 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:21,114 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3667ms
2014-07-11 01:07:21,114 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:21,129 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3720ms
2014-07-11 01:07:21,129 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:21,129 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3758ms
2014-07-11 01:07:21,129 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:21,129 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3798ms
2014-07-11 01:07:21,129 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:21,133 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3853ms
2014-07-11 01:07:21,133 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:21,137 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3906ms
2014-07-11 01:07:21,137 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:21,141 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3952ms
2014-07-11 01:07:21,141 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:21,141 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4738ms
2014-07-11 01:07:21,141 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:21,284 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29.
2014-07-11 01:07:21,808 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:07:21,860 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19070 synced till here 19054
2014-07-11 01:07:22,781 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066034182 with entries=91, filesize=77.3m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066041809
2014-07-11 01:07:22,822 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:07:23,427 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:07:23,460 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19145 synced till here 19142
2014-07-11 01:07:24,045 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066041809 with entries=75, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066043427
2014-07-11 01:07:25,246 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3647, memsize=323.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/ce209d59cfb24b48bd94c0d64f8c83bc
2014-07-11 01:07:25,263 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/ce209d59cfb24b48bd94c0d64f8c83bc as hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/ce209d59cfb24b48bd94c0d64f8c83bc
2014-07-11 01:07:25,278 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/ce209d59cfb24b48bd94c0d64f8c83bc, entries=1178580, sequenceid=3647, filesize=83.9m
2014-07-11 01:07:25,278 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~815.3m/854867280, currentsize=310.3m/325391360 for region usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. in 29414ms, sequenceid=3647, compaction requested=true
2014-07-11 01:07:25,279 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:07:25,279 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 20 blocking
2014-07-11 01:07:25,279 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-11 01:07:25,279 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224., current region memstore size 615.5m
2014-07-11 01:07:25,279 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:07:25,279 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:07:25,279 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. because compaction request was cancelled
2014-07-11 01:07:25,289 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2.
2014-07-11 01:07:25,675 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:07:25,988 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:07:26,332 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19270 synced till here 19269
2014-07-11 01:07:27,469 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066043427 with entries=125, filesize=107.2m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066045676
2014-07-11 01:07:27,472 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065956890
2014-07-11 01:07:27,472 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065959020
2014-07-11 01:07:27,472 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065960733
2014-07-11 01:07:27,472 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065962258
2014-07-11 01:07:27,472 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065964377
2014-07-11 01:07:27,472 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065966950
2014-07-11 01:07:27,472 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065968907
2014-07-11 01:07:27,472 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065970657
2014-07-11 01:07:28,391 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:07:29,992 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1207ms
GC pool 'ParNew' had collection(s): count=1 time=1585ms
2014-07-11 01:07:30,011 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19351 synced till here 19346
2014-07-11 01:07:30,047 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066045676 with entries=81, filesize=69.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066048392
2014-07-11 01:07:30,776 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:07:30,825 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19431 synced till here 19423
2014-07-11 01:07:30,888 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066048392 with entries=80, filesize=68.2m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066050777
2014-07-11 01:07:32,694 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:07:32,715 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19519 synced till here 19506
2014-07-11 01:07:32,818 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066050777 with entries=88, filesize=75.5m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066052694
2014-07-11 01:07:34,571 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:07:34,599 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19593 synced till here 19592
2014-07-11 01:07:34,621 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066052694 with entries=74, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066054571
2014-07-11 01:07:36,082 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:07:36,123 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19668 synced till here 19667
2014-07-11 01:07:36,155 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066054571 with entries=75, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066056083
2014-07-11 01:07:38,018 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:07:38,049 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19745 synced till here 19742
2014-07-11 01:07:38,092 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066056083 with entries=77, filesize=65.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066058019
2014-07-11 01:07:38,885 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:07:40,529 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19880 synced till here 19867
2014-07-11 01:07:40,685 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066058019 with entries=135, filesize=115.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066058885
2014-07-11 01:07:42,033 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:42,044 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:42,055 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:42,056 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:42,086 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:42,088 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:07:42,090 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:42,091 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:42,094 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:42,094 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:42,106 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19957 synced till here 19954
2014-07-11 01:07:42,120 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:42,140 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066058885 with entries=77, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066062088
2014-07-11 01:07:42,180 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:42,218 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:42,428 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:42,465 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:42,503 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:42,538 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:07:42,546 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3855, memsize=170.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/7f82ced9ad114035b67092a093806c6a
2014-07-11 01:07:42,560 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/7f82ced9ad114035b67092a093806c6a as hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/7f82ced9ad114035b67092a093806c6a
2014-07-11 01:07:42,573 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/7f82ced9ad114035b67092a093806c6a, entries=621400, sequenceid=3855, filesize=44.3m
2014-07-11 01:07:42,573 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~623.2m/653520880, currentsize=236.2m/247655440 for region usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. in 17294ms, sequenceid=3855, compaction requested=true
2014-07-11 01:07:42,574 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:07:42,574 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 20 blocking
2014-07-11 01:07:42,574 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 36ms
2014-07-11 01:07:42,574 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:42,574 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355., current region memstore size 931.0m
2014-07-11 01:07:42,574 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-11 01:07:42,574 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 71ms
2014-07-11 01:07:42,575 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:42,575 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:07:42,575 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:07:42,575 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. because compaction request was cancelled
2014-07-11 01:07:42,581 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 116ms
2014-07-11 01:07:42,581 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:42,581 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 153ms
2014-07-11 01:07:42,581 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:42,581 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 364ms
2014-07-11 01:07:42,581 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:42,581 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 401ms
2014-07-11 01:07:42,581 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:42,582 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 462ms
2014-07-11 01:07:42,582 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:42,582 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 488ms
2014-07-11 01:07:42,582 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:42,582 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 488ms
2014-07-11 01:07:42,582 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:42,582 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 491ms
2014-07-11 01:07:42,582 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:42,582 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 492ms
2014-07-11 01:07:42,582 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:42,586 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 501ms
2014-07-11 01:07:42,586 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:42,586 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 530ms
2014-07-11 01:07:42,586 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:42,587 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 532ms
2014-07-11 01:07:42,587 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:42,587 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 543ms
2014-07-11 01:07:42,587 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:42,587 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 554ms
2014-07-11 01:07:42,587 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:07:42,889 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224.
2014-07-11 01:07:44,548 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:07:44,598 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20054 synced till here 20040
2014-07-11 01:07:44,694 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066062088 with entries=97, filesize=83.2m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066064549
2014-07-11 01:07:44,815 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:07:46,320 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:07:46,370 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20137 synced till here 20129
2014-07-11 01:07:46,427 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066064549 with entries=83, filesize=71.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066066320
2014-07-11 01:07:48,341 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:07:48,358 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20213 synced till here 20208
2014-07-11 01:07:48,405 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066066320 with entries=76, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066068341
2014-07-11 01:07:49,240 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:07:49,263 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20288 synced till here 20284
2014-07-11 01:07:50,040 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066068341 with entries=75, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066069241
2014-07-11 01:07:50,366 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3812, memsize=237.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/b113be5f84a24b8abdbbc0916d5f3036
2014-07-11 01:07:50,385 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/b113be5f84a24b8abdbbc0916d5f3036 as hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/b113be5f84a24b8abdbbc0916d5f3036
2014-07-11 01:07:50,398 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/b113be5f84a24b8abdbbc0916d5f3036, entries=864890, sequenceid=3812, filesize=61.6m
2014-07-11 01:07:50,399 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~866.8m/908876160, currentsize=408.6m/428470080 for region usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. in 29311ms, sequenceid=3812, compaction requested=true
2014-07-11 01:07:50,399 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:07:50,399 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 20 blocking
2014-07-11 01:07:50,399 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-11 01:07:50,399 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29., current region memstore size 666.2m
2014-07-11 01:07:50,399 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:07:50,400 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:07:50,400 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. because compaction request was cancelled
2014-07-11 01:07:50,403 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2.
2014-07-11 01:07:50,789 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:07:50,954 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:07:52,481 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20394 synced till here 20392
2014-07-11 01:07:52,499 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066069241 with entries=106, filesize=89.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066070789
2014-07-11 01:07:52,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065971441
2014-07-11 01:07:52,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065972869
2014-07-11 01:07:52,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065974015
2014-07-11 01:07:52,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065975856
2014-07-11 01:07:52,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065977605
2014-07-11 01:07:52,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065980895
2014-07-11 01:07:52,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065982776
2014-07-11 01:07:52,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065983679
2014-07-11 01:07:53,294 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:07:53,320 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20471 synced till here 20469
2014-07-11 01:07:53,393 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066070789 with entries=77, filesize=64.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066073294
2014-07-11 01:07:55,188 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:07:55,212 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20552 synced till here 20544
2014-07-11 01:07:55,328 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066073294 with entries=81, filesize=69.5m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066075188
2014-07-11 01:07:56,858 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1367ms
GC pool 'ParNew' had collection(s): count=1 time=1464ms
2014-07-11 01:07:57,453 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:07:57,472 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20629 synced till here 20624
2014-07-11 01:07:57,529 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066075188 with entries=77, filesize=66.3m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066077453
2014-07-11 01:07:58,857 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:07:58,877 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20706 synced till here 20699
2014-07-11 01:07:58,963 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066077453 with entries=77, filesize=66.3m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066078857
2014-07-11 01:08:00,288 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:08:00,332 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20785 synced till here 20780
2014-07-11 01:08:00,387 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066078857 with entries=79, filesize=67.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066080288
2014-07-11 01:08:01,783 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:08:01,828 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20862 synced till here 20858
2014-07-11 01:08:01,889 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066080288 with entries=77, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066081784
2014-07-11 01:08:02,419 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:02,422 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:02,422 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:02,461 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:02,462 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:02,462 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:02,496 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:02,496 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:02,515 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:02,569 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:02,612 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:02,651 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:02,689 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:02,740 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:02,793 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:02,833 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:02,878 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:03,652 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:03,677 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:03,714 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:03,751 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:04,707 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:04,906 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:04,907 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:04,907 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:04,907 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:04,935 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:04,971 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:05,020 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:05,061 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:05,100 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:05,140 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:05,177 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:05,214 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:05,253 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:05,290 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:05,328 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:05,366 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:05,405 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:05,443 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:05,483 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:05,522 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:05,560 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:05,598 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:05,638 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:05,677 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:05,712 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:05,751 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:05,793 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:05,832 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:06,990 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4073, memsize=239.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/42de3e882a704660b8439b18ac85125a
2014-07-11 01:08:07,008 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/42de3e882a704660b8439b18ac85125a as hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/42de3e882a704660b8439b18ac85125a
2014-07-11 01:08:07,023 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/42de3e882a704660b8439b18ac85125a, entries=871850, sequenceid=4073, filesize=62.1m
2014-07-11 01:08:07,024 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~667.8m/700224400, currentsize=186.9m/195999360 for region usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. in 16625ms, sequenceid=4073, compaction requested=true
2014-07-11 01:08:07,024 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:08:07,024 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 20 blocking
2014-07-11 01:08:07,024 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1192ms
2014-07-11 01:08:07,025 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,025 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2., current region memstore size 853.3m
2014-07-11 01:08:07,024 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-11 01:08:07,025 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1232ms
2014-07-11 01:08:07,025 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,025 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:08:07,025 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1274ms
2014-07-11 01:08:07,025 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,025 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:08:07,025 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. because compaction request was cancelled
2014-07-11 01:08:07,029 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1317ms
2014-07-11 01:08:07,029 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,030 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1354ms
2014-07-11 01:08:07,030 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,030 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1392ms
2014-07-11 01:08:07,030 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,030 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1432ms
2014-07-11 01:08:07,030 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,031 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1470ms
2014-07-11 01:08:07,031 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,033 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1511ms
2014-07-11 01:08:07,033 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,037 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1554ms
2014-07-11 01:08:07,037 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,037 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1594ms
2014-07-11 01:08:07,037 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,039 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1634ms
2014-07-11 01:08:07,039 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,039 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1673ms
2014-07-11 01:08:07,040 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,040 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1712ms
2014-07-11 01:08:07,040 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,040 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1750ms
2014-07-11 01:08:07,040 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,042 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1789ms
2014-07-11 01:08:07,042 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,045 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1831ms
2014-07-11 01:08:07,045 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,055 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1878ms
2014-07-11 01:08:07,055 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,058 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1918ms
2014-07-11 01:08:07,058 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,058 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1958ms
2014-07-11 01:08:07,058 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,058 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1997ms
2014-07-11 01:08:07,058 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,058 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2038ms
2014-07-11 01:08:07,058 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,058 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2087ms
2014-07-11 01:08:07,058 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,062 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2127ms
2014-07-11 01:08:07,062 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,062 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2291ms
2014-07-11 01:08:07,062 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,065 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2248ms
2014-07-11 01:08:07,065 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,065 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2205ms
2014-07-11 01:08:07,065 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,065 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2168ms
2014-07-11 01:08:07,065 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,065 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2358ms
2014-07-11 01:08:07,065 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,066 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3315ms
2014-07-11 01:08:07,066 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,066 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3352ms
2014-07-11 01:08:07,066 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,069 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3392ms
2014-07-11 01:08:07,069 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,070 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3417ms
2014-07-11 01:08:07,070 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,070 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4192ms
2014-07-11 01:08:07,070 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,073 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4240ms
2014-07-11 01:08:07,073 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,073 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4280ms
2014-07-11 01:08:07,073 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,081 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4341ms
2014-07-11 01:08:07,081 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,081 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4392ms
2014-07-11 01:08:07,081 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,082 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4431ms
2014-07-11 01:08:07,082 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,082 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4470ms
2014-07-11 01:08:07,082 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,082 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4513ms
2014-07-11 01:08:07,082 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,082 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4567ms
2014-07-11 01:08:07,082 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,082 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4586ms
2014-07-11 01:08:07,083 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,083 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4587ms
2014-07-11 01:08:07,083 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,083 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4622ms
2014-07-11 01:08:07,083 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,083 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4622ms
2014-07-11 01:08:07,083 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,101 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4640ms
2014-07-11 01:08:07,101 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,102 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4679ms
2014-07-11 01:08:07,102 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,114 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4692ms
2014-07-11 01:08:07,114 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:07,121 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4702ms
2014-07-11 01:08:07,121 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:08,486 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1347ms
GC pool 'ParNew' had collection(s): count=1 time=1359ms
2014-07-11 01:08:08,771 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4014, memsize=294.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/2e0ebd7940ae46e5a63aa430e5d7e44e
2014-07-11 01:08:08,785 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/2e0ebd7940ae46e5a63aa430e5d7e44e as hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/2e0ebd7940ae46e5a63aa430e5d7e44e
2014-07-11 01:08:08,795 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/2e0ebd7940ae46e5a63aa430e5d7e44e, entries=1073070, sequenceid=4014, filesize=76.5m
2014-07-11 01:08:08,795 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~931.0m/976207520, currentsize=295.9m/310233680 for region usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. in 26221ms, sequenceid=4014, compaction requested=true
2014-07-11 01:08:08,796 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:08:08,796 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 20 blocking
2014-07-11 01:08:08,796 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-11 01:08:08,796 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224., current region memstore size 535.6m
2014-07-11 01:08:08,796 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:08:08,796 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:08:08,796 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. because compaction request was cancelled
2014-07-11 01:08:08,846 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:08:08,847 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355.
2014-07-11 01:08:08,896 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20973 synced till here 20947
2014-07-11 01:08:09,157 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066081784 with entries=111, filesize=95.3m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066088847
2014-07-11 01:08:09,158 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065985545
2014-07-11 01:08:09,158 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065987470
2014-07-11 01:08:09,158 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065989332
2014-07-11 01:08:09,158 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065991399
2014-07-11 01:08:09,158 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065993881
2014-07-11 01:08:09,158 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065995941
2014-07-11 01:08:09,158 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065997609
2014-07-11 01:08:09,158 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405065999170
2014-07-11 01:08:09,158 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066000930
2014-07-11 01:08:09,158 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066002756
2014-07-11 01:08:09,160 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066004117
2014-07-11 01:08:09,160 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066006159
2014-07-11 01:08:09,160 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066007941
2014-07-11 01:08:09,160 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066011577
2014-07-11 01:08:09,249 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:08:09,249 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:08:10,952 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:08:10,994 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21056 synced till here 21047
2014-07-11 01:08:11,104 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066088847 with entries=83, filesize=69.9m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066090952
2014-07-11 01:08:12,932 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:08:12,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21167 synced till here 21141
2014-07-11 01:08:13,155 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066090952 with entries=111, filesize=92.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066092932
2014-07-11 01:08:13,422 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10630,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405066082791,"queuetimems":1,"class":"HRegionServer","responsesize":19605,"method":"Multi"}
2014-07-11 01:08:14,706 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12288,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405066082417,"queuetimems":0,"class":"HRegionServer","responsesize":19759,"method":"Multi"}
2014-07-11 01:08:14,710 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12060,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405066082649,"queuetimems":0,"class":"HRegionServer","responsesize":19764,"method":"Multi"}
2014-07-11 01:08:14,711 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12252,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405066082459,"queuetimems":0,"class":"HRegionServer","responsesize":19726,"method":"Multi"}
2014-07-11 01:08:14,711 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12101,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405066082610,"queuetimems":0,"class":"HRegionServer","responsesize":19560,"method":"Multi"}
2014-07-11 01:08:14,732 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12165,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405066082567,"queuetimems":0,"class":"HRegionServer","responsesize":19953,"method":"Multi"}
2014-07-11 01:08:14,732 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11082,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405066083650,"queuetimems":0,"class":"HRegionServer","responsesize":19398,"method":"Multi"}
2014-07-11 01:08:14,732 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10983,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405066083749,"queuetimems":0,"class":"HRegionServer","responsesize":19816,"method":"Multi"}
2014-07-11 01:08:14,733 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11021,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405066083712,"queuetimems":0,"class":"HRegionServer","responsesize":19787,"method":"Multi"}
2014-07-11 01:08:14,733 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11856,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405066082876,"queuetimems":0,"class":"HRegionServer","responsesize":19908,"method":"Multi"}
2014-07-11 01:08:14,737 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29.
2014-07-11 01:08:14,742 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10037,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405066084705,"queuetimems":1,"class":"HRegionServer","responsesize":20020,"method":"Multi"}
2014-07-11 01:08:14,743 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12230,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405066082513,"queuetimems":0,"class":"HRegionServer","responsesize":19710,"method":"Multi"}
2014-07-11 01:08:14,742 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12004,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405066082738,"queuetimems":0,"class":"HRegionServer","responsesize":19339,"method":"Multi"}
2014-07-11 01:08:14,749 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12062,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405066082687,"queuetimems":0,"class":"HRegionServer","responsesize":19435,"method":"Multi"}
2014-07-11 01:08:15,051 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:08:15,054 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12221,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405066082832,"queuetimems":0,"class":"HRegionServer","responsesize":19734,"method":"Multi"}
2014-07-11 01:08:15,103 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21283 synced till here 21248
2014-07-11 01:08:15,423 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066092932 with entries=116, filesize=98.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066095052
2014-07-11 01:08:17,041 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1098ms
GC pool 'ParNew' had collection(s): count=1 time=1568ms
2014-07-11 01:08:18,051 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:08:18,088 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21397 synced till here 21380
2014-07-11 01:08:18,337 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066095052 with entries=114, filesize=97.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066098052
2014-07-11 01:08:19,787 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:08:19,879 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21509 synced till here 21472
2014-07-11 01:08:21,081 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1015ms
GC pool 'ParNew' had collection(s): count=1 time=1106ms
2014-07-11 01:08:21,291 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066098052 with entries=112, filesize=95.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066099787
2014-07-11 01:08:21,928 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:08:21,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21605 synced till here 21580
2014-07-11 01:08:22,361 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066099787 with entries=96, filesize=82.3m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066101929
2014-07-11 01:08:23,689 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:08:23,732 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21693 synced till here 21678
2014-07-11 01:08:23,890 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066101929 with entries=88, filesize=75.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066103689
2014-07-11 01:08:25,390 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:08:25,799 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21816 synced till here 21799
2014-07-11 01:08:26,406 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066103689 with entries=123, filesize=105.5m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066105390
2014-07-11 01:08:29,082 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:08:29,118 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21894 synced till here 21889
2014-07-11 01:08:29,721 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066105390 with entries=78, filesize=67.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066109082
2014-07-11 01:08:30,870 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:30,871 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:30,872 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:30,888 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:30,922 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:30,924 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:30,934 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:30,942 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:30,942 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:30,943 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:30,969 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:30,982 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:30,983 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:30,983 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:31,002 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:31,007 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:31,042 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:31,079 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:31,116 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:31,152 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:31,189 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:31,246 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:31,289 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:31,326 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:31,365 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:31,403 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:31,445 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:31,482 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:31,518 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:31,554 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:31,679 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:31,717 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:31,754 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:31,791 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:31,827 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:31,865 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:33,445 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1017ms
GC pool 'ParNew' had collection(s): count=1 time=1200ms
2014-07-11 01:08:33,465 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:33,503 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:33,539 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:33,576 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:33,601 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4201, memsize=333.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/59e278632b8d44afa8aa6bd0af3c69db
2014-07-11 01:08:33,614 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:33,617 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/59e278632b8d44afa8aa6bd0af3c69db as hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/59e278632b8d44afa8aa6bd0af3c69db
2014-07-11 01:08:33,627 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/59e278632b8d44afa8aa6bd0af3c69db, entries=1214220, sequenceid=4201, filesize=86.5m
2014-07-11 01:08:33,627 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~535.6m/561656960, currentsize=322.1m/337753360 for region usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. in 24831ms, sequenceid=4201, compaction requested=true
2014-07-11 01:08:33,628 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:08:33,628 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 20 blocking
2014-07-11 01:08:33,628 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-11 01:08:33,628 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14ms
2014-07-11 01:08:33,628 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,628 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2., current region memstore size 916.8m
2014-07-11 01:08:33,628 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:08:33,628 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:08:33,628 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 52ms
2014-07-11 01:08:33,629 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,629 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. because compaction request was cancelled
2014-07-11 01:08:33,629 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 90ms
2014-07-11 01:08:33,629 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,629 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 126ms
2014-07-11 01:08:33,629 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,629 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 164ms
2014-07-11 01:08:33,629 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,629 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1764ms
2014-07-11 01:08:33,629 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,633 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1806ms
2014-07-11 01:08:33,633 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,633 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1842ms
2014-07-11 01:08:33,633 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,633 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1879ms
2014-07-11 01:08:33,633 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,634 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1917ms
2014-07-11 01:08:33,634 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,634 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1955ms
2014-07-11 01:08:33,634 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,634 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2080ms
2014-07-11 01:08:33,634 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,645 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2127ms
2014-07-11 01:08:33,645 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,645 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2163ms
2014-07-11 01:08:33,645 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,645 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2201ms
2014-07-11 01:08:33,645 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,645 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2242ms
2014-07-11 01:08:33,645 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,646 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2281ms
2014-07-11 01:08:33,646 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,646 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2320ms
2014-07-11 01:08:33,646 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,649 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2360ms
2014-07-11 01:08:33,649 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,649 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2403ms
2014-07-11 01:08:33,649 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,649 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2460ms
2014-07-11 01:08:33,650 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,650 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2498ms
2014-07-11 01:08:33,650 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,650 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2534ms
2014-07-11 01:08:33,650 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,653 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2574ms
2014-07-11 01:08:33,653 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,653 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2611ms
2014-07-11 01:08:33,653 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,662 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2654ms
2014-07-11 01:08:33,662 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,669 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2667ms
2014-07-11 01:08:33,669 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,669 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2686ms
2014-07-11 01:08:33,669 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,676 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2694ms
2014-07-11 01:08:33,676 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,677 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2695ms
2014-07-11 01:08:33,677 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,677 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2708ms
2014-07-11 01:08:33,677 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,677 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2735ms
2014-07-11 01:08:33,677 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,685 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2743ms
2014-07-11 01:08:33,685 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,685 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2743ms
2014-07-11 01:08:33,685 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,685 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2751ms
2014-07-11 01:08:33,685 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,690 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2766ms
2014-07-11 01:08:33,690 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,690 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2769ms
2014-07-11 01:08:33,690 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,690 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2802ms
2014-07-11 01:08:33,690 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,693 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2821ms
2014-07-11 01:08:33,693 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,693 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2823ms
2014-07-11 01:08:33,693 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,693 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2823ms
2014-07-11 01:08:33,693 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:33,846 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:08:33,925 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21989 synced till here 21967
2014-07-11 01:08:33,927 DEBUG [RpcServer.handler=23,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224.
2014-07-11 01:08:34,059 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066109082 with entries=95, filesize=80.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066113846
2014-07-11 01:08:34,704 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:08:35,555 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:08:35,627 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22082 synced till here 22067
2014-07-11 01:08:35,719 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066113846 with entries=93, filesize=79.9m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066115555
2014-07-11 01:08:36,510 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:08:36,579 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22176 synced till here 22162
2014-07-11 01:08:37,198 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066115555 with entries=94, filesize=80.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066116510
2014-07-11 01:08:38,042 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:08:38,196 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4198, memsize=376.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/15985a5094ba476c8a206e6d843ca016
2014-07-11 01:08:38,209 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/15985a5094ba476c8a206e6d843ca016 as hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/15985a5094ba476c8a206e6d843ca016
2014-07-11 01:08:39,382 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22278 synced till here 22277
2014-07-11 01:08:39,393 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/15985a5094ba476c8a206e6d843ca016, entries=1372220, sequenceid=4198, filesize=97.7m
2014-07-11 01:08:39,394 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~853.3m/894722400, currentsize=419.5m/439877440 for region usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. in 32369ms, sequenceid=4198, compaction requested=true
2014-07-11 01:08:39,395 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:08:39,395 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 20 blocking
2014-07-11 01:08:39,395 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-11 01:08:39,395 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355., current region memstore size 714.4m
2014-07-11 01:08:39,395 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:08:39,395 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:08:39,395 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. because compaction request was cancelled
2014-07-11 01:08:39,617 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066116510 with entries=102, filesize=87.5m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066118043
2014-07-11 01:08:39,617 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066013466
2014-07-11 01:08:39,617 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066018575
2014-07-11 01:08:39,617 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066021110
2014-07-11 01:08:39,617 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066022924
2014-07-11 01:08:39,617 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066026798
2014-07-11 01:08:39,618 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066029509
2014-07-11 01:08:39,618 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066030480
2014-07-11 01:08:39,618 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066032068
2014-07-11 01:08:39,688 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2.
2014-07-11 01:08:40,391 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:08:40,842 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:08:40,866 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22352 synced till here 22351
2014-07-11 01:08:40,893 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066118043 with entries=74, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066120843
2014-07-11 01:08:42,368 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:08:42,386 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22426 synced till here 22424
2014-07-11 01:08:42,413 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066120843 with entries=74, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066122368
2014-07-11 01:08:43,902 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:08:43,943 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22505 synced till here 22498
2014-07-11 01:08:44,051 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066122368 with entries=79, filesize=67.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066123902
2014-07-11 01:08:45,773 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:08:45,799 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22595 synced till here 22582
2014-07-11 01:08:45,909 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066123902 with entries=90, filesize=77.2m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066125773
2014-07-11 01:08:47,554 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:08:47,576 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22678 synced till here 22668
2014-07-11 01:08:47,688 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066125773 with entries=83, filesize=71.0m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066127555
2014-07-11 01:08:49,217 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:08:49,242 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22753 synced till here 22751
2014-07-11 01:08:49,285 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066127555 with entries=75, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066129218
2014-07-11 01:08:51,626 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:08:51,820 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:51,820 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:51,836 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:51,841 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:51,841 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:51,842 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:51,855 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:51,855 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:51,873 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:51,929 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:51,965 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:52,163 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:52,359 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066129218 with entries=94, filesize=80.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066131627
2014-07-11 01:08:54,060 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:54,101 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:54,138 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:54,177 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:54,212 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:54,250 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:54,287 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:54,325 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:54,366 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:54,404 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:54,442 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:55,401 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:55,439 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:55,478 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:55,516 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:55,552 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:55,589 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:55,626 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:55,664 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:57,607 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5444ms
2014-07-11 01:08:57,608 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5788ms
2014-07-11 01:08:57,608 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1530ms
GC pool 'ParNew' had collection(s): count=1 time=1911ms
2014-07-11 01:08:57,609 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5788ms
2014-07-11 01:08:57,609 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5768ms
2014-07-11 01:08:57,609 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5773ms
2014-07-11 01:08:57,609 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5768ms
2014-07-11 01:08:57,610 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5754ms
2014-07-11 01:08:57,610 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5768ms
2014-07-11 01:08:57,610 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5755ms
2014-07-11 01:08:57,610 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5737ms
2014-07-11 01:08:57,611 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5681ms
2014-07-11 01:08:57,611 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5646ms
2014-07-11 01:08:57,615 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:57,628 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:57,656 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:57,694 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:57,732 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:57,770 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:57,807 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:57,845 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:57,886 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:57,921 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:57,960 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:57,997 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:58,035 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:58,171 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:58,216 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:58,255 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:58,293 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:58,337 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:58,382 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:08:59,061 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 01:08:59,101 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 01:08:59,138 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 01:08:59,177 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 01:08:59,213 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 01:08:59,250 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 01:08:59,288 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 01:08:59,316 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4476, memsize=279.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/011aedd6dae041daa80be8c943fe6e07
2014-07-11 01:08:59,326 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 01:08:59,336 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/011aedd6dae041daa80be8c943fe6e07 as hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/011aedd6dae041daa80be8c943fe6e07
2014-07-11 01:08:59,350 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/011aedd6dae041daa80be8c943fe6e07, entries=1017410, sequenceid=4476, filesize=72.5m
2014-07-11 01:08:59,350 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~715.9m/750690720, currentsize=180.3m/189084240 for region usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. in 19955ms, sequenceid=4476, compaction requested=true
2014-07-11 01:08:59,351 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:08:59,351 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 20 blocking
2014-07-11 01:08:59,351 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5026ms
2014-07-11 01:08:59,351 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-11 01:08:59,351 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,351 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29., current region memstore size 784.7m
2014-07-11 01:08:59,351 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5064ms
2014-07-11 01:08:59,351 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,351 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:08:59,352 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:08:59,352 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5101ms
2014-07-11 01:08:59,352 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,352 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. because compaction request was cancelled
2014-07-11 01:08:59,352 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5140ms
2014-07-11 01:08:59,352 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,352 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5176ms
2014-07-11 01:08:59,352 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,357 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5219ms
2014-07-11 01:08:59,357 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,357 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5257ms
2014-07-11 01:08:59,357 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,357 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5297ms
2014-07-11 01:08:59,357 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,361 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 980ms
2014-07-11 01:08:59,361 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,361 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1024ms
2014-07-11 01:08:59,361 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,361 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1068ms
2014-07-11 01:08:59,361 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,361 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1106ms
2014-07-11 01:08:59,361 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,361 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1146ms
2014-07-11 01:08:59,361 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,365 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1194ms
2014-07-11 01:08:59,365 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,366 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1331ms
2014-07-11 01:08:59,366 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,366 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1369ms
2014-07-11 01:08:59,366 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,366 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1406ms
2014-07-11 01:08:59,366 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,367 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 01:08:59,367 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,368 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1447ms
2014-07-11 01:08:59,368 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,368 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1482ms
2014-07-11 01:08:59,368 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,368 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1524ms
2014-07-11 01:08:59,368 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,369 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1562ms
2014-07-11 01:08:59,369 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,370 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1601ms
2014-07-11 01:08:59,370 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,373 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1641ms
2014-07-11 01:08:59,373 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,381 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1687ms
2014-07-11 01:08:59,381 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,381 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1725ms
2014-07-11 01:08:59,381 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,381 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1753ms
2014-07-11 01:08:59,381 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,381 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1766ms
2014-07-11 01:08:59,381 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,381 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7416ms
2014-07-11 01:08:59,381 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,382 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7453ms
2014-07-11 01:08:59,382 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,382 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7509ms
2014-07-11 01:08:59,382 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,384 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7529ms
2014-07-11 01:08:59,384 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,385 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7543ms
2014-07-11 01:08:59,385 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,388 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7533ms
2014-07-11 01:08:59,388 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,389 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7548ms
2014-07-11 01:08:59,389 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,389 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7553ms
2014-07-11 01:08:59,389 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,389 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7548ms
2014-07-11 01:08:59,389 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,389 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7569ms
2014-07-11 01:08:59,389 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,393 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7573ms
2014-07-11 01:08:59,393 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,405 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 01:08:59,406 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,406 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7243ms
2014-07-11 01:08:59,406 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,413 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3749ms
2014-07-11 01:08:59,413 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,421 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3795ms
2014-07-11 01:08:59,421 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,422 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3833ms
2014-07-11 01:08:59,422 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,422 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3870ms
2014-07-11 01:08:59,422 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,422 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3906ms
2014-07-11 01:08:59,422 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,422 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3944ms
2014-07-11 01:08:59,422 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,423 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3984ms
2014-07-11 01:08:59,423 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,433 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4032ms
2014-07-11 01:08:59,433 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:08:59,434 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4991ms
2014-07-11 01:08:59,434 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:01,321 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:01,339 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22963 synced till here 22927
2014-07-11 01:09:01,514 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:09:01,641 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066131627 with entries=116, filesize=95.9m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066141321
2014-07-11 01:09:01,981 DEBUG [RpcServer.handler=5,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355.
2014-07-11 01:09:02,199 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4408, memsize=401.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/24bb6a2f6ee44ef6bf0695cca568a541
2014-07-11 01:09:03,124 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/24bb6a2f6ee44ef6bf0695cca568a541 as hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/24bb6a2f6ee44ef6bf0695cca568a541
2014-07-11 01:09:03,144 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/24bb6a2f6ee44ef6bf0695cca568a541, entries=1461160, sequenceid=4408, filesize=104.1m
2014-07-11 01:09:03,144 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~916.8m/961321760, currentsize=292.4m/306587680 for region usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. in 29516ms, sequenceid=4408, compaction requested=true
2014-07-11 01:09:03,148 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 20 blocking
2014-07-11 01:09:03,148 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-11 01:09:03,148 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:09:03,148 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:09:03,148 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. because compaction request was cancelled
2014-07-11 01:09:03,149 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:09:03,149 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224., current region memstore size 676.5m
2014-07-11 01:09:03,331 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:03,332 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11462,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405066131870,"queuetimems":1,"class":"HRegionServer","responsesize":19366,"method":"Multi"}
2014-07-11 01:09:03,335 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2.
2014-07-11 01:09:03,371 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23064 synced till here 23041
2014-07-11 01:09:03,567 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066141321 with entries=101, filesize=83.9m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066143331
2014-07-11 01:09:03,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066034182
2014-07-11 01:09:03,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066041809
2014-07-11 01:09:03,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066043427
2014-07-11 01:09:03,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066045676
2014-07-11 01:09:03,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066048392
2014-07-11 01:09:03,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066050777
2014-07-11 01:09:03,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066052694
2014-07-11 01:09:03,570 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066054571
2014-07-11 01:09:03,570 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066056083
2014-07-11 01:09:03,570 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066058019
2014-07-11 01:09:03,574 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066058885
2014-07-11 01:09:03,574 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066062088
2014-07-11 01:09:03,574 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066064549
2014-07-11 01:09:03,574 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066066320
2014-07-11 01:09:03,574 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066068341
2014-07-11 01:09:03,876 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11715,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405066132161,"queuetimems":0,"class":"HRegionServer","responsesize":20018,"method":"Multi"}
2014-07-11 01:09:03,899 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11935,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405066131963,"queuetimems":0,"class":"HRegionServer","responsesize":19398,"method":"Multi"}
2014-07-11 01:09:04,029 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12098,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405066131927,"queuetimems":0,"class":"HRegionServer","responsesize":19231,"method":"Multi"}
2014-07-11 01:09:04,128 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:09:04,133 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:04,134 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12300,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405066131834,"queuetimems":1,"class":"HRegionServer","responsesize":20876,"method":"Multi"}
2014-07-11 01:09:04,157 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23142 synced till here 23137
2014-07-11 01:09:04,202 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066143331 with entries=78, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066144134
2014-07-11 01:09:04,306 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10095,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44933","starttimems":1405066134210,"queuetimems":0,"class":"HRegionServer","responsesize":19398,"method":"Multi"}
2014-07-11 01:09:05,345 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:05,364 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23218 synced till here 23215
2014-07-11 01:09:05,409 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066144134 with entries=76, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066145346
2014-07-11 01:09:06,697 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:06,716 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23292 synced till here 23290
2014-07-11 01:09:06,740 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066145346 with entries=74, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066146697
2014-07-11 01:09:08,234 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:08,679 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23406 synced till here 23402
2014-07-11 01:09:08,733 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066146697 with entries=114, filesize=97.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066148235
2014-07-11 01:09:10,108 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:10,122 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23486 synced till here 23478
2014-07-11 01:09:10,261 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066148235 with entries=80, filesize=68.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066150108
2014-07-11 01:09:12,146 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:12,198 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23578 synced till here 23560
2014-07-11 01:09:12,503 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066150108 with entries=92, filesize=75.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066152147
2014-07-11 01:09:14,356 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:14,392 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23669 synced till here 23652
2014-07-11 01:09:14,516 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066152147 with entries=91, filesize=75.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066154357
2014-07-11 01:09:16,572 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:16,595 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23744 synced till here 23742
2014-07-11 01:09:16,627 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066154357 with entries=75, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066156572
2014-07-11 01:09:18,381 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:18,398 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23827 synced till here 23820
2014-07-11 01:09:18,465 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066156572 with entries=83, filesize=69.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066158382
2014-07-11 01:09:19,393 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:19,412 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23901 synced till here 23900
2014-07-11 01:09:19,447 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066158382 with entries=74, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066159393
2014-07-11 01:09:20,178 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:20,179 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:20,191 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:20,219 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:20,226 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:20,227 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:20,227 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:20,227 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:20,227 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:20,243 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:20,243 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:20,269 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:20,307 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:20,345 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:20,382 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:20,419 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:20,456 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:20,493 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:20,534 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:20,582 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:20,620 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:20,661 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:20,701 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:20,737 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:20,905 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4645, memsize=216.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/df67a338f74e442d921e49d801b27651
2014-07-11 01:09:20,930 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/df67a338f74e442d921e49d801b27651 as hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/df67a338f74e442d921e49d801b27651
2014-07-11 01:09:20,947 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/df67a338f74e442d921e49d801b27651, entries=787620, sequenceid=4645, filesize=56.1m
2014-07-11 01:09:20,947 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~681.0m/714061280, currentsize=242.7m/254519520 for region usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. in 17798ms, sequenceid=4645, compaction requested=true
2014-07-11 01:09:20,948 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:09:20,948 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 20 blocking
2014-07-11 01:09:20,948 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 211ms
2014-07-11 01:09:20,948 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:20,948 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-11 01:09:20,948 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2., current region memstore size 923.9m
2014-07-11 01:09:20,948 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:09:20,948 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 247ms
2014-07-11 01:09:20,948 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:20,948 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:09:20,948 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 287ms
2014-07-11 01:09:20,948 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:20,948 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. because compaction request was cancelled
2014-07-11 01:09:20,953 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 333ms
2014-07-11 01:09:20,953 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:20,953 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 371ms
2014-07-11 01:09:20,953 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:20,953 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 419ms
2014-07-11 01:09:20,953 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:20,961 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 468ms
2014-07-11 01:09:20,961 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:20,961 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 505ms
2014-07-11 01:09:20,961 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:20,961 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 542ms
2014-07-11 01:09:20,961 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:20,961 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 579ms
2014-07-11 01:09:20,961 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:20,970 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 625ms
2014-07-11 01:09:20,970 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:20,970 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 664ms
2014-07-11 01:09:20,970 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:20,985 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 716ms
2014-07-11 01:09:20,985 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:20,985 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 742ms
2014-07-11 01:09:20,985 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:20,985 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 742ms
2014-07-11 01:09:20,985 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:20,987 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 760ms
2014-07-11 01:09:20,987 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:20,987 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 760ms
2014-07-11 01:09:20,987 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:20,987 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 760ms
2014-07-11 01:09:20,987 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:20,987 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 760ms
2014-07-11 01:09:20,988 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:20,988 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 762ms
2014-07-11 01:09:20,988 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:20,997 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 778ms
2014-07-11 01:09:20,997 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:20,997 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 807ms
2014-07-11 01:09:20,997 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:20,997 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 818ms
2014-07-11 01:09:20,997 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:21,001 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 823ms
2014-07-11 01:09:21,001 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:21,380 DEBUG [RpcServer.handler=30,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224.
2014-07-11 01:09:21,549 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:21,596 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23985 synced till here 23973
2014-07-11 01:09:22,682 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066159393 with entries=84, filesize=72.2m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066161550
2014-07-11 01:09:22,873 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:09:23,362 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:23,378 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24071 synced till here 24061
2014-07-11 01:09:23,453 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066161550 with entries=86, filesize=73.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066163363
2014-07-11 01:09:25,105 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:25,185 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24148 synced till here 24147
2014-07-11 01:09:25,218 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066163363 with entries=77, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066165106
2014-07-11 01:09:27,176 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:28,447 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24274 synced till here 24269
2014-07-11 01:09:28,500 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066165106 with entries=126, filesize=108.2m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066167176
2014-07-11 01:09:28,729 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4581, memsize=342.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/705d0db83c7346259f78cb7f40e4cf01
2014-07-11 01:09:28,766 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/705d0db83c7346259f78cb7f40e4cf01 as hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/705d0db83c7346259f78cb7f40e4cf01
2014-07-11 01:09:28,789 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/705d0db83c7346259f78cb7f40e4cf01, entries=1246430, sequenceid=4581, filesize=88.8m
2014-07-11 01:09:28,790 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~784.7m/822772480, currentsize=442.8m/464347760 for region usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. in 29438ms, sequenceid=4581, compaction requested=true
2014-07-11 01:09:28,790 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:09:28,790 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 20 blocking
2014-07-11 01:09:28,790 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-11 01:09:28,790 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355., current region memstore size 627.1m
2014-07-11 01:09:28,790 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:09:28,790 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:09:28,790 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. because compaction request was cancelled
2014-07-11 01:09:28,799 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29.
2014-07-11 01:09:29,397 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:09:29,464 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:29,483 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24348 synced till here 24347
2014-07-11 01:09:29,499 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066167176 with entries=74, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066169465
2014-07-11 01:09:29,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066069241
2014-07-11 01:09:29,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066070789
2014-07-11 01:09:29,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066073294
2014-07-11 01:09:29,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066075188
2014-07-11 01:09:29,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066077453
2014-07-11 01:09:29,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066078857
2014-07-11 01:09:29,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066080288
2014-07-11 01:09:30,813 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:30,839 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24423 synced till here 24421
2014-07-11 01:09:30,870 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066169465 with entries=75, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066170813
2014-07-11 01:09:32,052 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:32,077 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066170813 with entries=77, filesize=61.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066172053
2014-07-11 01:09:33,775 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:33,796 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24576 synced till here 24572
2014-07-11 01:09:33,838 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066172053 with entries=76, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066173776
2014-07-11 01:09:34,674 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:34,690 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24650 synced till here 24647
2014-07-11 01:09:35,117 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066173776 with entries=74, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066174675
2014-07-11 01:09:36,018 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:36,032 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24727 synced till here 24724
2014-07-11 01:09:36,078 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066174675 with entries=77, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066176018
2014-07-11 01:09:37,487 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:37,508 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24800 synced till here 24799
2014-07-11 01:09:37,531 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066176018 with entries=73, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066177487
2014-07-11 01:09:38,879 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:38,882 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:38,907 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:38,944 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:38,944 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:38,944 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:38,977 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:39,112 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:39,153 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:39,193 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:39,241 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:39,519 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:39,609 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:39,777 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:41,143 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:41,180 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:41,221 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:41,261 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:41,299 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:41,335 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:41,372 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:42,877 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:42,916 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:42,952 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:42,991 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:43,027 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:43,916 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5009ms
2014-07-11 01:09:43,916 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5037ms
2014-07-11 01:09:43,917 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5034ms
2014-07-11 01:09:43,923 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:43,934 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:43,945 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 01:09:43,945 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 01:09:43,945 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-11 01:09:43,961 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:43,977 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 01:09:43,999 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:44,036 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:44,075 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:44,113 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 01:09:44,114 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:44,150 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:09:44,153 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 01:09:44,194 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 01:09:44,241 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 01:09:44,384 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4797, memsize=310.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/9f0c15303a5a4be3998748c0363b1ee6
2014-07-11 01:09:44,409 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/9f0c15303a5a4be3998748c0363b1ee6 as hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/9f0c15303a5a4be3998748c0363b1ee6
2014-07-11 01:09:44,427 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/9f0c15303a5a4be3998748c0363b1ee6, entries=1129630, sequenceid=4797, filesize=80.4m
2014-07-11 01:09:44,427 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~923.9m/968746000, currentsize=293.1m/307293440 for region usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. in 23479ms, sequenceid=4797, compaction requested=true
2014-07-11 01:09:44,428 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:09:44,428 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 20 blocking
2014-07-11 01:09:44,428 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-11 01:09:44,428 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5187ms
2014-07-11 01:09:44,428 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,428 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:09:44,428 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5235ms
2014-07-11 01:09:44,428 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,428 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2., current region memstore size 897.7m
2014-07-11 01:09:44,429 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5276ms
2014-07-11 01:09:44,429 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,428 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:09:44,429 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 279ms
2014-07-11 01:09:44,429 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. because compaction request was cancelled
2014-07-11 01:09:44,429 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,429 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 315ms
2014-07-11 01:09:44,429 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,433 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5321ms
2014-07-11 01:09:44,433 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,433 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 358ms
2014-07-11 01:09:44,433 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,436 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 400ms
2014-07-11 01:09:44,436 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,436 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 437ms
2014-07-11 01:09:44,436 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,437 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5460ms
2014-07-11 01:09:44,437 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,437 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 476ms
2014-07-11 01:09:44,437 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,437 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5494ms
2014-07-11 01:09:44,437 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,437 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5493ms
2014-07-11 01:09:44,437 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,445 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5502ms
2014-07-11 01:09:44,445 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,445 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 511ms
2014-07-11 01:09:44,445 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,445 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 522ms
2014-07-11 01:09:44,445 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,445 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5563ms
2014-07-11 01:09:44,445 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,451 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5572ms
2014-07-11 01:09:44,451 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,451 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5544ms
2014-07-11 01:09:44,451 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,451 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1424ms
2014-07-11 01:09:44,451 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,451 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1461ms
2014-07-11 01:09:44,451 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,451 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1499ms
2014-07-11 01:09:44,451 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,451 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1535ms
2014-07-11 01:09:44,452 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,452 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1575ms
2014-07-11 01:09:44,452 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,452 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3080ms
2014-07-11 01:09:44,452 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,453 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3118ms
2014-07-11 01:09:44,453 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,453 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3155ms
2014-07-11 01:09:44,453 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,453 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3192ms
2014-07-11 01:09:44,481 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,481 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3260ms
2014-07-11 01:09:44,481 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,481 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3301ms
2014-07-11 01:09:44,481 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,483 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3340ms
2014-07-11 01:09:44,483 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,485 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4706ms
2014-07-11 01:09:44,485 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,485 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4876ms
2014-07-11 01:09:44,485 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,485 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4966ms
2014-07-11 01:09:44,485 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:09:44,558 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:44,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24899 synced till here 24876
2014-07-11 01:09:44,791 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2.
2014-07-11 01:09:44,844 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066177487 with entries=99, filesize=83.3m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066184558
2014-07-11 01:09:44,844 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066081784
2014-07-11 01:09:44,844 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066088847
2014-07-11 01:09:44,844 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066090952
2014-07-11 01:09:44,844 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066092932
2014-07-11 01:09:44,844 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066095052
2014-07-11 01:09:44,844 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066098052
2014-07-11 01:09:44,844 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066099787
2014-07-11 01:09:44,844 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066101929
2014-07-11 01:09:44,845 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066103689
2014-07-11 01:09:44,845 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066105390
2014-07-11 01:09:45,369 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:09:46,673 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:46,822 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25015 synced till here 24991
2014-07-11 01:09:46,968 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066184558 with entries=116, filesize=98.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066186674
2014-07-11 01:09:47,349 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4887, memsize=328.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/eaba82e82704477db005552986ec606c
2014-07-11 01:09:47,367 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/eaba82e82704477db005552986ec606c as hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/eaba82e82704477db005552986ec606c
2014-07-11 01:09:47,379 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/eaba82e82704477db005552986ec606c, entries=1194270, sequenceid=4887, filesize=85.1m
2014-07-11 01:09:47,380 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~631.8m/662513200, currentsize=229.2m/240295920 for region usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. in 18590ms, sequenceid=4887, compaction requested=true
2014-07-11 01:09:47,380 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:09:47,380 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 17 store files, 0 compacting, 17 eligible, 20 blocking
2014-07-11 01:09:47,380 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224., current region memstore size 602.9m
2014-07-11 01:09:47,380 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 17 files from compaction candidates
2014-07-11 01:09:47,381 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:09:47,381 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:09:47,381 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. because compaction request was cancelled
2014-07-11 01:09:48,443 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:48,464 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25098 synced till here 25088
2014-07-11 01:09:48,772 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:09:49,053 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066186674 with entries=83, filesize=71.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066188443
2014-07-11 01:09:50,072 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:50,088 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25170 synced till here 25169
2014-07-11 01:09:50,102 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066188443 with entries=72, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066190073
2014-07-11 01:09:50,584 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355.
2014-07-11 01:09:51,283 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:53,220 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1728ms
GC pool 'ParNew' had collection(s): count=1 time=1741ms
2014-07-11 01:09:53,236 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25259 synced till here 25258
2014-07-11 01:09:53,244 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066190073 with entries=89, filesize=76.2m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066191284
2014-07-11 01:09:53,904 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:53,922 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25334 synced till here 25331
2014-07-11 01:09:53,955 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066191284 with entries=75, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066193904
2014-07-11 01:09:55,312 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:55,346 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25409 synced till here 25406
2014-07-11 01:09:55,390 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066193904 with entries=75, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066195313
2014-07-11 01:09:56,700 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:56,730 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25486 synced till here 25483
2014-07-11 01:09:56,762 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066195313 with entries=77, filesize=66.0m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066196701
2014-07-11 01:09:58,270 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:09:58,305 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25566 synced till here 25563
2014-07-11 01:09:58,334 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066196701 with entries=80, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066198271
2014-07-11 01:10:00,211 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:10:00,235 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25643 synced till here 25642
2014-07-11 01:10:00,271 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066198271 with entries=77, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066200211
2014-07-11 01:10:01,241 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:10:01,831 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066200211 with entries=87, filesize=71.8m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066201243
2014-07-11 01:10:03,203 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:10:04,080 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066201243 with entries=91, filesize=73.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066203203
2014-07-11 01:10:05,583 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:10:05,597 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:10:05,615 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:10:05,621 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:10:05,636 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:10:05,639 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:10:05,674 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:10:05,712 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:10:07,975 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405065110777: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 01:10:09,202 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5039, memsize=386.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/9501fbfea70c449e8cd85518d34e6fa9
2014-07-11 01:10:09,214 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/9501fbfea70c449e8cd85518d34e6fa9 as hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/9501fbfea70c449e8cd85518d34e6fa9
2014-07-11 01:10:09,222 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/9501fbfea70c449e8cd85518d34e6fa9, entries=1408810, sequenceid=5039, filesize=100.3m
2014-07-11 01:10:09,223 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~602.9m/632198320, currentsize=243.4m/255272160 for region usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. in 21843ms, sequenceid=5039, compaction requested=true
2014-07-11 01:10:09,224 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:10:09,224 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 17 store files, 0 compacting, 17 eligible, 20 blocking
2014-07-11 01:10:09,224 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1249ms
2014-07-11 01:10:09,224 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:10:09,224 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 17 files from compaction candidates
2014-07-11 01:10:09,224 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3512ms
2014-07-11 01:10:09,224 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:10:09,224 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:10:09,225 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3551ms
2014-07-11 01:10:09,225 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:10:09,225 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3587ms
2014-07-11 01:10:09,225 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:10:09,224 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29., current region memstore size 930.0m
2014-07-11 01:10:09,225 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:10:09,225 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. because compaction request was cancelled
2014-07-11 01:10:09,226 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3589ms
2014-07-11 01:10:09,226 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:10:09,226 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3605ms
2014-07-11 01:10:09,226 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:10:09,229 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3615ms
2014-07-11 01:10:09,229 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:10:09,230 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3632ms
2014-07-11 01:10:09,230 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:10:09,230 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3647ms
2014-07-11 01:10:09,230 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405065110777
2014-07-11 01:10:09,416 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:10:09,429 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25900 synced till here 25895
2014-07-11 01:10:09,474 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066203203 with entries=79, filesize=65.4m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066209416
2014-07-11 01:10:10,056 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224.
2014-07-11 01:10:10,157 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:10:11,934 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:10:11,964 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066209416 with entries=71, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066211935
2014-07-11 01:10:14,105 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4990, memsize=516.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/89339ab25ff54edf84c049120b0a4714
2014-07-11 01:10:14,122 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/.tmp/89339ab25ff54edf84c049120b0a4714 as hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/89339ab25ff54edf84c049120b0a4714
2014-07-11 01:10:14,133 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e40f437ea2cd04f5e80b9966a93834b2/family/89339ab25ff54edf84c049120b0a4714, entries=1879830, sequenceid=4990, filesize=133.9m
2014-07-11 01:10:14,133 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~897.7m/941314240, currentsize=360.7m/378259520 for region usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. in 29705ms, sequenceid=4990, compaction requested=true
2014-07-11 01:10:14,134 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 20 blocking
2014-07-11 01:10:14,134 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-11 01:10:14,134 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:10:14,134 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:10:14,134 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2. because compaction request was cancelled
2014-07-11 01:10:14,134 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:10:14,135 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2., current region memstore size 649.9m
2014-07-11 01:10:14,605 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:10:14,668 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2.
2014-07-11 01:10:15,131 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:10:16,666 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066211935 with entries=106, filesize=88.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066214605
2014-07-11 01:10:16,666 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066109082
2014-07-11 01:10:16,666 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066113846
2014-07-11 01:10:16,666 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066115555
2014-07-11 01:10:16,667 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066116510
2014-07-11 01:10:16,667 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066118043
2014-07-11 01:10:16,667 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066120843
2014-07-11 01:10:16,667 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066122368
2014-07-11 01:10:16,667 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066123902
2014-07-11 01:10:16,667 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066125773
2014-07-11 01:10:16,667 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066127555
2014-07-11 01:10:16,667 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066129218
2014-07-11 01:10:28,234 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:10:28,263 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26154 synced till here 26152
2014-07-11 01:10:28,295 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066214605 with entries=77, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066228235
2014-07-11 01:10:30,360 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:10:30,378 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26228 synced till here 26227
2014-07-11 01:10:30,395 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066228235 with entries=74, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066230360
2014-07-11 01:10:31,470 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:10:31,484 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26301 synced till here 26300
2014-07-11 01:10:31,496 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066230360 with entries=73, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066231471
2014-07-11 01:10:32,970 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5188, memsize=576.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/b7f65afd4e264ab387f942223b94f3ca
2014-07-11 01:10:32,985 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/.tmp/b7f65afd4e264ab387f942223b94f3ca as hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/b7f65afd4e264ab387f942223b94f3ca
2014-07-11 01:10:32,997 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/f5085e3c4a405ca169db6dcb31b38b29/family/b7f65afd4e264ab387f942223b94f3ca, entries=2099460, sequenceid=5188, filesize=149.4m
2014-07-11 01:10:32,997 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~930.0m/975162240, currentsize=148.7m/155970880 for region usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. in 23773ms, sequenceid=5188, compaction requested=true
2014-07-11 01:10:32,998 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:10:32,998 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 20 blocking
2014-07-11 01:10:32,998 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-11 01:10:32,998 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355., current region memstore size 623.6m
2014-07-11 01:10:32,998 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:10:32,998 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:10:32,998 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user8,1405065489083.f5085e3c4a405ca169db6dcb31b38b29. because compaction request was cancelled
2014-07-11 01:10:33,012 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:10:33,027 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26375 synced till here 26374
2014-07-11 01:10:33,045 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066231471 with entries=74, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066233012
2014-07-11 01:10:33,045 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066131627
2014-07-11 01:10:33,045 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066141321
2014-07-11 01:10:33,045 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066143331
2014-07-11 01:10:33,045 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066144134
2014-07-11 01:10:33,045 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066145346
2014-07-11 01:10:33,045 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066146697
2014-07-11 01:10:33,045 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066148235
2014-07-11 01:10:33,045 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066150108
2014-07-11 01:10:33,045 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066152147
2014-07-11 01:10:33,045 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066154357
2014-07-11 01:10:33,045 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066156572
2014-07-11 01:10:33,046 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066158382
2014-07-11 01:10:34,835 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1168ms
GC pool 'ParNew' had collection(s): count=1 time=1452ms
2014-07-11 01:10:35,027 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:10:35,136 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5222, memsize=491.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/f54b8e1a144c4188a30faa3541824ab1
2014-07-11 01:10:35,149 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/.tmp/f54b8e1a144c4188a30faa3541824ab1 as hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/f54b8e1a144c4188a30faa3541824ab1
2014-07-11 01:10:35,168 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/bc1bf2a12fb05f36e29f0cabc7669ca2/family/f54b8e1a144c4188a30faa3541824ab1, entries=1789770, sequenceid=5222, filesize=127.4m
2014-07-11 01:10:35,168 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~649.9m/681510160, currentsize=119.7m/125477120 for region usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. in 21033ms, sequenceid=5222, compaction requested=true
2014-07-11 01:10:35,169 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:10:35,169 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 20 blocking
2014-07-11 01:10:35,169 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224., current region memstore size 411.8m
2014-07-11 01:10:35,169 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-11 01:10:35,169 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:10:35,169 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:10:35,169 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user6,1405065489083.bc1bf2a12fb05f36e29f0cabc7669ca2. because compaction request was cancelled
2014-07-11 01:10:35,369 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:10:35,384 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26448 synced till here 26447
2014-07-11 01:10:35,400 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066233012 with entries=73, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066235369
2014-07-11 01:10:35,400 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066159393
2014-07-11 01:10:35,400 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066161550
2014-07-11 01:10:35,400 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066163363
2014-07-11 01:10:35,400 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066165106
2014-07-11 01:10:35,448 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:10:37,055 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:10:37,072 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26527 synced till here 26524
2014-07-11 01:10:37,102 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066235369 with entries=79, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066237055
2014-07-11 01:10:39,290 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 01:10:39,312 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066237055 with entries=77, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405065110777/slave1%2C60020%2C1405065110777.1405066239290
2014-07-11 01:10:47,943 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5310, memsize=372.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/e7b2b2acc7b540a98aaff3b17808b9f6
2014-07-11 01:10:47,958 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/.tmp/e7b2b2acc7b540a98aaff3b17808b9f6 as hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/e7b2b2acc7b540a98aaff3b17808b9f6
2014-07-11 01:10:47,971 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2c9ec51d492f963529797ef469f19224/family/e7b2b2acc7b540a98aaff3b17808b9f6, entries=1356540, sequenceid=5310, filesize=96.6m
2014-07-11 01:10:47,972 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~413.5m/433547920, currentsize=57.8m/60648320 for region usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. in 12803ms, sequenceid=5310, compaction requested=true
2014-07-11 01:10:47,972 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:10:47,972 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 18 store files, 0 compacting, 18 eligible, 20 blocking
2014-07-11 01:10:47,973 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 18 files from compaction candidates
2014-07-11 01:10:47,973 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:10:47,973 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:10:47,973 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405065489083.e40f437ea2cd04f5e80b9966a93834b2., current region memstore size 541.2m
2014-07-11 01:10:47,973 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user5,1405065489083.2c9ec51d492f963529797ef469f19224. because compaction request was cancelled
2014-07-11 01:10:48,386 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 01:10:52,604 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5298, memsize=509.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/eb5771f4b40f453e95fd880183db2ebb
2014-07-11 01:10:52,622 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/.tmp/eb5771f4b40f453e95fd880183db2ebb as hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/eb5771f4b40f453e95fd880183db2ebb
2014-07-11 01:10:52,633 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/e468587ab0e54014075f4520dcd9c355/family/eb5771f4b40f453e95fd880183db2ebb, entries=1855680, sequenceid=5298, filesize=132.2m
2014-07-11 01:10:52,633 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~625.3m/655625280, currentsize=74.8m/78466080 for region usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. in 19635ms, sequenceid=5298, compaction requested=true
2014-07-11 01:10:52,634 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 01:10:52,634 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Selecting compaction from 18 store files, 0 compacting, 18 eligible, 20 blocking
2014-07-11 01:10:52,634 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 18 files from compaction candidates
2014-07-11 01:10:52,634 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 01:10:52,634 DEBUG [regionserver60020-smallCompactions-1405065148661] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 01:10:52,634 DEBUG [regionserver60020-smallCompactions-1405065148661] regionserver.CompactSplitThread: Not compacting usertable,user3,1405065489083.e468587ab0e54014075f4520dcd9c355. because compaction request was cancelled
