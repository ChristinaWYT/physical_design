Mon Jul 14 03:25:53 PDT 2014 Starting regionserver on sceplus-vm49
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 128203
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 32768
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 128203
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
2014-07-14 03:25:53,762 INFO  [main] util.VersionInfo: HBase 0.98.3-hadoop1
2014-07-14 03:25:53,763 INFO  [main] util.VersionInfo: Subversion git://acer/usr/src/Hadoop/hbase -r d5e65a9144e315bb0a964e7730871af32f5018d5
2014-07-14 03:25:53,763 INFO  [main] util.VersionInfo: Compiled by apurtell on Sat May 31 19:34:57 PDT 2014
2014-07-14 03:25:53,991 INFO  [main] util.ServerCommandLine: env:JAVA_HOME=/usr/lib/jvm/java-1.6.0-openjdk-amd64/
2014-07-14 03:25:53,991 INFO  [main] util.ServerCommandLine: env:SHLVL=3
2014-07-14 03:25:53,991 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_DIR=/home/hadoop/hbase/bin/../logs
2014-07-14 03:25:53,991 INFO  [main] util.ServerCommandLine: env:HBASE_HOME=/home/hadoop/hbase/bin/..
2014-07-14 03:25:53,991 INFO  [main] util.ServerCommandLine: env:HBASE_OPTS=-XX:+UseConcMarkSweepGC   -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm49.log -Dhbase.home.dir=/home/hadoop/hbase/bin/.. -Dhbase.id.str=hadoop -Dhbase.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64 -Dhbase.security.logger=INFO,RFAS
2014-07-14 03:25:53,991 INFO  [main] util.ServerCommandLine: env:HBASE_ENV_INIT=true
2014-07-14 03:25:53,991 INFO  [main] util.ServerCommandLine: env:SSH_CLIENT=9.1.143.58 37778 22
2014-07-14 03:25:53,991 INFO  [main] util.ServerCommandLine: env:HBASE_HEAPSIZE=10240
2014-07-14 03:25:53,992 INFO  [main] util.ServerCommandLine: env:MAIL=/var/mail/hadoop
2014-07-14 03:25:53,992 INFO  [main] util.ServerCommandLine: env:HBASE_ZNODE_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.znode
2014-07-14 03:25:53,992 INFO  [main] util.ServerCommandLine: env:PWD=/home/hadoop/hbase
2014-07-14 03:25:53,992 INFO  [main] util.ServerCommandLine: env:LOGNAME=hadoop
2014-07-14 03:25:53,992 INFO  [main] util.ServerCommandLine: env:HBASE_MANAGES_ZK=true
2014-07-14 03:25:53,992 INFO  [main] util.ServerCommandLine: env:HBASE_NICENESS=0
2014-07-14 03:25:53,992 INFO  [main] util.ServerCommandLine: env:HBASE_REST_OPTS=
2014-07-14 03:25:53,992 INFO  [main] util.ServerCommandLine: env:LD_LIBRARY_PATH=/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64/server:/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64:/usr/lib/jvm/java-6-openjdk-amd64/jre/../lib/amd64::/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-07-14 03:25:53,992 INFO  [main] util.ServerCommandLine: env:MALLOC_ARENA_MAX=4
2014-07-14 03:25:53,992 INFO  [main] util.ServerCommandLine: env:SSH_CONNECTION=9.1.143.58 37778 9.1.143.59 22
2014-07-14 03:25:53,993 INFO  [main] util.ServerCommandLine: env:SHELL=/bin/bash
2014-07-14 03:25:53,993 INFO  [main] util.ServerCommandLine: env:HBASE_PID_DIR=/var/hadoop/pids
2014-07-14 03:25:53,993 INFO  [main] util.ServerCommandLine: env:HBASE_ROOT_LOGGER=INFO,RFA
2014-07-14 03:25:53,995 INFO  [main] util.ServerCommandLine: env:CLASSPATH=/home/hadoop/hbase/bin/../conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-07-14 03:25:53,995 INFO  [main] util.ServerCommandLine: env:HBASE_THRIFT_OPTS=
2014-07-14 03:25:53,995 INFO  [main] util.ServerCommandLine: env:PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games
2014-07-14 03:25:53,995 INFO  [main] util.ServerCommandLine: env:USER=hadoop
2014-07-14 03:25:53,995 INFO  [main] util.ServerCommandLine: env:HBASE_SECURITY_LOGGER=INFO,RFAS
2014-07-14 03:25:53,995 INFO  [main] util.ServerCommandLine: env:HOME=/home/hadoop
2014-07-14 03:25:53,995 INFO  [main] util.ServerCommandLine: env:HBASE_LIBRARY_PATH=/home/hadoop/hbase/lib/native/Linux-amd64-64
2014-07-14 03:25:53,996 INFO  [main] util.ServerCommandLine: env:HBASE_START_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.autorestart
2014-07-14 03:25:53,996 INFO  [main] util.ServerCommandLine: env:XDG_SESSION_ID=982
2014-07-14 03:25:53,996 INFO  [main] util.ServerCommandLine: env:HBASE_LOGFILE=hbase-hadoop-regionserver-sceplus-vm49.log
2014-07-14 03:25:53,996 INFO  [main] util.ServerCommandLine: env:XDG_RUNTIME_DIR=/run/user/1001
2014-07-14 03:25:53,996 INFO  [main] util.ServerCommandLine: env:LANG=en_US.UTF-8
2014-07-14 03:25:53,996 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_PREFIX=hbase-hadoop-regionserver-sceplus-vm49
2014-07-14 03:25:53,996 INFO  [main] util.ServerCommandLine: env:HBASE_IDENT_STRING=hadoop
2014-07-14 03:25:53,998 INFO  [main] util.ServerCommandLine: vmName=OpenJDK 64-Bit Server VM, vmVendor=Sun Microsystems Inc., vmVersion=23.25-b01
2014-07-14 03:25:53,999 INFO  [main] util.ServerCommandLine: vmInputArguments=[-Dproc_regionserver, -XX:OnOutOfMemoryError=kill -9 %p, -Xmx10240m, -XX:+UseConcMarkSweepGC, -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs, -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm49.log, -Dhbase.home.dir=/home/hadoop/hbase/bin/.., -Dhbase.id.str=hadoop, -Dhbase.root.logger=INFO,RFA, -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64, -Dhbase.security.logger=INFO,RFAS]
2014-07-14 03:25:54,234 DEBUG [main] regionserver.HRegionServer: regionserver/sceplus-vm49.almaden.ibm.com/9.1.143.59:60020 HConnection server-to-server retries=350
2014-07-14 03:25:54,675 INFO  [main] ipc.RpcServer: regionserver/sceplus-vm49.almaden.ibm.com/9.1.143.59:60020: started 10 reader(s).
2014-07-14 03:25:54,796 INFO  [main] impl.MetricsConfig: loaded properties from hadoop-metrics2-hbase.properties
2014-07-14 03:25:54,809 INFO  [main] impl.MetricsSinkAdapter: Sink file-all started
2014-07-14 03:25:54,874 INFO  [main] impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.
2014-07-14 03:25:54,876 INFO  [main] impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2014-07-14 03:25:54,876 INFO  [main] impl.MetricsSystemImpl: HBase metrics system started
2014-07-14 03:25:54,881 INFO  [main] impl.MetricsSourceAdapter: MBean for source jvm registered.
2014-07-14 03:25:54,886 INFO  [main] impl.MetricsSourceAdapter: MBean for source IPC,sub=IPC registered.
2014-07-14 03:25:54,976 INFO  [main] impl.MetricsSourceAdapter: MBean for source ugi registered.
2014-07-14 03:25:54,976 WARN  [main] impl.MetricsSystemImpl: Source name ugi already exists!
2014-07-14 03:25:54,981 DEBUG [main] util.DirectMemoryUtils: Failed to retrieve nio.BufferPool direct MemoryUsed attribute.
javax.management.InstanceNotFoundException: java.nio:type=BufferPool,name=direct
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1117)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:678)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:682)
	at org.apache.hadoop.hbase.util.DirectMemoryUtils.<clinit>(DirectMemoryUtils.java:72)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.instantiateBlockCache(CacheConfig.java:396)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.<init>(CacheConfig.java:179)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:621)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:534)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2393)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:61)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:85)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2410)
2014-07-14 03:25:54,984 INFO  [main] hfile.CacheConfig: Allocating LruBlockCache with maximum size 4.0g
2014-07-14 03:25:55,059 INFO  [main] mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2014-07-14 03:25:55,122 INFO  [main] http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
2014-07-14 03:25:55,132 INFO  [main] http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 60030
2014-07-14 03:25:55,134 INFO  [main] http.HttpServer: listener.getLocalPort() returned 60030 webServer.getConnectors()[0].getLocalPort() returned 60030
2014-07-14 03:25:55,134 INFO  [main] http.HttpServer: Jetty bound to port 60030
2014-07-14 03:25:55,135 INFO  [main] mortbay.log: jetty-6.1.26
2014-07-14 03:25:55,458 INFO  [main] mortbay.log: Started SelectChannelConnector@0.0.0.0:60030
2014-07-14 03:25:55,510 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2014-07-14 03:25:55,511 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:host.name=sceplus-vm49.almaden.ibm.com
2014-07-14 03:25:55,511 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.version=1.6.0_31
2014-07-14 03:25:55,511 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.vendor=Sun Microsystems Inc.
2014-07-14 03:25:55,511 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-6-openjdk-amd64/jre
2014-07-14 03:25:55,511 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.class.path=/home/hadoop/hbase/bin/../conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-07-14 03:25:55,511 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-07-14 03:25:55,511 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
2014-07-14 03:25:55,511 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
2014-07-14 03:25:55,511 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.name=Linux
2014-07-14 03:25:55,511 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.arch=amd64
2014-07-14 03:25:55,511 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.version=3.13.0-24-generic
2014-07-14 03:25:55,511 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.name=hadoop
2014-07-14 03:25:55,511 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.home=/home/hadoop
2014-07-14 03:25:55,511 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.dir=/home/hadoop/hbase-0.98.3-hadoop1
2014-07-14 03:25:55,514 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=regionserver:60020, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-14 03:25:55,518 INFO  [main] regionserver.ShutdownHook: Installed shutdown hook thread: Shutdownhook:regionserver60020
2014-07-14 03:25:55,539 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=regionserver:60020 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-14 03:25:55,543 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-14 03:25:55,546 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-14 03:25:55,555 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, sessionid = 0x147346847d30001, negotiated timeout = 90000
2014-07-14 03:26:27,275 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x1c2d6398, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-14 03:26:27,276 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x1c2d6398 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-14 03:26:27,276 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-14 03:26:27,277 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-14 03:26:27,280 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, sessionid = 0x147346847d30003, negotiated timeout = 90000
2014-07-14 03:26:27,570 DEBUG [regionserver60020] catalog.CatalogTracker: Starting catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@4bbaa9f0
2014-07-14 03:26:27,573 INFO  [regionserver60020] regionserver.HRegionServer: ClusterId : 3b61b992-e8ee-43f8-b0c6-14cd23a8afbe
2014-07-14 03:26:27,579 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initializing
2014-07-14 03:26:27,594 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Node /hbase/online-snapshot/acquired already exists and this is not a retry
2014-07-14 03:26:27,631 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initialized
2014-07-14 03:26:27,638 INFO  [regionserver60020] regionserver.MemStoreFlusher: globalMemStoreLimit=4.0g, globalMemStoreLimitLowMark=3.8g, maxHeap=9.9g
2014-07-14 03:26:27,648 INFO  [regionserver60020] regionserver.HRegionServer: CompactionChecker runs every 10sec
2014-07-14 03:26:27,678 INFO  [regionserver60020] regionserver.HRegionServer: reportForDuty to master=sceplus-vm48.almaden.ibm.com,60000,1405333553204 with port=60020, startcode=1405333554898
2014-07-14 03:26:28,016 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: hbase.rootdir=hdfs://master:54310/hbase
2014-07-14 03:26:28,016 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: fs.default.name=hdfs://master:54310
2014-07-14 03:26:28,017 INFO  [regionserver60020] regionserver.HRegionServer: Master passed us a different hostname to use; was=sceplus-vm49.almaden.ibm.com, but now=slave1
2014-07-14 03:26:28,043 INFO  [regionserver60020] fs.HFileSystem: Added intercepting call to namenode#getBlockLocations so can do block reordering using class class org.apache.hadoop.hbase.fs.HFileSystem$ReorderWALBlocks
2014-07-14 03:26:28,053 DEBUG [regionserver60020] regionserver.HRegionServer: logdir=hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898
2014-07-14 03:26:28,094 DEBUG [regionserver60020] regionserver.Replication: ReplicationStatisticsThread 300
2014-07-14 03:26:28,105 INFO  [regionserver60020] wal.FSHLog: WAL/HLog configuration: blocksize=64 MB, rollsize=60.8 MB, enabled=true
2014-07-14 03:26:28,206 INFO  [regionserver60020] wal.FSHLog: New WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333588115
2014-07-14 03:26:28,224 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=WAL registered.
2014-07-14 03:26:28,229 INFO  [regionserver60020] regionserver.MetricsRegionServerWrapperImpl: Computing regionserver metrics every 5000 milliseconds
2014-07-14 03:26:28,233 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Server registered.
2014-07-14 03:26:28,237 INFO  [regionserver60020] trace.SpanReceiverHost: SpanReceiver org.cloudera.htrace.impl.LocalFileSpanReceiver was loaded successfully.
2014-07-14 03:26:28,240 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_REGION-slave1:60020, corePoolSize=3, maxPoolSize=3
2014-07-14 03:26:28,240 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_META-slave1:60020, corePoolSize=1, maxPoolSize=1
2014-07-14 03:26:28,240 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_REGION-slave1:60020, corePoolSize=3, maxPoolSize=3
2014-07-14 03:26:28,240 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_META-slave1:60020, corePoolSize=1, maxPoolSize=1
2014-07-14 03:26:28,240 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_LOG_REPLAY_OPS-slave1:60020, corePoolSize=2, maxPoolSize=2
2014-07-14 03:26:28,248 INFO  [regionserver60020] regionserver.ReplicationSourceManager: Current list of replicators: [sceplus-vm48.almaden.ibm.com,60020,1405333555121, slave1,60020,1405333554898] other RSs: [sceplus-vm48.almaden.ibm.com,60020,1405333555121, slave1,60020,1405333554898]
2014-07-14 03:26:28,271 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Replication registered.
2014-07-14 03:26:28,273 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x16b3237b, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-14 03:26:28,274 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x16b3237b connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-14 03:26:28,274 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Opening socket connection to server master/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-14 03:26:28,275 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Socket connection established to master/9.1.143.58:2181, initiating session
2014-07-14 03:26:28,279 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Session establishment complete on server master/9.1.143.58:2181, sessionid = 0x47346849080003, negotiated timeout = 90000
2014-07-14 03:26:28,286 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: starting
2014-07-14 03:26:28,286 INFO  [RpcServer.listener,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: starting
2014-07-14 03:26:28,331 INFO  [regionserver60020] regionserver.HRegionServer: Serving as slave1,60020,1405333554898, RpcServer on sceplus-vm49.almaden.ibm.com/9.1.143.59:60020, sessionid=0x147346847d30001
2014-07-14 03:26:28,331 INFO  [SplitLogWorker-slave1,60020,1405333554898] regionserver.SplitLogWorker: SplitLogWorker slave1,60020,1405333554898 starting
2014-07-14 03:26:28,332 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is starting
2014-07-14 03:26:28,332 DEBUG [regionserver60020] snapshot.RegionServerSnapshotManager: Start Snapshot Manager slave1,60020,1405333554898
2014-07-14 03:26:28,332 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Starting procedure member 'slave1,60020,1405333554898'
2014-07-14 03:26:28,332 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Checking for aborted procedures on node: '/hbase/online-snapshot/abort'
2014-07-14 03:26:28,333 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Looking for new procedures under znode:'/hbase/online-snapshot/acquired'
2014-07-14 03:26:28,334 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is started
2014-07-14 03:26:32,957 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user2,1405331185400.fd2af1df8ba9259ec0c538eeceae443e.
2014-07-14 03:26:33,094 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user3,1405331185400.5cdea4a8c4f1b79cac96dfb3d518efe1.
2014-07-14 03:26:33,094 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning fd2af1df8ba9259ec0c538eeceae443e from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 03:26:33,096 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-14 03:26:33,097 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 5cdea4a8c4f1b79cac96dfb3d518efe1 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 03:26:33,114 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user8,1405331185400.7f4e87d2eea7a637326e2204c730bf5a.
2014-07-14 03:26:33,115 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e5ee55a21ff19d69490518939b0887e0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 03:26:33,115 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user5,1405331185400.f874ab9cace3c84c3e27af574e5b4d27.
2014-07-14 03:26:33,115 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,,1405331185400.e09ff48a9fe69e3f6c8e6a97470cf37b.
2014-07-14 03:26:33,118 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node fd2af1df8ba9259ec0c538eeceae443e from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 03:26:33,118 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 5cdea4a8c4f1b79cac96dfb3d518efe1 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 03:26:33,120 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e5ee55a21ff19d69490518939b0887e0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 03:26:33,141 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => 5cdea4a8c4f1b79cac96dfb3d518efe1, NAME => 'usertable,user3,1405331185400.5cdea4a8c4f1b79cac96dfb3d518efe1.', STARTKEY => 'user3', ENDKEY => 'user4'}
2014-07-14 03:26:33,141 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => fd2af1df8ba9259ec0c538eeceae443e, NAME => 'usertable,user2,1405331185400.fd2af1df8ba9259ec0c538eeceae443e.', STARTKEY => 'user2', ENDKEY => 'user3'}
2014-07-14 03:26:33,141 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => e5ee55a21ff19d69490518939b0887e0, NAME => 'hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.', STARTKEY => '', ENDKEY => ''}
2014-07-14 03:26:33,169 INFO  [RS_OPEN_REGION-slave1:60020-0] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Regions registered.
2014-07-14 03:26:33,170 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable fd2af1df8ba9259ec0c538eeceae443e
2014-07-14 03:26:33,170 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 5cdea4a8c4f1b79cac96dfb3d518efe1
2014-07-14 03:26:33,170 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table namespace e5ee55a21ff19d69490518939b0887e0
2014-07-14 03:26:33,170 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,user2,1405331185400.fd2af1df8ba9259ec0c538eeceae443e.
2014-07-14 03:26:33,170 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user3,1405331185400.5cdea4a8c4f1b79cac96dfb3d518efe1.
2014-07-14 03:26:33,170 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-14 03:26:33,182 INFO  [RS_OPEN_REGION-slave1:60020-1] util.NativeCodeLoader: Loaded the native-hadoop library
2014-07-14 03:26:33,184 INFO  [RS_OPEN_REGION-slave1:60020-1] zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
2014-07-14 03:26:33,187 INFO  [RS_OPEN_REGION-slave1:60020-1] compress.CodecPool: Got brand-new compressor
2014-07-14 03:26:33,187 INFO  [RS_OPEN_REGION-slave1:60020-0] compress.CodecPool: Got brand-new compressor
2014-07-14 03:26:33,251 INFO  [StoreOpener-5cdea4a8c4f1b79cac96dfb3d518efe1-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-14 03:26:33,251 INFO  [StoreOpener-e5ee55a21ff19d69490518939b0887e0-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 604800000, major jitter 0.500000
2014-07-14 03:26:33,257 INFO  [StoreOpener-fd2af1df8ba9259ec0c538eeceae443e-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-14 03:26:33,285 INFO  [StoreFileOpenerThread-info-1] util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc32
2014-07-14 03:26:33,351 DEBUG [StoreOpener-e5ee55a21ff19d69490518939b0887e0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/hbase/namespace/e5ee55a21ff19d69490518939b0887e0/info/5b0102065d284f308d4c0a8d64d9fab5, isReference=false, isBulkLoadResult=false, seqid=4, majorCompaction=false
2014-07-14 03:26:33,401 INFO  [StoreFileOpenerThread-family-1] compress.CodecPool: Got brand-new decompressor
2014-07-14 03:26:33,404 DEBUG [StoreOpener-fd2af1df8ba9259ec0c538eeceae443e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/fd2af1df8ba9259ec0c538eeceae443e/family/0199c89cb3154940ba863e9565941484, isReference=false, isBulkLoadResult=false, seqid=15322, majorCompaction=false
2014-07-14 03:26:33,407 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/hbase/namespace/e5ee55a21ff19d69490518939b0887e0
2014-07-14 03:26:33,411 DEBUG [StoreOpener-5cdea4a8c4f1b79cac96dfb3d518efe1-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/5cdea4a8c4f1b79cac96dfb3d518efe1/family/06fc19c3af1247b3b6319c34d793aff9, isReference=false, isBulkLoadResult=false, seqid=8125, majorCompaction=false
2014-07-14 03:26:33,419 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined e5ee55a21ff19d69490518939b0887e0; next sequenceid=5
2014-07-14 03:26:33,419 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node e5ee55a21ff19d69490518939b0887e0
2014-07-14 03:26:33,423 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] regionserver.HRegionServer: Post open deploy tasks for region=hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-14 03:26:33,442 DEBUG [StoreOpener-fd2af1df8ba9259ec0c538eeceae443e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/fd2af1df8ba9259ec0c538eeceae443e/family/14290ffa364a4c92bb6b7d8c3d4259a7, isReference=false, isBulkLoadResult=false, seqid=18550, majorCompaction=false
2014-07-14 03:26:33,446 DEBUG [StoreOpener-5cdea4a8c4f1b79cac96dfb3d518efe1-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/5cdea4a8c4f1b79cac96dfb3d518efe1/family/097d8652b5f5413e97858f82dab4c36a, isReference=false, isBulkLoadResult=false, seqid=20870, majorCompaction=false
2014-07-14 03:26:33,538 DEBUG [StoreOpener-fd2af1df8ba9259ec0c538eeceae443e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/fd2af1df8ba9259ec0c538eeceae443e/family/16c81cb79f3b47ceb65f7af1f6ffa379, isReference=false, isBulkLoadResult=false, seqid=5200, majorCompaction=false
2014-07-14 03:26:33,542 DEBUG [StoreOpener-5cdea4a8c4f1b79cac96dfb3d518efe1-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/5cdea4a8c4f1b79cac96dfb3d518efe1/family/185e1a9ea88e4c54b1c083646bbea3ba, isReference=false, isBulkLoadResult=false, seqid=10838, majorCompaction=false
2014-07-14 03:26:33,574 DEBUG [StoreOpener-fd2af1df8ba9259ec0c538eeceae443e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/fd2af1df8ba9259ec0c538eeceae443e/family/1ed89338438041d4bbab49645bee8449, isReference=false, isBulkLoadResult=false, seqid=12047, majorCompaction=false
2014-07-14 03:26:33,576 DEBUG [StoreOpener-5cdea4a8c4f1b79cac96dfb3d518efe1-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/5cdea4a8c4f1b79cac96dfb3d518efe1/family/3b558572af19428d9bc7c40fae0c17b7, isReference=false, isBulkLoadResult=false, seqid=9222, majorCompaction=false
2014-07-14 03:26:33,600 DEBUG [StoreOpener-5cdea4a8c4f1b79cac96dfb3d518efe1-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/5cdea4a8c4f1b79cac96dfb3d518efe1/family/3ea214aca1df4285b7c0c269926b8960, isReference=false, isBulkLoadResult=false, seqid=20473, majorCompaction=false
2014-07-14 03:26:33,600 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] catalog.MetaEditor: Updated row hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0. with server=slave1,60020,1405333554898
2014-07-14 03:26:33,600 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] regionserver.HRegionServer: Finished post open deploy task for hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-14 03:26:33,601 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e5ee55a21ff19d69490518939b0887e0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 03:26:33,602 DEBUG [StoreOpener-fd2af1df8ba9259ec0c538eeceae443e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/fd2af1df8ba9259ec0c538eeceae443e/family/237c9d8bc7bd49a589a920c64a0cf6cb, isReference=false, isBulkLoadResult=false, seqid=5664, majorCompaction=false
2014-07-14 03:26:33,605 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e5ee55a21ff19d69490518939b0887e0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 03:26:33,605 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned e5ee55a21ff19d69490518939b0887e0 to OPENED in zk on slave1,60020,1405333554898
2014-07-14 03:26:33,606 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0. on slave1,60020,1405333554898
2014-07-14 03:26:33,606 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 7f4e87d2eea7a637326e2204c730bf5a from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 03:26:33,610 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 7f4e87d2eea7a637326e2204c730bf5a from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 03:26:33,610 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => 7f4e87d2eea7a637326e2204c730bf5a, NAME => 'usertable,user8,1405331185400.7f4e87d2eea7a637326e2204c730bf5a.', STARTKEY => 'user8', ENDKEY => 'user9'}
2014-07-14 03:26:33,611 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 7f4e87d2eea7a637326e2204c730bf5a
2014-07-14 03:26:33,611 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user8,1405331185400.7f4e87d2eea7a637326e2204c730bf5a.
2014-07-14 03:26:33,622 INFO  [StoreOpener-7f4e87d2eea7a637326e2204c730bf5a-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-14 03:26:33,629 DEBUG [StoreOpener-fd2af1df8ba9259ec0c538eeceae443e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/fd2af1df8ba9259ec0c538eeceae443e/family/25647c0be23c4b7788c835645397ef1c, isReference=false, isBulkLoadResult=false, seqid=6261, majorCompaction=false
2014-07-14 03:26:33,631 DEBUG [StoreOpener-5cdea4a8c4f1b79cac96dfb3d518efe1-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/5cdea4a8c4f1b79cac96dfb3d518efe1/family/415f49c9abb54236a6059529ed9fdd81, isReference=false, isBulkLoadResult=false, seqid=17593, majorCompaction=false
2014-07-14 03:26:33,652 DEBUG [StoreOpener-fd2af1df8ba9259ec0c538eeceae443e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/fd2af1df8ba9259ec0c538eeceae443e/family/2c2d8417e6404818b975445fd90e3ac1, isReference=false, isBulkLoadResult=false, seqid=17848, majorCompaction=false
2014-07-14 03:26:33,654 DEBUG [StoreOpener-5cdea4a8c4f1b79cac96dfb3d518efe1-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/5cdea4a8c4f1b79cac96dfb3d518efe1/family/4c50a526e0234ed695a89e7561d561f2, isReference=false, isBulkLoadResult=false, seqid=8716, majorCompaction=false
2014-07-14 03:26:33,678 DEBUG [StoreOpener-fd2af1df8ba9259ec0c538eeceae443e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/fd2af1df8ba9259ec0c538eeceae443e/family/321d34ad5a954eb99f8b6f025ea73a60, isReference=false, isBulkLoadResult=false, seqid=16493, majorCompaction=false
2014-07-14 03:26:33,679 DEBUG [StoreOpener-7f4e87d2eea7a637326e2204c730bf5a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/7f4e87d2eea7a637326e2204c730bf5a/family/03e6c897201c42f4be55e18b23f11dcb, isReference=false, isBulkLoadResult=false, seqid=15902, majorCompaction=false
2014-07-14 03:26:33,680 DEBUG [StoreOpener-5cdea4a8c4f1b79cac96dfb3d518efe1-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/5cdea4a8c4f1b79cac96dfb3d518efe1/family/5e0c07179b4f4cd0bfd39cae96201dad, isReference=false, isBulkLoadResult=false, seqid=16754, majorCompaction=false
2014-07-14 03:26:33,700 DEBUG [StoreOpener-fd2af1df8ba9259ec0c538eeceae443e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/fd2af1df8ba9259ec0c538eeceae443e/family/37d15ced13484d38bd0e25683eb753bc, isReference=false, isBulkLoadResult=false, seqid=15826, majorCompaction=false
2014-07-14 03:26:33,706 DEBUG [StoreOpener-7f4e87d2eea7a637326e2204c730bf5a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/7f4e87d2eea7a637326e2204c730bf5a/family/064fa18c9c58461ab0e7606cc6865d8d, isReference=false, isBulkLoadResult=false, seqid=9144, majorCompaction=false
2014-07-14 03:26:33,716 DEBUG [StoreOpener-5cdea4a8c4f1b79cac96dfb3d518efe1-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/5cdea4a8c4f1b79cac96dfb3d518efe1/family/83b46a02ba044306ac57e459923fc5ee, isReference=false, isBulkLoadResult=false, seqid=10284, majorCompaction=false
2014-07-14 03:26:33,716 DEBUG [StoreOpener-fd2af1df8ba9259ec0c538eeceae443e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/fd2af1df8ba9259ec0c538eeceae443e/family/3af780c5311a47818d06bd2ba0deee80, isReference=false, isBulkLoadResult=false, seqid=19951, majorCompaction=false
2014-07-14 03:26:33,726 DEBUG [StoreOpener-7f4e87d2eea7a637326e2204c730bf5a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/7f4e87d2eea7a637326e2204c730bf5a/family/0ada75bc29ec4759b30f16359097935a, isReference=false, isBulkLoadResult=false, seqid=20481, majorCompaction=false
2014-07-14 03:26:33,751 DEBUG [StoreOpener-fd2af1df8ba9259ec0c538eeceae443e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/fd2af1df8ba9259ec0c538eeceae443e/family/52c857a38a484232bbdb434da2b6c601, isReference=false, isBulkLoadResult=false, seqid=2080, majorCompaction=true
2014-07-14 03:26:33,757 DEBUG [StoreOpener-5cdea4a8c4f1b79cac96dfb3d518efe1-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/5cdea4a8c4f1b79cac96dfb3d518efe1/family/91b5546b51d74db28da15c89ee467f3e, isReference=false, isBulkLoadResult=false, seqid=16020, majorCompaction=false
2014-07-14 03:26:33,764 DEBUG [StoreOpener-7f4e87d2eea7a637326e2204c730bf5a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/7f4e87d2eea7a637326e2204c730bf5a/family/1d1dfa3a059d42ca8c3fdb47c302a3ee, isReference=false, isBulkLoadResult=false, seqid=16689, majorCompaction=false
2014-07-14 03:26:33,786 DEBUG [StoreOpener-5cdea4a8c4f1b79cac96dfb3d518efe1-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/5cdea4a8c4f1b79cac96dfb3d518efe1/family/9d5ef2bbdd9d4362b1edbdf9dbb3fabf, isReference=false, isBulkLoadResult=false, seqid=9699, majorCompaction=false
2014-07-14 03:26:33,805 DEBUG [StoreOpener-fd2af1df8ba9259ec0c538eeceae443e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/fd2af1df8ba9259ec0c538eeceae443e/family/5f36168043d04b54915caf036159ed28, isReference=false, isBulkLoadResult=false, seqid=11513, majorCompaction=false
2014-07-14 03:26:33,814 DEBUG [StoreOpener-7f4e87d2eea7a637326e2204c730bf5a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/7f4e87d2eea7a637326e2204c730bf5a/family/211eab77913b4dd795661ff72dbd6ff8, isReference=false, isBulkLoadResult=false, seqid=9535, majorCompaction=false
2014-07-14 03:26:33,822 DEBUG [StoreOpener-5cdea4a8c4f1b79cac96dfb3d518efe1-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/5cdea4a8c4f1b79cac96dfb3d518efe1/family/a6e687c088e2486eb3aed019945ea232, isReference=false, isBulkLoadResult=false, seqid=3387, majorCompaction=true
2014-07-14 03:26:33,847 DEBUG [StoreOpener-fd2af1df8ba9259ec0c538eeceae443e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/fd2af1df8ba9259ec0c538eeceae443e/family/643b11b0fc1d40c3a689dc4578d8bb27, isReference=false, isBulkLoadResult=false, seqid=14345, majorCompaction=false
2014-07-14 03:26:33,865 DEBUG [StoreOpener-5cdea4a8c4f1b79cac96dfb3d518efe1-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/5cdea4a8c4f1b79cac96dfb3d518efe1/family/a778297c325c4d6d84bd5b773ec43cbc, isReference=false, isBulkLoadResult=false, seqid=15652, majorCompaction=false
2014-07-14 03:26:33,868 DEBUG [StoreOpener-7f4e87d2eea7a637326e2204c730bf5a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/7f4e87d2eea7a637326e2204c730bf5a/family/426ef96c49254a58b811f31fa1491e98, isReference=false, isBulkLoadResult=false, seqid=18016, majorCompaction=false
2014-07-14 03:26:33,872 DEBUG [StoreOpener-fd2af1df8ba9259ec0c538eeceae443e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/fd2af1df8ba9259ec0c538eeceae443e/family/915013d897ce4a6d888db886ff6c3e68, isReference=false, isBulkLoadResult=false, seqid=2248, majorCompaction=false
2014-07-14 03:26:33,880 DEBUG [StoreOpener-7f4e87d2eea7a637326e2204c730bf5a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/7f4e87d2eea7a637326e2204c730bf5a/family/5d475845739d43e1b8ab42085cd41e57, isReference=false, isBulkLoadResult=false, seqid=17307, majorCompaction=false
2014-07-14 03:26:33,886 INFO  [StoreFileOpenerThread-family-1] compress.CodecPool: Got brand-new decompressor
2014-07-14 03:26:33,886 DEBUG [StoreOpener-5cdea4a8c4f1b79cac96dfb3d518efe1-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/5cdea4a8c4f1b79cac96dfb3d518efe1/family/c6a246182e90430c9fffc2de251fe2fd, isReference=false, isBulkLoadResult=false, seqid=14123, majorCompaction=false
2014-07-14 03:26:33,887 DEBUG [StoreOpener-fd2af1df8ba9259ec0c538eeceae443e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/fd2af1df8ba9259ec0c538eeceae443e/family/927118a6bb1045a2914e89135ae5c314, isReference=false, isBulkLoadResult=false, seqid=21029, majorCompaction=false
2014-07-14 03:26:33,914 DEBUG [StoreOpener-fd2af1df8ba9259ec0c538eeceae443e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/fd2af1df8ba9259ec0c538eeceae443e/family/a217cda44d994a5e862cc8f2206932e1, isReference=false, isBulkLoadResult=false, seqid=13849, majorCompaction=false
2014-07-14 03:26:33,919 DEBUG [StoreOpener-5cdea4a8c4f1b79cac96dfb3d518efe1-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/5cdea4a8c4f1b79cac96dfb3d518efe1/family/c8ce92bc0e9f4bff9b6b45071ff3ec66, isReference=false, isBulkLoadResult=false, seqid=11405, majorCompaction=false
2014-07-14 03:26:33,919 DEBUG [StoreOpener-7f4e87d2eea7a637326e2204c730bf5a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/7f4e87d2eea7a637326e2204c730bf5a/family/63be9a876d9b4eaabcdc9ea6100496b7, isReference=false, isBulkLoadResult=false, seqid=6522, majorCompaction=false
2014-07-14 03:26:33,928 DEBUG [StoreOpener-fd2af1df8ba9259ec0c538eeceae443e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/fd2af1df8ba9259ec0c538eeceae443e/family/a2707bd2f2d74543b155ef628c212691, isReference=false, isBulkLoadResult=false, seqid=18718, majorCompaction=false
2014-07-14 03:26:33,945 DEBUG [StoreOpener-7f4e87d2eea7a637326e2204c730bf5a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/7f4e87d2eea7a637326e2204c730bf5a/family/63c27f55635045f3b502b39c2602ad7f, isReference=false, isBulkLoadResult=false, seqid=5492, majorCompaction=false
2014-07-14 03:26:33,946 DEBUG [StoreOpener-fd2af1df8ba9259ec0c538eeceae443e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/fd2af1df8ba9259ec0c538eeceae443e/family/be306e2648ac4631bf760f19ee78b0eb, isReference=false, isBulkLoadResult=false, seqid=13279, majorCompaction=false
2014-07-14 03:26:33,954 DEBUG [StoreOpener-5cdea4a8c4f1b79cac96dfb3d518efe1-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/5cdea4a8c4f1b79cac96dfb3d518efe1/family/cab3364468e0438d861ac2c8dabbde37, isReference=false, isBulkLoadResult=false, seqid=14640, majorCompaction=false
2014-07-14 03:26:33,958 DEBUG [StoreOpener-7f4e87d2eea7a637326e2204c730bf5a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/7f4e87d2eea7a637326e2204c730bf5a/family/652ef9e9c1bb4ba6855ac12f35d985dc, isReference=false, isBulkLoadResult=false, seqid=21022, majorCompaction=false
2014-07-14 03:26:33,975 DEBUG [StoreOpener-5cdea4a8c4f1b79cac96dfb3d518efe1-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/5cdea4a8c4f1b79cac96dfb3d518efe1/family/d13f96e8bd864bb792cce97448934fe5, isReference=false, isBulkLoadResult=false, seqid=21027, majorCompaction=false
2014-07-14 03:26:33,980 DEBUG [StoreOpener-fd2af1df8ba9259ec0c538eeceae443e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/fd2af1df8ba9259ec0c538eeceae443e/family/ca69b55992994d9c8fc20213a8ab8d73, isReference=false, isBulkLoadResult=false, seqid=2415, majorCompaction=false
2014-07-14 03:26:33,994 DEBUG [StoreOpener-fd2af1df8ba9259ec0c538eeceae443e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/fd2af1df8ba9259ec0c538eeceae443e/family/cf57e75d7646443696f549bb3e7a940a, isReference=false, isBulkLoadResult=false, seqid=20989, majorCompaction=false
2014-07-14 03:26:33,995 DEBUG [StoreOpener-7f4e87d2eea7a637326e2204c730bf5a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/7f4e87d2eea7a637326e2204c730bf5a/family/8fc3572095c44b8dafb1fe179035591c, isReference=false, isBulkLoadResult=false, seqid=15013, majorCompaction=false
2014-07-14 03:26:34,002 DEBUG [StoreOpener-5cdea4a8c4f1b79cac96dfb3d518efe1-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/5cdea4a8c4f1b79cac96dfb3d518efe1/family/ebae0da248eb4d6ba738430fbf3cfeb9, isReference=false, isBulkLoadResult=false, seqid=15832, majorCompaction=false
2014-07-14 03:26:34,017 DEBUG [StoreOpener-fd2af1df8ba9259ec0c538eeceae443e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/fd2af1df8ba9259ec0c538eeceae443e/family/f3d6525daaa549a68b37582ca7b7e315, isReference=false, isBulkLoadResult=false, seqid=19420, majorCompaction=false
2014-07-14 03:26:34,039 DEBUG [StoreOpener-7f4e87d2eea7a637326e2204c730bf5a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/7f4e87d2eea7a637326e2204c730bf5a/family/a25d5d6c3a5c4b2b8d21014308e12d4f, isReference=false, isBulkLoadResult=false, seqid=17848, majorCompaction=false
2014-07-14 03:26:34,044 DEBUG [StoreOpener-5cdea4a8c4f1b79cac96dfb3d518efe1-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/5cdea4a8c4f1b79cac96dfb3d518efe1/family/ecad64f904434c04afadee4cc4000e24, isReference=false, isBulkLoadResult=false, seqid=11990, majorCompaction=false
2014-07-14 03:26:34,057 DEBUG [StoreOpener-fd2af1df8ba9259ec0c538eeceae443e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/fd2af1df8ba9259ec0c538eeceae443e/family/f8c771e9ff7743a38f7c65a324903301, isReference=false, isBulkLoadResult=false, seqid=12678, majorCompaction=false
2014-07-14 03:26:34,062 DEBUG [StoreOpener-5cdea4a8c4f1b79cac96dfb3d518efe1-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/5cdea4a8c4f1b79cac96dfb3d518efe1/family/ee72d6f2ec3d4c5dac3b902040f8fc67, isReference=false, isBulkLoadResult=false, seqid=19643, majorCompaction=false
2014-07-14 03:26:34,077 DEBUG [StoreOpener-7f4e87d2eea7a637326e2204c730bf5a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/7f4e87d2eea7a637326e2204c730bf5a/family/a4fc0db2c67344c09a7b7cc4071e753e, isReference=false, isBulkLoadResult=false, seqid=5977, majorCompaction=false
2014-07-14 03:26:34,077 DEBUG [StoreOpener-fd2af1df8ba9259ec0c538eeceae443e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/fd2af1df8ba9259ec0c538eeceae443e/family/fd166056397042a085a13fa0c7d58591, isReference=false, isBulkLoadResult=false, seqid=15993, majorCompaction=false
2014-07-14 03:26:34,085 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/fd2af1df8ba9259ec0c538eeceae443e
2014-07-14 03:26:34,088 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined fd2af1df8ba9259ec0c538eeceae443e; next sequenceid=21030
2014-07-14 03:26:34,088 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node fd2af1df8ba9259ec0c538eeceae443e
2014-07-14 03:26:34,090 INFO  [PostOpenDeployTasks:fd2af1df8ba9259ec0c538eeceae443e] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user2,1405331185400.fd2af1df8ba9259ec0c538eeceae443e.
2014-07-14 03:26:34,093 DEBUG [StoreOpener-5cdea4a8c4f1b79cac96dfb3d518efe1-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/5cdea4a8c4f1b79cac96dfb3d518efe1/family/f9d31adf23a743a0b9e58111f0d5fe83, isReference=false, isBulkLoadResult=false, seqid=20702, majorCompaction=false
2014-07-14 03:26:34,101 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/5cdea4a8c4f1b79cac96dfb3d518efe1
2014-07-14 03:26:34,102 DEBUG [PostOpenDeployTasks:fd2af1df8ba9259ec0c538eeceae443e] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:26:34,102 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 20 blocking
2014-07-14 03:26:34,104 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined 5cdea4a8c4f1b79cac96dfb3d518efe1; next sequenceid=21028
2014-07-14 03:26:34,104 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 5cdea4a8c4f1b79cac96dfb3d518efe1
2014-07-14 03:26:34,107 INFO  [PostOpenDeployTasks:5cdea4a8c4f1b79cac96dfb3d518efe1] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user3,1405331185400.5cdea4a8c4f1b79cac96dfb3d518efe1.
2014-07-14 03:26:34,108 DEBUG [PostOpenDeployTasks:5cdea4a8c4f1b79cac96dfb3d518efe1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-14 03:26:34,110 DEBUG [StoreOpener-7f4e87d2eea7a637326e2204c730bf5a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/7f4e87d2eea7a637326e2204c730bf5a/family/ad11889ca28a48cdb276b117afa89896, isReference=false, isBulkLoadResult=false, seqid=2344, majorCompaction=true
2014-07-14 03:26:34,112 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 340624181 starting at candidate #8 after considering 140 permutations with 115 in ratio
2014-07-14 03:26:34,114 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.HStore: fd2af1df8ba9259ec0c538eeceae443e - family: Initiating minor compaction
2014-07-14 03:26:34,114 INFO  [regionserver60020-smallCompactions-1405333594100] regionserver.HRegion: Starting compaction on family in region usertable,user2,1405331185400.fd2af1df8ba9259ec0c538eeceae443e.
2014-07-14 03:26:34,114 INFO  [regionserver60020-smallCompactions-1405333594100] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user2,1405331185400.fd2af1df8ba9259ec0c538eeceae443e. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/fd2af1df8ba9259ec0c538eeceae443e/.tmp, totalSize=324.8m
2014-07-14 03:26:34,115 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/fd2af1df8ba9259ec0c538eeceae443e/family/f8c771e9ff7743a38f7c65a324903301, keycount=122609, bloomtype=ROW, size=87.3m, encoding=NONE, seqNum=12678
2014-07-14 03:26:34,115 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/fd2af1df8ba9259ec0c538eeceae443e/family/be306e2648ac4631bf760f19ee78b0eb, keycount=209279, bloomtype=ROW, size=149.0m, encoding=NONE, seqNum=13279
2014-07-14 03:26:34,115 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/fd2af1df8ba9259ec0c538eeceae443e/family/a217cda44d994a5e862cc8f2206932e1, keycount=124260, bloomtype=ROW, size=88.5m, encoding=NONE, seqNum=13849
2014-07-14 03:26:34,118 INFO  [PostOpenDeployTasks:fd2af1df8ba9259ec0c538eeceae443e] catalog.MetaEditor: Updated row usertable,user2,1405331185400.fd2af1df8ba9259ec0c538eeceae443e. with server=slave1,60020,1405333554898
2014-07-14 03:26:34,119 INFO  [PostOpenDeployTasks:fd2af1df8ba9259ec0c538eeceae443e] regionserver.HRegionServer: Finished post open deploy task for usertable,user2,1405331185400.fd2af1df8ba9259ec0c538eeceae443e.
2014-07-14 03:26:34,120 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning fd2af1df8ba9259ec0c538eeceae443e from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 03:26:34,120 INFO  [PostOpenDeployTasks:5cdea4a8c4f1b79cac96dfb3d518efe1] catalog.MetaEditor: Updated row usertable,user3,1405331185400.5cdea4a8c4f1b79cac96dfb3d518efe1. with server=slave1,60020,1405333554898
2014-07-14 03:26:34,120 INFO  [PostOpenDeployTasks:5cdea4a8c4f1b79cac96dfb3d518efe1] regionserver.HRegionServer: Finished post open deploy task for usertable,user3,1405331185400.5cdea4a8c4f1b79cac96dfb3d518efe1.
2014-07-14 03:26:34,121 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 5cdea4a8c4f1b79cac96dfb3d518efe1 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 03:26:34,124 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node fd2af1df8ba9259ec0c538eeceae443e from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 03:26:34,124 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned fd2af1df8ba9259ec0c538eeceae443e to OPENED in zk on slave1,60020,1405333554898
2014-07-14 03:26:34,125 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 5cdea4a8c4f1b79cac96dfb3d518efe1 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 03:26:34,125 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,user2,1405331185400.fd2af1df8ba9259ec0c538eeceae443e. on slave1,60020,1405333554898
2014-07-14 03:26:34,125 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned 5cdea4a8c4f1b79cac96dfb3d518efe1 to OPENED in zk on slave1,60020,1405333554898
2014-07-14 03:26:34,125 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning f874ab9cace3c84c3e27af574e5b4d27 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 03:26:34,125 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user3,1405331185400.5cdea4a8c4f1b79cac96dfb3d518efe1. on slave1,60020,1405333554898
2014-07-14 03:26:34,126 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e09ff48a9fe69e3f6c8e6a97470cf37b from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 03:26:34,126 DEBUG [StoreOpener-7f4e87d2eea7a637326e2204c730bf5a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/7f4e87d2eea7a637326e2204c730bf5a/family/b5c40ac6a85b4a02912592b629dd092e, isReference=false, isBulkLoadResult=false, seqid=19037, majorCompaction=false
2014-07-14 03:26:34,129 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node f874ab9cace3c84c3e27af574e5b4d27 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 03:26:34,130 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e09ff48a9fe69e3f6c8e6a97470cf37b from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 03:26:34,130 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => f874ab9cace3c84c3e27af574e5b4d27, NAME => 'usertable,user5,1405331185400.f874ab9cace3c84c3e27af574e5b4d27.', STARTKEY => 'user5', ENDKEY => 'user6'}
2014-07-14 03:26:34,130 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => e09ff48a9fe69e3f6c8e6a97470cf37b, NAME => 'usertable,,1405331185400.e09ff48a9fe69e3f6c8e6a97470cf37b.', STARTKEY => '', ENDKEY => 'user1'}
2014-07-14 03:26:34,130 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable f874ab9cace3c84c3e27af574e5b4d27
2014-07-14 03:26:34,131 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,user5,1405331185400.f874ab9cace3c84c3e27af574e5b4d27.
2014-07-14 03:26:34,131 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable e09ff48a9fe69e3f6c8e6a97470cf37b
2014-07-14 03:26:34,131 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,,1405331185400.e09ff48a9fe69e3f6c8e6a97470cf37b.
2014-07-14 03:26:34,141 INFO  [StoreOpener-e09ff48a9fe69e3f6c8e6a97470cf37b-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-14 03:26:34,141 INFO  [StoreOpener-f874ab9cace3c84c3e27af574e5b4d27-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-14 03:26:34,146 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/e09ff48a9fe69e3f6c8e6a97470cf37b
2014-07-14 03:26:34,148 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined e09ff48a9fe69e3f6c8e6a97470cf37b; next sequenceid=1
2014-07-14 03:26:34,148 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node e09ff48a9fe69e3f6c8e6a97470cf37b
2014-07-14 03:26:34,150 INFO  [PostOpenDeployTasks:e09ff48a9fe69e3f6c8e6a97470cf37b] regionserver.HRegionServer: Post open deploy tasks for region=usertable,,1405331185400.e09ff48a9fe69e3f6c8e6a97470cf37b.
2014-07-14 03:26:34,154 DEBUG [StoreOpener-7f4e87d2eea7a637326e2204c730bf5a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/7f4e87d2eea7a637326e2204c730bf5a/family/c940c5b9e5a8437ba5e86b6aa7a29ad7, isReference=false, isBulkLoadResult=false, seqid=15695, majorCompaction=false
2014-07-14 03:26:34,159 INFO  [PostOpenDeployTasks:e09ff48a9fe69e3f6c8e6a97470cf37b] catalog.MetaEditor: Updated row usertable,,1405331185400.e09ff48a9fe69e3f6c8e6a97470cf37b. with server=slave1,60020,1405333554898
2014-07-14 03:26:34,159 INFO  [PostOpenDeployTasks:e09ff48a9fe69e3f6c8e6a97470cf37b] regionserver.HRegionServer: Finished post open deploy task for usertable,,1405331185400.e09ff48a9fe69e3f6c8e6a97470cf37b.
2014-07-14 03:26:34,160 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e09ff48a9fe69e3f6c8e6a97470cf37b from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 03:26:34,165 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e09ff48a9fe69e3f6c8e6a97470cf37b from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 03:26:34,165 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned e09ff48a9fe69e3f6c8e6a97470cf37b to OPENED in zk on slave1,60020,1405333554898
2014-07-14 03:26:34,165 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,,1405331185400.e09ff48a9fe69e3f6c8e6a97470cf37b. on slave1,60020,1405333554898
2014-07-14 03:26:34,166 DEBUG [StoreOpener-f874ab9cace3c84c3e27af574e5b4d27-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f874ab9cace3c84c3e27af574e5b4d27/family/055a90e66bb2470fa4410d79ba790d5a, isReference=false, isBulkLoadResult=false, seqid=10549, majorCompaction=false
2014-07-14 03:26:34,176 DEBUG [StoreOpener-7f4e87d2eea7a637326e2204c730bf5a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/7f4e87d2eea7a637326e2204c730bf5a/family/cf4c5e74e60e4031904ec99b9ba76440, isReference=false, isBulkLoadResult=false, seqid=19878, majorCompaction=false
2014-07-14 03:26:34,237 DEBUG [StoreOpener-7f4e87d2eea7a637326e2204c730bf5a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/7f4e87d2eea7a637326e2204c730bf5a/family/d382c38aff004178bbc81e32bfc10884, isReference=false, isBulkLoadResult=false, seqid=7067, majorCompaction=false
2014-07-14 03:26:34,238 DEBUG [regionserver60020-smallCompactions-1405333594100] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:26:34,240 DEBUG [StoreOpener-f874ab9cace3c84c3e27af574e5b4d27-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f874ab9cace3c84c3e27af574e5b4d27/family/1b4d44d9ce2448d787d885ec8419ffab, isReference=false, isBulkLoadResult=false, seqid=6315, majorCompaction=false
2014-07-14 03:26:34,262 DEBUG [StoreOpener-7f4e87d2eea7a637326e2204c730bf5a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/7f4e87d2eea7a637326e2204c730bf5a/family/e4714ba6bf094f48be31b1edf00d5bf1, isReference=false, isBulkLoadResult=false, seqid=15507, majorCompaction=false
2014-07-14 03:26:34,265 DEBUG [StoreOpener-f874ab9cace3c84c3e27af574e5b4d27-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f874ab9cace3c84c3e27af574e5b4d27/family/1c4dd76b0c39414791203e57ef58ff95, isReference=false, isBulkLoadResult=false, seqid=18915, majorCompaction=false
2014-07-14 03:26:34,326 DEBUG [StoreOpener-f874ab9cace3c84c3e27af574e5b4d27-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f874ab9cace3c84c3e27af574e5b4d27/family/28a71a409299425892e95273033d7bcb, isReference=false, isBulkLoadResult=false, seqid=9468, majorCompaction=false
2014-07-14 03:26:34,328 DEBUG [StoreOpener-7f4e87d2eea7a637326e2204c730bf5a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/7f4e87d2eea7a637326e2204c730bf5a/family/f01de928b17045a0a000fc4558682c0c, isReference=false, isBulkLoadResult=false, seqid=4962, majorCompaction=false
2014-07-14 03:26:34,349 DEBUG [StoreOpener-f874ab9cace3c84c3e27af574e5b4d27-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f874ab9cace3c84c3e27af574e5b4d27/family/2ad7aad5404449e68ae401cf9c9a2c28, isReference=false, isBulkLoadResult=false, seqid=14377, majorCompaction=false
2014-07-14 03:26:34,379 DEBUG [StoreOpener-f874ab9cace3c84c3e27af574e5b4d27-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f874ab9cace3c84c3e27af574e5b4d27/family/2c896d5e8b204d4cbc0f756b985ed184, isReference=false, isBulkLoadResult=false, seqid=12234, majorCompaction=false
2014-07-14 03:26:34,380 DEBUG [StoreOpener-7f4e87d2eea7a637326e2204c730bf5a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/7f4e87d2eea7a637326e2204c730bf5a/family/f4a9aac2605e44cbad07b997569b0ce5, isReference=false, isBulkLoadResult=false, seqid=18185, majorCompaction=false
2014-07-14 03:26:34,384 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/7f4e87d2eea7a637326e2204c730bf5a
2014-07-14 03:26:34,387 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined 7f4e87d2eea7a637326e2204c730bf5a; next sequenceid=21023
2014-07-14 03:26:34,387 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 7f4e87d2eea7a637326e2204c730bf5a
2014-07-14 03:26:34,389 INFO  [PostOpenDeployTasks:7f4e87d2eea7a637326e2204c730bf5a] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user8,1405331185400.7f4e87d2eea7a637326e2204c730bf5a.
2014-07-14 03:26:34,389 DEBUG [PostOpenDeployTasks:7f4e87d2eea7a637326e2204c730bf5a] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:2), split_queue=0, merge_queue=0
2014-07-14 03:26:34,395 DEBUG [StoreOpener-f874ab9cace3c84c3e27af574e5b4d27-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f874ab9cace3c84c3e27af574e5b4d27/family/2e40f34af51b450aa2cad025ade6dd25, isReference=false, isBulkLoadResult=false, seqid=12814, majorCompaction=false
2014-07-14 03:26:34,396 INFO  [PostOpenDeployTasks:7f4e87d2eea7a637326e2204c730bf5a] catalog.MetaEditor: Updated row usertable,user8,1405331185400.7f4e87d2eea7a637326e2204c730bf5a. with server=slave1,60020,1405333554898
2014-07-14 03:26:34,397 INFO  [PostOpenDeployTasks:7f4e87d2eea7a637326e2204c730bf5a] regionserver.HRegionServer: Finished post open deploy task for usertable,user8,1405331185400.7f4e87d2eea7a637326e2204c730bf5a.
2014-07-14 03:26:34,398 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 7f4e87d2eea7a637326e2204c730bf5a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 03:26:34,402 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 7f4e87d2eea7a637326e2204c730bf5a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 03:26:34,402 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned 7f4e87d2eea7a637326e2204c730bf5a to OPENED in zk on slave1,60020,1405333554898
2014-07-14 03:26:34,403 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user8,1405331185400.7f4e87d2eea7a637326e2204c730bf5a. on slave1,60020,1405333554898
2014-07-14 03:26:34,410 DEBUG [StoreOpener-f874ab9cace3c84c3e27af574e5b4d27-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f874ab9cace3c84c3e27af574e5b4d27/family/476477d25b2d47a6aa2c1908239b6a9c, isReference=false, isBulkLoadResult=false, seqid=15746, majorCompaction=false
2014-07-14 03:26:34,422 DEBUG [StoreOpener-f874ab9cace3c84c3e27af574e5b4d27-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f874ab9cace3c84c3e27af574e5b4d27/family/5b51702cf93e4501b45d7826c9669bee, isReference=false, isBulkLoadResult=false, seqid=7889, majorCompaction=false
2014-07-14 03:26:34,445 DEBUG [StoreOpener-f874ab9cace3c84c3e27af574e5b4d27-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f874ab9cace3c84c3e27af574e5b4d27/family/5e71e4cbe7934faf94e9cb2390621b29, isReference=false, isBulkLoadResult=false, seqid=16559, majorCompaction=false
2014-07-14 03:26:34,471 DEBUG [StoreOpener-f874ab9cace3c84c3e27af574e5b4d27-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f874ab9cace3c84c3e27af574e5b4d27/family/5fe96c37d07d4bc6a7d78ba9688c8e27, isReference=false, isBulkLoadResult=false, seqid=11685, majorCompaction=false
2014-07-14 03:26:34,494 DEBUG [StoreOpener-f874ab9cace3c84c3e27af574e5b4d27-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f874ab9cace3c84c3e27af574e5b4d27/family/60075c8a74a34b48aaee5113db11e4e6, isReference=false, isBulkLoadResult=false, seqid=15579, majorCompaction=false
2014-07-14 03:26:34,510 DEBUG [StoreOpener-f874ab9cace3c84c3e27af574e5b4d27-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f874ab9cace3c84c3e27af574e5b4d27/family/6f0d8cc2309d408a812b0764c8c4bda5, isReference=false, isBulkLoadResult=false, seqid=20625, majorCompaction=false
2014-07-14 03:26:34,547 DEBUG [StoreOpener-f874ab9cace3c84c3e27af574e5b4d27-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f874ab9cace3c84c3e27af574e5b4d27/family/79a968ec33ee4d0fad3b492c609aa496, isReference=false, isBulkLoadResult=false, seqid=17220, majorCompaction=false
2014-07-14 03:26:34,559 DEBUG [StoreOpener-f874ab9cace3c84c3e27af574e5b4d27-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f874ab9cace3c84c3e27af574e5b4d27/family/9a0cba8daeac4c00be37e355a01bd65e, isReference=false, isBulkLoadResult=false, seqid=21013, majorCompaction=false
2014-07-14 03:26:34,568 DEBUG [StoreOpener-f874ab9cace3c84c3e27af574e5b4d27-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f874ab9cace3c84c3e27af574e5b4d27/family/ba449af0a5ee408f906ddfa55c4d3a52, isReference=false, isBulkLoadResult=false, seqid=13389, majorCompaction=false
2014-07-14 03:26:34,590 DEBUG [StoreOpener-f874ab9cace3c84c3e27af574e5b4d27-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f874ab9cace3c84c3e27af574e5b4d27/family/c39512310e004251843b48e9966ed083, isReference=false, isBulkLoadResult=false, seqid=11135, majorCompaction=false
2014-07-14 03:26:34,607 DEBUG [StoreOpener-f874ab9cace3c84c3e27af574e5b4d27-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f874ab9cace3c84c3e27af574e5b4d27/family/cadb4eca65514b03b986228873287046, isReference=false, isBulkLoadResult=false, seqid=19882, majorCompaction=false
2014-07-14 03:26:34,622 DEBUG [StoreOpener-f874ab9cace3c84c3e27af574e5b4d27-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f874ab9cace3c84c3e27af574e5b4d27/family/db087f6d8de34bd4b87c1ccebb04e49e, isReference=false, isBulkLoadResult=false, seqid=13884, majorCompaction=false
2014-07-14 03:26:34,642 DEBUG [StoreOpener-f874ab9cace3c84c3e27af574e5b4d27-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f874ab9cace3c84c3e27af574e5b4d27/family/dbbc7c5baa924fcb939e3992f3907d32, isReference=false, isBulkLoadResult=false, seqid=10008, majorCompaction=false
2014-07-14 03:26:34,663 DEBUG [StoreOpener-f874ab9cace3c84c3e27af574e5b4d27-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f874ab9cace3c84c3e27af574e5b4d27/family/e509b05a45ba4c7288b8d83244e56573, isReference=false, isBulkLoadResult=false, seqid=2757, majorCompaction=true
2014-07-14 03:26:34,689 DEBUG [StoreOpener-f874ab9cace3c84c3e27af574e5b4d27-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f874ab9cace3c84c3e27af574e5b4d27/family/f1e6a90bcaf342f092de2d61948dfe71, isReference=false, isBulkLoadResult=false, seqid=18323, majorCompaction=false
2014-07-14 03:26:34,691 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/f874ab9cace3c84c3e27af574e5b4d27
2014-07-14 03:26:34,694 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined f874ab9cace3c84c3e27af574e5b4d27; next sequenceid=21014
2014-07-14 03:26:34,694 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node f874ab9cace3c84c3e27af574e5b4d27
2014-07-14 03:26:34,696 INFO  [PostOpenDeployTasks:f874ab9cace3c84c3e27af574e5b4d27] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user5,1405331185400.f874ab9cace3c84c3e27af574e5b4d27.
2014-07-14 03:26:34,696 DEBUG [PostOpenDeployTasks:f874ab9cace3c84c3e27af574e5b4d27] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:3), split_queue=0, merge_queue=0
2014-07-14 03:26:34,703 INFO  [PostOpenDeployTasks:f874ab9cace3c84c3e27af574e5b4d27] catalog.MetaEditor: Updated row usertable,user5,1405331185400.f874ab9cace3c84c3e27af574e5b4d27. with server=slave1,60020,1405333554898
2014-07-14 03:26:34,703 INFO  [PostOpenDeployTasks:f874ab9cace3c84c3e27af574e5b4d27] regionserver.HRegionServer: Finished post open deploy task for usertable,user5,1405331185400.f874ab9cace3c84c3e27af574e5b4d27.
2014-07-14 03:26:34,704 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning f874ab9cace3c84c3e27af574e5b4d27 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 03:26:34,715 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node f874ab9cace3c84c3e27af574e5b4d27 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 03:26:34,715 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned f874ab9cace3c84c3e27af574e5b4d27 to OPENED in zk on slave1,60020,1405333554898
2014-07-14 03:26:34,715 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,user5,1405331185400.f874ab9cace3c84c3e27af574e5b4d27. on slave1,60020,1405333554898
2014-07-14 03:26:38,244 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:4), split_queue=0, merge_queue=0
2014-07-14 03:26:38,245 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:5), split_queue=0, merge_queue=0
2014-07-14 03:26:38,245 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:6), split_queue=0, merge_queue=0
2014-07-14 03:26:38,245 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:7), split_queue=0, merge_queue=0
2014-07-14 03:27:02,539 INFO  [Priority.RpcServer.handler=2,port=60020] regionserver.HRegionServer: Close fd2af1df8ba9259ec0c538eeceae443e, via zk=yes, znode version=0, on null
2014-07-14 03:27:02,539 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Close f874ab9cace3c84c3e27af574e5b4d27, via zk=yes, znode version=0, on null
2014-07-14 03:27:02,540 INFO  [Priority.RpcServer.handler=4,port=60020] regionserver.HRegionServer: Close 5cdea4a8c4f1b79cac96dfb3d518efe1, via zk=yes, znode version=0, on null
2014-07-14 03:27:02,539 INFO  [Priority.RpcServer.handler=3,port=60020] regionserver.HRegionServer: Close e09ff48a9fe69e3f6c8e6a97470cf37b, via zk=yes, znode version=0, on null
2014-07-14 03:27:02,540 INFO  [Priority.RpcServer.handler=5,port=60020] regionserver.HRegionServer: Close 7f4e87d2eea7a637326e2204c730bf5a, via zk=yes, znode version=0, on null
2014-07-14 03:27:02,542 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Processing close of usertable,user5,1405331185400.f874ab9cace3c84c3e27af574e5b4d27.
2014-07-14 03:27:02,543 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Processing close of usertable,user3,1405331185400.5cdea4a8c4f1b79cac96dfb3d518efe1.
2014-07-14 03:27:02,543 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Processing close of usertable,user2,1405331185400.fd2af1df8ba9259ec0c538eeceae443e.
2014-07-14 03:27:02,545 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closing usertable,user5,1405331185400.f874ab9cace3c84c3e27af574e5b4d27.: disabling compactions & flushes
2014-07-14 03:27:02,546 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closing usertable,user3,1405331185400.5cdea4a8c4f1b79cac96dfb3d518efe1.: disabling compactions & flushes
2014-07-14 03:27:02,546 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Updates disabled for region usertable,user5,1405331185400.f874ab9cace3c84c3e27af574e5b4d27.
2014-07-14 03:27:02,546 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Updates disabled for region usertable,user3,1405331185400.5cdea4a8c4f1b79cac96dfb3d518efe1.
2014-07-14 03:27:02,547 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closing usertable,user2,1405331185400.fd2af1df8ba9259ec0c538eeceae443e.: disabling compactions & flushes
2014-07-14 03:27:02,547 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: waiting for 1 compactions to complete for region usertable,user2,1405331185400.fd2af1df8ba9259ec0c538eeceae443e.
2014-07-14 03:27:02,586 INFO  [StoreCloserThread-usertable,user3,1405331185400.5cdea4a8c4f1b79cac96dfb3d518efe1.-1] regionserver.HStore: Closed family
2014-07-14 03:27:02,587 INFO  [StoreCloserThread-usertable,user5,1405331185400.f874ab9cace3c84c3e27af574e5b4d27.-1] regionserver.HStore: Closed family
2014-07-14 03:27:02,590 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closed usertable,user3,1405331185400.5cdea4a8c4f1b79cac96dfb3d518efe1.
2014-07-14 03:27:02,590 INFO  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closed usertable,user5,1405331185400.f874ab9cace3c84c3e27af574e5b4d27.
2014-07-14 03:27:02,590 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 5cdea4a8c4f1b79cac96dfb3d518efe1 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-14 03:27:02,590 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning f874ab9cace3c84c3e27af574e5b4d27 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-14 03:27:02,597 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 5cdea4a8c4f1b79cac96dfb3d518efe1 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-14 03:27:02,597 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Set closed state in zk for usertable,user3,1405331185400.5cdea4a8c4f1b79cac96dfb3d518efe1. on slave1,60020,1405333554898
2014-07-14 03:27:02,597 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Closed usertable,user3,1405331185400.5cdea4a8c4f1b79cac96dfb3d518efe1.
2014-07-14 03:27:02,597 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node f874ab9cace3c84c3e27af574e5b4d27 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-14 03:27:02,597 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Set closed state in zk for usertable,user5,1405331185400.f874ab9cace3c84c3e27af574e5b4d27. on slave1,60020,1405333554898
2014-07-14 03:27:02,597 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Processing close of usertable,,1405331185400.e09ff48a9fe69e3f6c8e6a97470cf37b.
2014-07-14 03:27:02,598 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Closed usertable,user5,1405331185400.f874ab9cace3c84c3e27af574e5b4d27.
2014-07-14 03:27:02,598 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Processing close of usertable,user8,1405331185400.7f4e87d2eea7a637326e2204c730bf5a.
2014-07-14 03:27:02,599 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closing usertable,,1405331185400.e09ff48a9fe69e3f6c8e6a97470cf37b.: disabling compactions & flushes
2014-07-14 03:27:02,599 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Updates disabled for region usertable,,1405331185400.e09ff48a9fe69e3f6c8e6a97470cf37b.
2014-07-14 03:27:02,599 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closing usertable,user8,1405331185400.7f4e87d2eea7a637326e2204c730bf5a.: disabling compactions & flushes
2014-07-14 03:27:02,600 INFO  [StoreCloserThread-usertable,,1405331185400.e09ff48a9fe69e3f6c8e6a97470cf37b.-1] regionserver.HStore: Closed family
2014-07-14 03:27:02,600 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Updates disabled for region usertable,user8,1405331185400.7f4e87d2eea7a637326e2204c730bf5a.
2014-07-14 03:27:02,600 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closed usertable,,1405331185400.e09ff48a9fe69e3f6c8e6a97470cf37b.
2014-07-14 03:27:02,600 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e09ff48a9fe69e3f6c8e6a97470cf37b from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-14 03:27:02,604 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e09ff48a9fe69e3f6c8e6a97470cf37b from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-14 03:27:02,604 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Set closed state in zk for usertable,,1405331185400.e09ff48a9fe69e3f6c8e6a97470cf37b. on slave1,60020,1405333554898
2014-07-14 03:27:02,604 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Closed usertable,,1405331185400.e09ff48a9fe69e3f6c8e6a97470cf37b.
2014-07-14 03:27:02,604 INFO  [StoreCloserThread-usertable,user8,1405331185400.7f4e87d2eea7a637326e2204c730bf5a.-1] regionserver.HStore: Closed family
2014-07-14 03:27:02,605 INFO  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closed usertable,user8,1405331185400.7f4e87d2eea7a637326e2204c730bf5a.
2014-07-14 03:27:02,605 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 7f4e87d2eea7a637326e2204c730bf5a from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-14 03:27:02,612 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 7f4e87d2eea7a637326e2204c730bf5a from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-14 03:27:02,612 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Set closed state in zk for usertable,user8,1405331185400.7f4e87d2eea7a637326e2204c730bf5a. on slave1,60020,1405333554898
2014-07-14 03:27:02,612 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Closed usertable,user8,1405331185400.7f4e87d2eea7a637326e2204c730bf5a.
2014-07-14 03:27:02,671 INFO  [regionserver60020-smallCompactions-1405333594100] regionserver.HRegion: compaction interrupted
java.io.InterruptedIOException: Aborting compaction of store family in region usertable,user2,1405331185400.fd2af1df8ba9259ec0c538eeceae443e. because it was interrupted.
	at org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.compact(DefaultCompactor.java:81)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext.compact(DefaultStoreEngine.java:109)
	at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:1086)
	at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1481)
	at org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.run(CompactSplitThread.java:475)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-14 03:27:02,671 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Updates disabled for region usertable,user2,1405331185400.fd2af1df8ba9259ec0c538eeceae443e.
2014-07-14 03:27:02,677 INFO  [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Aborted compaction: Request = regionName=usertable,user2,1405331185400.fd2af1df8ba9259ec0c538eeceae443e., storeName=family, fileCount=3, fileSize=324.8m, priority=-3, time=287501540655238; duration=28sec
2014-07-14 03:27:02,677 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:7), split_queue=0, merge_queue=0
2014-07-14 03:27:02,677 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user8,1405331185400.7f4e87d2eea7a637326e2204c730bf5a. because compaction request was cancelled
2014-07-14 03:27:02,677 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405331185400.5cdea4a8c4f1b79cac96dfb3d518efe1. because compaction request was cancelled
2014-07-14 03:27:02,678 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user2,1405331185400.fd2af1df8ba9259ec0c538eeceae443e. because compaction request was cancelled
2014-07-14 03:27:02,678 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405331185400.f874ab9cace3c84c3e27af574e5b4d27. because compaction request was cancelled
2014-07-14 03:27:02,678 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405331185400.f874ab9cace3c84c3e27af574e5b4d27. because compaction request was cancelled
2014-07-14 03:27:02,678 INFO  [StoreCloserThread-usertable,user2,1405331185400.fd2af1df8ba9259ec0c538eeceae443e.-1] regionserver.HStore: Closed family
2014-07-14 03:27:02,678 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user8,1405331185400.7f4e87d2eea7a637326e2204c730bf5a. because compaction request was cancelled
2014-07-14 03:27:02,679 INFO  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closed usertable,user2,1405331185400.fd2af1df8ba9259ec0c538eeceae443e.
2014-07-14 03:27:02,679 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405331185400.5cdea4a8c4f1b79cac96dfb3d518efe1. because compaction request was cancelled
2014-07-14 03:27:02,679 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning fd2af1df8ba9259ec0c538eeceae443e from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-14 03:27:02,684 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node fd2af1df8ba9259ec0c538eeceae443e from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-14 03:27:02,684 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Set closed state in zk for usertable,user2,1405331185400.fd2af1df8ba9259ec0c538eeceae443e. on slave1,60020,1405333554898
2014-07-14 03:27:02,684 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Closed usertable,user2,1405331185400.fd2af1df8ba9259ec0c538eeceae443e.
2014-07-14 03:30:54,994 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=12794, hits=5369, hitRatio=41.96%, , cachingAccesses=5373, cachingHits=5368, cachingHitsRatio=99.90%, evictions=0, evicted=3, evictedPerRun=Infinity
2014-07-14 03:32:14,034 INFO  [Priority.RpcServer.handler=6,port=60020] regionserver.HRegionServer: Open usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:32:14,046 INFO  [Priority.RpcServer.handler=6,port=60020] regionserver.HRegionServer: Open usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:32:14,047 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 71c27911c72dddd675be84dade80b522 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 03:32:14,047 INFO  [Priority.RpcServer.handler=6,port=60020] regionserver.HRegionServer: Open usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:32:14,048 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning cb0b7b4d7da9eff851ee6768c30cfe8c from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 03:32:14,049 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning cd10ec3e9e9086b0a4afff5b7f27579b from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 03:32:14,049 INFO  [Priority.RpcServer.handler=6,port=60020] regionserver.HRegionServer: Open usertable,,1405333934035.0336d276e196289cfa77a2ae46169b31.
2014-07-14 03:32:14,049 INFO  [Priority.RpcServer.handler=6,port=60020] regionserver.HRegionServer: Open usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b.
2014-07-14 03:32:14,054 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 71c27911c72dddd675be84dade80b522 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 03:32:14,054 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => 71c27911c72dddd675be84dade80b522, NAME => 'usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.', STARTKEY => 'user4', ENDKEY => 'user5'}
2014-07-14 03:32:14,054 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node cb0b7b4d7da9eff851ee6768c30cfe8c from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 03:32:14,054 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => cb0b7b4d7da9eff851ee6768c30cfe8c, NAME => 'usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.', STARTKEY => 'user5', ENDKEY => 'user6'}
2014-07-14 03:32:14,055 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 71c27911c72dddd675be84dade80b522
2014-07-14 03:32:14,055 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable cb0b7b4d7da9eff851ee6768c30cfe8c
2014-07-14 03:32:14,055 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:32:14,055 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:32:14,056 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node cd10ec3e9e9086b0a4afff5b7f27579b from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 03:32:14,056 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => cd10ec3e9e9086b0a4afff5b7f27579b, NAME => 'usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.', STARTKEY => 'user3', ENDKEY => 'user4'}
2014-07-14 03:32:14,056 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable cd10ec3e9e9086b0a4afff5b7f27579b
2014-07-14 03:32:14,057 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:32:14,066 INFO  [StoreOpener-71c27911c72dddd675be84dade80b522-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-14 03:32:14,067 INFO  [StoreOpener-cb0b7b4d7da9eff851ee6768c30cfe8c-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-14 03:32:14,070 INFO  [StoreOpener-cd10ec3e9e9086b0a4afff5b7f27579b-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-14 03:32:14,072 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522
2014-07-14 03:32:14,072 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c
2014-07-14 03:32:14,074 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b
2014-07-14 03:32:14,075 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined 71c27911c72dddd675be84dade80b522; next sequenceid=1
2014-07-14 03:32:14,075 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 71c27911c72dddd675be84dade80b522
2014-07-14 03:32:14,075 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined cb0b7b4d7da9eff851ee6768c30cfe8c; next sequenceid=1
2014-07-14 03:32:14,075 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node cb0b7b4d7da9eff851ee6768c30cfe8c
2014-07-14 03:32:14,077 INFO  [PostOpenDeployTasks:71c27911c72dddd675be84dade80b522] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:32:14,077 INFO  [PostOpenDeployTasks:cb0b7b4d7da9eff851ee6768c30cfe8c] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:32:14,088 INFO  [PostOpenDeployTasks:cb0b7b4d7da9eff851ee6768c30cfe8c] catalog.MetaEditor: Updated row usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. with server=slave1,60020,1405333554898
2014-07-14 03:32:14,088 INFO  [PostOpenDeployTasks:cb0b7b4d7da9eff851ee6768c30cfe8c] regionserver.HRegionServer: Finished post open deploy task for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:32:14,089 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning cb0b7b4d7da9eff851ee6768c30cfe8c from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 03:32:14,092 INFO  [PostOpenDeployTasks:71c27911c72dddd675be84dade80b522] catalog.MetaEditor: Updated row usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. with server=slave1,60020,1405333554898
2014-07-14 03:32:14,092 INFO  [PostOpenDeployTasks:71c27911c72dddd675be84dade80b522] regionserver.HRegionServer: Finished post open deploy task for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:32:14,093 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 71c27911c72dddd675be84dade80b522 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 03:32:14,094 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node cb0b7b4d7da9eff851ee6768c30cfe8c from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 03:32:14,094 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned cb0b7b4d7da9eff851ee6768c30cfe8c to OPENED in zk on slave1,60020,1405333554898
2014-07-14 03:32:14,095 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. on slave1,60020,1405333554898
2014-07-14 03:32:14,095 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 0336d276e196289cfa77a2ae46169b31 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 03:32:14,098 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 71c27911c72dddd675be84dade80b522 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 03:32:14,098 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned 71c27911c72dddd675be84dade80b522 to OPENED in zk on slave1,60020,1405333554898
2014-07-14 03:32:14,098 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. on slave1,60020,1405333554898
2014-07-14 03:32:14,099 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 331f54c5b3020b63e58b94456232194b from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 03:32:14,100 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 0336d276e196289cfa77a2ae46169b31 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 03:32:14,101 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => 0336d276e196289cfa77a2ae46169b31, NAME => 'usertable,,1405333934035.0336d276e196289cfa77a2ae46169b31.', STARTKEY => '', ENDKEY => 'user1'}
2014-07-14 03:32:14,102 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 0336d276e196289cfa77a2ae46169b31
2014-07-14 03:32:14,102 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,,1405333934035.0336d276e196289cfa77a2ae46169b31.
2014-07-14 03:32:14,107 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined cd10ec3e9e9086b0a4afff5b7f27579b; next sequenceid=1
2014-07-14 03:32:14,107 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node cd10ec3e9e9086b0a4afff5b7f27579b
2014-07-14 03:32:14,109 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 331f54c5b3020b63e58b94456232194b from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 03:32:14,109 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => 331f54c5b3020b63e58b94456232194b, NAME => 'usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b.', STARTKEY => 'user9', ENDKEY => ''}
2014-07-14 03:32:14,110 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 331f54c5b3020b63e58b94456232194b
2014-07-14 03:32:14,110 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b.
2014-07-14 03:32:14,110 INFO  [PostOpenDeployTasks:cd10ec3e9e9086b0a4afff5b7f27579b] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:32:14,116 INFO  [StoreOpener-0336d276e196289cfa77a2ae46169b31-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-14 03:32:14,118 INFO  [PostOpenDeployTasks:cd10ec3e9e9086b0a4afff5b7f27579b] catalog.MetaEditor: Updated row usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. with server=slave1,60020,1405333554898
2014-07-14 03:32:14,118 INFO  [PostOpenDeployTasks:cd10ec3e9e9086b0a4afff5b7f27579b] regionserver.HRegionServer: Finished post open deploy task for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:32:14,119 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning cd10ec3e9e9086b0a4afff5b7f27579b from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 03:32:14,121 INFO  [StoreOpener-331f54c5b3020b63e58b94456232194b-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-14 03:32:14,123 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/0336d276e196289cfa77a2ae46169b31
2014-07-14 03:32:14,123 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node cd10ec3e9e9086b0a4afff5b7f27579b from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 03:32:14,123 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned cd10ec3e9e9086b0a4afff5b7f27579b to OPENED in zk on slave1,60020,1405333554898
2014-07-14 03:32:14,123 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. on slave1,60020,1405333554898
2014-07-14 03:32:14,126 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b
2014-07-14 03:32:14,127 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined 0336d276e196289cfa77a2ae46169b31; next sequenceid=1
2014-07-14 03:32:14,127 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 0336d276e196289cfa77a2ae46169b31
2014-07-14 03:32:14,129 INFO  [PostOpenDeployTasks:0336d276e196289cfa77a2ae46169b31] regionserver.HRegionServer: Post open deploy tasks for region=usertable,,1405333934035.0336d276e196289cfa77a2ae46169b31.
2014-07-14 03:32:14,136 INFO  [PostOpenDeployTasks:0336d276e196289cfa77a2ae46169b31] catalog.MetaEditor: Updated row usertable,,1405333934035.0336d276e196289cfa77a2ae46169b31. with server=slave1,60020,1405333554898
2014-07-14 03:32:14,137 INFO  [PostOpenDeployTasks:0336d276e196289cfa77a2ae46169b31] regionserver.HRegionServer: Finished post open deploy task for usertable,,1405333934035.0336d276e196289cfa77a2ae46169b31.
2014-07-14 03:32:14,138 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 0336d276e196289cfa77a2ae46169b31 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 03:32:14,141 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 0336d276e196289cfa77a2ae46169b31 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 03:32:14,142 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned 0336d276e196289cfa77a2ae46169b31 to OPENED in zk on slave1,60020,1405333554898
2014-07-14 03:32:14,142 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,,1405333934035.0336d276e196289cfa77a2ae46169b31. on slave1,60020,1405333554898
2014-07-14 03:32:14,166 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined 331f54c5b3020b63e58b94456232194b; next sequenceid=1
2014-07-14 03:32:14,166 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 331f54c5b3020b63e58b94456232194b
2014-07-14 03:32:14,168 INFO  [PostOpenDeployTasks:331f54c5b3020b63e58b94456232194b] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b.
2014-07-14 03:32:14,175 INFO  [PostOpenDeployTasks:331f54c5b3020b63e58b94456232194b] catalog.MetaEditor: Updated row usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. with server=slave1,60020,1405333554898
2014-07-14 03:32:14,175 INFO  [PostOpenDeployTasks:331f54c5b3020b63e58b94456232194b] regionserver.HRegionServer: Finished post open deploy task for usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b.
2014-07-14 03:32:14,176 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 331f54c5b3020b63e58b94456232194b from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 03:32:14,179 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x147346847d30001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 331f54c5b3020b63e58b94456232194b from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 03:32:14,179 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned 331f54c5b3020b63e58b94456232194b to OPENED in zk on slave1,60020,1405333554898
2014-07-14 03:32:14,179 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. on slave1,60020,1405333554898
2014-07-14 03:32:33,934 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:32:34,072 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 122 synced till here 85
2014-07-14 03:32:34,762 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333588115 with entries=122, filesize=98.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333953934
2014-07-14 03:32:35,904 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:32:35,922 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 238 synced till here 228
2014-07-14 03:32:36,089 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333953934 with entries=116, filesize=70.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333955905
2014-07-14 03:32:37,640 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:32:37,762 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 333 synced till here 326
2014-07-14 03:32:38,025 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333955905 with entries=95, filesize=68.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333957640
2014-07-14 03:32:39,848 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:32:39,870 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 440 synced till here 422
2014-07-14 03:32:40,220 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333957640 with entries=107, filesize=77.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333959848
2014-07-14 03:32:42,051 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:32:42,177 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 546 synced till here 525
2014-07-14 03:32:42,769 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333959848 with entries=106, filesize=79.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333962052
2014-07-14 03:32:43,855 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:32:43,858 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 261.1m
2014-07-14 03:32:45,069 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:32:45,141 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:32:45,407 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:32:45,407 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 266.0m
2014-07-14 03:32:45,442 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 665 synced till here 656
2014-07-14 03:32:45,624 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333962052 with entries=119, filesize=84.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333965141
2014-07-14 03:32:47,192 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:32:47,253 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:32:47,351 INFO  [MemStoreFlusher.0] compress.CodecPool: Got brand-new compressor
2014-07-14 03:32:47,352 INFO  [MemStoreFlusher.0] compress.CodecPool: Got brand-new compressor
2014-07-14 03:32:47,906 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:32:48,050 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 784 synced till here 762
2014-07-14 03:32:48,554 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333965141 with entries=119, filesize=83.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333967907
2014-07-14 03:32:50,230 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:32:50,710 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333967907 with entries=102, filesize=74.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333970230
2014-07-14 03:32:52,303 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:32:52,459 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1003 synced till here 986
2014-07-14 03:32:52,829 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333970230 with entries=117, filesize=76.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333972303
2014-07-14 03:32:54,546 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:32:54,671 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1117 synced till here 1086
2014-07-14 03:32:54,886 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=177, memsize=96.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/353e3a15c5db4c34b60cba40429ded9f
2014-07-14 03:32:54,911 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/353e3a15c5db4c34b60cba40429ded9f as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/353e3a15c5db4c34b60cba40429ded9f
2014-07-14 03:32:55,050 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/353e3a15c5db4c34b60cba40429ded9f, entries=349530, sequenceid=177, filesize=24.9m
2014-07-14 03:32:55,051 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~267.1m/280085920, currentsize=199.4m/209078880 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 11193ms, sequenceid=177, compaction requested=false
2014-07-14 03:32:55,055 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 397.3m
2014-07-14 03:32:55,353 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333972303 with entries=114, filesize=88.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333974547
2014-07-14 03:32:56,326 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:32:56,344 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:32:56,365 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1244 synced till here 1201
2014-07-14 03:32:57,085 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333974547 with entries=127, filesize=79.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333976345
2014-07-14 03:32:57,169 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=183, memsize=96.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/f1e97ce4cf88494bbab130ef72c6a72e
2014-07-14 03:32:57,265 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/f1e97ce4cf88494bbab130ef72c6a72e as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/f1e97ce4cf88494bbab130ef72c6a72e
2014-07-14 03:32:57,296 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/f1e97ce4cf88494bbab130ef72c6a72e, entries=350860, sequenceid=183, filesize=25.0m
2014-07-14 03:32:57,298 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~281.6m/295282880, currentsize=193.3m/202639040 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 11890ms, sequenceid=183, compaction requested=false
2014-07-14 03:32:58,056 DEBUG [RpcServer.handler=24,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:32:58,057 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 256.8m
2014-07-14 03:32:58,784 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:32:58,928 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:32:58,961 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1360 synced till here 1342
2014-07-14 03:32:59,091 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:32:59,279 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333976345 with entries=116, filesize=92.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333978928
2014-07-14 03:33:01,391 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:33:01,569 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1488 synced till here 1472
2014-07-14 03:33:01,877 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333978928 with entries=128, filesize=82.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333981391
2014-07-14 03:33:03,559 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:33:03,591 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1606 synced till here 1597
2014-07-14 03:33:03,928 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333981391 with entries=118, filesize=87.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333983560
2014-07-14 03:33:05,268 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=362, memsize=80.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/8e467deae9a24c83b7ecbd31c439410f
2014-07-14 03:33:05,356 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/8e467deae9a24c83b7ecbd31c439410f as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/8e467deae9a24c83b7ecbd31c439410f
2014-07-14 03:33:05,364 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=275, memsize=99.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/ac33b1b1f0374507b8218954078e9d70
2014-07-14 03:33:05,407 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/8e467deae9a24c83b7ecbd31c439410f, entries=292430, sequenceid=362, filesize=20.8m
2014-07-14 03:33:05,407 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~281.4m/295063680, currentsize=112.9m/118352640 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 7350ms, sequenceid=362, compaction requested=false
2014-07-14 03:33:05,408 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 380.4m
2014-07-14 03:33:05,412 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/ac33b1b1f0374507b8218954078e9d70 as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/ac33b1b1f0374507b8218954078e9d70
2014-07-14 03:33:05,446 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/ac33b1b1f0374507b8218954078e9d70, entries=363480, sequenceid=275, filesize=25.9m
2014-07-14 03:33:05,446 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~425.3m/445978560, currentsize=230.9m/242115840 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 10391ms, sequenceid=275, compaction requested=false
2014-07-14 03:33:05,628 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:33:05,652 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1712 synced till here 1701
2014-07-14 03:33:05,728 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333983560 with entries=106, filesize=67.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333985628
2014-07-14 03:33:06,063 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:33:08,805 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:33:08,805 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 257.4m
2014-07-14 03:33:09,195 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:33:09,949 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=431, memsize=81.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/c89fb1f0581740afb8d13d78d928c31f
2014-07-14 03:33:09,962 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/c89fb1f0581740afb8d13d78d928c31f as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/c89fb1f0581740afb8d13d78d928c31f
2014-07-14 03:33:09,974 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/c89fb1f0581740afb8d13d78d928c31f, entries=295120, sequenceid=431, filesize=21.0m
2014-07-14 03:33:09,974 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~385.2m/403869360, currentsize=31.1m/32651840 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 4567ms, sequenceid=431, compaction requested=false
2014-07-14 03:33:10,199 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:33:10,224 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333985628 with entries=91, filesize=61.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333990200
2014-07-14 03:33:12,745 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:33:13,693 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1940 synced till here 1929
2014-07-14 03:33:14,166 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=443, memsize=81.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/b27f4be55bf34cabb26f58bebee91de2
2014-07-14 03:33:14,223 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/b27f4be55bf34cabb26f58bebee91de2 as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/b27f4be55bf34cabb26f58bebee91de2
2014-07-14 03:33:14,239 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333990200 with entries=137, filesize=99.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333992746
2014-07-14 03:33:14,367 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/b27f4be55bf34cabb26f58bebee91de2, entries=295090, sequenceid=443, filesize=21.0m
2014-07-14 03:33:14,367 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~259.0m/271580240, currentsize=68.5m/71818720 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 5562ms, sequenceid=443, compaction requested=false
2014-07-14 03:33:15,792 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:33:15,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2034 synced till here 2030
2014-07-14 03:33:15,925 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333992746 with entries=94, filesize=65.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333995792
2014-07-14 03:33:17,255 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:33:17,256 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 259.2m
2014-07-14 03:33:17,602 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:33:18,130 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:33:18,145 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2125 synced till here 2123
2014-07-14 03:33:18,172 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333995792 with entries=91, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333998131
2014-07-14 03:33:21,131 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b.
2014-07-14 03:33:21,131 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b., current region memstore size 256.0m
2014-07-14 03:33:21,147 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:33:21,174 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333998131 with entries=88, filesize=61.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334001148
2014-07-14 03:33:21,401 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:33:24,828 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=554, memsize=58.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/b13c7a20e2a34951b999abc2c4bc4ca0
2014-07-14 03:33:24,877 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:33:24,890 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/b13c7a20e2a34951b999abc2c4bc4ca0 as hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/b13c7a20e2a34951b999abc2c4bc4ca0
2014-07-14 03:33:24,905 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/b13c7a20e2a34951b999abc2c4bc4ca0, entries=213680, sequenceid=554, filesize=15.2m
2014-07-14 03:33:24,905 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~257.1m/269587600, currentsize=9.8m/10280800 for region usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. in 3774ms, sequenceid=554, compaction requested=false
2014-07-14 03:33:24,909 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334001148 with entries=88, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334004878
2014-07-14 03:33:24,909 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333588115
2014-07-14 03:33:24,909 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333953934
2014-07-14 03:33:24,909 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333955905
2014-07-14 03:33:24,909 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333957640
2014-07-14 03:33:24,910 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333959848
2014-07-14 03:33:24,910 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333962052
2014-07-14 03:33:24,910 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333965141
2014-07-14 03:33:24,910 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333967907
2014-07-14 03:33:24,910 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333970230
2014-07-14 03:33:24,910 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333972303
2014-07-14 03:33:24,910 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333974547
2014-07-14 03:33:25,557 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=533, memsize=141.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/e6a8f584a42f4408aba0436f0dece4be
2014-07-14 03:33:25,569 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/e6a8f584a42f4408aba0436f0dece4be as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/e6a8f584a42f4408aba0436f0dece4be
2014-07-14 03:33:25,581 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/e6a8f584a42f4408aba0436f0dece4be, entries=514920, sequenceid=533, filesize=36.7m
2014-07-14 03:33:25,581 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~263.8m/276664480, currentsize=90.5m/94910000 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 8325ms, sequenceid=533, compaction requested=true
2014-07-14 03:33:25,582 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:33:25,583 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-14 03:33:25,583 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-14 03:33:25,583 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:33:25,583 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:33:25,583 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:33:27,474 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:33:27,554 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 261.8m
2014-07-14 03:33:27,555 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:33:27,582 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2407 synced till here 2391
2014-07-14 03:33:27,774 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334004878 with entries=106, filesize=80.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334007556
2014-07-14 03:33:27,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333976345
2014-07-14 03:33:27,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333978928
2014-07-14 03:33:27,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333981391
2014-07-14 03:33:27,944 DEBUG [RpcServer.handler=5,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:33:27,945 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 257.9m
2014-07-14 03:33:27,966 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:33:28,277 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:33:29,316 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:33:29,381 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2510 synced till here 2500
2014-07-14 03:33:30,099 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334007556 with entries=103, filesize=69.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334009316
2014-07-14 03:33:31,965 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:33:32,032 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2602 synced till here 2588
2014-07-14 03:33:32,211 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334009316 with entries=92, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334011966
2014-07-14 03:33:34,914 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:33:36,441 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:33:37,204 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2785 synced till here 2747
2014-07-14 03:33:38,035 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334011966 with entries=183, filesize=138.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334014915
2014-07-14 03:33:39,744 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:33:39,925 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2916 synced till here 2884
2014-07-14 03:33:40,527 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334014915 with entries=131, filesize=93.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334019744
2014-07-14 03:33:41,736 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:33:41,992 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3037 synced till here 3007
2014-07-14 03:33:42,401 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334019744 with entries=121, filesize=82.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334021737
2014-07-14 03:33:42,924 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=615, memsize=206.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/992d77ba9a984267832c6f19a4fdaf5c
2014-07-14 03:33:42,938 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/992d77ba9a984267832c6f19a4fdaf5c as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/992d77ba9a984267832c6f19a4fdaf5c
2014-07-14 03:33:43,046 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/992d77ba9a984267832c6f19a4fdaf5c, entries=752950, sequenceid=615, filesize=53.7m
2014-07-14 03:33:43,046 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~265.9m/278786240, currentsize=226.3m/237273120 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 15101ms, sequenceid=615, compaction requested=true
2014-07-14 03:33:43,047 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-14 03:33:43,047 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-14 03:33:43,047 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:33:43,047 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:33:43,047 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:33:43,047 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:33:43,048 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 400.4m
2014-07-14 03:33:43,104 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=609, memsize=216.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/e2384083583d4c79a38471757ee55f70
2014-07-14 03:33:43,206 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/e2384083583d4c79a38471757ee55f70 as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/e2384083583d4c79a38471757ee55f70
2014-07-14 03:33:43,220 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/e2384083583d4c79a38471757ee55f70, entries=789610, sequenceid=609, filesize=56.3m
2014-07-14 03:33:43,220 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~275.9m/289285840, currentsize=260.1m/272686880 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 15666ms, sequenceid=609, compaction requested=true
2014-07-14 03:33:43,222 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:33:43,222 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-14 03:33:43,222 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-14 03:33:43,222 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:33:43,222 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:33:43,222 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:33:43,371 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:33:43,373 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 264.8m
2014-07-14 03:33:44,075 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:33:44,111 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:33:44,127 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:33:44,242 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3177 synced till here 3149
2014-07-14 03:33:44,416 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:33:44,600 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334021737 with entries=140, filesize=98.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334024112
2014-07-14 03:33:44,601 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333983560
2014-07-14 03:33:44,601 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333985628
2014-07-14 03:33:44,601 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333990200
2014-07-14 03:33:44,601 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333992746
2014-07-14 03:33:46,457 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:33:46,610 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3291 synced till here 3270
2014-07-14 03:33:46,781 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334024112 with entries=114, filesize=78.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334026457
2014-07-14 03:33:48,425 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:33:48,561 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3394 synced till here 3383
2014-07-14 03:33:48,646 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334026457 with entries=103, filesize=69.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334028425
2014-07-14 03:33:50,526 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:33:50,560 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3489 synced till here 3483
2014-07-14 03:33:50,716 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334028425 with entries=95, filesize=68.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334030526
2014-07-14 03:33:51,887 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=787, memsize=92.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/9e5117529eda427f99371d50cc7a76c8
2014-07-14 03:33:52,008 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/9e5117529eda427f99371d50cc7a76c8 as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/9e5117529eda427f99371d50cc7a76c8
2014-07-14 03:33:52,021 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/9e5117529eda427f99371d50cc7a76c8, entries=335980, sequenceid=787, filesize=23.9m
2014-07-14 03:33:52,022 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~275.7m/289116800, currentsize=163.5m/171420880 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 8649ms, sequenceid=787, compaction requested=true
2014-07-14 03:33:52,023 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:33:52,023 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-14 03:33:52,023 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-14 03:33:52,023 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 423.3m
2014-07-14 03:33:52,023 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:33:52,023 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:33:52,023 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:33:52,262 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:33:52,344 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3584 synced till here 3577
2014-07-14 03:33:52,974 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:33:53,021 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334030526 with entries=95, filesize=67.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334032262
2014-07-14 03:33:54,508 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:33:55,432 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3722 synced till here 3721
2014-07-14 03:33:55,449 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334032262 with entries=138, filesize=93.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334034508
2014-07-14 03:33:56,465 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=811, memsize=159.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/730686f4480349f7a8911a76058478c4
2014-07-14 03:33:56,480 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/730686f4480349f7a8911a76058478c4 as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/730686f4480349f7a8911a76058478c4
2014-07-14 03:33:56,492 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/730686f4480349f7a8911a76058478c4, entries=582340, sequenceid=811, filesize=41.5m
2014-07-14 03:33:56,493 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~431.2m/452182160, currentsize=217.5m/228022240 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 13445ms, sequenceid=811, compaction requested=true
2014-07-14 03:33:56,493 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:33:56,493 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-14 03:33:56,493 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-14 03:33:56,493 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:33:56,494 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:33:56,494 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:33:57,015 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:33:57,015 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 257.1m
2014-07-14 03:33:57,376 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:33:57,473 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3812 synced till here 3808
2014-07-14 03:33:57,552 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334034508 with entries=90, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334037377
2014-07-14 03:33:57,553 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333995792
2014-07-14 03:33:57,553 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405333998131
2014-07-14 03:33:57,555 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:33:58,398 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:34:00,245 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:34:00,377 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3905 synced till here 3903
2014-07-14 03:34:00,414 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334037377 with entries=93, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334040246
2014-07-14 03:34:00,755 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=893, memsize=108.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/bb0945e3182b4d7094ee1e14cf2524aa
2014-07-14 03:34:00,774 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/bb0945e3182b4d7094ee1e14cf2524aa as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/bb0945e3182b4d7094ee1e14cf2524aa
2014-07-14 03:34:00,880 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/bb0945e3182b4d7094ee1e14cf2524aa, entries=394560, sequenceid=893, filesize=28.1m
2014-07-14 03:34:00,881 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~429.5m/450374160, currentsize=140.9m/147720320 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 8858ms, sequenceid=893, compaction requested=true
2014-07-14 03:34:00,882 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:34:00,882 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 284.2m
2014-07-14 03:34:00,882 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-14 03:34:00,882 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-14 03:34:00,882 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:34:00,883 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:34:00,883 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:34:01,614 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:34:02,182 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:34:02,209 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334040246 with entries=88, filesize=61.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334042183
2014-07-14 03:34:03,819 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:34:03,926 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4082 synced till here 4078
2014-07-14 03:34:03,976 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334042183 with entries=89, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334043820
2014-07-14 03:34:05,403 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=954, memsize=150.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/e60c8d9017714423aa4a337023fdcf7f
2014-07-14 03:34:05,420 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/e60c8d9017714423aa4a337023fdcf7f as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/e60c8d9017714423aa4a337023fdcf7f
2014-07-14 03:34:05,512 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/e60c8d9017714423aa4a337023fdcf7f, entries=548990, sequenceid=954, filesize=39.2m
2014-07-14 03:34:05,513 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~258.7m/271218480, currentsize=131.1m/137493120 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 8498ms, sequenceid=954, compaction requested=true
2014-07-14 03:34:05,513 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:34:05,513 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-14 03:34:05,513 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-14 03:34:05,514 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:34:05,514 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:34:05,514 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:34:06,193 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:34:06,218 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4174 synced till here 4172
2014-07-14 03:34:06,318 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334043820 with entries=92, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334046194
2014-07-14 03:34:07,094 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:34:07,100 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 256.1m
2014-07-14 03:34:07,409 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:34:07,752 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:34:07,777 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4264 synced till here 4263
2014-07-14 03:34:07,793 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334046194 with entries=90, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334047752
2014-07-14 03:34:11,102 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:34:12,405 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b.
2014-07-14 03:34:12,738 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=997, memsize=206.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/5aa1f1bab4954258a7cebe4984695942
2014-07-14 03:34:12,738 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4424 synced till here 4421
2014-07-14 03:34:12,752 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/5aa1f1bab4954258a7cebe4984695942 as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/5aa1f1bab4954258a7cebe4984695942
2014-07-14 03:34:12,761 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334047752 with entries=160, filesize=112.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334051103
2014-07-14 03:34:12,771 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/5aa1f1bab4954258a7cebe4984695942, entries=752820, sequenceid=997, filesize=53.7m
2014-07-14 03:34:12,771 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~287.2m/301191920, currentsize=186.2m/195215280 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 11889ms, sequenceid=997, compaction requested=true
2014-07-14 03:34:12,772 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:34:12,772 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-14 03:34:12,772 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-14 03:34:12,772 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:34:12,772 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:34:12,772 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b., current region memstore size 256.5m
2014-07-14 03:34:12,772 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:34:13,101 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:34:13,546 DEBUG [RpcServer.handler=46,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:34:16,434 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:34:16,457 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4515 synced till here 4512
2014-07-14 03:34:16,494 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334051103 with entries=91, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334056434
2014-07-14 03:34:16,892 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1061, memsize=239.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/1f32861f2e8a4dabbb0b15c9c420d2c7
2014-07-14 03:34:16,923 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/1f32861f2e8a4dabbb0b15c9c420d2c7 as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/1f32861f2e8a4dabbb0b15c9c420d2c7
2014-07-14 03:34:16,956 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/1f32861f2e8a4dabbb0b15c9c420d2c7, entries=870120, sequenceid=1061, filesize=62.0m
2014-07-14 03:34:16,956 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~259.2m/271764400, currentsize=121.0m/126860640 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 9856ms, sequenceid=1061, compaction requested=true
2014-07-14 03:34:16,956 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:34:16,957 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-14 03:34:16,957 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-14 03:34:16,957 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 285.6m
2014-07-14 03:34:16,957 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:34:16,957 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:34:16,957 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:34:17,205 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:34:18,807 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:34:19,013 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:34:19,127 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4613 synced till here 4612
2014-07-14 03:34:19,141 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1107, memsize=134.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/f3fe50315a9f400692a0ce9b01efd1dc
2014-07-14 03:34:19,146 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334056434 with entries=98, filesize=68.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334058807
2014-07-14 03:34:19,203 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/f3fe50315a9f400692a0ce9b01efd1dc as hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/f3fe50315a9f400692a0ce9b01efd1dc
2014-07-14 03:34:19,222 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/f3fe50315a9f400692a0ce9b01efd1dc, entries=489950, sequenceid=1107, filesize=34.9m
2014-07-14 03:34:19,222 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~257.0m/269490080, currentsize=21.7m/22726560 for region usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. in 6450ms, sequenceid=1107, compaction requested=false
2014-07-14 03:34:19,223 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 259.2m
2014-07-14 03:34:19,509 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:34:22,107 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:34:22,124 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4701 synced till here 4700
2014-07-14 03:34:22,143 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334058807 with entries=88, filesize=61.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334062107
2014-07-14 03:34:22,143 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334001148
2014-07-14 03:34:22,143 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334004878
2014-07-14 03:34:22,143 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334007556
2014-07-14 03:34:22,143 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334009316
2014-07-14 03:34:22,143 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334011966
2014-07-14 03:34:22,143 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334014915
2014-07-14 03:34:22,143 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334019744
2014-07-14 03:34:22,143 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334021737
2014-07-14 03:34:22,144 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334024112
2014-07-14 03:34:22,144 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334026457
2014-07-14 03:34:22,144 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334028425
2014-07-14 03:34:22,144 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334030526
2014-07-14 03:34:22,144 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334032262
2014-07-14 03:34:25,928 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:34:26,029 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334062107 with entries=86, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334065929
2014-07-14 03:34:27,533 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:34:27,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4877 synced till here 4875
2014-07-14 03:34:27,682 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334065929 with entries=90, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334067534
2014-07-14 03:34:27,730 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1139, memsize=273.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/828589d4475847b9a160260701fd28a4
2014-07-14 03:34:27,745 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/828589d4475847b9a160260701fd28a4 as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/828589d4475847b9a160260701fd28a4
2014-07-14 03:34:27,756 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/828589d4475847b9a160260701fd28a4, entries=994190, sequenceid=1139, filesize=70.8m
2014-07-14 03:34:27,757 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~285.6m/299493760, currentsize=136.3m/142916960 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 10800ms, sequenceid=1139, compaction requested=true
2014-07-14 03:34:27,757 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:34:27,757 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-14 03:34:27,757 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-14 03:34:27,757 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:34:27,758 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:34:27,758 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:34:27,760 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:34:27,760 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 256.0m
2014-07-14 03:34:28,052 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:34:29,463 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1167, memsize=262.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/40675e8fdb5d47a2a601dd49ecc9baa6
2014-07-14 03:34:29,475 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/40675e8fdb5d47a2a601dd49ecc9baa6 as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/40675e8fdb5d47a2a601dd49ecc9baa6
2014-07-14 03:34:29,484 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/40675e8fdb5d47a2a601dd49ecc9baa6, entries=954990, sequenceid=1167, filesize=68.0m
2014-07-14 03:34:29,484 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~262.3m/275029440, currentsize=122.5m/128451120 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 10261ms, sequenceid=1167, compaction requested=true
2014-07-14 03:34:29,485 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:34:29,485 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-14 03:34:29,485 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-14 03:34:29,485 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:34:29,485 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:34:29,485 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:34:30,033 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:34:30,051 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4966 synced till here 4965
2014-07-14 03:34:30,063 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334067534 with entries=89, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334070034
2014-07-14 03:34:30,063 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334034508
2014-07-14 03:34:30,063 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334037377
2014-07-14 03:34:30,063 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334040246
2014-07-14 03:34:30,063 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334042183
2014-07-14 03:34:30,063 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334043820
2014-07-14 03:34:32,526 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:34:32,561 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334070034 with entries=89, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334072526
2014-07-14 03:34:35,592 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:34:35,623 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334072526 with entries=87, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334075592
2014-07-14 03:34:36,899 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:34:36,900 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 256.9m
2014-07-14 03:34:37,137 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:34:37,555 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1228, memsize=257.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/98162aa5388543859174c8d0424f2f05
2014-07-14 03:34:37,609 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/98162aa5388543859174c8d0424f2f05 as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/98162aa5388543859174c8d0424f2f05
2014-07-14 03:34:37,622 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/98162aa5388543859174c8d0424f2f05, entries=937890, sequenceid=1228, filesize=66.9m
2014-07-14 03:34:37,622 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~257.6m/270103680, currentsize=130.4m/136722400 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 9862ms, sequenceid=1228, compaction requested=true
2014-07-14 03:34:37,623 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:34:37,623 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-14 03:34:37,623 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-14 03:34:37,623 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:34:37,623 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:34:37,623 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:34:37,830 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:34:37,848 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5231 synced till here 5229
2014-07-14 03:34:37,873 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334075592 with entries=89, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334077831
2014-07-14 03:34:37,873 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334046194
2014-07-14 03:34:37,873 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334047752
2014-07-14 03:34:40,735 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:34:40,736 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 257.2m
2014-07-14 03:34:41,270 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:34:41,291 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:34:42,431 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5358 synced till here 5356
2014-07-14 03:34:42,451 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334077831 with entries=127, filesize=89.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334081291
2014-07-14 03:34:45,483 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:34:45,506 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5446 synced till here 5445
2014-07-14 03:34:45,529 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334081291 with entries=88, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334085484
2014-07-14 03:34:46,411 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1306, memsize=258.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/d20abd7895cd4e3092ae285cdf1e6060
2014-07-14 03:34:46,427 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/d20abd7895cd4e3092ae285cdf1e6060 as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/d20abd7895cd4e3092ae285cdf1e6060
2014-07-14 03:34:46,439 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/d20abd7895cd4e3092ae285cdf1e6060, entries=940760, sequenceid=1306, filesize=67.0m
2014-07-14 03:34:46,440 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~258.4m/270933440, currentsize=105.4m/110482400 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 9540ms, sequenceid=1306, compaction requested=true
2014-07-14 03:34:46,440 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:34:46,440 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-14 03:34:46,440 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-14 03:34:46,440 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:34:46,441 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:34:46,441 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:34:50,257 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1335, memsize=260.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/5515144288f1463f82e69f2d2bc77682
2014-07-14 03:34:50,274 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/5515144288f1463f82e69f2d2bc77682 as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/5515144288f1463f82e69f2d2bc77682
2014-07-14 03:34:50,283 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/5515144288f1463f82e69f2d2bc77682, entries=947790, sequenceid=1335, filesize=67.5m
2014-07-14 03:34:50,283 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~260.3m/272958000, currentsize=76.0m/79655920 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 9547ms, sequenceid=1335, compaction requested=true
2014-07-14 03:34:50,284 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:34:50,284 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-14 03:34:50,284 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-14 03:34:50,284 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:34:50,284 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:34:50,284 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:35:13,317 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:35:13,444 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5534 synced till here 5533
2014-07-14 03:35:13,451 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334085484 with entries=88, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334113317
2014-07-14 03:35:13,679 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:35:13,679 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 261.1m
2014-07-14 03:35:13,834 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:35:17,519 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:35:17,620 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5624 synced till here 5623
2014-07-14 03:35:17,629 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334113317 with entries=90, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334117519
2014-07-14 03:35:19,826 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:35:19,845 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5717 synced till here 5705
2014-07-14 03:35:20,125 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334117519 with entries=93, filesize=70.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334119827
2014-07-14 03:35:22,138 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:35:22,319 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5840 synced till here 5809
2014-07-14 03:35:22,568 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334119827 with entries=123, filesize=88.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334122138
2014-07-14 03:35:22,813 DEBUG [RpcServer.handler=15,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:35:22,817 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 259.6m
2014-07-14 03:35:23,073 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:35:24,346 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:35:24,473 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334122138 with entries=97, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334124347
2014-07-14 03:35:24,657 DEBUG [RpcServer.handler=37,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:35:26,079 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:35:26,102 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6026 synced till here 6025
2014-07-14 03:35:26,209 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334124347 with entries=89, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334126080
2014-07-14 03:35:26,637 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1398, memsize=262.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/36b718cf776042ddb22d4021c961eebd
2014-07-14 03:35:26,661 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/36b718cf776042ddb22d4021c961eebd as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/36b718cf776042ddb22d4021c961eebd
2014-07-14 03:35:26,762 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/36b718cf776042ddb22d4021c961eebd, entries=956310, sequenceid=1398, filesize=68.1m
2014-07-14 03:35:26,762 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~262.7m/275410560, currentsize=187.1m/196148400 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 13083ms, sequenceid=1398, compaction requested=true
2014-07-14 03:35:26,763 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:35:26,763 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-14 03:35:26,763 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-14 03:35:26,763 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:35:26,763 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 298.6m
2014-07-14 03:35:26,763 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:35:26,764 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:35:27,211 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:35:27,748 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:35:27,898 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6123 synced till here 6119
2014-07-14 03:35:28,038 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334126080 with entries=97, filesize=69.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334127861
2014-07-14 03:35:29,590 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:35:29,609 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6219 synced till here 6212
2014-07-14 03:35:29,735 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334127861 with entries=96, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334129591
2014-07-14 03:35:29,915 DEBUG [RpcServer.handler=19,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:35:31,457 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:35:31,733 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6321 synced till here 6304
2014-07-14 03:35:31,913 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334129591 with entries=102, filesize=74.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334131457
2014-07-14 03:35:33,977 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:35:34,128 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6432 synced till here 6405
2014-07-14 03:35:34,510 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334131457 with entries=111, filesize=81.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334133977
2014-07-14 03:35:36,054 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:35:36,071 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6531 synced till here 6520
2014-07-14 03:35:36,281 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334133977 with entries=99, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334136055
2014-07-14 03:35:38,223 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:35:38,321 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6632 synced till here 6619
2014-07-14 03:35:38,607 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334136055 with entries=101, filesize=74.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334138224
2014-07-14 03:35:38,785 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b.
2014-07-14 03:35:39,776 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1479, memsize=231.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/1bec6a25f035497a98a39e631842c4a2
2014-07-14 03:35:39,799 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/1bec6a25f035497a98a39e631842c4a2 as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/1bec6a25f035497a98a39e631842c4a2
2014-07-14 03:35:39,870 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/1bec6a25f035497a98a39e631842c4a2, entries=842430, sequenceid=1479, filesize=60.0m
2014-07-14 03:35:39,870 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~267.4m/280427040, currentsize=317.3m/332699040 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 17053ms, sequenceid=1479, compaction requested=true
2014-07-14 03:35:39,871 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:35:39,871 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 20 blocking
2014-07-14 03:35:39,871 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 445.6m
2014-07-14 03:35:39,872 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-14 03:35:39,872 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:35:39,872 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:35:39,872 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:35:39,874 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:35:40,222 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:35:40,238 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6730 synced till here 6729
2014-07-14 03:35:40,252 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334138224 with entries=98, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334140222
2014-07-14 03:35:40,832 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:35:42,074 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:35:42,486 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6833 synced till here 6817
2014-07-14 03:35:42,717 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334140222 with entries=103, filesize=74.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334142075
2014-07-14 03:35:44,323 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:35:44,351 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6926 synced till here 6922
2014-07-14 03:35:44,481 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334142075 with entries=93, filesize=66.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334144324
2014-07-14 03:35:44,653 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1529, memsize=222.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/9c59a9cfe2db4a479bedc6ce84708fa3
2014-07-14 03:35:44,678 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/9c59a9cfe2db4a479bedc6ce84708fa3 as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/9c59a9cfe2db4a479bedc6ce84708fa3
2014-07-14 03:35:44,696 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/9c59a9cfe2db4a479bedc6ce84708fa3, entries=809000, sequenceid=1529, filesize=57.6m
2014-07-14 03:35:44,697 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~300.1m/314675200, currentsize=343.8m/360549200 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 17934ms, sequenceid=1529, compaction requested=true
2014-07-14 03:35:44,697 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:35:44,697 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 20 blocking
2014-07-14 03:35:44,697 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-14 03:35:44,697 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b., current region memstore size 294.5m
2014-07-14 03:35:44,698 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:35:44,698 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:35:44,698 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:35:44,986 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:35:45,098 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:35:46,685 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:35:46,812 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7027 synced till here 7018
2014-07-14 03:35:46,991 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334144324 with entries=101, filesize=68.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334146686
2014-07-14 03:35:49,097 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:35:49,208 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7130 synced till here 7116
2014-07-14 03:35:49,485 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334146686 with entries=103, filesize=76.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334149097
2014-07-14 03:35:51,218 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:35:51,554 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7238 synced till here 7217
2014-07-14 03:35:52,140 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334149097 with entries=108, filesize=75.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334151219
2014-07-14 03:35:54,348 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:35:55,020 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=12794, hits=5369, hitRatio=41.96%, , cachingAccesses=5373, cachingHits=5368, cachingHitsRatio=99.90%, evictions=0, evicted=3, evictedPerRun=Infinity
2014-07-14 03:35:55,115 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7348 synced till here 7316
2014-07-14 03:35:55,796 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334151219 with entries=110, filesize=83.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334154348
2014-07-14 03:35:58,180 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:35:58,398 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7472 synced till here 7438
2014-07-14 03:35:59,077 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334154348 with entries=124, filesize=86.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334158181
2014-07-14 03:36:01,172 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1686, memsize=172.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/e77bf307db494670aac8d92e8a2cf784
2014-07-14 03:36:01,330 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/e77bf307db494670aac8d92e8a2cf784 as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/e77bf307db494670aac8d92e8a2cf784
2014-07-14 03:36:01,353 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/e77bf307db494670aac8d92e8a2cf784, entries=628380, sequenceid=1686, filesize=44.8m
2014-07-14 03:36:01,354 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~447.2m/468897760, currentsize=311.5m/326635200 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 21483ms, sequenceid=1686, compaction requested=true
2014-07-14 03:36:01,355 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 20 blocking
2014-07-14 03:36:01,355 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-14 03:36:01,356 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:36:01,356 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:36:01,356 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:36:01,356 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:36:01,356 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 644.9m
2014-07-14 03:36:01,660 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:36:01,676 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7597 synced till here 7564
2014-07-14 03:36:01,966 DEBUG [RpcServer.handler=41,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:36:02,140 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334158181 with entries=125, filesize=94.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334161660
2014-07-14 03:36:03,316 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:36:04,442 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:36:04,692 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7721 synced till here 7702
2014-07-14 03:36:05,132 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334161660 with entries=124, filesize=86.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334164442
2014-07-14 03:36:05,431 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1736, memsize=201.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/a3f83e92375b4a379d1839a08b998891
2014-07-14 03:36:05,447 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/a3f83e92375b4a379d1839a08b998891 as hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/a3f83e92375b4a379d1839a08b998891
2014-07-14 03:36:05,460 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/a3f83e92375b4a379d1839a08b998891, entries=732840, sequenceid=1736, filesize=52.2m
2014-07-14 03:36:05,461 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~295.0m/309277920, currentsize=79.8m/83670880 for region usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. in 20764ms, sequenceid=1736, compaction requested=true
2014-07-14 03:36:05,461 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:36:05,461 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-14 03:36:05,461 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-14 03:36:05,461 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:36:05,461 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 669.6m
2014-07-14 03:36:05,461 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:36:05,462 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. because compaction request was cancelled
2014-07-14 03:36:06,856 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:36:07,540 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7865 synced till here 7845
2014-07-14 03:36:07,821 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:36:08,045 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334164442 with entries=144, filesize=100.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334166856
2014-07-14 03:36:08,045 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334051103
2014-07-14 03:36:08,045 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334056434
2014-07-14 03:36:08,046 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334058807
2014-07-14 03:36:08,046 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334062107
2014-07-14 03:36:08,046 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334065929
2014-07-14 03:36:08,046 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334067534
2014-07-14 03:36:08,046 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334070034
2014-07-14 03:36:08,047 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334072526
2014-07-14 03:36:08,047 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334075592
2014-07-14 03:36:08,047 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334077831
2014-07-14 03:36:08,047 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334081291
2014-07-14 03:36:08,049 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334085484
2014-07-14 03:36:08,049 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334113317
2014-07-14 03:36:08,049 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334117519
2014-07-14 03:36:08,049 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334119827
2014-07-14 03:36:09,920 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:36:10,287 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8009 synced till here 7971
2014-07-14 03:36:10,557 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334166856 with entries=144, filesize=96.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334169920
2014-07-14 03:36:12,254 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:36:12,339 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8124 synced till here 8093
2014-07-14 03:36:12,860 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334169920 with entries=115, filesize=86.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334172254
2014-07-14 03:36:14,932 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:36:14,951 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8229 synced till here 8220
2014-07-14 03:36:15,149 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334172254 with entries=105, filesize=68.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334174933
2014-07-14 03:36:17,170 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:36:17,203 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334174933 with entries=92, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334177171
2014-07-14 03:36:19,764 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:36:19,909 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8410 synced till here 8407
2014-07-14 03:36:19,963 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334177171 with entries=89, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334179765
2014-07-14 03:36:22,046 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:36:22,416 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8517 synced till here 8509
2014-07-14 03:36:22,461 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334179765 with entries=107, filesize=76.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334182047
2014-07-14 03:36:22,499 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1915, memsize=238.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/d8566ce322664481809552f449d3b244
2014-07-14 03:36:22,516 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/d8566ce322664481809552f449d3b244 as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/d8566ce322664481809552f449d3b244
2014-07-14 03:36:22,591 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/d8566ce322664481809552f449d3b244, entries=868460, sequenceid=1915, filesize=61.8m
2014-07-14 03:36:22,592 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~673.1m/705809280, currentsize=350.6m/367601520 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 21236ms, sequenceid=1915, compaction requested=true
2014-07-14 03:36:22,592 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:36:22,592 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 20 blocking
2014-07-14 03:36:22,592 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-14 03:36:22,592 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 705.2m
2014-07-14 03:36:22,592 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:36:22,593 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:36:22,593 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:36:22,613 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:36:24,336 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:36:24,480 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1972, memsize=209.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/a7534e2bd3134ac1a467fd89f0b0f688
2014-07-14 03:36:24,486 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:36:24,498 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/a7534e2bd3134ac1a467fd89f0b0f688 as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/a7534e2bd3134ac1a467fd89f0b0f688
2014-07-14 03:36:24,506 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8615 synced till here 8604
2014-07-14 03:36:24,514 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/a7534e2bd3134ac1a467fd89f0b0f688, entries=762240, sequenceid=1972, filesize=54.3m
2014-07-14 03:36:24,630 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~688.3m/721768160, currentsize=305.5m/320319600 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 19169ms, sequenceid=1972, compaction requested=true
2014-07-14 03:36:24,630 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:36:24,630 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 20 blocking
2014-07-14 03:36:24,631 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-14 03:36:24,631 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:36:24,631 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 383.4m
2014-07-14 03:36:24,631 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:36:24,631 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:36:24,678 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:36:24,713 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334182047 with entries=98, filesize=69.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334184486
2014-07-14 03:36:24,713 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334122138
2014-07-14 03:36:24,713 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334124347
2014-07-14 03:36:24,713 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334126080
2014-07-14 03:36:24,713 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334127861
2014-07-14 03:36:24,713 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334129591
2014-07-14 03:36:24,713 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334131457
2014-07-14 03:36:24,713 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334133977
2014-07-14 03:36:24,713 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334136055
2014-07-14 03:36:25,602 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:36:26,646 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:36:26,669 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8714 synced till here 8700
2014-07-14 03:36:26,987 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334184486 with entries=99, filesize=73.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334186647
2014-07-14 03:36:29,098 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:36:29,283 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8824 synced till here 8814
2014-07-14 03:36:29,724 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334186647 with entries=110, filesize=76.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334189099
2014-07-14 03:36:31,060 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:36:31,382 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8933 synced till here 8930
2014-07-14 03:36:31,393 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334189099 with entries=109, filesize=74.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334191061
2014-07-14 03:36:33,087 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:36:33,106 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9026 synced till here 9022
2014-07-14 03:36:33,287 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334191061 with entries=93, filesize=65.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334193087
2014-07-14 03:36:35,500 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:36:35,516 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9117 synced till here 9116
2014-07-14 03:36:35,616 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334193087 with entries=91, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334195501
2014-07-14 03:36:36,697 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b.
2014-07-14 03:36:37,278 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:36:37,305 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9210 synced till here 9205
2014-07-14 03:36:37,439 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334195501 with entries=93, filesize=66.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334197279
2014-07-14 03:36:39,213 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:36:39,358 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9302 synced till here 9299
2014-07-14 03:36:39,398 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334197279 with entries=92, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334199214
2014-07-14 03:36:40,841 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2165, memsize=182.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/2b5240a7ee5244e6844febc09012d291
2014-07-14 03:36:40,864 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/2b5240a7ee5244e6844febc09012d291 as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/2b5240a7ee5244e6844febc09012d291
2014-07-14 03:36:40,882 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/2b5240a7ee5244e6844febc09012d291, entries=662830, sequenceid=2165, filesize=47.3m
2014-07-14 03:36:40,883 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~388.0m/406894960, currentsize=295.6m/309937040 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 16252ms, sequenceid=2165, compaction requested=true
2014-07-14 03:36:40,883 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:36:40,883 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 20 blocking
2014-07-14 03:36:40,883 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-14 03:36:40,884 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:36:40,884 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:36:40,884 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 600.8m
2014-07-14 03:36:40,884 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:36:41,189 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:36:41,394 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:36:41,412 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9392 synced till here 9390
2014-07-14 03:36:41,536 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334199214 with entries=90, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334201394
2014-07-14 03:36:41,595 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:36:43,353 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:36:43,373 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9482 synced till here 9479
2014-07-14 03:36:43,409 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334201394 with entries=90, filesize=64.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334203354
2014-07-14 03:36:45,287 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:36:45,390 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9576 synced till here 9571
2014-07-14 03:36:45,438 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334203354 with entries=94, filesize=66.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334205288
2014-07-14 03:36:47,051 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:36:47,296 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334205288 with entries=101, filesize=70.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334207051
2014-07-14 03:36:48,737 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2141, memsize=239.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/75b9cce52fc641319c27cd213410151d
2014-07-14 03:36:48,770 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/75b9cce52fc641319c27cd213410151d as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/75b9cce52fc641319c27cd213410151d
2014-07-14 03:36:48,787 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/75b9cce52fc641319c27cd213410151d, entries=871690, sequenceid=2141, filesize=62.1m
2014-07-14 03:36:48,788 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~711.4m/745981040, currentsize=465.1m/487673840 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 26196ms, sequenceid=2141, compaction requested=true
2014-07-14 03:36:48,788 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:36:48,788 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 20 blocking
2014-07-14 03:36:48,789 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-14 03:36:48,789 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b., current region memstore size 320.7m
2014-07-14 03:36:48,789 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:36:48,789 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:36:48,789 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:36:48,803 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:36:49,263 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:36:49,412 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9774 synced till here 9762
2014-07-14 03:36:49,531 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334207051 with entries=97, filesize=71.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334209263
2014-07-14 03:36:49,532 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334138224
2014-07-14 03:36:49,532 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334140222
2014-07-14 03:36:49,532 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334142075
2014-07-14 03:36:49,660 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:36:51,410 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:36:51,429 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9868 synced till here 9862
2014-07-14 03:36:51,554 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334209263 with entries=94, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334211410
2014-07-14 03:36:53,317 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:36:53,341 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9959 synced till here 9956
2014-07-14 03:36:53,381 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334211410 with entries=91, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334213317
2014-07-14 03:36:55,957 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:36:56,111 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10053 synced till here 10048
2014-07-14 03:36:56,166 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334213317 with entries=94, filesize=67.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334215958
2014-07-14 03:36:58,721 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:36:58,871 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10157 synced till here 10143
2014-07-14 03:36:59,183 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334215958 with entries=104, filesize=74.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334218722
2014-07-14 03:37:01,286 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:37:01,706 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10262 synced till here 10238
2014-07-14 03:37:15,044 WARN  [regionserver60020] util.Sleeper: We slept 13065ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-14 03:37:15,102 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 12883ms
GC pool 'ParNew' had collection(s): count=2 time=204ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=12935ms
2014-07-14 03:37:15,143 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334218722 with entries=105, filesize=78.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334221287
2014-07-14 03:37:15,243 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16261,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334218844,"queuetimems":1,"class":"HRegionServer","responsesize":13073,"method":"Multi"}
2014-07-14 03:37:15,244 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15706,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334219415,"queuetimems":0,"class":"HRegionServer","responsesize":13393,"method":"Multi"}
2014-07-14 03:37:15,243 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15947,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334219166,"queuetimems":0,"class":"HRegionServer","responsesize":13166,"method":"Multi"}
2014-07-14 03:37:15,244 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4168 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,284 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,284 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4163 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,285 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,285 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4161 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,285 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,285 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15918,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334219195,"queuetimems":0,"class":"HRegionServer","responsesize":13249,"method":"Multi"}
2014-07-14 03:37:15,285 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4162 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,285 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,321 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15858,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334219462,"queuetimems":0,"class":"HRegionServer","responsesize":12941,"method":"Multi"}
2014-07-14 03:37:15,321 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15709,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334219611,"queuetimems":0,"class":"HRegionServer","responsesize":12671,"method":"Multi"}
2014-07-14 03:37:15,321 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15882,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334219438,"queuetimems":0,"class":"HRegionServer","responsesize":12967,"method":"Multi"}
2014-07-14 03:37:15,321 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15735,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334219585,"queuetimems":1,"class":"HRegionServer","responsesize":13135,"method":"Multi"}
2014-07-14 03:37:15,321 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4159 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,322 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,322 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4156 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,322 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,322 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4153 service: ClientService methodName: Multi size: 2.2m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,322 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,322 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4160 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,322 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,435 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15641,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334219794,"queuetimems":1,"class":"HRegionServer","responsesize":13430,"method":"Multi"}
2014-07-14 03:37:15,436 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4151 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,436 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,445 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15680,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334219764,"queuetimems":0,"class":"HRegionServer","responsesize":12908,"method":"Multi"}
2014-07-14 03:37:15,445 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4152 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,445 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,447 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15348,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334220098,"queuetimems":2,"class":"HRegionServer","responsesize":13156,"method":"Multi"}
2014-07-14 03:37:15,447 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4142 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,447 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,473 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15351,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334220122,"queuetimems":0,"class":"HRegionServer","responsesize":13102,"method":"Multi"}
2014-07-14 03:37:15,474 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4139 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,474 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,473 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15519,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334219954,"queuetimems":0,"class":"HRegionServer","responsesize":12535,"method":"Multi"}
2014-07-14 03:37:15,474 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4148 service: ClientService methodName: Multi size: 2.2m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,474 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,497 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15565,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334219931,"queuetimems":0,"class":"HRegionServer","responsesize":13056,"method":"Multi"}
2014-07-14 03:37:15,497 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4149 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,497 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,500 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15589,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334219911,"queuetimems":0,"class":"HRegionServer","responsesize":13097,"method":"Multi"}
2014-07-14 03:37:15,500 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4150 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,500 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,501 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14979,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334220521,"queuetimems":1,"class":"HRegionServer","responsesize":12996,"method":"Multi"}
2014-07-14 03:37:15,501 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4134 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,501 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,504 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15146,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334220357,"queuetimems":4,"class":"HRegionServer","responsesize":12910,"method":"Multi"}
2014-07-14 03:37:15,504 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15112,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334220391,"queuetimems":1,"class":"HRegionServer","responsesize":13325,"method":"Multi"}
2014-07-14 03:37:15,504 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4138 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,504 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,504 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4136 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,504 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,510 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15144,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334220365,"queuetimems":0,"class":"HRegionServer","responsesize":13071,"method":"Multi"}
2014-07-14 03:37:15,510 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4137 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,510 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,700 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14913,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334220787,"queuetimems":0,"class":"HRegionServer","responsesize":12535,"method":"Multi"}
2014-07-14 03:37:15,701 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4246 service: ClientService methodName: Multi size: 2.2m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,701 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,749 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15179,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334220570,"queuetimems":1,"class":"HRegionServer","responsesize":12910,"method":"Multi"}
2014-07-14 03:37:15,750 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4249 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,750 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,749 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14948,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334220801,"queuetimems":3,"class":"HRegionServer","responsesize":13097,"method":"Multi"}
2014-07-14 03:37:15,749 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15207,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334220542,"queuetimems":1,"class":"HRegionServer","responsesize":12729,"method":"Multi"}
2014-07-14 03:37:15,749 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14804,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334220944,"queuetimems":0,"class":"HRegionServer","responsesize":13102,"method":"Multi"}
2014-07-14 03:37:15,750 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4243 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,750 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,750 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4133 service: ClientService methodName: Multi size: 2.2m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,750 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,750 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4242 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,751 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,774 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14469,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334221304,"queuetimems":1,"class":"HRegionServer","responsesize":12967,"method":"Multi"}
2014-07-14 03:37:15,774 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4230 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,774 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,787 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14826,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334220960,"queuetimems":0,"class":"HRegionServer","responsesize":13156,"method":"Multi"}
2014-07-14 03:37:15,787 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4241 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,787 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,788 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14029,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334221758,"queuetimems":1,"class":"HRegionServer","responsesize":12945,"method":"Multi"}
2014-07-14 03:37:15,788 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4219 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,788 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,818 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14828,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334220990,"queuetimems":0,"class":"HRegionServer","responsesize":13056,"method":"Multi"}
2014-07-14 03:37:15,820 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4240 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,820 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,839 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14710,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334221128,"queuetimems":0,"class":"HRegionServer","responsesize":13430,"method":"Multi"}
2014-07-14 03:37:15,839 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14692,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334221146,"queuetimems":1,"class":"HRegionServer","responsesize":12908,"method":"Multi"}
2014-07-14 03:37:15,839 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4237 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,840 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,840 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4234 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,840 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,899 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:37:15,910 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13923,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334221987,"queuetimems":0,"class":"HRegionServer","responsesize":13166,"method":"Multi"}
2014-07-14 03:37:15,910 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4216 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,910 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,916 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10361 synced till here 10355
2014-07-14 03:37:15,919 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14199,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334221720,"queuetimems":0,"class":"HRegionServer","responsesize":13249,"method":"Multi"}
2014-07-14 03:37:15,919 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14243,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334221676,"queuetimems":1,"class":"HRegionServer","responsesize":12941,"method":"Multi"}
2014-07-14 03:37:15,919 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4212 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,919 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14253,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334221666,"queuetimems":0,"class":"HRegionServer","responsesize":13135,"method":"Multi"}
2014-07-14 03:37:15,919 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,919 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4226 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,920 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,920 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14633,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334221286,"queuetimems":0,"class":"HRegionServer","responsesize":12671,"method":"Multi"}
2014-07-14 03:37:15,920 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4227 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,920 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,920 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4222 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,920 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,920 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4233 service: ClientService methodName: Multi size: 2.2m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,920 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,926 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14232,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334221693,"queuetimems":1,"class":"HRegionServer","responsesize":13393,"method":"Multi"}
2014-07-14 03:37:15,926 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4214 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,926 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,926 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13908,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47275","starttimems":1405334222018,"queuetimems":1,"class":"HRegionServer","responsesize":13111,"method":"Multi"}
2014-07-14 03:37:15,926 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4213 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,926 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,926 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4225 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,926 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,927 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4215 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47275: output error
2014-07-14 03:37:15,927 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:37:15,935 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334221287 with entries=99, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334235899
2014-07-14 03:37:17,261 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2434, memsize=155.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/30a130e60f1d420fb25091fba27f64c6
2014-07-14 03:37:17,280 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/30a130e60f1d420fb25091fba27f64c6 as hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/30a130e60f1d420fb25091fba27f64c6
2014-07-14 03:37:18,564 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/30a130e60f1d420fb25091fba27f64c6, entries=567600, sequenceid=2434, filesize=40.5m
2014-07-14 03:37:18,564 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~322.0m/337637600, currentsize=75.0m/78662960 for region usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. in 29775ms, sequenceid=2434, compaction requested=true
2014-07-14 03:37:18,565 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-14 03:37:18,565 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-14 03:37:18,565 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:37:18,565 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:37:18,565 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. because compaction request was cancelled
2014-07-14 03:37:18,565 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:37:18,565 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 678.4m
2014-07-14 03:37:19,191 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:37:19,212 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10449 synced till here 10443
2014-07-14 03:37:19,252 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:37:19,257 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334235899 with entries=88, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334239192
2014-07-14 03:37:19,257 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334144324
2014-07-14 03:37:19,257 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334146686
2014-07-14 03:37:19,257 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334149097
2014-07-14 03:37:19,257 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334151219
2014-07-14 03:37:19,257 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334154348
2014-07-14 03:37:19,257 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334158181
2014-07-14 03:37:19,257 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334161660
2014-07-14 03:37:22,116 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2359, memsize=380.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/ddba5c2f25a64c96b636f42e8f0f5b91
2014-07-14 03:37:22,130 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/ddba5c2f25a64c96b636f42e8f0f5b91 as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/ddba5c2f25a64c96b636f42e8f0f5b91
2014-07-14 03:37:22,141 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/ddba5c2f25a64c96b636f42e8f0f5b91, entries=1384300, sequenceid=2359, filesize=98.6m
2014-07-14 03:37:22,142 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~600.8m/630026800, currentsize=421.3m/441731680 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 41257ms, sequenceid=2359, compaction requested=true
2014-07-14 03:37:22,142 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:37:22,142 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 20 blocking
2014-07-14 03:37:22,142 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-14 03:37:22,142 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 755.7m
2014-07-14 03:37:22,142 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:37:22,143 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:37:22,143 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:37:22,486 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:37:23,260 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:37:24,110 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:37:24,127 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10540 synced till here 10539
2014-07-14 03:37:24,151 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334239192 with entries=91, filesize=62.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334244111
2014-07-14 03:37:24,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334164442
2014-07-14 03:37:24,152 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334166856
2014-07-14 03:37:24,152 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334169920
2014-07-14 03:37:24,152 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334172254
2014-07-14 03:37:24,152 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334174933
2014-07-14 03:37:24,152 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334177171
2014-07-14 03:37:24,152 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334179765
2014-07-14 03:37:26,590 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:37:26,668 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10632 synced till here 10630
2014-07-14 03:37:26,684 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334244111 with entries=92, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334246591
2014-07-14 03:37:28,923 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:37:28,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10720 synced till here 10719
2014-07-14 03:37:28,963 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334246591 with entries=88, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334248924
2014-07-14 03:37:30,544 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:37:30,843 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10837 synced till here 10834
2014-07-14 03:37:30,863 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334248924 with entries=117, filesize=83.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334250545
2014-07-14 03:37:32,217 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:37:32,236 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10929 synced till here 10925
2014-07-14 03:37:32,287 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334250545 with entries=92, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334252217
2014-07-14 03:37:33,854 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:37:34,068 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11039 synced till here 11033
2014-07-14 03:37:34,129 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334252217 with entries=110, filesize=78.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334253854
2014-07-14 03:37:36,265 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:37:36,303 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334253854 with entries=90, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334256265
2014-07-14 03:37:37,873 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2610, memsize=430.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/818dcf7b6c8e4648b0ee824ee7a26b6f
2014-07-14 03:37:37,888 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/818dcf7b6c8e4648b0ee824ee7a26b6f as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/818dcf7b6c8e4648b0ee824ee7a26b6f
2014-07-14 03:37:37,907 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/818dcf7b6c8e4648b0ee824ee7a26b6f, entries=1566660, sequenceid=2610, filesize=111.5m
2014-07-14 03:37:37,907 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~689.3m/722778000, currentsize=305.1m/319924240 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 19342ms, sequenceid=2610, compaction requested=true
2014-07-14 03:37:37,908 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:37:37,908 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 20 blocking
2014-07-14 03:37:37,908 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-14 03:37:37,908 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 700.5m
2014-07-14 03:37:37,908 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:37:37,908 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:37:37,908 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:37:37,983 DEBUG [RpcServer.handler=30,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:37:38,690 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:37:38,735 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:37:39,068 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11244 synced till here 11243
2014-07-14 03:37:39,097 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334256265 with entries=115, filesize=81.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334258691
2014-07-14 03:37:40,693 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:37:40,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11344 synced till here 11343
2014-07-14 03:37:40,903 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334258691 with entries=100, filesize=70.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334260694
2014-07-14 03:37:42,169 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:37:42,614 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334260694 with entries=101, filesize=71.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334262170
2014-07-14 03:37:44,089 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2627, memsize=460.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/6df660e53a614cceaeaf8ef0a3da102d
2014-07-14 03:37:44,357 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/6df660e53a614cceaeaf8ef0a3da102d as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/6df660e53a614cceaeaf8ef0a3da102d
2014-07-14 03:37:44,374 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/6df660e53a614cceaeaf8ef0a3da102d, entries=1677430, sequenceid=2627, filesize=119.4m
2014-07-14 03:37:44,375 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~755.7m/792362640, currentsize=399.7m/419094560 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 22233ms, sequenceid=2627, compaction requested=true
2014-07-14 03:37:44,376 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:37:44,376 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 20 blocking
2014-07-14 03:37:44,376 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-14 03:37:44,376 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 427.6m
2014-07-14 03:37:44,376 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:37:44,376 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:37:44,376 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:37:44,394 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:37:44,645 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:37:44,695 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:37:44,720 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11535 synced till here 11532
2014-07-14 03:37:44,775 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334262170 with entries=90, filesize=64.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334264695
2014-07-14 03:37:44,775 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334182047
2014-07-14 03:37:44,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334184486
2014-07-14 03:37:44,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334186647
2014-07-14 03:37:44,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334189099
2014-07-14 03:37:44,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334191061
2014-07-14 03:37:44,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334193087
2014-07-14 03:37:44,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334195501
2014-07-14 03:37:44,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334197279
2014-07-14 03:37:46,829 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:37:46,844 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11625 synced till here 11624
2014-07-14 03:37:46,860 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334264695 with entries=90, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334266829
2014-07-14 03:37:48,397 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:37:48,437 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11714 synced till here 11713
2014-07-14 03:37:48,461 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334266829 with entries=89, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334268398
2014-07-14 03:37:50,268 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:37:50,441 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11818 synced till here 11817
2014-07-14 03:37:50,465 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334268398 with entries=104, filesize=73.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334270268
2014-07-14 03:37:53,741 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:37:53,759 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11907 synced till here 11906
2014-07-14 03:37:53,780 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334270268 with entries=89, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334273741
2014-07-14 03:37:53,963 DEBUG [RpcServer.handler=41,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b.
2014-07-14 03:37:54,970 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2812, memsize=429.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/90aca105ac1a42af8e304ed98acd2abb
2014-07-14 03:37:54,983 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/90aca105ac1a42af8e304ed98acd2abb as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/90aca105ac1a42af8e304ed98acd2abb
2014-07-14 03:37:54,995 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/90aca105ac1a42af8e304ed98acd2abb, entries=1564320, sequenceid=2812, filesize=111.3m
2014-07-14 03:37:54,995 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~702.1m/736177680, currentsize=305.2m/319984160 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 17087ms, sequenceid=2812, compaction requested=true
2014-07-14 03:37:54,996 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:37:54,996 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 20 blocking
2014-07-14 03:37:54,996 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-14 03:37:54,996 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 584.0m
2014-07-14 03:37:54,996 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:37:54,996 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:37:54,996 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:37:55,073 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:37:55,314 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:37:55,780 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:37:55,800 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11996 synced till here 11995
2014-07-14 03:37:55,812 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334273741 with entries=89, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334275780
2014-07-14 03:37:55,812 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334199214
2014-07-14 03:37:55,812 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334201394
2014-07-14 03:37:55,812 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334203354
2014-07-14 03:37:55,812 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334205288
2014-07-14 03:37:57,638 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:37:57,655 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12084 synced till here 12081
2014-07-14 03:37:57,682 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334275780 with entries=88, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334277638
2014-07-14 03:37:57,712 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2887, memsize=389.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/105ff833daf44a88b7a5117b35b7e226
2014-07-14 03:37:57,725 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/105ff833daf44a88b7a5117b35b7e226 as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/105ff833daf44a88b7a5117b35b7e226
2014-07-14 03:37:57,736 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/105ff833daf44a88b7a5117b35b7e226, entries=1417510, sequenceid=2887, filesize=101.0m
2014-07-14 03:37:57,736 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~427.6m/448401920, currentsize=230.5m/241652080 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 13360ms, sequenceid=2887, compaction requested=true
2014-07-14 03:37:57,737 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:37:57,737 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 20 blocking
2014-07-14 03:37:57,737 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-14 03:37:57,737 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:37:57,737 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b., current region memstore size 276.6m
2014-07-14 03:37:57,737 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:37:57,737 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:37:57,944 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:37:58,705 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:37:59,069 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:37:59,119 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334277638 with entries=89, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334279069
2014-07-14 03:38:01,450 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:38:02,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12275 synced till here 12274
2014-07-14 03:38:02,029 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334279069 with entries=102, filesize=72.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334281450
2014-07-14 03:38:03,931 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:38:03,966 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334281450 with entries=88, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334283931
2014-07-14 03:38:05,140 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3027, memsize=200.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/7b5ac043da47425e8332526cf4dc74e5
2014-07-14 03:38:05,163 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/7b5ac043da47425e8332526cf4dc74e5 as hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/7b5ac043da47425e8332526cf4dc74e5
2014-07-14 03:38:05,175 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/7b5ac043da47425e8332526cf4dc74e5, entries=730320, sequenceid=3027, filesize=52.0m
2014-07-14 03:38:05,176 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~276.9m/290381120, currentsize=34.7m/36405360 for region usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. in 7439ms, sequenceid=3027, compaction requested=true
2014-07-14 03:38:05,176 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:38:05,176 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-14 03:38:05,176 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-14 03:38:05,177 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:38:05,177 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:38:05,177 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. because compaction request was cancelled
2014-07-14 03:38:05,177 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 477.3m
2014-07-14 03:38:05,446 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:38:06,062 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:38:06,111 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334283931 with entries=88, filesize=61.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334286062
2014-07-14 03:38:06,111 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334207051
2014-07-14 03:38:06,112 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334209263
2014-07-14 03:38:06,112 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334211410
2014-07-14 03:38:06,112 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334213317
2014-07-14 03:38:06,112 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334215958
2014-07-14 03:38:06,112 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334218722
2014-07-14 03:38:06,112 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334221287
2014-07-14 03:38:06,112 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334235899
2014-07-14 03:38:08,684 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:38:08,708 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334286062 with entries=87, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334288684
2014-07-14 03:38:11,045 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:38:11,074 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12627 synced till here 12625
2014-07-14 03:38:11,093 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334288684 with entries=89, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334291045
2014-07-14 03:38:13,039 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:38:13,343 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334291045 with entries=110, filesize=76.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334293040
2014-07-14 03:38:16,143 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:38:16,180 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334293040 with entries=86, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334296143
2014-07-14 03:38:16,708 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3004, memsize=571.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/ac6ec8e1792449b294d122d7d467a564
2014-07-14 03:38:16,724 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/ac6ec8e1792449b294d122d7d467a564 as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/ac6ec8e1792449b294d122d7d467a564
2014-07-14 03:38:16,735 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/ac6ec8e1792449b294d122d7d467a564, entries=2080650, sequenceid=3004, filesize=148.1m
2014-07-14 03:38:16,735 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~584.0m/612343040, currentsize=335.8m/352127280 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 21739ms, sequenceid=3004, compaction requested=true
2014-07-14 03:38:16,735 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:38:16,735 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 20 blocking
2014-07-14 03:38:16,736 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-14 03:38:16,736 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 517.8m
2014-07-14 03:38:16,736 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:38:16,736 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:38:16,736 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:38:16,818 DEBUG [RpcServer.handler=37,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:38:17,030 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:38:19,165 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:38:19,185 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12912 synced till here 12908
2014-07-14 03:38:19,256 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334296143 with entries=89, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334299165
2014-07-14 03:38:19,256 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334239192
2014-07-14 03:38:19,256 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334244111
2014-07-14 03:38:19,256 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334246591
2014-07-14 03:38:19,256 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334248924
2014-07-14 03:38:19,256 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334250545
2014-07-14 03:38:19,256 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334252217
2014-07-14 03:38:19,256 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334253854
2014-07-14 03:38:20,952 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1054ms
GC pool 'ParNew' had collection(s): count=1 time=1331ms
2014-07-14 03:38:21,716 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:38:21,732 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13001 synced till here 13000
2014-07-14 03:38:21,750 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334299165 with entries=89, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334301717
2014-07-14 03:38:23,310 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:38:23,705 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334301717 with entries=88, filesize=61.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334303311
2014-07-14 03:38:24,063 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3121, memsize=478.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/c92e99dad921421d879d5b8ca71e3795
2014-07-14 03:38:24,091 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/c92e99dad921421d879d5b8ca71e3795 as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/c92e99dad921421d879d5b8ca71e3795
2014-07-14 03:38:24,103 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/c92e99dad921421d879d5b8ca71e3795, entries=1743450, sequenceid=3121, filesize=124.1m
2014-07-14 03:38:24,103 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~478.8m/502102400, currentsize=279.4m/292972320 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 18926ms, sequenceid=3121, compaction requested=true
2014-07-14 03:38:24,104 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:38:24,104 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 20 blocking
2014-07-14 03:38:24,104 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-14 03:38:24,104 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 444.8m
2014-07-14 03:38:24,104 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:38:24,104 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:38:24,104 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:38:24,113 DEBUG [RpcServer.handler=7,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:38:24,457 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:38:26,393 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1349ms
GC pool 'ParNew' had collection(s): count=1 time=1619ms
2014-07-14 03:38:26,561 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:38:26,597 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334303311 with entries=86, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334306562
2014-07-14 03:38:26,597 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334256265
2014-07-14 03:38:26,597 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334258691
2014-07-14 03:38:26,597 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334260694
2014-07-14 03:38:28,161 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:38:28,395 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13288 synced till here 13283
2014-07-14 03:38:28,427 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334306562 with entries=113, filesize=80.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334308161
2014-07-14 03:38:29,773 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:38:29,794 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13378 synced till here 13377
2014-07-14 03:38:29,815 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334308161 with entries=90, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334309773
2014-07-14 03:38:32,143 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:38:32,237 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13495 synced till here 13476
2014-07-14 03:38:32,315 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334309773 with entries=117, filesize=85.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334312143
2014-07-14 03:38:33,663 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:38:33,679 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13586 synced till here 13585
2014-07-14 03:38:33,686 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334312143 with entries=91, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334313664
2014-07-14 03:38:35,004 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:38:35,113 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13679 synced till here 13676
2014-07-14 03:38:35,189 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334313664 with entries=93, filesize=66.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334315005
2014-07-14 03:38:39,118 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3222, memsize=519.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/9a1634c3942d47f6baa28c1a6d4cfcad
2014-07-14 03:38:39,134 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/9a1634c3942d47f6baa28c1a6d4cfcad as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/9a1634c3942d47f6baa28c1a6d4cfcad
2014-07-14 03:38:39,154 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/9a1634c3942d47f6baa28c1a6d4cfcad, entries=1890340, sequenceid=3222, filesize=134.5m
2014-07-14 03:38:39,154 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~519.2m/544403600, currentsize=345.1m/361882400 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 22418ms, sequenceid=3222, compaction requested=true
2014-07-14 03:38:39,155 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:38:39,155 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 20 blocking
2014-07-14 03:38:39,155 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-14 03:38:39,155 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:38:39,155 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:38:39,155 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 507.9m
2014-07-14 03:38:39,155 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:38:39,167 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:38:39,497 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:38:40,227 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:38:40,258 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13772 synced till here 13767
2014-07-14 03:38:40,320 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334315005 with entries=93, filesize=66.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334320227
2014-07-14 03:38:40,321 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334262170
2014-07-14 03:38:40,321 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334264695
2014-07-14 03:38:40,321 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334266829
2014-07-14 03:38:40,321 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334268398
2014-07-14 03:38:40,321 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334270268
2014-07-14 03:38:41,315 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:38:41,332 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13862 synced till here 13860
2014-07-14 03:38:41,343 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334320227 with entries=90, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334321315
2014-07-14 03:38:43,261 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3296, memsize=443.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/ca2b8cfbd1da487db72cf0b85631f729
2014-07-14 03:38:43,278 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/ca2b8cfbd1da487db72cf0b85631f729 as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/ca2b8cfbd1da487db72cf0b85631f729
2014-07-14 03:38:43,311 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/ca2b8cfbd1da487db72cf0b85631f729, entries=1613340, sequenceid=3296, filesize=114.9m
2014-07-14 03:38:43,312 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~452.5m/474487760, currentsize=315.8m/331106000 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 19208ms, sequenceid=3296, compaction requested=true
2014-07-14 03:38:43,313 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:38:43,313 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 20 blocking
2014-07-14 03:38:43,313 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 432.5m
2014-07-14 03:38:43,314 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-14 03:38:43,314 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:38:43,314 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:38:43,314 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:38:43,333 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:38:43,333 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:38:43,525 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334321315 with entries=98, filesize=68.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334323333
2014-07-14 03:38:43,525 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334273741
2014-07-14 03:38:43,525 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334275780
2014-07-14 03:38:43,637 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:38:50,129 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:38:50,163 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334323333 with entries=87, filesize=61.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334330129
2014-07-14 03:38:51,612 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:38:51,630 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14140 synced till here 14135
2014-07-14 03:38:51,685 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334330129 with entries=93, filesize=67.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334331613
2014-07-14 03:38:53,642 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:38:53,662 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14230 synced till here 14227
2014-07-14 03:38:53,701 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334331613 with entries=90, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334333642
2014-07-14 03:38:54,672 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b.
2014-07-14 03:38:54,857 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3449, memsize=436.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/39eb784a24114037928a2bac887f8d93
2014-07-14 03:38:54,871 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/39eb784a24114037928a2bac887f8d93 as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/39eb784a24114037928a2bac887f8d93
2014-07-14 03:38:54,880 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/39eb784a24114037928a2bac887f8d93, entries=1590120, sequenceid=3449, filesize=113.2m
2014-07-14 03:38:54,881 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~507.9m/532533520, currentsize=217.7m/228245200 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 15726ms, sequenceid=3449, compaction requested=true
2014-07-14 03:38:54,881 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:38:54,881 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 20 blocking
2014-07-14 03:38:54,882 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-14 03:38:54,882 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:38:54,882 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 448.0m
2014-07-14 03:38:54,882 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:38:54,882 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:38:55,129 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:38:55,813 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3503, memsize=363.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/150f48ee1ce945779d5a9d338ac42703
2014-07-14 03:38:55,835 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/150f48ee1ce945779d5a9d338ac42703 as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/150f48ee1ce945779d5a9d338ac42703
2014-07-14 03:38:55,903 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/150f48ee1ce945779d5a9d338ac42703, entries=1324090, sequenceid=3503, filesize=94.3m
2014-07-14 03:38:55,904 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~435.6m/456723280, currentsize=135.2m/141750880 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 12591ms, sequenceid=3503, compaction requested=true
2014-07-14 03:38:55,904 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:38:55,904 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 20 blocking
2014-07-14 03:38:55,904 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-14 03:38:55,904 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b., current region memstore size 259.5m
2014-07-14 03:38:55,904 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:38:55,904 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:38:55,904 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:38:56,059 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:39:02,976 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3582, memsize=237.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/f264baddb4024098bb41e981e252e0ce
2014-07-14 03:39:03,010 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/f264baddb4024098bb41e981e252e0ce as hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/f264baddb4024098bb41e981e252e0ce
2014-07-14 03:39:03,038 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/f264baddb4024098bb41e981e252e0ce, entries=865280, sequenceid=3582, filesize=61.6m
2014-07-14 03:39:03,039 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~259.5m/272130320, currentsize=0.0/0 for region usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. in 7135ms, sequenceid=3582, compaction requested=true
2014-07-14 03:39:03,039 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:39:03,039 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-14 03:39:03,039 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-14 03:39:03,040 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:39:03,040 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:39:03,040 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. because compaction request was cancelled
2014-07-14 03:39:06,500 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3585, memsize=388.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/ceae01581c37409895ce2e976419bea6
2014-07-14 03:39:06,522 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/ceae01581c37409895ce2e976419bea6 as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/ceae01581c37409895ce2e976419bea6
2014-07-14 03:39:06,535 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/ceae01581c37409895ce2e976419bea6, entries=1415330, sequenceid=3585, filesize=100.8m
2014-07-14 03:39:06,536 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~448.0m/469713920, currentsize=7.4m/7795920 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 11654ms, sequenceid=3585, compaction requested=true
2014-07-14 03:39:06,537 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:39:06,537 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 20 blocking
2014-07-14 03:39:06,537 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-14 03:39:06,537 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:39:06,538 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:39:06,538 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:39:10,113 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:39:10,145 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334333642 with entries=89, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334350113
2014-07-14 03:39:10,145 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334277638
2014-07-14 03:39:10,146 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334279069
2014-07-14 03:39:10,146 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334281450
2014-07-14 03:39:10,146 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334283931
2014-07-14 03:39:10,146 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334286062
2014-07-14 03:39:10,146 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334288684
2014-07-14 03:39:10,146 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334291045
2014-07-14 03:39:10,147 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334293040
2014-07-14 03:39:10,147 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334296143
2014-07-14 03:39:10,147 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334299165
2014-07-14 03:39:10,147 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334301717
2014-07-14 03:39:10,147 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334303311
2014-07-14 03:39:10,147 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334306562
2014-07-14 03:39:10,147 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334308161
2014-07-14 03:39:10,147 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334309773
2014-07-14 03:39:10,148 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334312143
2014-07-14 03:39:10,148 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334313664
2014-07-14 03:39:12,258 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:39:12,258 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 256.2m
2014-07-14 03:39:12,445 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:39:12,562 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:39:12,587 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14410 synced till here 14408
2014-07-14 03:39:12,629 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334350113 with entries=91, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334352562
2014-07-14 03:39:13,765 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:39:13,846 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334352562 with entries=88, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334353766
2014-07-14 03:39:16,036 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:39:16,074 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334353766 with entries=89, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334356036
2014-07-14 03:39:16,525 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:39:16,527 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 256.4m
2014-07-14 03:39:16,717 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:39:17,666 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:39:17,693 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334356036 with entries=87, filesize=61.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334357666
2014-07-14 03:39:20,319 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3616, memsize=257.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/1209229bcb54465ba8328653cf527fd4
2014-07-14 03:39:20,333 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/1209229bcb54465ba8328653cf527fd4 as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/1209229bcb54465ba8328653cf527fd4
2014-07-14 03:39:20,346 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/1209229bcb54465ba8328653cf527fd4, entries=938580, sequenceid=3616, filesize=66.9m
2014-07-14 03:39:20,346 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~257.8m/270303600, currentsize=136.5m/143105760 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 8088ms, sequenceid=3616, compaction requested=true
2014-07-14 03:39:20,347 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:39:20,347 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 20 blocking
2014-07-14 03:39:20,347 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-14 03:39:20,347 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:39:20,347 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:39:20,347 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:39:21,812 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:39:21,843 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334357666 with entries=88, filesize=61.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334361812
2014-07-14 03:39:21,843 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334315005
2014-07-14 03:39:21,843 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334320227
2014-07-14 03:39:24,234 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:39:24,248 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14850 synced till here 14849
2014-07-14 03:39:24,266 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334361812 with entries=88, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334364235
2014-07-14 03:39:24,546 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3670, memsize=258.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/a95cd84c3fb64018b9dd590eb7321341
2014-07-14 03:39:24,562 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/a95cd84c3fb64018b9dd590eb7321341 as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/a95cd84c3fb64018b9dd590eb7321341
2014-07-14 03:39:24,575 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/a95cd84c3fb64018b9dd590eb7321341, entries=939220, sequenceid=3670, filesize=66.9m
2014-07-14 03:39:24,575 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~258.0m/270486800, currentsize=98.2m/102974960 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 8048ms, sequenceid=3670, compaction requested=true
2014-07-14 03:39:24,576 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:39:24,576 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 20 blocking
2014-07-14 03:39:24,576 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-14 03:39:24,576 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:39:24,576 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:39:24,576 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:39:25,125 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:39:25,221 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334364235 with entries=91, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334365125
2014-07-14 03:39:25,221 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334321315
2014-07-14 03:39:25,221 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334323333
2014-07-14 03:39:25,221 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334330129
2014-07-14 03:39:25,221 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334331613
2014-07-14 03:39:25,385 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:39:25,386 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 257.3m
2014-07-14 03:39:25,538 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:39:28,167 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:39:28,498 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334365125 with entries=99, filesize=70.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334368167
2014-07-14 03:39:29,059 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:39:29,059 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 257.6m
2014-07-14 03:39:29,499 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:39:30,286 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:39:30,306 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15129 synced till here 15128
2014-07-14 03:39:30,327 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334368167 with entries=89, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334370287
2014-07-14 03:39:33,400 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3752, memsize=257.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/d2f9ba8198b74e6081fe0a1aec5f67e0
2014-07-14 03:39:33,420 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/d2f9ba8198b74e6081fe0a1aec5f67e0 as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/d2f9ba8198b74e6081fe0a1aec5f67e0
2014-07-14 03:39:33,432 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/d2f9ba8198b74e6081fe0a1aec5f67e0, entries=936950, sequenceid=3752, filesize=66.7m
2014-07-14 03:39:33,433 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~257.3m/269834240, currentsize=77.8m/81626000 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 8048ms, sequenceid=3752, compaction requested=true
2014-07-14 03:39:33,433 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:39:33,433 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 20 blocking
2014-07-14 03:39:33,434 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-14 03:39:33,434 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:39:33,434 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:39:33,434 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:39:36,795 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3784, memsize=259.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/e0b9a0797c5f4f3bbb7d2f76a703d2a0
2014-07-14 03:39:36,814 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/e0b9a0797c5f4f3bbb7d2f76a703d2a0 as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/e0b9a0797c5f4f3bbb7d2f76a703d2a0
2014-07-14 03:39:36,830 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/e0b9a0797c5f4f3bbb7d2f76a703d2a0, entries=943570, sequenceid=3784, filesize=67.2m
2014-07-14 03:39:36,831 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~259.2m/271740800, currentsize=33.9m/35550080 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 7772ms, sequenceid=3784, compaction requested=true
2014-07-14 03:39:36,832 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:39:36,832 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 20 blocking
2014-07-14 03:39:36,832 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-14 03:39:36,832 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:39:36,833 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:39:36,833 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:39:49,536 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:39:49,556 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15219 synced till here 15217
2014-07-14 03:39:49,574 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334370287 with entries=90, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334389536
2014-07-14 03:39:50,329 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:39:50,330 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 256.1m
2014-07-14 03:39:50,466 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:39:50,609 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:39:50,975 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334389536 with entries=114, filesize=80.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334390609
2014-07-14 03:39:52,952 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:39:52,971 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15421 synced till here 15417
2014-07-14 03:39:53,003 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334390609 with entries=88, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334392953
2014-07-14 03:39:54,127 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:39:54,143 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15510 synced till here 15507
2014-07-14 03:39:54,183 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334392953 with entries=89, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334394128
2014-07-14 03:39:55,772 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:39:55,801 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334394128 with entries=89, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334395772
2014-07-14 03:39:56,065 DEBUG [RpcServer.handler=24,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:39:56,066 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 256.8m
2014-07-14 03:39:56,286 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:39:57,596 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:39:57,624 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15689 synced till here 15687
2014-07-14 03:39:57,638 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334395772 with entries=90, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334397596
2014-07-14 03:39:58,343 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:39:59,089 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3836, memsize=256.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/a57a0fd0350942fd8d5a84f22619e0c1
2014-07-14 03:39:59,111 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/a57a0fd0350942fd8d5a84f22619e0c1 as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/a57a0fd0350942fd8d5a84f22619e0c1
2014-07-14 03:39:59,123 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/a57a0fd0350942fd8d5a84f22619e0c1, entries=932310, sequenceid=3836, filesize=66.4m
2014-07-14 03:39:59,124 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~256.1m/268499040, currentsize=188.1m/197258160 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 8794ms, sequenceid=3836, compaction requested=true
2014-07-14 03:39:59,125 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:39:59,125 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 20 blocking
2014-07-14 03:39:59,125 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-14 03:39:59,126 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 276.8m
2014-07-14 03:39:59,126 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:39:59,126 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:39:59,126 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:39:59,197 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:39:59,220 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15777 synced till here 15775
2014-07-14 03:39:59,238 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334397596 with entries=88, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334399197
2014-07-14 03:39:59,565 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:40:01,579 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:40:01,608 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15867 synced till here 15866
2014-07-14 03:40:01,620 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334399197 with entries=90, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334401580
2014-07-14 03:40:02,659 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:40:02,962 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:40:02,988 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15956 synced till here 15955
2014-07-14 03:40:02,998 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334401580 with entries=89, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334402962
2014-07-14 03:40:04,729 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3919, memsize=253.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/80c8c966a8c54bf9b9c3ab029f02197b
2014-07-14 03:40:04,740 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/80c8c966a8c54bf9b9c3ab029f02197b as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/80c8c966a8c54bf9b9c3ab029f02197b
2014-07-14 03:40:04,749 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/80c8c966a8c54bf9b9c3ab029f02197b, entries=923120, sequenceid=3919, filesize=65.8m
2014-07-14 03:40:04,749 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~258.4m/270916800, currentsize=145.8m/152920640 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 8683ms, sequenceid=3919, compaction requested=true
2014-07-14 03:40:04,750 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:40:04,750 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 20 blocking
2014-07-14 03:40:04,751 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 275.1m
2014-07-14 03:40:04,751 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-14 03:40:04,751 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:40:04,751 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:40:04,751 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:40:04,879 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:40:07,917 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3965, memsize=275.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/20375cf2470e44b98eb3315cd2c2c417
2014-07-14 03:40:07,942 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/20375cf2470e44b98eb3315cd2c2c417 as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/20375cf2470e44b98eb3315cd2c2c417
2014-07-14 03:40:07,970 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/20375cf2470e44b98eb3315cd2c2c417, entries=1001900, sequenceid=3965, filesize=71.4m
2014-07-14 03:40:07,971 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~280.0m/293613280, currentsize=91.9m/96313360 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 8846ms, sequenceid=3965, compaction requested=true
2014-07-14 03:40:07,971 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:40:07,971 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 20 blocking
2014-07-14 03:40:07,971 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-14 03:40:07,971 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:40:07,971 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:40:07,971 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:40:12,178 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4014, memsize=270.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/7b87163527974e7cbbc40c94bf2e79ad
2014-07-14 03:40:12,192 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/7b87163527974e7cbbc40c94bf2e79ad as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/7b87163527974e7cbbc40c94bf2e79ad
2014-07-14 03:40:12,349 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/7b87163527974e7cbbc40c94bf2e79ad, entries=984810, sequenceid=4014, filesize=70.1m
2014-07-14 03:40:12,350 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~275.1m/288433280, currentsize=10.8m/11294960 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 7599ms, sequenceid=4014, compaction requested=true
2014-07-14 03:40:12,351 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:40:12,351 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 17 store files, 0 compacting, 17 eligible, 20 blocking
2014-07-14 03:40:12,351 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 17 files from compaction candidates
2014-07-14 03:40:12,351 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:40:12,352 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:40:12,352 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:40:20,846 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:40:21,173 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16062 synced till here 16061
2014-07-14 03:40:21,199 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334402962 with entries=106, filesize=75.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334420847
2014-07-14 03:40:22,925 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:40:22,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16152 synced till here 16150
2014-07-14 03:40:22,978 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334420847 with entries=90, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334422925
2014-07-14 03:40:24,149 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:40:24,622 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16252 synced till here 16250
2014-07-14 03:40:24,695 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334422925 with entries=100, filesize=70.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334424149
2014-07-14 03:40:25,115 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:40:25,115 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 257.6m
2014-07-14 03:40:25,323 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:40:25,947 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:40:25,973 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334424149 with entries=88, filesize=61.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334425947
2014-07-14 03:40:28,289 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:40:28,310 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16430 synced till here 16429
2014-07-14 03:40:28,336 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334425947 with entries=90, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334428289
2014-07-14 03:40:28,409 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:40:28,409 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 256.4m
2014-07-14 03:40:28,555 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:40:30,316 DEBUG [RpcServer.handler=15,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b.
2014-07-14 03:40:31,312 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:40:31,613 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334428289 with entries=97, filesize=67.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334431313
2014-07-14 03:40:33,376 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:40:33,565 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4087, memsize=259.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/b76d703decbd4388a3fa4a6cd7b8c745
2014-07-14 03:40:33,756 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16619 synced till here 16616
2014-07-14 03:40:33,822 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/b76d703decbd4388a3fa4a6cd7b8c745 as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/b76d703decbd4388a3fa4a6cd7b8c745
2014-07-14 03:40:33,824 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334431313 with entries=92, filesize=65.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334433377
2014-07-14 03:40:33,886 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/b76d703decbd4388a3fa4a6cd7b8c745, entries=943610, sequenceid=4087, filesize=67.1m
2014-07-14 03:40:33,887 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~259.2m/271752720, currentsize=132.1m/138548880 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 8772ms, sequenceid=4087, compaction requested=true
2014-07-14 03:40:33,887 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:40:33,888 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 20 blocking
2014-07-14 03:40:33,888 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-14 03:40:33,888 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:40:33,888 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b., current region memstore size 270.6m
2014-07-14 03:40:33,888 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:40:33,888 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:40:34,073 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:40:34,507 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:40:35,919 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:40:35,945 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334433377 with entries=88, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334435919
2014-07-14 03:40:37,217 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4132, memsize=257.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/7eb1de9287ae493ca613b7cfe2a37b96
2014-07-14 03:40:37,240 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/7eb1de9287ae493ca613b7cfe2a37b96 as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/7eb1de9287ae493ca613b7cfe2a37b96
2014-07-14 03:40:37,264 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/7eb1de9287ae493ca613b7cfe2a37b96, entries=939190, sequenceid=4132, filesize=66.8m
2014-07-14 03:40:37,264 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~257.9m/270479440, currentsize=131.7m/138100320 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 8855ms, sequenceid=4132, compaction requested=true
2014-07-14 03:40:37,264 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:40:37,264 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 17 store files, 0 compacting, 17 eligible, 20 blocking
2014-07-14 03:40:37,265 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 17 files from compaction candidates
2014-07-14 03:40:37,265 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:40:37,265 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 309.3m
2014-07-14 03:40:37,265 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:40:37,265 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:40:37,403 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:40:37,689 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:40:37,709 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16795 synced till here 16793
2014-07-14 03:40:37,731 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334435919 with entries=88, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334437690
2014-07-14 03:40:42,360 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4162, memsize=269.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/44f6a4b5c4dc49d1ba34b64176216fc4
2014-07-14 03:40:42,376 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/44f6a4b5c4dc49d1ba34b64176216fc4 as hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/44f6a4b5c4dc49d1ba34b64176216fc4
2014-07-14 03:40:42,394 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/44f6a4b5c4dc49d1ba34b64176216fc4, entries=982610, sequenceid=4162, filesize=69.9m
2014-07-14 03:40:42,395 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~271.2m/284359360, currentsize=30.1m/31564000 for region usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. in 8507ms, sequenceid=4162, compaction requested=true
2014-07-14 03:40:42,395 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:40:42,395 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-14 03:40:42,395 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-14 03:40:42,395 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:40:42,395 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:40:42,395 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. because compaction request was cancelled
2014-07-14 03:40:46,359 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4214, memsize=309.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/cb7154a7fa9f4076a5b3424f7109132d
2014-07-14 03:40:46,407 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/cb7154a7fa9f4076a5b3424f7109132d as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/cb7154a7fa9f4076a5b3424f7109132d
2014-07-14 03:40:46,423 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/cb7154a7fa9f4076a5b3424f7109132d, entries=1126240, sequenceid=4214, filesize=80.2m
2014-07-14 03:40:46,423 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~309.3m/324348000, currentsize=37.3m/39065680 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 9158ms, sequenceid=4214, compaction requested=true
2014-07-14 03:40:46,424 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:40:46,424 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 18 store files, 0 compacting, 18 eligible, 20 blocking
2014-07-14 03:40:46,424 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 18 files from compaction candidates
2014-07-14 03:40:46,425 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:40:46,425 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:40:46,425 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:40:54,244 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:40:54,270 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334437690 with entries=88, filesize=61.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334454245
2014-07-14 03:40:54,270 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334333642
2014-07-14 03:40:54,270 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334350113
2014-07-14 03:40:54,270 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334352562
2014-07-14 03:40:54,270 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334353766
2014-07-14 03:40:54,270 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334356036
2014-07-14 03:40:54,270 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334357666
2014-07-14 03:40:54,270 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334361812
2014-07-14 03:40:54,270 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334364235
2014-07-14 03:40:54,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334365125
2014-07-14 03:40:54,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334368167
2014-07-14 03:40:54,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334370287
2014-07-14 03:40:54,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334389536
2014-07-14 03:40:54,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334390609
2014-07-14 03:40:54,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334392953
2014-07-14 03:40:54,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334394128
2014-07-14 03:40:54,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334395772
2014-07-14 03:40:54,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334397596
2014-07-14 03:40:54,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334399197
2014-07-14 03:40:54,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334401580
2014-07-14 03:40:54,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334402962
2014-07-14 03:40:54,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334420847
2014-07-14 03:40:54,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334422925
2014-07-14 03:40:54,992 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=12794, hits=5369, hitRatio=41.96%, , cachingAccesses=5373, cachingHits=5368, cachingHitsRatio=99.90%, evictions=0, evicted=3, evictedPerRun=Infinity
2014-07-14 03:40:57,745 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:40:57,746 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 256.4m
2014-07-14 03:40:57,878 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:40:58,119 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:40:58,169 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16976 synced till here 16971
2014-07-14 03:40:58,215 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334454245 with entries=93, filesize=66.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334458120
2014-07-14 03:40:59,383 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:40:59,677 DEBUG [RpcServer.handler=19,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:40:59,677 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 256.2m
2014-07-14 03:40:59,713 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17099 synced till here 17096
2014-07-14 03:40:59,739 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334458120 with entries=123, filesize=87.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334459384
2014-07-14 03:40:59,855 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:41:02,192 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:41:02,215 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17190 synced till here 17189
2014-07-14 03:41:02,227 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334459384 with entries=91, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334462193
2014-07-14 03:41:04,297 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:41:04,407 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17285 synced till here 17284
2014-07-14 03:41:04,428 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334462193 with entries=95, filesize=67.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334464297
2014-07-14 03:41:05,843 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4253, memsize=256.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/e7147cebef6f4827936fd815bd0417ce
2014-07-14 03:41:05,855 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/e7147cebef6f4827936fd815bd0417ce as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/e7147cebef6f4827936fd815bd0417ce
2014-07-14 03:41:05,864 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/e7147cebef6f4827936fd815bd0417ce, entries=933550, sequenceid=4253, filesize=66.5m
2014-07-14 03:41:05,864 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~256.4m/268853440, currentsize=149.0m/156252720 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 8118ms, sequenceid=4253, compaction requested=true
2014-07-14 03:41:05,865 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:41:05,865 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 17 store files, 0 compacting, 17 eligible, 20 blocking
2014-07-14 03:41:05,865 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 17 files from compaction candidates
2014-07-14 03:41:05,865 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:41:05,865 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:41:05,865 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:41:06,444 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:41:06,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17378 synced till here 17374
2014-07-14 03:41:06,627 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334464297 with entries=93, filesize=66.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334466445
2014-07-14 03:41:06,627 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334424149
2014-07-14 03:41:06,627 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334425947
2014-07-14 03:41:07,774 DEBUG [RpcServer.handler=46,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:41:07,774 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 256.5m
2014-07-14 03:41:07,833 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4299, memsize=257.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/2fe366d6a24549f3870ea81091b1713e
2014-07-14 03:41:07,856 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/2fe366d6a24549f3870ea81091b1713e as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/2fe366d6a24549f3870ea81091b1713e
2014-07-14 03:41:07,874 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/2fe366d6a24549f3870ea81091b1713e, entries=938190, sequenceid=4299, filesize=66.9m
2014-07-14 03:41:07,875 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~257.7m/270189920, currentsize=135.7m/142274080 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 8198ms, sequenceid=4299, compaction requested=true
2014-07-14 03:41:07,875 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:41:07,875 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 18 store files, 0 compacting, 18 eligible, 20 blocking
2014-07-14 03:41:07,876 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 18 files from compaction candidates
2014-07-14 03:41:07,876 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:41:07,876 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:41:07,876 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:41:07,968 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:41:08,025 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:41:08,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17467 synced till here 17466
2014-07-14 03:41:08,070 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334466445 with entries=89, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334468026
2014-07-14 03:41:08,071 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334428289
2014-07-14 03:41:08,071 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334431313
2014-07-14 03:41:10,337 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:41:10,681 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334468026 with entries=112, filesize=78.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334470337
2014-07-14 03:41:11,579 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:41:11,579 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 256.5m
2014-07-14 03:41:11,779 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:41:12,443 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:41:12,471 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334470337 with entries=88, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334472443
2014-07-14 03:41:15,736 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4381, memsize=258.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/9951bdec2e49466796e541b25ae988b9
2014-07-14 03:41:15,752 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/9951bdec2e49466796e541b25ae988b9 as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/9951bdec2e49466796e541b25ae988b9
2014-07-14 03:41:15,766 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/9951bdec2e49466796e541b25ae988b9, entries=939300, sequenceid=4381, filesize=66.9m
2014-07-14 03:41:15,766 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~258.0m/270510000, currentsize=88.7m/93050240 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 7992ms, sequenceid=4381, compaction requested=true
2014-07-14 03:41:15,767 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:41:15,767 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 19 store files, 0 compacting, 19 eligible, 20 blocking
2014-07-14 03:41:15,767 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 19 files from compaction candidates
2014-07-14 03:41:15,767 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:41:15,767 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:41:15,767 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:41:18,926 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4420, memsize=258.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/0dc2947453ac43da9cb4c390906bcea8
2014-07-14 03:41:18,957 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/0dc2947453ac43da9cb4c390906bcea8 as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/0dc2947453ac43da9cb4c390906bcea8
2014-07-14 03:41:19,193 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/0dc2947453ac43da9cb4c390906bcea8, entries=939680, sequenceid=4420, filesize=66.8m
2014-07-14 03:41:19,194 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~258.1m/270620720, currentsize=28.0m/29398560 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 7615ms, sequenceid=4420, compaction requested=true
2014-07-14 03:41:19,195 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:41:19,195 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 18 store files, 0 compacting, 18 eligible, 20 blocking
2014-07-14 03:41:19,195 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 18 files from compaction candidates
2014-07-14 03:41:19,195 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:41:19,196 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:41:19,196 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:41:29,677 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:41:29,807 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:41:29,808 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 258.4m
2014-07-14 03:41:30,028 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:41:30,098 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334472443 with entries=119, filesize=83.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334489678
2014-07-14 03:41:32,624 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:41:32,649 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17876 synced till here 17875
2014-07-14 03:41:32,900 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334489678 with entries=90, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334492627
2014-07-14 03:41:34,248 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:41:34,301 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334492627 with entries=88, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334494248
2014-07-14 03:41:35,466 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:41:35,483 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18054 synced till here 18051
2014-07-14 03:41:35,577 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334494248 with entries=90, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334495466
2014-07-14 03:41:36,192 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:41:36,193 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 256.7m
2014-07-14 03:41:36,386 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:41:37,535 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:41:37,588 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18146 synced till here 18144
2014-07-14 03:41:37,619 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334495466 with entries=92, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334497536
2014-07-14 03:41:39,379 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4468, memsize=261.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/71d403ae16d146148c71a2b891f103fe
2014-07-14 03:41:39,397 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/71d403ae16d146148c71a2b891f103fe as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/71d403ae16d146148c71a2b891f103fe
2014-07-14 03:41:39,420 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/71d403ae16d146148c71a2b891f103fe, entries=952430, sequenceid=4468, filesize=67.8m
2014-07-14 03:41:39,421 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~261.6m/274291600, currentsize=167.5m/175663520 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 9614ms, sequenceid=4468, compaction requested=true
2014-07-14 03:41:39,421 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:41:39,421 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 19 store files, 0 compacting, 19 eligible, 20 blocking
2014-07-14 03:41:39,421 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 19 files from compaction candidates
2014-07-14 03:41:39,421 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:41:39,421 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:41:39,421 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:41:40,315 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:41:40,762 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18263 synced till here 18262
2014-07-14 03:41:40,783 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334497536 with entries=117, filesize=82.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334500315
2014-07-14 03:41:40,821 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:41:40,821 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 256.9m
2014-07-14 03:41:40,980 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:41:42,495 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:41:42,516 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18354 synced till here 18352
2014-07-14 03:41:42,559 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334500315 with entries=91, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334502495
2014-07-14 03:41:44,893 DEBUG [RpcServer.handler=46,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:41:44,963 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:41:44,981 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18445 synced till here 18441
2014-07-14 03:41:45,011 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334502495 with entries=91, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334504964
2014-07-14 03:41:45,882 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4548, memsize=255.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/4a417e336ff94ee6a76e4defb2cec270
2014-07-14 03:41:45,901 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/4a417e336ff94ee6a76e4defb2cec270 as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/4a417e336ff94ee6a76e4defb2cec270
2014-07-14 03:41:45,914 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/4a417e336ff94ee6a76e4defb2cec270, entries=929450, sequenceid=4548, filesize=66.2m
2014-07-14 03:41:45,914 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~258.3m/270841360, currentsize=132.0m/138423600 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 9721ms, sequenceid=4548, compaction requested=true
2014-07-14 03:41:45,915 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:41:45,915 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 20 store files, 0 compacting, 20 eligible, 20 blocking
2014-07-14 03:41:45,915 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 20 files from compaction candidates
2014-07-14 03:41:45,915 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 260.1m
2014-07-14 03:41:45,915 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:41:45,915 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:41:45,915 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:41:46,036 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:41:49,997 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4586, memsize=253.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/065553b6ff3543d0ac31ce3297a8cfe8
2014-07-14 03:41:50,017 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/065553b6ff3543d0ac31ce3297a8cfe8 as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/065553b6ff3543d0ac31ce3297a8cfe8
2014-07-14 03:41:50,038 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/065553b6ff3543d0ac31ce3297a8cfe8, entries=923600, sequenceid=4586, filesize=65.8m
2014-07-14 03:41:50,039 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~256.9m/269330720, currentsize=72.6m/76173280 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 9218ms, sequenceid=4586, compaction requested=true
2014-07-14 03:41:50,040 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:41:50,040 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 19 store files, 0 compacting, 19 eligible, 20 blocking
2014-07-14 03:41:50,041 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 19 files from compaction candidates
2014-07-14 03:41:50,041 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:41:50,041 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:41:50,041 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:41:53,290 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4637, memsize=256.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/bbf9f9b2cea54587b8b5d7728b9fb0a0
2014-07-14 03:41:53,317 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/bbf9f9b2cea54587b8b5d7728b9fb0a0 as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/bbf9f9b2cea54587b8b5d7728b9fb0a0
2014-07-14 03:41:53,328 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/bbf9f9b2cea54587b8b5d7728b9fb0a0, entries=935400, sequenceid=4637, filesize=66.6m
2014-07-14 03:41:53,329 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~260.1m/272734000, currentsize=0.0/0 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 7414ms, sequenceid=4637, compaction requested=true
2014-07-14 03:41:53,330 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:41:53,330 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 20 store files, 0 compacting, 20 eligible, 20 blocking
2014-07-14 03:41:53,330 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 20 files from compaction candidates
2014-07-14 03:41:53,330 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:41:53,330 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:41:53,331 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:41:58,507 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:41:58,655 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18540 synced till here 18536
2014-07-14 03:41:58,761 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334504964 with entries=95, filesize=67.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334518507
2014-07-14 03:42:00,469 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:42:00,485 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18631 synced till here 18626
2014-07-14 03:42:00,554 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334518507 with entries=91, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334520469
2014-07-14 03:42:02,973 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:42:02,991 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18721 synced till here 18718
2014-07-14 03:42:03,012 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334520469 with entries=90, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334522973
2014-07-14 03:42:03,541 DEBUG [RpcServer.handler=13,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:42:03,541 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 256.2m
2014-07-14 03:42:03,757 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:42:04,694 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:42:04,750 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334522973 with entries=91, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334524694
2014-07-14 03:42:04,765 DEBUG [RpcServer.handler=24,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b.
2014-07-14 03:42:04,765 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b., current region memstore size 256.3m
2014-07-14 03:42:04,928 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:42:06,695 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:42:06,861 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18911 synced till here 18910
2014-07-14 03:42:06,874 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334524694 with entries=99, filesize=70.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334526695
2014-07-14 03:42:07,734 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:42:08,227 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:42:08,244 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19002 synced till here 18998
2014-07-14 03:42:08,305 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334526695 with entries=91, filesize=64.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334528227
2014-07-14 03:42:11,487 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:42:11,905 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334528227 with entries=99, filesize=69.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334531488
2014-07-14 03:42:13,143 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4715, memsize=254.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/a0c8149463bf4822b5d02ea5a4e70d4d
2014-07-14 03:42:13,162 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/a0c8149463bf4822b5d02ea5a4e70d4d as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/a0c8149463bf4822b5d02ea5a4e70d4d
2014-07-14 03:42:13,181 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/a0c8149463bf4822b5d02ea5a4e70d4d, entries=926940, sequenceid=4715, filesize=66.1m
2014-07-14 03:42:13,182 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~257.8m/270286960, currentsize=127.2m/133381440 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 9641ms, sequenceid=4715, compaction requested=true
2014-07-14 03:42:13,182 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:42:13,182 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 20 blocking
2014-07-14 03:42:13,182 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-14 03:42:13,182 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 326.0m
2014-07-14 03:42:13,182 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:42:13,182 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:42:13,183 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:42:13,336 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:42:13,344 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:42:14,542 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4711, memsize=254.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/b2baa99aadc3422bb2324e4c9be453fd
2014-07-14 03:42:14,560 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/b2baa99aadc3422bb2324e4c9be453fd as hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/b2baa99aadc3422bb2324e4c9be453fd
2014-07-14 03:42:14,575 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/b2baa99aadc3422bb2324e4c9be453fd, entries=926410, sequenceid=4711, filesize=66.0m
2014-07-14 03:42:14,575 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~256.3m/268708400, currentsize=41.0m/42963120 for region usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. in 9810ms, sequenceid=4711, compaction requested=true
2014-07-14 03:42:14,576 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:42:14,576 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 20 blocking
2014-07-14 03:42:14,576 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-14 03:42:14,576 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 282.0m
2014-07-14 03:42:14,576 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:42:14,576 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:42:14,576 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. because compaction request was cancelled
2014-07-14 03:42:14,737 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:42:14,775 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:42:14,803 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19188 synced till here 19187
2014-07-14 03:42:14,824 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334531488 with entries=87, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334534775
2014-07-14 03:42:14,824 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334433377
2014-07-14 03:42:14,824 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334435919
2014-07-14 03:42:14,825 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334437690
2014-07-14 03:42:14,825 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334454245
2014-07-14 03:42:14,825 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334458120
2014-07-14 03:42:14,825 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334459384
2014-07-14 03:42:14,826 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334462193
2014-07-14 03:42:14,826 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334464297
2014-07-14 03:42:14,826 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334466445
2014-07-14 03:42:14,826 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334468026
2014-07-14 03:42:14,826 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334470337
2014-07-14 03:42:14,826 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334472443
2014-07-14 03:42:14,826 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334489678
2014-07-14 03:42:14,826 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334492627
2014-07-14 03:42:14,826 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334494248
2014-07-14 03:42:14,826 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334495466
2014-07-14 03:42:14,826 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334497536
2014-07-14 03:42:16,666 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:42:16,691 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19279 synced till here 19276
2014-07-14 03:42:16,774 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334534775 with entries=91, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334536667
2014-07-14 03:42:22,996 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4819, memsize=277.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/571de650bb514bdf820bc4535a5cd725
2014-07-14 03:42:23,016 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/571de650bb514bdf820bc4535a5cd725 as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/571de650bb514bdf820bc4535a5cd725
2014-07-14 03:42:23,033 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/571de650bb514bdf820bc4535a5cd725, entries=1009740, sequenceid=4819, filesize=71.9m
2014-07-14 03:42:23,034 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~282.0m/295651760, currentsize=56.0m/58767920 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 8458ms, sequenceid=4819, compaction requested=true
2014-07-14 03:42:23,034 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:42:23,034 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 20 blocking
2014-07-14 03:42:23,034 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-14 03:42:23,035 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:42:23,035 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:42:23,035 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:42:23,117 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4797, memsize=321.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/e5647fa036114ee88aab9adf3f44acb4
2014-07-14 03:42:23,136 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/e5647fa036114ee88aab9adf3f44acb4 as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/e5647fa036114ee88aab9adf3f44acb4
2014-07-14 03:42:23,152 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/e5647fa036114ee88aab9adf3f44acb4, entries=1169950, sequenceid=4797, filesize=83.3m
2014-07-14 03:42:23,153 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~326.0m/341832160, currentsize=84.0m/88119360 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 9971ms, sequenceid=4797, compaction requested=true
2014-07-14 03:42:23,153 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:42:23,153 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 20 store files, 0 compacting, 20 eligible, 20 blocking
2014-07-14 03:42:23,154 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 20 files from compaction candidates
2014-07-14 03:42:23,154 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:42:23,154 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:42:23,154 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:42:36,947 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:42:37,141 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19375 synced till here 19367
2014-07-14 03:42:37,249 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334536667 with entries=96, filesize=68.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334557113
2014-07-14 03:42:37,251 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334500315
2014-07-14 03:42:37,251 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334502495
2014-07-14 03:42:37,252 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334504964
2014-07-14 03:42:37,252 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334518507
2014-07-14 03:42:37,252 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334520469
2014-07-14 03:42:37,708 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:42:37,708 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. has too many store files; delaying flush up to 90000ms
2014-07-14 03:42:37,709 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:42:37,709 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 20 blocking
2014-07-14 03:42:37,709 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-14 03:42:37,709 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:42:37,709 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:42:37,709 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:42:38,076 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:42:38,095 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19466 synced till here 19464
2014-07-14 03:42:38,117 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334557113 with entries=91, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334558076
2014-07-14 03:42:39,420 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:42:39,445 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334558076 with entries=87, filesize=60.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334559421
2014-07-14 03:42:41,070 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:42:41,100 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334559421 with entries=87, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334561071
2014-07-14 03:42:42,797 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:42:43,046 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19750 synced till here 19749
2014-07-14 03:42:43,056 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334561071 with entries=110, filesize=78.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334562797
2014-07-14 03:42:43,292 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:42:43,292 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 256.6m
2014-07-14 03:42:43,432 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:42:44,955 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:42:44,955 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. has too many store files; delaying flush up to 90000ms
2014-07-14 03:42:44,956 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:42:44,956 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 20 blocking
2014-07-14 03:42:44,956 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-14 03:42:44,956 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:42:44,956 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:42:44,956 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:42:45,044 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:42:45,165 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19849 synced till here 19847
2014-07-14 03:42:45,183 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334562797 with entries=99, filesize=69.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334565044
2014-07-14 03:42:47,582 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:42:47,612 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19938 synced till here 19937
2014-07-14 03:42:47,637 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334565044 with entries=89, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334567583
2014-07-14 03:42:49,242 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:42:49,265 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20027 synced till here 20025
2014-07-14 03:42:49,296 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334567583 with entries=89, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334569242
2014-07-14 03:42:51,217 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4963, memsize=256.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/127dc71dbe174b088edb2517994b9001
2014-07-14 03:42:51,228 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/127dc71dbe174b088edb2517994b9001 as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/127dc71dbe174b088edb2517994b9001
2014-07-14 03:42:51,238 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/127dc71dbe174b088edb2517994b9001, entries=934110, sequenceid=4963, filesize=66.5m
2014-07-14 03:42:51,239 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~256.6m/269016560, currentsize=127.5m/133686240 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 7947ms, sequenceid=4963, compaction requested=true
2014-07-14 03:42:51,239 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:42:51,239 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 20 blocking
2014-07-14 03:42:51,240 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-14 03:42:51,240 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:42:51,240 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:42:51,240 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:42:58,382 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:42:58,404 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20115 synced till here 20114
2014-07-14 03:42:58,416 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334569242 with entries=88, filesize=61.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334578382
2014-07-14 03:43:02,144 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:43:02,163 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20203 synced till here 20202
2014-07-14 03:43:02,179 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334578382 with entries=88, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334582144
2014-07-14 03:43:03,412 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:43:03,426 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20293 synced till here 20288
2014-07-14 03:43:03,461 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334582144 with entries=90, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334583412
2014-07-14 03:43:12,662 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:43:12,920 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20383 synced till here 20382
2014-07-14 03:43:12,939 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334583412 with entries=90, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334592662
2014-07-14 03:43:14,166 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:43:14,166 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. has too many store files; delaying flush up to 90000ms
2014-07-14 03:43:14,169 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:43:14,169 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 20 blocking
2014-07-14 03:43:14,169 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-14 03:43:14,169 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:43:14,169 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:43:14,169 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:43:14,649 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:43:15,341 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20492 synced till here 20489
2014-07-14 03:43:15,553 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334592662 with entries=109, filesize=79.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334594649
2014-07-14 03:43:17,087 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:43:17,105 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20585 synced till here 20582
2014-07-14 03:43:17,148 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334594649 with entries=93, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334597088
2014-07-14 03:43:18,454 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:43:18,471 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20675 synced till here 20673
2014-07-14 03:43:18,492 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334597088 with entries=90, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334598454
2014-07-14 03:43:20,480 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:43:20,506 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334598454 with entries=88, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334600480
2014-07-14 03:43:37,490 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:43:37,521 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20851 synced till here 20849
2014-07-14 03:43:37,549 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334600480 with entries=88, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334617491
2014-07-14 03:43:38,595 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:43:38,638 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334617491 with entries=87, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334618596
2014-07-14 03:43:39,842 DEBUG [RpcServer.handler=39,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b.
2014-07-14 03:43:39,843 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b., current region memstore size 256.0m
2014-07-14 03:43:39,997 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:43:40,872 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:43:41,359 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21064 synced till here 21063
2014-07-14 03:43:41,379 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334618596 with entries=126, filesize=89.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334620873
2014-07-14 03:43:43,137 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:43:43,152 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21151 synced till here 21150
2014-07-14 03:43:43,171 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334620873 with entries=87, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334623137
2014-07-14 03:43:45,991 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:43:46,046 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334623137 with entries=91, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334625991
2014-07-14 03:43:47,807 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5262, memsize=256.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/89091c992c414a0d81bb99a7da194538
2014-07-14 03:43:47,822 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/89091c992c414a0d81bb99a7da194538 as hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/89091c992c414a0d81bb99a7da194538
2014-07-14 03:43:47,833 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/89091c992c414a0d81bb99a7da194538, entries=935570, sequenceid=5262, filesize=66.6m
2014-07-14 03:43:47,834 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~256.9m/269419680, currentsize=35.4m/37067920 for region usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. in 7991ms, sequenceid=5262, compaction requested=true
2014-07-14 03:43:47,834 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:43:47,834 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 20 blocking
2014-07-14 03:43:47,834 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-14 03:43:47,835 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:43:47,835 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:43:47,835 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. because compaction request was cancelled
2014-07-14 03:43:47,901 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:43:47,921 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21329 synced till here 21328
2014-07-14 03:43:47,932 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334625991 with entries=87, filesize=62.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334627902
2014-07-14 03:43:50,856 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:43:50,893 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334627902 with entries=88, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334630856
2014-07-14 03:43:53,256 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:43:53,299 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334630856 with entries=89, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334633257
2014-07-14 03:43:55,246 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:43:55,267 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21595 synced till here 21592
2014-07-14 03:43:55,319 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334633257 with entries=89, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334635247
2014-07-14 03:44:08,067 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90359ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:44:08,068 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 1.1g
2014-07-14 03:44:08,681 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:44:11,610 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:44:11,636 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21685 synced till here 21684
2014-07-14 03:44:11,650 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334635247 with entries=90, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334651611
2014-07-14 03:44:13,094 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:44:13,226 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21785 synced till here 21777
2014-07-14 03:44:13,284 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334651611 with entries=100, filesize=71.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334653094
2014-07-14 03:44:14,430 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:44:14,708 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334653094 with entries=92, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334654430
2014-07-14 03:44:15,539 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90584ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:44:15,540 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 1.0g
2014-07-14 03:44:16,166 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:44:16,283 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334654430 with entries=90, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334656166
2014-07-14 03:44:16,320 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:44:17,910 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:44:17,926 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22054 synced till here 22053
2014-07-14 03:44:17,935 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334656166 with entries=87, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334657910
2014-07-14 03:44:19,998 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:44:20,552 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334657910 with entries=112, filesize=79.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334659999
2014-07-14 03:44:22,948 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:44:22,972 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22255 synced till here 22253
2014-07-14 03:44:22,996 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334659999 with entries=89, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334662948
2014-07-14 03:44:24,816 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:44:24,858 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334662948 with entries=89, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334664816
2014-07-14 03:44:40,221 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:44:40,241 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22432 synced till here 22430
2014-07-14 03:44:40,256 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334664816 with entries=88, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334680221
2014-07-14 03:44:42,684 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5431, memsize=1.1g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/f9107bebce934fe2b96007afba09d0ab
2014-07-14 03:44:42,699 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/f9107bebce934fe2b96007afba09d0ab as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/f9107bebce934fe2b96007afba09d0ab
2014-07-14 03:44:42,710 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/f9107bebce934fe2b96007afba09d0ab, entries=4045320, sequenceid=5431, filesize=287.9m
2014-07-14 03:44:42,710 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.1g/1165020640, currentsize=334.5m/350795040 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 34642ms, sequenceid=5431, compaction requested=true
2014-07-14 03:44:42,711 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:44:42,711 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 20 blocking
2014-07-14 03:44:42,711 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-14 03:44:42,711 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:44:42,711 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:44:42,711 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:44:42,725 DEBUG [RpcServer.handler=19,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:44:42,725 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. has too many store files; delaying flush up to 90000ms
2014-07-14 03:44:42,725 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:44:42,725 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 20 blocking
2014-07-14 03:44:42,725 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-14 03:44:42,725 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:44:42,725 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:44:42,725 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:44:43,149 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:44:43,235 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22528 synced till here 22527
2014-07-14 03:44:43,285 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334680221 with entries=96, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334683150
2014-07-14 03:44:43,285 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334522973
2014-07-14 03:44:43,285 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334524694
2014-07-14 03:44:43,285 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334526695
2014-07-14 03:44:43,285 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334528227
2014-07-14 03:44:44,350 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:44:44,384 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334683150 with entries=86, filesize=61.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334684351
2014-07-14 03:44:44,799 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90633ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:44:44,800 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 1.1g
2014-07-14 03:44:45,429 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:44:48,875 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5509, memsize=1.0g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/6646be569d714c518ba8781a1b7e0fd5
2014-07-14 03:44:48,890 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/6646be569d714c518ba8781a1b7e0fd5 as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/6646be569d714c518ba8781a1b7e0fd5
2014-07-14 03:44:48,904 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/6646be569d714c518ba8781a1b7e0fd5, entries=3898400, sequenceid=5509, filesize=277.5m
2014-07-14 03:44:48,905 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.0g/1122705360, currentsize=295.6m/309959120 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 33365ms, sequenceid=5509, compaction requested=true
2014-07-14 03:44:48,905 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:44:48,905 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 20 blocking
2014-07-14 03:44:48,905 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-14 03:44:48,905 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:44:48,905 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:44:48,905 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:44:50,020 DEBUG [RpcServer.handler=5,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:44:50,020 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. has too many store files; delaying flush up to 90000ms
2014-07-14 03:44:50,021 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:44:50,021 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 20 blocking
2014-07-14 03:44:50,021 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-14 03:44:50,021 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:44:50,021 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:44:50,021 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:44:50,141 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:44:50,190 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334684351 with entries=89, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334690141
2014-07-14 03:44:50,191 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334531488
2014-07-14 03:44:50,191 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334534775
2014-07-14 03:44:50,191 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334536667
2014-07-14 03:44:50,191 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334557113
2014-07-14 03:44:50,191 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334558076
2014-07-14 03:44:50,191 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334559421
2014-07-14 03:44:50,191 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334561071
2014-07-14 03:44:53,303 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:44:53,321 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22793 synced till here 22790
2014-07-14 03:44:53,354 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334690141 with entries=90, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334693303
2014-07-14 03:44:54,418 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:44:54,463 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334693303 with entries=90, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334694419
2014-07-14 03:44:56,713 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:44:56,740 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334694419 with entries=88, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334696713
2014-07-14 03:44:58,319 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:44:58,351 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334696713 with entries=88, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334698320
2014-07-14 03:45:00,856 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:45:00,911 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23147 synced till here 23146
2014-07-14 03:45:00,931 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334698320 with entries=88, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334700857
2014-07-14 03:45:14,028 DEBUG [RpcServer.handler=39,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b.
2014-07-14 03:45:14,029 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b., current region memstore size 256.3m
2014-07-14 03:45:14,198 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:45:14,444 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:45:14,476 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23237 synced till here 23235
2014-07-14 03:45:14,527 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334700857 with entries=90, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334714445
2014-07-14 03:45:15,819 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:45:15,833 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23325 synced till here 23324
2014-07-14 03:45:15,844 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334714445 with entries=88, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334715820
2014-07-14 03:45:18,785 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:45:19,068 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334715820 with entries=104, filesize=74.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334718786
2014-07-14 03:45:19,426 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5680, memsize=1.1g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/e3f4bc556be44ea0abd76c6145a60551
2014-07-14 03:45:19,447 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/e3f4bc556be44ea0abd76c6145a60551 as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/e3f4bc556be44ea0abd76c6145a60551
2014-07-14 03:45:19,470 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/e3f4bc556be44ea0abd76c6145a60551, entries=4051440, sequenceid=5680, filesize=288.3m
2014-07-14 03:45:19,471 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.1g/1166781520, currentsize=318.7m/334199680 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 34670ms, sequenceid=5680, compaction requested=true
2014-07-14 03:45:19,471 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:45:19,472 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 20 blocking
2014-07-14 03:45:19,472 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-14 03:45:19,472 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:45:19,472 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:45:19,473 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:45:19,543 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:45:19,543 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. has too many store files; delaying flush up to 90000ms
2014-07-14 03:45:19,544 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:45:19,544 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 20 blocking
2014-07-14 03:45:19,544 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-14 03:45:19,545 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:45:19,545 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:45:19,545 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:45:20,570 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:45:20,592 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23520 synced till here 23518
2014-07-14 03:45:20,637 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334718786 with entries=91, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334720570
2014-07-14 03:45:20,637 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334562797
2014-07-14 03:45:20,638 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334565044
2014-07-14 03:45:20,638 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334567583
2014-07-14 03:45:20,638 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334569242
2014-07-14 03:45:20,638 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334578382
2014-07-14 03:45:20,639 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334582144
2014-07-14 03:45:20,639 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334583412
2014-07-14 03:45:20,639 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334592662
2014-07-14 03:45:20,639 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334594649
2014-07-14 03:45:20,639 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334597088
2014-07-14 03:45:20,639 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334598454
2014-07-14 03:45:20,639 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334600480
2014-07-14 03:45:20,639 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334617491
2014-07-14 03:45:22,097 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:45:22,259 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23621 synced till here 23619
2014-07-14 03:45:22,288 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334720570 with entries=101, filesize=70.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334722097
2014-07-14 03:45:23,455 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5810, memsize=256.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/09f91c94229243d5a33b3dced99b9964
2014-07-14 03:45:23,470 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/09f91c94229243d5a33b3dced99b9964 as hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/09f91c94229243d5a33b3dced99b9964
2014-07-14 03:45:23,674 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/09f91c94229243d5a33b3dced99b9964, entries=933180, sequenceid=5810, filesize=66.4m
2014-07-14 03:45:23,675 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~256.3m/268729200, currentsize=54.6m/57286800 for region usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. in 9646ms, sequenceid=5810, compaction requested=true
2014-07-14 03:45:23,676 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 20 blocking
2014-07-14 03:45:23,676 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-14 03:45:23,676 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:45:23,676 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:45:23,677 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. because compaction request was cancelled
2014-07-14 03:45:23,677 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:45:24,527 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:45:24,545 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23708 synced till here 23706
2014-07-14 03:45:24,563 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334722097 with entries=87, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334724528
2014-07-14 03:45:24,563 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334618596
2014-07-14 03:45:24,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334620873
2014-07-14 03:45:24,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334623137
2014-07-14 03:45:24,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334625991
2014-07-14 03:45:24,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334627902
2014-07-14 03:45:24,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334630856
2014-07-14 03:45:24,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334633257
2014-07-14 03:45:26,250 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:45:26,269 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23799 synced till here 23797
2014-07-14 03:45:26,293 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334724528 with entries=91, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334726250
2014-07-14 03:45:27,960 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:45:28,105 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23891 synced till here 23888
2014-07-14 03:45:28,162 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334726250 with entries=92, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334727961
2014-07-14 03:45:42,906 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:45:42,922 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23979 synced till here 23978
2014-07-14 03:45:42,936 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334727961 with entries=88, filesize=61.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334742907
2014-07-14 03:45:45,212 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:45:45,233 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24073 synced till here 24066
2014-07-14 03:45:45,302 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334742907 with entries=94, filesize=69.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334745213
2014-07-14 03:45:46,289 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:45:46,352 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334745213 with entries=90, filesize=60.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334746289
2014-07-14 03:45:50,053 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:45:50,069 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24250 synced till here 24249
2014-07-14 03:45:50,086 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334746289 with entries=87, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334750053
2014-07-14 03:45:53,417 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:45:53,434 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24338 synced till here 24337
2014-07-14 03:45:53,451 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334750053 with entries=88, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334753417
2014-07-14 03:45:55,583 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=12794, hits=5369, hitRatio=41.96%, , cachingAccesses=5373, cachingHits=5368, cachingHitsRatio=99.90%, evictions=0, evicted=3, evictedPerRun=Infinity
2014-07-14 03:45:56,037 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:45:56,088 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24429 synced till here 24424
2014-07-14 03:45:56,119 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334753417 with entries=91, filesize=67.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334756037
2014-07-14 03:45:58,554 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:45:58,602 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24523 synced till here 24520
2014-07-14 03:45:58,649 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334756037 with entries=94, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334758555
2014-07-14 03:46:00,026 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:46:00,948 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24632 synced till here 24630
2014-07-14 03:46:00,993 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334758555 with entries=109, filesize=77.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334760026
2014-07-14 03:46:00,995 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 71c27911c72dddd675be84dade80b522
2014-07-14 03:46:01,923 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:46:01,940 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24721 synced till here 24720
2014-07-14 03:46:01,955 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334760026 with entries=89, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334761923
2014-07-14 03:46:01,956 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 71c27911c72dddd675be84dade80b522
2014-07-14 03:46:03,648 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:46:03,969 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24832 synced till here 24831
2014-07-14 03:46:04,004 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334761923 with entries=111, filesize=77.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334763648
2014-07-14 03:46:04,005 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 71c27911c72dddd675be84dade80b522
2014-07-14 03:46:13,550 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90826ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:46:13,551 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 1.2g
2014-07-14 03:46:14,567 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:46:18,124 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:46:18,178 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24921 synced till here 24920
2014-07-14 03:46:18,205 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334763648 with entries=89, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334778125
2014-07-14 03:46:21,083 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 91063ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:46:21,083 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 1.1g
2014-07-14 03:46:21,523 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:46:21,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25010 synced till here 25008
2014-07-14 03:46:21,575 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334778125 with entries=89, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334781524
2014-07-14 03:46:22,018 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:46:23,215 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:46:23,267 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334781524 with entries=89, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334783215
2014-07-14 03:46:26,348 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:46:26,651 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25211 synced till here 25209
2014-07-14 03:46:26,665 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334783215 with entries=112, filesize=79.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334786348
2014-07-14 03:46:29,053 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:46:29,080 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25300 synced till here 25299
2014-07-14 03:46:29,133 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334786348 with entries=89, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334789053
2014-07-14 03:46:30,216 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:30,217 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:30,246 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:30,262 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:30,273 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:30,293 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:30,334 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:30,368 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:30,409 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:30,448 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:30,482 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:30,511 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:30,548 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:30,583 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:30,611 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:30,644 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:30,687 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:30,721 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:31,250 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:31,262 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:31,276 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:31,317 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:31,344 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:31,388 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:31,422 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:31,463 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:31,578 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:31,602 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:31,752 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:32,588 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:32,626 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:32,669 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:32,691 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:32,724 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:32,765 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:32,802 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:32,835 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:32,873 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:32,915 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:32,951 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:32,997 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:33,037 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:33,077 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:33,237 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:33,282 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:33,342 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:33,375 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:33,403 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:33,456 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:33,508 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:46:35,217 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:46:35,217 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:46:35,246 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:46:35,262 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:46:35,273 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:46:35,294 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:46:35,334 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:46:35,368 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:46:35,409 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:46:35,448 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:46:35,482 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:46:35,511 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:46:35,549 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:46:35,583 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:46:35,611 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:46:35,644 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:46:35,687 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:46:35,721 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:46:36,621 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5018ms
2014-07-14 03:46:36,621 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5371ms
2014-07-14 03:46:36,621 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5359ms
2014-07-14 03:46:36,622 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5345ms
2014-07-14 03:46:36,622 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5305ms
2014-07-14 03:46:36,622 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5278ms
2014-07-14 03:46:36,623 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5235ms
2014-07-14 03:46:36,623 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5201ms
2014-07-14 03:46:36,623 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5160ms
2014-07-14 03:46:36,624 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5046ms
2014-07-14 03:46:36,753 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:46:37,589 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-14 03:46:37,627 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:46:37,670 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:46:37,692 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:46:37,724 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:46:37,765 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:46:37,802 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:46:37,835 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:46:37,874 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:46:37,916 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:46:37,951 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:46:37,998 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:46:38,037 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:46:38,077 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:46:38,238 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:46:38,283 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:46:38,343 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:46:38,376 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:46:38,404 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:46:38,457 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:46:38,508 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:46:40,217 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:46:40,217 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:46:40,247 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:46:40,263 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:46:40,274 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:46:40,294 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:46:40,334 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:46:40,368 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:46:40,410 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:46:40,449 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:46:40,483 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:46:40,511 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:46:40,549 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:46:40,584 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:46:40,612 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:46:40,645 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:46:40,688 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:46:40,722 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:46:42,108 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10355ms
2014-07-14 03:46:42,108 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10506ms
2014-07-14 03:46:42,109 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10859ms
2014-07-14 03:46:42,109 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10847ms
2014-07-14 03:46:42,109 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10833ms
2014-07-14 03:46:42,110 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10792ms
2014-07-14 03:46:42,110 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10766ms
2014-07-14 03:46:42,111 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10722ms
2014-07-14 03:46:42,111 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10689ms
2014-07-14 03:46:42,111 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10648ms
2014-07-14 03:46:42,111 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10534ms
2014-07-14 03:46:42,590 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-14 03:46:42,627 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:46:42,670 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:46:42,692 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:46:42,724 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:46:42,765 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:46:42,802 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:46:42,835 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:46:42,874 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:46:42,916 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:46:42,951 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:46:42,998 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:46:43,038 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 03:46:43,077 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:46:43,239 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 03:46:43,283 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:46:43,343 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:46:43,376 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:46:43,404 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:46:43,457 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:46:43,508 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:46:45,217 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 03:46:45,218 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 03:46:45,247 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 03:46:45,263 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 03:46:45,274 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 03:46:45,294 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 03:46:45,335 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 03:46:45,369 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 03:46:45,410 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 03:46:45,449 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 03:46:45,483 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 03:46:45,512 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 03:46:45,550 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 03:46:45,584 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 03:46:45,612 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 03:46:45,645 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 03:46:45,688 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 03:46:45,722 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 03:46:47,808 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15007ms
2014-07-14 03:46:47,808 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16420ms
2014-07-14 03:46:47,809 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16206ms
2014-07-14 03:46:47,809 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16387ms
2014-07-14 03:46:47,809 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16346ms
2014-07-14 03:46:47,809 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16232ms
2014-07-14 03:46:47,810 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15223ms
2014-07-14 03:46:47,810 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15184ms
2014-07-14 03:46:47,810 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15141ms
2014-07-14 03:46:47,810 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15119ms
2014-07-14 03:46:47,810 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15086ms
2014-07-14 03:46:47,810 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15045ms
2014-07-14 03:46:47,811 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16059ms
2014-07-14 03:46:47,811 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16561ms
2014-07-14 03:46:47,811 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16549ms
2014-07-14 03:46:47,811 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16535ms
2014-07-14 03:46:47,811 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16494ms
2014-07-14 03:46:47,812 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16468ms
2014-07-14 03:46:47,835 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 03:46:47,875 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 03:46:47,916 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 03:46:47,952 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 03:46:47,998 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 03:46:48,038 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 03:46:48,078 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 03:46:48,239 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 03:46:48,283 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 03:46:48,343 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 03:46:48,376 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 03:46:48,405 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 03:46:48,457 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 03:46:48,509 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 03:46:50,218 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 03:46:50,218 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 03:46:50,247 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 03:46:50,263 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 03:46:50,274 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 03:46:50,294 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 03:46:50,335 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 03:46:50,369 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 03:46:50,410 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 03:46:50,449 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 03:46:50,483 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 03:46:50,512 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 03:46:50,550 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 03:46:50,584 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 03:46:50,612 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 03:46:50,645 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 03:46:50,688 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 03:46:50,722 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 03:46:52,810 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20007ms
2014-07-14 03:46:52,810 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20141ms
2014-07-14 03:46:52,811 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20185ms
2014-07-14 03:46:52,811 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21059ms
2014-07-14 03:46:52,811 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20224ms
2014-07-14 03:46:52,811 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21234ms
2014-07-14 03:46:52,812 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21349ms
2014-07-14 03:46:52,812 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21390ms
2014-07-14 03:46:52,812 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21210ms
2014-07-14 03:46:52,812 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21424ms
2014-07-14 03:46:52,813 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21468ms
2014-07-14 03:46:52,813 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21496ms
2014-07-14 03:46:52,814 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21538ms
2014-07-14 03:46:52,815 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21553ms
2014-07-14 03:46:52,815 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21565ms
2014-07-14 03:46:52,815 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20050ms
2014-07-14 03:46:52,815 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20091ms
2014-07-14 03:46:52,816 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20125ms
2014-07-14 03:46:52,836 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 03:46:52,875 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 03:46:52,917 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 03:46:52,952 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 03:46:52,999 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 03:46:53,038 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 03:46:53,078 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 03:46:53,240 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20003ms
2014-07-14 03:46:53,284 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 03:46:54,196 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20687ms
2014-07-14 03:46:54,196 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20854ms
2014-07-14 03:46:54,197 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20822ms
2014-07-14 03:46:54,197 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20794ms
2014-07-14 03:46:54,197 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20741ms
2014-07-14 03:46:55,218 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 03:46:55,219 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 03:46:55,247 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 03:46:55,263 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 03:46:55,275 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 03:46:55,294 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 03:46:55,335 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 03:46:55,369 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 03:46:55,410 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 03:46:55,450 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 03:46:55,484 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 03:46:55,512 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 03:46:55,550 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 03:46:55,584 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 03:46:55,612 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 03:46:55,645 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 03:46:55,689 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 03:46:55,723 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 03:46:57,811 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25009ms
2014-07-14 03:46:57,811 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25142ms
2014-07-14 03:46:57,811 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25185ms
2014-07-14 03:46:57,811 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26059ms
2014-07-14 03:46:57,812 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25224ms
2014-07-14 03:46:57,812 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26235ms
2014-07-14 03:46:57,812 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26349ms
2014-07-14 03:46:57,812 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26390ms
2014-07-14 03:46:57,812 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26210ms
2014-07-14 03:46:57,812 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26424ms
2014-07-14 03:46:57,813 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26469ms
2014-07-14 03:46:57,814 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26497ms
2014-07-14 03:46:57,815 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26539ms
2014-07-14 03:46:57,815 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26553ms
2014-07-14 03:46:57,816 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25051ms
2014-07-14 03:46:57,816 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25092ms
2014-07-14 03:46:57,816 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26566ms
2014-07-14 03:46:57,817 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25125ms
2014-07-14 03:46:57,836 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 03:46:57,876 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25003ms
2014-07-14 03:46:57,917 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 03:46:57,952 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 03:46:57,999 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 03:46:58,039 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25003ms
2014-07-14 03:46:58,078 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 03:46:58,168 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6250, memsize=1.2g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/6ff60444342f409a823f0c8d2aa77f30
2014-07-14 03:46:58,185 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/6ff60444342f409a823f0c8d2aa77f30 as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/6ff60444342f409a823f0c8d2aa77f30
2014-07-14 03:46:58,195 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/6ff60444342f409a823f0c8d2aa77f30, entries=4475190, sequenceid=6250, filesize=318.4m
2014-07-14 03:46:58,195 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.2g/1332994240, currentsize=164.4m/172415280 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 44644ms, sequenceid=6250, compaction requested=true
2014-07-14 03:46:58,196 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:46:58,196 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 20 blocking
2014-07-14 03:46:58,196 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25119ms
2014-07-14 03:46:58,196 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-14 03:46:58,196 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,196 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 98653ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:46:58,196 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25160ms
2014-07-14 03:46:58,196 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,196 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:46:58,196 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 1.0g
2014-07-14 03:46:58,197 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:46:58,197 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25200ms
2014-07-14 03:46:58,197 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:46:58,197 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,201 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25250ms
2014-07-14 03:46:58,201 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,201 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25286ms
2014-07-14 03:46:58,201 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,201 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25328ms
2014-07-14 03:46:58,202 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,202 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25367ms
2014-07-14 03:46:58,202 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,202 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25511ms
2014-07-14 03:46:58,202 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,211 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26961ms
2014-07-14 03:46:58,211 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,211 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25487ms
2014-07-14 03:46:58,211 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,211 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25446ms
2014-07-14 03:46:58,211 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,213 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26951ms
2014-07-14 03:46:58,213 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,214 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26938ms
2014-07-14 03:46:58,214 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,214 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26897ms
2014-07-14 03:46:58,214 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,214 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26870ms
2014-07-14 03:46:58,214 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,214 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26826ms
2014-07-14 03:46:58,214 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,215 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26613ms
2014-07-14 03:46:58,215 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,215 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26793ms
2014-07-14 03:46:58,215 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,218 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26755ms
2014-07-14 03:46:58,218 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,218 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26641ms
2014-07-14 03:46:58,219 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,221 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25634ms
2014-07-14 03:46:58,221 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,223 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26471ms
2014-07-14 03:46:58,223 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,223 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25597ms
2014-07-14 03:46:58,223 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,223 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25554ms
2014-07-14 03:46:58,224 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,224 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25423ms
2014-07-14 03:46:58,224 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,225 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27504ms
2014-07-14 03:46:58,225 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,225 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27538ms
2014-07-14 03:46:58,225 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,226 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27582ms
2014-07-14 03:46:58,226 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,226 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27615ms
2014-07-14 03:46:58,226 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,226 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27643ms
2014-07-14 03:46:58,226 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,226 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27678ms
2014-07-14 03:46:58,227 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,227 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27716ms
2014-07-14 03:46:58,227 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,228 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27746ms
2014-07-14 03:46:58,228 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,229 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27780ms
2014-07-14 03:46:58,229 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,230 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27821ms
2014-07-14 03:46:58,231 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,233 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27865ms
2014-07-14 03:46:58,233 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,235 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27900ms
2014-07-14 03:46:58,235 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,237 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27944ms
2014-07-14 03:46:58,237 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,237 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27964ms
2014-07-14 03:46:58,237 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,240 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27978ms
2014-07-14 03:46:58,240 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,240 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25003ms
2014-07-14 03:46:58,240 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,241 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27995ms
2014-07-14 03:46:58,241 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,241 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28025ms
2014-07-14 03:46:58,241 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,241 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28025ms
2014-07-14 03:46:58,242 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,242 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24786ms
2014-07-14 03:46:58,242 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,254 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24850ms
2014-07-14 03:46:58,254 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,254 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24879ms
2014-07-14 03:46:58,254 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,254 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24912ms
2014-07-14 03:46:58,254 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,254 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24746ms
2014-07-14 03:46:58,254 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,255 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24973ms
2014-07-14 03:46:58,256 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:46:58,291 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28256,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334790033,"queuetimems":0,"class":"HRegionServer","responsesize":12927,"method":"Multi"}
2014-07-14 03:46:58,350 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28279,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334790070,"queuetimems":0,"class":"HRegionServer","responsesize":12906,"method":"Multi"}
2014-07-14 03:46:59,491 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29298,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334790193,"queuetimems":0,"class":"HRegionServer","responsesize":12892,"method":"Multi"}
2014-07-14 03:46:59,655 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29525,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334790129,"queuetimems":1,"class":"HRegionServer","responsesize":13227,"method":"Multi"}
2014-07-14 03:46:59,759 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:46:59,865 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25399 synced till here 25398
2014-07-14 03:46:59,888 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334789053 with entries=99, filesize=75.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334819760
2014-07-14 03:46:59,889 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334635247
2014-07-14 03:46:59,889 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334651611
2014-07-14 03:46:59,889 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334653094
2014-07-14 03:46:59,929 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26854,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334793074,"queuetimems":0,"class":"HRegionServer","responsesize":12927,"method":"Multi"}
2014-07-14 03:46:59,961 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26926,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334793034,"queuetimems":0,"class":"HRegionServer","responsesize":12966,"method":"Multi"}
2014-07-14 03:47:00,037 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27123,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334792913,"queuetimems":0,"class":"HRegionServer","responsesize":13362,"method":"Multi"}
2014-07-14 03:47:00,037 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27087,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334792949,"queuetimems":1,"class":"HRegionServer","responsesize":13198,"method":"Multi"}
2014-07-14 03:47:00,037 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27041,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334792995,"queuetimems":1,"class":"HRegionServer","responsesize":13227,"method":"Multi"}
2014-07-14 03:47:00,037 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27166,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334792870,"queuetimems":2,"class":"HRegionServer","responsesize":13094,"method":"Multi"}
2014-07-14 03:47:00,230 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:47:01,261 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:01,262 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30817,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334790444,"queuetimems":0,"class":"HRegionServer","responsesize":12878,"method":"Multi"}
2014-07-14 03:47:01,278 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25523 synced till here 25489
2014-07-14 03:47:01,451 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31125,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334790326,"queuetimems":0,"class":"HRegionServer","responsesize":13146,"method":"Multi"}
2014-07-14 03:47:01,451 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27999,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334793452,"queuetimems":1,"class":"HRegionServer","responsesize":13710,"method":"Multi"}
2014-07-14 03:47:01,463 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30949,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334790508,"queuetimems":1,"class":"HRegionServer","responsesize":12501,"method":"Multi"}
2014-07-14 03:47:01,464 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27960,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334793503,"queuetimems":0,"class":"HRegionServer","responsesize":13077,"method":"Multi"}
2014-07-14 03:47:01,465 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30045,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334791420,"queuetimems":1,"class":"HRegionServer","responsesize":13152,"method":"Multi"}
2014-07-14 03:47:01,465 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30849,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334790608,"queuetimems":0,"class":"HRegionServer","responsesize":12966,"method":"Multi"}
2014-07-14 03:47:01,465 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b.
2014-07-14 03:47:01,466 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29866,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334791600,"queuetimems":1,"class":"HRegionServer","responsesize":12706,"method":"Multi"}
2014-07-14 03:47:01,466 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28801,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334792665,"queuetimems":0,"class":"HRegionServer","responsesize":13094,"method":"Multi"}
2014-07-14 03:47:01,473 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28894,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334792579,"queuetimems":0,"class":"HRegionServer","responsesize":13192,"method":"Multi"}
2014-07-14 03:47:01,473 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30896,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334790577,"queuetimems":0,"class":"HRegionServer","responsesize":13077,"method":"Multi"}
2014-07-14 03:47:01,621 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334819760 with entries=124, filesize=94.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334821261
2014-07-14 03:47:01,822 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28422,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334793399,"queuetimems":0,"class":"HRegionServer","responsesize":12829,"method":"Multi"}
2014-07-14 03:47:01,822 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30573,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334791248,"queuetimems":491,"class":"HRegionServer","responsesize":13094,"method":"Multi"}
2014-07-14 03:47:01,822 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30547,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334791274,"queuetimems":1,"class":"HRegionServer","responsesize":12839,"method":"Multi"}
2014-07-14 03:47:01,829 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28458,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334793371,"queuetimems":0,"class":"HRegionServer","responsesize":12752,"method":"Multi"}
2014-07-14 03:47:01,837 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31358,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334790479,"queuetimems":1,"class":"HRegionServer","responsesize":12827,"method":"Multi"}
2014-07-14 03:47:01,837 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28603,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334793234,"queuetimems":0,"class":"HRegionServer","responsesize":12785,"method":"Multi"}
2014-07-14 03:47:01,841 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28562,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334793279,"queuetimems":1,"class":"HRegionServer","responsesize":12892,"method":"Multi"}
2014-07-14 03:47:01,845 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30503,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334791342,"queuetimems":1,"class":"HRegionServer","responsesize":12606,"method":"Multi"}
2014-07-14 03:47:01,847 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29157,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334792690,"queuetimems":0,"class":"HRegionServer","responsesize":12906,"method":"Multi"}
2014-07-14 03:47:01,849 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30273,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334791576,"queuetimems":1,"class":"HRegionServer","responsesize":12973,"method":"Multi"}
2014-07-14 03:47:01,849 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31132,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334790717,"queuetimems":0,"class":"HRegionServer","responsesize":13094,"method":"Multi"}
2014-07-14 03:47:01,849 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30534,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334791315,"queuetimems":0,"class":"HRegionServer","responsesize":12805,"method":"Multi"}
2014-07-14 03:47:01,849 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30463,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334791386,"queuetimems":0,"class":"HRegionServer","responsesize":13392,"method":"Multi"}
2014-07-14 03:47:01,850 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31487,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334790363,"queuetimems":0,"class":"HRegionServer","responsesize":12508,"method":"Multi"}
2014-07-14 03:47:01,850 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30590,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334791260,"queuetimems":0,"class":"HRegionServer","responsesize":13107,"method":"Multi"}
2014-07-14 03:47:01,850 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29128,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334792722,"queuetimems":0,"class":"HRegionServer","responsesize":13150,"method":"Multi"}
2014-07-14 03:47:01,850 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31166,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334790684,"queuetimems":0,"class":"HRegionServer","responsesize":13192,"method":"Multi"}
2014-07-14 03:47:01,851 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29087,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334792763,"queuetimems":1,"class":"HRegionServer","responsesize":13008,"method":"Multi"}
2014-07-14 03:47:01,853 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30392,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334791461,"queuetimems":0,"class":"HRegionServer","responsesize":12657,"method":"Multi"}
2014-07-14 03:47:01,853 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29020,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334792833,"queuetimems":0,"class":"HRegionServer","responsesize":13077,"method":"Multi"}
2014-07-14 03:47:01,853 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31563,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334790290,"queuetimems":0,"class":"HRegionServer","responsesize":13362,"method":"Multi"}
2014-07-14 03:47:01,850 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30100,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334791750,"queuetimems":0,"class":"HRegionServer","responsesize":12873,"method":"Multi"}
2014-07-14 03:47:01,865 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29067,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334792798,"queuetimems":1,"class":"HRegionServer","responsesize":13082,"method":"Multi"}
2014-07-14 03:47:01,873 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31467,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334790406,"queuetimems":1,"class":"HRegionServer","responsesize":12902,"method":"Multi"}
2014-07-14 03:47:01,894 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31350,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334790543,"queuetimems":0,"class":"HRegionServer","responsesize":13008,"method":"Multi"}
2014-07-14 03:47:01,897 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31657,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334790240,"queuetimems":1,"class":"HRegionServer","responsesize":13150,"method":"Multi"}
2014-07-14 03:47:01,901 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31260,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334790641,"queuetimems":1,"class":"HRegionServer","responsesize":13082,"method":"Multi"}
2014-07-14 03:47:01,945 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29321,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334792624,"queuetimems":0,"class":"HRegionServer","responsesize":13107,"method":"Multi"}
2014-07-14 03:47:01,945 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28612,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334793333,"queuetimems":0,"class":"HRegionServer","responsesize":12501,"method":"Multi"}
2014-07-14 03:47:02,101 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:47:02,642 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:03,227 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25652 synced till here 25642
2014-07-14 03:47:03,292 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334821261 with entries=129, filesize=80.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334822642
2014-07-14 03:47:03,929 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:03,976 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25744 synced till here 25731
2014-07-14 03:47:04,036 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334822642 with entries=92, filesize=69.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334823929
2014-07-14 03:47:05,412 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:05,431 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25888 synced till here 25848
2014-07-14 03:47:06,629 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334823929 with entries=144, filesize=99.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334825412
2014-07-14 03:47:07,472 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:07,488 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26008 synced till here 25973
2014-07-14 03:47:07,675 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334825412 with entries=120, filesize=92.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334827472
2014-07-14 03:47:08,286 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6270, memsize=1.1g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/455c070e8e3f4267ab9adf17316991d9
2014-07-14 03:47:08,311 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/455c070e8e3f4267ab9adf17316991d9 as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/455c070e8e3f4267ab9adf17316991d9
2014-07-14 03:47:08,325 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/455c070e8e3f4267ab9adf17316991d9, entries=4147950, sequenceid=6270, filesize=295.1m
2014-07-14 03:47:08,326 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.2g/1238875680, currentsize=424.4m/445048240 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 47243ms, sequenceid=6270, compaction requested=true
2014-07-14 03:47:08,326 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:47:08,326 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 20 blocking
2014-07-14 03:47:08,326 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-14 03:47:08,326 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b., current region memstore size 323.5m
2014-07-14 03:47:08,326 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:47:08,327 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:47:08,327 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:47:08,354 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:47:08,683 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:47:08,716 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:09,226 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26164 synced till here 26131
2014-07-14 03:47:09,386 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334827472 with entries=156, filesize=105.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334828716
2014-07-14 03:47:09,386 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334654430
2014-07-14 03:47:09,386 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334656166
2014-07-14 03:47:09,386 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334657910
2014-07-14 03:47:09,386 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334659999
2014-07-14 03:47:09,387 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334662948
2014-07-14 03:47:09,387 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334664816
2014-07-14 03:47:09,387 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334680221
2014-07-14 03:47:09,387 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334683150
2014-07-14 03:47:10,714 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:10,847 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26290 synced till here 26264
2014-07-14 03:47:11,032 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334828716 with entries=126, filesize=83.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334830715
2014-07-14 03:47:12,607 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:12,679 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26415 synced till here 26403
2014-07-14 03:47:12,763 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334830715 with entries=125, filesize=94.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334832608
2014-07-14 03:47:14,106 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:14,182 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26548 synced till here 26531
2014-07-14 03:47:14,503 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334832608 with entries=133, filesize=83.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334834107
2014-07-14 03:47:15,426 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:16,052 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26653 synced till here 26636
2014-07-14 03:47:16,345 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334834107 with entries=105, filesize=86.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334835426
2014-07-14 03:47:17,121 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:17,182 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26775 synced till here 26763
2014-07-14 03:47:17,923 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334835426 with entries=122, filesize=74.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334837121
2014-07-14 03:47:18,632 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:18,656 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26874 synced till here 26863
2014-07-14 03:47:18,759 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334837121 with entries=99, filesize=68.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334838632
2014-07-14 03:47:21,044 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:21,434 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6514, memsize=254.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/9c93b74d37ef4f92b9075c0c64deb9c0
2014-07-14 03:47:21,489 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334838632 with entries=105, filesize=70.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334841044
2014-07-14 03:47:21,496 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/9c93b74d37ef4f92b9075c0c64deb9c0 as hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/9c93b74d37ef4f92b9075c0c64deb9c0
2014-07-14 03:47:21,508 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/9c93b74d37ef4f92b9075c0c64deb9c0, entries=927520, sequenceid=6514, filesize=66.0m
2014-07-14 03:47:21,508 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~328.5m/344452000, currentsize=110.4m/115814800 for region usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. in 13182ms, sequenceid=6514, compaction requested=true
2014-07-14 03:47:21,508 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:47:21,509 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 20 blocking
2014-07-14 03:47:21,509 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. has too many store files; delaying flush up to 90000ms
2014-07-14 03:47:21,509 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-14 03:47:21,509 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-14 03:47:21,509 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:47:21,509 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. has too many store files; delaying flush up to 90000ms
2014-07-14 03:47:21,509 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:47:21,509 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:2), split_queue=0, merge_queue=0
2014-07-14 03:47:21,509 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. because compaction request was cancelled
2014-07-14 03:47:21,509 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 20 blocking
2014-07-14 03:47:21,509 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-14 03:47:21,509 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:47:21,510 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:47:21,510 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:47:21,510 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 20 blocking
2014-07-14 03:47:21,510 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-14 03:47:21,510 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:47:21,510 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:47:21,510 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:47:23,900 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:24,245 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27081 synced till here 27079
2014-07-14 03:47:24,262 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334841044 with entries=102, filesize=72.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334843901
2014-07-14 03:47:26,914 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:26,930 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27169 synced till here 27168
2014-07-14 03:47:26,940 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334843901 with entries=88, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334846914
2014-07-14 03:47:28,637 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:28,653 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27258 synced till here 27253
2014-07-14 03:47:28,757 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334846914 with entries=89, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334848638
2014-07-14 03:47:30,350 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:30,368 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27352 synced till here 27346
2014-07-14 03:47:30,446 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 03:47:30,452 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334848638 with entries=94, filesize=67.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334850351
2014-07-14 03:47:30,506 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. has too many store files, but is 944.6m vs best flushable region's 152.0m. Choosing the bigger.
2014-07-14 03:47:30,507 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. due to global heap pressure
2014-07-14 03:47:30,507 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 944.6m
2014-07-14 03:47:31,674 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:31,691 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27448 synced till here 27444
2014-07-14 03:47:31,716 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:47:32,117 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334850351 with entries=96, filesize=65.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334851674
2014-07-14 03:47:32,835 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:32,836 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:32,838 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:32,838 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:32,842 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:32,907 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:32,910 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:32,911 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:32,911 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:32,913 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:33,059 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:33,102 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:33,136 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:33,166 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:33,203 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:33,230 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:33,264 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:33,291 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:34,183 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:34,204 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:34,229 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:34,279 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:34,359 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:34,628 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:34,708 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:34,930 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:35,046 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:35,075 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:35,099 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:35,138 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:35,163 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:35,193 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:35,222 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:35,271 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:35,309 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:35,340 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:35,376 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:35,407 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:35,622 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:36,391 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:36,762 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:37,073 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:37,106 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:37,141 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:37,171 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:37,205 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:37,233 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:37,265 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:37,295 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:37,330 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:47:37,836 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:47:37,836 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:47:37,838 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:47:37,839 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:47:37,842 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:47:37,908 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:47:37,911 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:47:37,911 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:47:37,911 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:47:37,913 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:47:38,059 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:47:38,103 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:47:38,137 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:47:38,167 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:47:38,203 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:47:38,231 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:47:38,265 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:47:38,292 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:47:39,185 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:47:39,204 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:47:39,229 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:47:39,280 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:47:39,360 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:47:40,306 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5035ms
2014-07-14 03:47:40,307 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5207ms
2014-07-14 03:47:40,307 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5169ms
2014-07-14 03:47:40,307 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5144ms
2014-07-14 03:47:40,308 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5114ms
2014-07-14 03:47:40,308 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5086ms
2014-07-14 03:47:40,309 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5680ms
2014-07-14 03:47:40,309 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5601ms
2014-07-14 03:47:40,309 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5379ms
2014-07-14 03:47:40,310 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5264ms
2014-07-14 03:47:40,310 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5235ms
2014-07-14 03:47:40,310 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:47:40,341 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:47:40,377 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:47:40,407 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:47:40,622 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:47:41,392 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:47:41,762 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:47:42,074 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:47:42,107 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:47:42,142 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:47:42,172 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:47:42,205 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:47:42,234 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:47:42,266 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:47:42,295 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:47:42,330 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:47:42,836 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:47:42,837 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:47:42,839 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:47:42,839 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:47:42,843 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:47:42,860 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6356, memsize=1004.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/63de64c91c9b49efaddbfbfd1855a04c
2014-07-14 03:47:42,871 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/63de64c91c9b49efaddbfbfd1855a04c as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/63de64c91c9b49efaddbfbfd1855a04c
2014-07-14 03:47:42,888 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/63de64c91c9b49efaddbfbfd1855a04c, entries=3657650, sequenceid=6356, filesize=260.4m
2014-07-14 03:47:42,889 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.0g/1099141760, currentsize=847.3m/888442160 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 44693ms, sequenceid=6356, compaction requested=true
2014-07-14 03:47:42,889 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:47:42,890 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 20 blocking
2014-07-14 03:47:42,890 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-14 03:47:42,890 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10048ms
2014-07-14 03:47:42,890 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:47:42,890 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,890 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:47:42,890 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10052ms
2014-07-14 03:47:42,890 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,890 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10052ms
2014-07-14 03:47:42,890 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,890 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10054ms
2014-07-14 03:47:42,890 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,893 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10058ms
2014-07-14 03:47:42,893 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,893 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5563ms
2014-07-14 03:47:42,893 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,897 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5602ms
2014-07-14 03:47:42,890 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:47:42,897 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,897 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5632ms
2014-07-14 03:47:42,897 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,897 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5664ms
2014-07-14 03:47:42,897 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,897 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5693ms
2014-07-14 03:47:42,897 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,898 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5726ms
2014-07-14 03:47:42,898 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,905 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5764ms
2014-07-14 03:47:42,905 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,905 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5799ms
2014-07-14 03:47:42,905 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,905 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5832ms
2014-07-14 03:47:42,905 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,905 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6143ms
2014-07-14 03:47:42,905 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,905 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6514ms
2014-07-14 03:47:42,905 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,908 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:47:42,908 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,911 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:47:42,911 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,911 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:47:42,912 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,912 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:47:42,912 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,914 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:47:42,914 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,918 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7296ms
2014-07-14 03:47:42,918 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,918 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7511ms
2014-07-14 03:47:42,918 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,918 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7542ms
2014-07-14 03:47:42,919 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,919 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7579ms
2014-07-14 03:47:42,919 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,920 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7610ms
2014-07-14 03:47:42,920 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,925 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7850ms
2014-07-14 03:47:42,925 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,926 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7880ms
2014-07-14 03:47:42,926 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,926 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7996ms
2014-07-14 03:47:42,927 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,927 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8219ms
2014-07-14 03:47:42,927 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,932 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8304ms
2014-07-14 03:47:42,932 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,932 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7710ms
2014-07-14 03:47:42,933 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,937 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7744ms
2014-07-14 03:47:42,937 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,945 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7782ms
2014-07-14 03:47:42,945 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,953 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7815ms
2014-07-14 03:47:42,953 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,953 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7854ms
2014-07-14 03:47:42,953 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,954 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7682ms
2014-07-14 03:47:42,954 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,954 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8595ms
2014-07-14 03:47:42,954 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,954 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8675ms
2014-07-14 03:47:42,954 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,955 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8726ms
2014-07-14 03:47:42,955 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,955 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8751ms
2014-07-14 03:47:42,955 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,955 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8772ms
2014-07-14 03:47:42,955 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,956 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10536,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334852419,"queuetimems":1,"class":"HRegionServer","responsesize":12987,"method":"Multi"}
2014-07-14 03:47:42,956 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10580,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334852375,"queuetimems":0,"class":"HRegionServer","responsesize":12912,"method":"Multi"}
2014-07-14 03:47:42,956 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10660,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334852295,"queuetimems":1,"class":"HRegionServer","responsesize":13060,"method":"Multi"}
2014-07-14 03:47:42,956 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9665ms
2014-07-14 03:47:42,956 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,958 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9693ms
2014-07-14 03:47:42,958 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,958 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9728ms
2014-07-14 03:47:42,958 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,961 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9758ms
2014-07-14 03:47:42,961 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,961 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9795ms
2014-07-14 03:47:42,961 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,968 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9832ms
2014-07-14 03:47:42,969 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,969 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9867ms
2014-07-14 03:47:42,969 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:42,969 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9910ms
2014-07-14 03:47:42,969 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:47:43,255 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:43,256 DEBUG [RpcServer.handler=38,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:47:43,256 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. has too many store files; delaying flush up to 90000ms
2014-07-14 03:47:43,257 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 20 blocking
2014-07-14 03:47:43,257 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:47:43,257 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-14 03:47:43,258 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:47:43,258 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:47:43,258 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:47:43,289 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27576 synced till here 27538
2014-07-14 03:47:43,566 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11004,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334852561,"queuetimems":0,"class":"HRegionServer","responsesize":13541,"method":"Multi"}
2014-07-14 03:47:43,566 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11071,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334852494,"queuetimems":0,"class":"HRegionServer","responsesize":12791,"method":"Multi"}
2014-07-14 03:47:43,626 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334851674 with entries=128, filesize=93.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334863255
2014-07-14 03:47:43,629 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334684351
2014-07-14 03:47:43,629 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334690141
2014-07-14 03:47:43,629 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334693303
2014-07-14 03:47:43,629 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334694419
2014-07-14 03:47:43,629 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334696713
2014-07-14 03:47:43,629 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334698320
2014-07-14 03:47:43,629 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334700857
2014-07-14 03:47:43,629 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334714445
2014-07-14 03:47:43,630 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334715820
2014-07-14 03:47:43,630 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334718786
2014-07-14 03:47:43,630 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334720570
2014-07-14 03:47:43,630 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334722097
2014-07-14 03:47:43,630 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334724528
2014-07-14 03:47:43,630 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334726250
2014-07-14 03:47:43,630 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334727961
2014-07-14 03:47:43,630 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334742907
2014-07-14 03:47:43,630 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334745213
2014-07-14 03:47:43,630 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334746289
2014-07-14 03:47:43,631 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334750053
2014-07-14 03:47:43,631 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334753417
2014-07-14 03:47:43,631 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334756037
2014-07-14 03:47:43,631 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334758555
2014-07-14 03:47:43,631 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334760026
2014-07-14 03:47:43,631 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334761923
2014-07-14 03:47:44,659 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12026,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334852633,"queuetimems":0,"class":"HRegionServer","responsesize":12752,"method":"Multi"}
2014-07-14 03:47:44,998 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12260,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334852738,"queuetimems":1,"class":"HRegionServer","responsesize":13063,"method":"Multi"}
2014-07-14 03:47:44,998 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12307,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334852691,"queuetimems":1,"class":"HRegionServer","responsesize":13127,"method":"Multi"}
2014-07-14 03:47:44,998 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12216,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334852782,"queuetimems":5,"class":"HRegionServer","responsesize":13059,"method":"Multi"}
2014-07-14 03:47:45,325 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:45,328 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12436,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334852892,"queuetimems":0,"class":"HRegionServer","responsesize":13065,"method":"Multi"}
2014-07-14 03:47:45,359 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27705 synced till here 27675
2014-07-14 03:47:45,472 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10135,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334855336,"queuetimems":0,"class":"HRegionServer","responsesize":13063,"method":"Multi"}
2014-07-14 03:47:45,665 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334863255 with entries=129, filesize=96.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334865326
2014-07-14 03:47:46,542 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11140,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334855401,"queuetimems":0,"class":"HRegionServer","responsesize":13238,"method":"Multi"}
2014-07-14 03:47:46,542 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11275,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334855266,"queuetimems":0,"class":"HRegionServer","responsesize":13060,"method":"Multi"}
2014-07-14 03:47:46,542 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11350,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334855191,"queuetimems":0,"class":"HRegionServer","responsesize":12851,"method":"Multi"}
2014-07-14 03:47:46,542 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12185,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334854357,"queuetimems":0,"class":"HRegionServer","responsesize":12952,"method":"Multi"}
2014-07-14 03:47:46,542 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13254,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334853288,"queuetimems":0,"class":"HRegionServer","responsesize":12900,"method":"Multi"}
2014-07-14 03:47:46,543 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13284,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334853258,"queuetimems":0,"class":"HRegionServer","responsesize":12943,"method":"Multi"}
2014-07-14 03:47:46,543 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12341,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334854202,"queuetimems":1,"class":"HRegionServer","responsesize":13035,"method":"Multi"}
2014-07-14 03:47:46,550 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12322,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334854227,"queuetimems":1,"class":"HRegionServer","responsesize":13345,"method":"Multi"}
2014-07-14 03:47:46,565 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11522,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334855043,"queuetimems":0,"class":"HRegionServer","responsesize":12791,"method":"Multi"}
2014-07-14 03:47:46,565 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11940,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334854625,"queuetimems":0,"class":"HRegionServer","responsesize":13040,"method":"Multi"}
2014-07-14 03:47:46,565 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11193,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334855372,"queuetimems":1,"class":"HRegionServer","responsesize":13059,"method":"Multi"}
2014-07-14 03:47:46,565 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11346,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334855219,"queuetimems":0,"class":"HRegionServer","responsesize":12912,"method":"Multi"}
2014-07-14 03:47:46,566 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12385,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334854181,"queuetimems":0,"class":"HRegionServer","responsesize":12980,"method":"Multi"}
2014-07-14 03:47:46,567 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11433,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334855133,"queuetimems":0,"class":"HRegionServer","responsesize":13065,"method":"Multi"}
2014-07-14 03:47:46,566 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13408,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334853158,"queuetimems":0,"class":"HRegionServer","responsesize":12715,"method":"Multi"}
2014-07-14 03:47:46,574 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11413,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334855160,"queuetimems":0,"class":"HRegionServer","responsesize":12752,"method":"Multi"}
2014-07-14 03:47:46,581 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11275,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334855306,"queuetimems":0,"class":"HRegionServer","responsesize":13541,"method":"Multi"}
2014-07-14 03:47:46,581 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11510,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334855071,"queuetimems":0,"class":"HRegionServer","responsesize":12987,"method":"Multi"}
2014-07-14 03:47:46,581 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11874,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334854707,"queuetimems":1,"class":"HRegionServer","responsesize":13251,"method":"Multi"}
2014-07-14 03:47:46,582 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13490,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334853092,"queuetimems":1,"class":"HRegionServer","responsesize":13040,"method":"Multi"}
2014-07-14 03:47:46,587 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11662,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334854925,"queuetimems":0,"class":"HRegionServer","responsesize":13169,"method":"Multi"}
2014-07-14 03:47:46,589 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13461,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334853128,"queuetimems":0,"class":"HRegionServer","responsesize":13050,"method":"Multi"}
2014-07-14 03:47:46,589 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13396,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334853193,"queuetimems":0,"class":"HRegionServer","responsesize":12707,"method":"Multi"}
2014-07-14 03:47:46,589 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11492,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334855097,"queuetimems":0,"class":"HRegionServer","responsesize":13127,"method":"Multi"}
2014-07-14 03:47:46,596 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12319,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334854277,"queuetimems":0,"class":"HRegionServer","responsesize":13033,"method":"Multi"}
2014-07-14 03:47:46,597 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10213,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334856384,"queuetimems":0,"class":"HRegionServer","responsesize":13228,"method":"Multi"}
2014-07-14 03:47:46,618 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13391,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334853226,"queuetimems":0,"class":"HRegionServer","responsesize":12944,"method":"Multi"}
2014-07-14 03:47:46,618 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13564,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47291","starttimems":1405334853054,"queuetimems":0,"class":"HRegionServer","responsesize":12851,"method":"Multi"}
2014-07-14 03:47:47,300 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:47,361 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27833 synced till here 27818
2014-07-14 03:47:47,400 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334865326 with entries=128, filesize=82.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334867301
2014-07-14 03:47:49,013 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:49,063 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27939 synced till here 27914
2014-07-14 03:47:49,211 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334867301 with entries=106, filesize=81.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334869013
2014-07-14 03:47:50,441 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:50,734 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28085 synced till here 28036
2014-07-14 03:47:50,965 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334869013 with entries=146, filesize=99.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334870442
2014-07-14 03:47:52,566 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:52,572 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 03:47:52,572 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. has too many store files, but is 1.2g vs best flushable region's 238.2m. Choosing the bigger.
2014-07-14 03:47:52,572 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. due to global heap pressure
2014-07-14 03:47:52,573 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 1.2g
2014-07-14 03:47:52,807 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28218 synced till here 28217
2014-07-14 03:47:52,860 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334870442 with entries=133, filesize=86.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334872566
2014-07-14 03:47:52,903 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6870, memsize=392.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/c057103cc0ec439d825f50a0d07dc10e
2014-07-14 03:47:52,916 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/c057103cc0ec439d825f50a0d07dc10e as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/c057103cc0ec439d825f50a0d07dc10e
2014-07-14 03:47:52,924 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/c057103cc0ec439d825f50a0d07dc10e, entries=1427370, sequenceid=6870, filesize=101.7m
2014-07-14 03:47:52,925 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~955.3m/1001725200, currentsize=327.4m/343274080 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 22418ms, sequenceid=6870, compaction requested=true
2014-07-14 03:47:52,925 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:47:52,926 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 20 blocking
2014-07-14 03:47:52,926 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-14 03:47:52,926 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:47:52,926 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:47:52,926 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:47:52,981 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b.
2014-07-14 03:47:52,982 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b., current region memstore size 256.8m
2014-07-14 03:47:53,134 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:47:53,199 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:47:54,115 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:47:54,331 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:54,347 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28312 synced till here 28310
2014-07-14 03:47:54,361 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334872566 with entries=94, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334874331
2014-07-14 03:47:54,362 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334763648
2014-07-14 03:47:55,501 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:55,515 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28400 synced till here 28399
2014-07-14 03:47:55,526 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334874331 with entries=88, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334875502
2014-07-14 03:47:56,565 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:57,023 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28501 synced till here 28500
2014-07-14 03:47:57,041 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334875502 with entries=101, filesize=70.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334876566
2014-07-14 03:47:57,778 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7068, memsize=96.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/4970b47aa4804114bacd6f207a99f1a9
2014-07-14 03:47:57,798 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/4970b47aa4804114bacd6f207a99f1a9 as hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/4970b47aa4804114bacd6f207a99f1a9
2014-07-14 03:47:57,811 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/4970b47aa4804114bacd6f207a99f1a9, entries=349440, sequenceid=7068, filesize=24.9m
2014-07-14 03:47:57,812 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~257.7m/270220640, currentsize=37.6m/39383440 for region usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. in 4830ms, sequenceid=7068, compaction requested=true
2014-07-14 03:47:57,813 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:47:57,813 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. has too many store files; delaying flush up to 90000ms
2014-07-14 03:47:57,813 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-14 03:47:57,813 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 20 blocking
2014-07-14 03:47:57,813 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-14 03:47:57,813 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:47:57,813 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:47:57,813 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. because compaction request was cancelled
2014-07-14 03:47:57,814 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 20 blocking
2014-07-14 03:47:57,814 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-14 03:47:57,814 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:47:57,814 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:47:57,814 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:47:58,023 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:58,038 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28590 synced till here 28589
2014-07-14 03:47:58,046 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334876566 with entries=89, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334878023
2014-07-14 03:47:59,455 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:47:59,674 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28694 synced till here 28692
2014-07-14 03:47:59,703 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334878023 with entries=104, filesize=75.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334879456
2014-07-14 03:48:01,077 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:48:01,405 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28802 synced till here 28801
2014-07-14 03:48:01,452 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334879456 with entries=108, filesize=75.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334881078
2014-07-14 03:48:03,499 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:48:03,514 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28890 synced till here 28889
2014-07-14 03:48:03,527 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334881078 with entries=88, filesize=62.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334883499
2014-07-14 03:48:06,139 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:48:06,164 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334883499 with entries=88, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334886139
2014-07-14 03:48:08,060 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:48:08,077 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29067 synced till here 29063
2014-07-14 03:48:08,113 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334886139 with entries=89, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334888060
2014-07-14 03:48:09,010 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 03:48:09,011 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. has too many store files, but is 1.4g vs best flushable region's 103.1m. Choosing the bigger.
2014-07-14 03:48:09,011 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. due to global heap pressure
2014-07-14 03:48:09,011 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 1.4g
2014-07-14 03:48:09,344 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:48:09,364 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29158 synced till here 29156
2014-07-14 03:48:09,386 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334888060 with entries=91, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334889344
2014-07-14 03:48:10,435 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:48:10,980 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:48:11,002 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334889344 with entries=86, filesize=61.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334890980
2014-07-14 03:48:11,892 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:48:12,087 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:48:12,237 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:48:12,294 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:48:12,340 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:48:12,383 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:48:12,625 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:48:13,539 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:48:13,583 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:48:13,734 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:48:14,043 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:48:14,178 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:48:14,285 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7088, memsize=453.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/d7559ae8182a4078b9497ff3c9911d5d
2014-07-14 03:48:14,298 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:48:14,305 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/d7559ae8182a4078b9497ff3c9911d5d as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/d7559ae8182a4078b9497ff3c9911d5d
2014-07-14 03:48:14,320 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/d7559ae8182a4078b9497ff3c9911d5d, entries=1650880, sequenceid=7088, filesize=117.7m
2014-07-14 03:48:14,321 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.2g/1321481360, currentsize=407.2m/427031680 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 21748ms, sequenceid=7088, compaction requested=true
2014-07-14 03:48:14,321 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:48:14,321 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 20 blocking
2014-07-14 03:48:14,322 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25ms
2014-07-14 03:48:14,322 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:48:14,322 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-14 03:48:14,322 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 144ms
2014-07-14 03:48:14,322 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:48:14,322 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:48:14,322 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:48:14,322 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:48:14,322 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 279ms
2014-07-14 03:48:14,322 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:48:14,322 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 588ms
2014-07-14 03:48:14,322 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:48:14,322 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 740ms
2014-07-14 03:48:14,323 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:48:14,323 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 784ms
2014-07-14 03:48:14,323 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:48:14,324 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1698ms
2014-07-14 03:48:14,324 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:48:14,324 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1941ms
2014-07-14 03:48:14,324 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:48:14,329 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1989ms
2014-07-14 03:48:14,329 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:48:14,331 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2035ms
2014-07-14 03:48:14,331 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:48:14,331 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2094ms
2014-07-14 03:48:14,331 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:48:14,331 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2245ms
2014-07-14 03:48:14,331 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:48:14,333 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2441ms
2014-07-14 03:48:14,333 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:48:14,432 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:48:14,432 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. has too many store files; delaying flush up to 90000ms
2014-07-14 03:48:14,433 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:48:14,433 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 20 blocking
2014-07-14 03:48:14,433 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-14 03:48:14,433 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:48:14,433 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:48:14,433 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:48:14,839 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:48:14,869 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29337 synced till here 29327
2014-07-14 03:48:14,987 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334890980 with entries=93, filesize=70.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334894839
2014-07-14 03:48:14,987 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334778125
2014-07-14 03:48:14,987 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334781524
2014-07-14 03:48:14,987 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334783215
2014-07-14 03:48:14,988 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334786348
2014-07-14 03:48:16,387 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:48:16,584 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334894839 with entries=115, filesize=77.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334896388
2014-07-14 03:48:17,694 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:48:17,725 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29546 synced till here 29536
2014-07-14 03:48:18,101 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334896388 with entries=94, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334897695
2014-07-14 03:48:19,927 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:48:19,944 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29637 synced till here 29636
2014-07-14 03:48:19,961 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334897695 with entries=91, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334899928
2014-07-14 03:48:21,242 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:48:21,255 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29725 synced till here 29724
2014-07-14 03:48:21,266 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334899928 with entries=88, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334901242
2014-07-14 03:48:25,035 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:48:25,051 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29816 synced till here 29812
2014-07-14 03:48:25,121 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334901242 with entries=91, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334905036
2014-07-14 03:48:26,270 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:48:26,300 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334905036 with entries=89, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334906271
2014-07-14 03:48:29,673 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:48:29,694 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334906271 with entries=88, filesize=61.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334909674
2014-07-14 03:48:32,520 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:48:32,557 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30086 synced till here 30084
2014-07-14 03:48:32,600 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334909674 with entries=93, filesize=67.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334912521
2014-07-14 03:48:33,050 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 03:48:33,051 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. has too many store files, but is 1.0g vs best flushable region's 218.8m. Choosing the bigger.
2014-07-14 03:48:33,051 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. due to global heap pressure
2014-07-14 03:48:33,051 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 1.0g
2014-07-14 03:48:34,013 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:48:34,929 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:48:34,949 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30176 synced till here 30174
2014-07-14 03:48:34,967 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334912521 with entries=90, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334914929
2014-07-14 03:48:36,192 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7312, memsize=603.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/7fbc039e38fd48d98eca11e7f3a1fc24
2014-07-14 03:48:36,205 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/7fbc039e38fd48d98eca11e7f3a1fc24 as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/7fbc039e38fd48d98eca11e7f3a1fc24
2014-07-14 03:48:36,221 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/7fbc039e38fd48d98eca11e7f3a1fc24, entries=2197630, sequenceid=7312, filesize=156.5m
2014-07-14 03:48:36,222 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.4g/1541613200, currentsize=429.9m/450835200 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 27211ms, sequenceid=7312, compaction requested=true
2014-07-14 03:48:36,223 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:48:36,223 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 20 blocking
2014-07-14 03:48:36,223 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-14 03:48:36,224 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:48:36,224 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:48:36,224 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:48:36,301 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:48:36,301 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. has too many store files; delaying flush up to 90000ms
2014-07-14 03:48:36,301 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:48:36,302 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 20 blocking
2014-07-14 03:48:36,302 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-14 03:48:36,302 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:48:36,302 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:48:36,302 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:48:36,520 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:48:36,547 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30265 synced till here 30263
2014-07-14 03:48:36,586 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334914929 with entries=89, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334916521
2014-07-14 03:48:36,586 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334789053
2014-07-14 03:48:36,586 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334819760
2014-07-14 03:48:36,586 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334821261
2014-07-14 03:48:36,586 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334822642
2014-07-14 03:48:36,587 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334823929
2014-07-14 03:48:36,587 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334825412
2014-07-14 03:48:36,587 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334827472
2014-07-14 03:48:36,587 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334828716
2014-07-14 03:48:36,587 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334830715
2014-07-14 03:48:36,587 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334832608
2014-07-14 03:48:36,587 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334834107
2014-07-14 03:48:36,587 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334835426
2014-07-14 03:48:36,587 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334837121
2014-07-14 03:48:36,587 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334838632
2014-07-14 03:48:36,587 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334841044
2014-07-14 03:48:36,587 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334843901
2014-07-14 03:48:36,587 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334846914
2014-07-14 03:48:36,588 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334848638
2014-07-14 03:48:38,305 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:48:38,322 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30355 synced till here 30354
2014-07-14 03:48:38,346 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334916521 with entries=90, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334918306
2014-07-14 03:48:39,708 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b.
2014-07-14 03:48:39,708 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b., current region memstore size 256.3m
2014-07-14 03:48:39,857 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:48:41,289 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:48:41,315 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30446 synced till here 30445
2014-07-14 03:48:41,403 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334918306 with entries=91, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334921290
2014-07-14 03:48:43,044 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:48:43,294 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30534 synced till here 30533
2014-07-14 03:48:43,312 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334921290 with entries=88, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334923044
2014-07-14 03:48:44,387 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:48:44,407 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30622 synced till here 30619
2014-07-14 03:48:44,431 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334923044 with entries=88, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334924388
2014-07-14 03:48:48,520 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7617, memsize=250.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/586bb95094424e51a88a06ca9ff0715f
2014-07-14 03:48:48,540 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/586bb95094424e51a88a06ca9ff0715f as hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/586bb95094424e51a88a06ca9ff0715f
2014-07-14 03:48:48,554 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/586bb95094424e51a88a06ca9ff0715f, entries=913280, sequenceid=7617, filesize=65.0m
2014-07-14 03:48:48,554 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~256.3m/268716880, currentsize=32.4m/33931440 for region usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. in 8846ms, sequenceid=7617, compaction requested=true
2014-07-14 03:48:48,555 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:48:48,555 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 20 blocking
2014-07-14 03:48:48,555 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-14 03:48:48,556 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:48:48,556 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:48:48,556 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. because compaction request was cancelled
2014-07-14 03:48:53,263 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:48:53,658 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334924388 with entries=94, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334933263
2014-07-14 03:48:55,460 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:48:55,487 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30805 synced till here 30803
2014-07-14 03:48:55,509 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334933263 with entries=89, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334935461
2014-07-14 03:48:56,699 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:48:56,728 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334935461 with entries=87, filesize=61.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334936700
2014-07-14 03:48:59,533 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:48:59,557 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30980 synced till here 30979
2014-07-14 03:48:59,580 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334936700 with entries=88, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334939534
2014-07-14 03:49:01,853 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7552, memsize=754.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/c0a6bb911ddf4bcd9012bcea1f6dea94
2014-07-14 03:49:01,865 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/c0a6bb911ddf4bcd9012bcea1f6dea94 as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/c0a6bb911ddf4bcd9012bcea1f6dea94
2014-07-14 03:49:01,876 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/c0a6bb911ddf4bcd9012bcea1f6dea94, entries=2748200, sequenceid=7552, filesize=195.6m
2014-07-14 03:49:01,876 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.0g/1105727280, currentsize=355.5m/372793440 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 28825ms, sequenceid=7552, compaction requested=true
2014-07-14 03:49:01,877 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:49:01,877 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 20 blocking
2014-07-14 03:49:01,877 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-14 03:49:01,877 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:49:01,877 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:49:01,877 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:49:01,918 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:49:01,918 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. has too many store files; delaying flush up to 90000ms
2014-07-14 03:49:01,918 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:49:01,918 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 20 blocking
2014-07-14 03:49:01,918 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-14 03:49:01,918 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:49:01,918 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:49:01,918 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:49:04,222 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:49:04,238 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31067 synced till here 31066
2014-07-14 03:49:04,257 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334939534 with entries=87, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334944222
2014-07-14 03:49:04,258 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334850351
2014-07-14 03:49:04,258 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334851674
2014-07-14 03:49:04,258 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334863255
2014-07-14 03:49:04,258 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334865326
2014-07-14 03:49:04,258 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334867301
2014-07-14 03:49:04,258 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334869013
2014-07-14 03:49:04,259 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334870442
2014-07-14 03:49:05,974 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:49:06,258 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31180 synced till here 31178
2014-07-14 03:49:06,275 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334944222 with entries=113, filesize=80.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334945975
2014-07-14 03:49:07,806 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:49:07,828 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31273 synced till here 31267
2014-07-14 03:49:07,875 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334945975 with entries=93, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334947806
2014-07-14 03:49:07,876 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): cd10ec3e9e9086b0a4afff5b7f27579b
2014-07-14 03:49:09,656 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:49:09,682 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334947806 with entries=90, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334949656
2014-07-14 03:49:09,682 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): cd10ec3e9e9086b0a4afff5b7f27579b
2014-07-14 03:49:14,005 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90749ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:49:14,006 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 876.7m
2014-07-14 03:49:14,472 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:49:30,244 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:49:30,411 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334949656 with entries=94, filesize=66.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334970244
2014-07-14 03:49:30,412 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): cd10ec3e9e9086b0a4afff5b7f27579b
2014-07-14 03:49:32,014 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:49:32,277 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31564 synced till here 31554
2014-07-14 03:49:32,305 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334970244 with entries=107, filesize=77.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334972014
2014-07-14 03:49:32,305 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): cd10ec3e9e9086b0a4afff5b7f27579b
2014-07-14 03:49:33,444 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:49:33,463 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31652 synced till here 31651
2014-07-14 03:49:33,476 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334972014 with entries=88, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334973444
2014-07-14 03:49:33,477 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): cd10ec3e9e9086b0a4afff5b7f27579b
2014-07-14 03:49:35,751 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:49:35,779 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334973444 with entries=89, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334975752
2014-07-14 03:49:35,779 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): cd10ec3e9e9086b0a4afff5b7f27579b
2014-07-14 03:49:36,985 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:49:37,612 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31851 synced till here 31850
2014-07-14 03:49:37,643 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334975752 with entries=110, filesize=77.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334976985
2014-07-14 03:49:37,644 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): cd10ec3e9e9086b0a4afff5b7f27579b
2014-07-14 03:49:39,730 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:49:39,748 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31939 synced till here 31938
2014-07-14 03:49:39,763 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334976985 with entries=88, filesize=61.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334979731
2014-07-14 03:49:39,764 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): cd10ec3e9e9086b0a4afff5b7f27579b
2014-07-14 03:49:42,234 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:49:42,264 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334979731 with entries=86, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334982235
2014-07-14 03:49:42,264 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): cd10ec3e9e9086b0a4afff5b7f27579b
2014-07-14 03:49:42,336 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7878, memsize=865.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/b4ac83be7e584ba98ef3264190053233
2014-07-14 03:49:42,355 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/b4ac83be7e584ba98ef3264190053233 as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/b4ac83be7e584ba98ef3264190053233
2014-07-14 03:49:42,367 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/b4ac83be7e584ba98ef3264190053233, entries=3152860, sequenceid=7878, filesize=224.5m
2014-07-14 03:49:42,368 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~876.7m/919309760, currentsize=247.4m/259382240 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 28362ms, sequenceid=7878, compaction requested=true
2014-07-14 03:49:42,369 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:49:42,369 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 20 blocking
2014-07-14 03:49:42,369 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-14 03:49:42,369 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:49:42,369 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:49:42,369 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:49:42,659 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:49:42,659 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. has too many store files; delaying flush up to 90000ms
2014-07-14 03:49:42,660 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:49:42,660 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 20 blocking
2014-07-14 03:49:42,660 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-14 03:49:42,660 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:49:42,660 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:49:42,660 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:49:43,702 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:49:44,161 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32143 synced till here 32142
2014-07-14 03:49:44,200 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334982235 with entries=118, filesize=83.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334983702
2014-07-14 03:49:44,201 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): cd10ec3e9e9086b0a4afff5b7f27579b
2014-07-14 03:49:45,188 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90755ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:49:45,188 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 1.5g
2014-07-14 03:49:45,733 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:49:45,823 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334983702 with entries=89, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334985733
2014-07-14 03:49:46,340 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:50:01,088 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:50:01,676 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32339 synced till here 32338
2014-07-14 03:50:01,709 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334985733 with entries=107, filesize=76.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335001088
2014-07-14 03:50:02,671 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:50:02,809 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32435 synced till here 32434
2014-07-14 03:50:02,832 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335001088 with entries=96, filesize=66.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335002672
2014-07-14 03:50:05,024 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:50:05,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32531 synced till here 32530
2014-07-14 03:50:05,312 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335002672 with entries=96, filesize=68.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335005024
2014-07-14 03:50:07,077 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90775ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:50:07,077 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 448.2m
2014-07-14 03:50:07,427 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:50:10,887 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:50:11,432 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32629 synced till here 32627
2014-07-14 03:50:11,456 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335005024 with entries=98, filesize=70.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335010888
2014-07-14 03:50:11,538 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b.
2014-07-14 03:50:12,306 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:50:12,324 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32719 synced till here 32718
2014-07-14 03:50:12,345 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335010888 with entries=90, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335012307
2014-07-14 03:50:14,669 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:50:14,696 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32810 synced till here 32808
2014-07-14 03:50:14,727 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335012307 with entries=91, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335014670
2014-07-14 03:50:16,970 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:50:16,984 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32898 synced till here 32896
2014-07-14 03:50:16,999 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335014670 with entries=88, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335016970
2014-07-14 03:50:19,558 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:50:19,581 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32986 synced till here 32984
2014-07-14 03:50:19,601 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335016970 with entries=88, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335019559
2014-07-14 03:50:22,278 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8167, memsize=446.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/fefa8e17e38a4f6fac75abcb9422ec8e
2014-07-14 03:50:22,299 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/fefa8e17e38a4f6fac75abcb9422ec8e as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/fefa8e17e38a4f6fac75abcb9422ec8e
2014-07-14 03:50:22,347 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/fefa8e17e38a4f6fac75abcb9422ec8e, entries=1626200, sequenceid=8167, filesize=115.8m
2014-07-14 03:50:22,347 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~448.2m/469960880, currentsize=175.3m/183818080 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 15270ms, sequenceid=8167, compaction requested=true
2014-07-14 03:50:22,348 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:50:22,348 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 20 blocking
2014-07-14 03:50:22,348 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-14 03:50:22,348 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:50:22,348 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b., current region memstore size 300.0m
2014-07-14 03:50:22,348 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:50:22,348 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:50:22,543 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:50:31,492 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8259, memsize=299.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/fc8babafc6834bfa96f4e7a4f42d271d
2014-07-14 03:50:31,512 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/fc8babafc6834bfa96f4e7a4f42d271d as hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/fc8babafc6834bfa96f4e7a4f42d271d
2014-07-14 03:50:31,545 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/fc8babafc6834bfa96f4e7a4f42d271d, entries=1090440, sequenceid=8259, filesize=77.6m
2014-07-14 03:50:31,545 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~300.0m/314523040, currentsize=0.0/0 for region usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. in 9197ms, sequenceid=8259, compaction requested=true
2014-07-14 03:50:31,546 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:50:31,546 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 20 blocking
2014-07-14 03:50:31,546 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-14 03:50:31,546 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:50:31,546 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:50:31,546 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. because compaction request was cancelled
2014-07-14 03:50:32,447 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90529ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:50:32,447 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 1.1g
2014-07-14 03:50:33,247 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:50:34,733 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8079, memsize=1.5g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/f1d86e73d6df4960999adab96b563678
2014-07-14 03:50:34,754 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/f1d86e73d6df4960999adab96b563678 as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/f1d86e73d6df4960999adab96b563678
2014-07-14 03:50:34,769 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/f1d86e73d6df4960999adab96b563678, entries=5531050, sequenceid=8079, filesize=393.6m
2014-07-14 03:50:34,770 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.5g/1612501040, currentsize=309.5m/324522480 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 49582ms, sequenceid=8079, compaction requested=true
2014-07-14 03:50:34,770 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:50:34,770 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 20 blocking
2014-07-14 03:50:34,770 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-14 03:50:34,771 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:50:34,771 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:50:34,771 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:50:36,713 DEBUG [RpcServer.handler=13,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:50:36,713 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. has too many store files; delaying flush up to 90000ms
2014-07-14 03:50:36,713 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:50:36,713 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 20 blocking
2014-07-14 03:50:36,713 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-14 03:50:36,714 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:50:36,714 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:50:36,714 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:50:38,663 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:50:38,688 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33076 synced till here 33075
2014-07-14 03:50:38,727 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335019559 with entries=90, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335038664
2014-07-14 03:50:38,727 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334872566
2014-07-14 03:50:38,727 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334874331
2014-07-14 03:50:38,727 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334875502
2014-07-14 03:50:38,727 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334876566
2014-07-14 03:50:38,728 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334878023
2014-07-14 03:50:38,728 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334879456
2014-07-14 03:50:38,728 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334881078
2014-07-14 03:50:38,728 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334883499
2014-07-14 03:50:38,728 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334886139
2014-07-14 03:50:38,728 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334888060
2014-07-14 03:50:38,728 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334889344
2014-07-14 03:50:38,728 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334890980
2014-07-14 03:50:38,728 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334894839
2014-07-14 03:50:38,728 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334896388
2014-07-14 03:50:38,728 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334897695
2014-07-14 03:50:38,728 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334899928
2014-07-14 03:50:38,728 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334901242
2014-07-14 03:50:38,728 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334905036
2014-07-14 03:50:38,728 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334906271
2014-07-14 03:50:38,728 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334909674
2014-07-14 03:50:40,703 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:50:40,968 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335038664 with entries=108, filesize=74.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335040728
2014-07-14 03:50:43,410 DEBUG [RpcServer.handler=46,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:50:43,410 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. has too many store files; delaying flush up to 90000ms
2014-07-14 03:50:43,411 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:50:43,411 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 20 blocking
2014-07-14 03:50:43,411 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-14 03:50:43,411 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:50:43,412 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:50:43,412 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:50:44,306 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:50:44,329 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33272 synced till here 33270
2014-07-14 03:50:44,348 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335040728 with entries=88, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335044307
2014-07-14 03:50:46,766 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:50:46,790 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335044307 with entries=88, filesize=61.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335046766
2014-07-14 03:50:48,097 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:50:48,257 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33452 synced till here 33450
2014-07-14 03:50:48,717 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335046766 with entries=92, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335048097
2014-07-14 03:50:51,668 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:50:51,687 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33540 synced till here 33539
2014-07-14 03:50:51,704 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335048097 with entries=88, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335051668
2014-07-14 03:50:52,958 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:50:52,986 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33632 synced till here 33629
2014-07-14 03:50:53,043 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335051668 with entries=92, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335052958
2014-07-14 03:50:54,992 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=12794, hits=5369, hitRatio=41.96%, , cachingAccesses=5373, cachingHits=5368, cachingHitsRatio=99.90%, evictions=0, evicted=3, evictedPerRun=Infinity
2014-07-14 03:50:55,508 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:50:55,852 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33725 synced till here 33723
2014-07-14 03:50:55,870 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335052958 with entries=93, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335055838
2014-07-14 03:50:57,725 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:50:57,747 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33815 synced till here 33814
2014-07-14 03:50:57,770 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335055838 with entries=90, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335057725
2014-07-14 03:51:09,024 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:51:09,042 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33909 synced till here 33901
2014-07-14 03:51:09,097 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335057725 with entries=94, filesize=67.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335069025
2014-07-14 03:51:10,059 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:51:10,588 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34003 synced till here 33997
2014-07-14 03:51:10,622 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335069025 with entries=94, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335070060
2014-07-14 03:51:10,664 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8274, memsize=1.1g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/89999132579d4cd9803a491fdd1fa7dc
2014-07-14 03:51:10,679 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/89999132579d4cd9803a491fdd1fa7dc as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/89999132579d4cd9803a491fdd1fa7dc
2014-07-14 03:51:10,691 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/89999132579d4cd9803a491fdd1fa7dc, entries=4075180, sequenceid=8274, filesize=290.0m
2014-07-14 03:51:10,692 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.1g/1175417600, currentsize=395.7m/414916800 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 38245ms, sequenceid=8274, compaction requested=true
2014-07-14 03:51:10,694 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 20 blocking
2014-07-14 03:51:10,694 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-14 03:51:10,694 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:51:10,694 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:51:10,694 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:51:10,696 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:51:10,714 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. has too many store files; delaying flush up to 90000ms
2014-07-14 03:51:10,715 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 20 blocking
2014-07-14 03:51:10,715 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-14 03:51:10,715 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:51:10,715 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:51:10,715 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:51:10,715 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:51:10,715 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:51:11,486 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:51:11,505 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34095 synced till here 34092
2014-07-14 03:51:11,549 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335070060 with entries=92, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335071487
2014-07-14 03:51:11,549 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334912521
2014-07-14 03:51:11,549 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334914929
2014-07-14 03:51:11,549 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334916521
2014-07-14 03:51:11,549 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334918306
2014-07-14 03:51:11,549 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334921290
2014-07-14 03:51:11,549 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334923044
2014-07-14 03:51:11,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334924388
2014-07-14 03:51:11,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334933263
2014-07-14 03:51:11,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334935461
2014-07-14 03:51:11,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334936700
2014-07-14 03:51:11,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334939534
2014-07-14 03:51:11,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334944222
2014-07-14 03:51:11,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334945975
2014-07-14 03:51:11,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334947806
2014-07-14 03:51:11,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334949656
2014-07-14 03:51:11,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334970244
2014-07-14 03:51:11,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334972014
2014-07-14 03:51:11,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334973444
2014-07-14 03:51:11,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334975752
2014-07-14 03:51:11,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334976985
2014-07-14 03:51:11,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334979731
2014-07-14 03:51:11,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334982235
2014-07-14 03:51:13,121 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:51:13,217 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34186 synced till here 34185
2014-07-14 03:51:13,232 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335071487 with entries=91, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335073121
2014-07-14 03:51:13,270 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90611ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:51:13,270 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 637.2m
2014-07-14 03:51:13,673 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:51:19,654 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:51:19,668 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34274 synced till here 34273
2014-07-14 03:51:19,690 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335073121 with entries=88, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335079655
2014-07-14 03:51:21,754 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:51:22,009 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34366 synced till here 34363
2014-07-14 03:51:22,068 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335079655 with entries=92, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335081754
2014-07-14 03:51:23,352 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:51:23,524 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34471 synced till here 34470
2014-07-14 03:51:23,590 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335081754 with entries=105, filesize=73.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335083353
2014-07-14 03:51:24,704 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:51:25,075 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335083353 with entries=111, filesize=78.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335084704
2014-07-14 03:51:33,322 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8579, memsize=628.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/1c844a17e35d4838bdb3fa7ee7ac67b8
2014-07-14 03:51:33,572 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/1c844a17e35d4838bdb3fa7ee7ac67b8 as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/1c844a17e35d4838bdb3fa7ee7ac67b8
2014-07-14 03:51:33,589 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/1c844a17e35d4838bdb3fa7ee7ac67b8, entries=2286440, sequenceid=8579, filesize=162.8m
2014-07-14 03:51:33,590 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~638.7m/669746560, currentsize=160.1m/167876640 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 20320ms, sequenceid=8579, compaction requested=true
2014-07-14 03:51:33,591 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:51:33,591 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 20 blocking
2014-07-14 03:51:33,591 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-14 03:51:33,591 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:51:33,591 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:51:33,592 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:51:47,125 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:51:47,165 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335084704 with entries=88, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335107125
2014-07-14 03:51:48,740 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:51:48,760 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34758 synced till here 34757
2014-07-14 03:51:48,778 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335107125 with entries=88, filesize=62.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335108740
2014-07-14 03:51:50,084 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:51:50,112 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34847 synced till here 34846
2014-07-14 03:51:50,125 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335108740 with entries=89, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335110085
2014-07-14 03:51:50,166 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:51:50,167 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. has too many store files; delaying flush up to 90000ms
2014-07-14 03:51:50,168 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:51:50,168 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 20 blocking
2014-07-14 03:51:50,168 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-14 03:51:50,168 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:51:50,168 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:51:50,169 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:51:52,254 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:51:52,670 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335110085 with entries=125, filesize=87.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335112254
2014-07-14 03:51:54,296 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:51:54,329 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35063 synced till here 35059
2014-07-14 03:51:54,366 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335112254 with entries=91, filesize=66.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335114296
2014-07-14 03:51:55,806 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:51:55,839 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35158 synced till here 35153
2014-07-14 03:51:55,896 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335114296 with entries=95, filesize=67.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335115806
2014-07-14 03:51:56,366 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b.
2014-07-14 03:51:56,366 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b., current region memstore size 256.5m
2014-07-14 03:51:56,544 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:51:57,320 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:51:57,351 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35250 synced till here 35246
2014-07-14 03:51:57,397 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335115806 with entries=92, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335117320
2014-07-14 03:51:57,398 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): cd10ec3e9e9086b0a4afff5b7f27579b
2014-07-14 03:51:58,813 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:51:58,835 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35339 synced till here 35338
2014-07-14 03:51:58,848 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335117320 with entries=89, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335118813
2014-07-14 03:51:58,848 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): cd10ec3e9e9086b0a4afff5b7f27579b
2014-07-14 03:52:01,121 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:52:01,157 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335118813 with entries=87, filesize=61.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335121121
2014-07-14 03:52:01,157 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): cd10ec3e9e9086b0a4afff5b7f27579b
2014-07-14 03:52:04,196 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:52:04,219 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335121121 with entries=88, filesize=62.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335124197
2014-07-14 03:52:04,220 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): cd10ec3e9e9086b0a4afff5b7f27579b
2014-07-14 03:52:04,885 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8809, memsize=245.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/570dd2a012ac46909bd147d0cc6216b2
2014-07-14 03:52:04,917 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/570dd2a012ac46909bd147d0cc6216b2 as hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/570dd2a012ac46909bd147d0cc6216b2
2014-07-14 03:52:04,940 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/570dd2a012ac46909bd147d0cc6216b2, entries=895030, sequenceid=8809, filesize=63.7m
2014-07-14 03:52:04,940 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~256.5m/268952960, currentsize=38.3m/40186320 for region usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. in 8574ms, sequenceid=8809, compaction requested=true
2014-07-14 03:52:04,941 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:52:04,941 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 20 blocking
2014-07-14 03:52:04,941 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-14 03:52:04,941 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:52:04,941 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:52:04,942 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. because compaction request was cancelled
2014-07-14 03:52:06,993 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90281ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:52:06,994 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 1.3g
2014-07-14 03:52:07,776 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:52:13,469 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90059ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:52:13,469 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 540.6m
2014-07-14 03:52:13,597 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:52:13,646 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35603 synced till here 35600
2014-07-14 03:52:13,697 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335124197 with entries=89, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335133598
2014-07-14 03:52:14,303 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:52:15,734 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:52:15,764 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335133598 with entries=87, filesize=60.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335135735
2014-07-14 03:52:17,259 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:52:17,278 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35778 synced till here 35776
2014-07-14 03:52:17,312 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335135735 with entries=88, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335137260
2014-07-14 03:52:22,229 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:52:22,247 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35867 synced till here 35866
2014-07-14 03:52:22,263 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335137260 with entries=89, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335142230
2014-07-14 03:52:23,702 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:52:23,764 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35964 synced till here 35954
2014-07-14 03:52:23,895 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335142230 with entries=97, filesize=71.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335143702
2014-07-14 03:52:25,175 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:52:25,402 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335143702 with entries=108, filesize=73.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335145176
2014-07-14 03:52:28,079 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:52:28,258 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335145176 with entries=98, filesize=70.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335148080
2014-07-14 03:52:41,473 WARN  [regionserver60020.compactionChecker] util.Sleeper: We slept 20706ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-14 03:52:41,473 WARN  [regionserver60020] util.Sleeper: We slept 13787ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-14 03:52:41,473 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 12407ms
GC pool 'ParNew' had collection(s): count=1 time=985ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=11914ms
2014-07-14 03:52:41,473 WARN  [regionserver60020.periodicFlusher] util.Sleeper: We slept 20706ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-14 03:52:41,479 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13107,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47295","starttimems":1405335148371,"queuetimems":0,"class":"HRegionServer","responsesize":13031,"method":"Multi"}
2014-07-14 03:52:41,581 WARN  [RpcServer.reader=6,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: count of bytes read: 0
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:251)
	at sun.nio.ch.IOUtil.read(IOUtil.java:224)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:254)
	at org.apache.hadoop.hbase.ipc.RpcServer.channelIO(RpcServer.java:2263)
	at org.apache.hadoop.hbase.ipc.RpcServer.channelRead(RpcServer.java:2229)
	at org.apache.hadoop.hbase.ipc.RpcServer$Connection.readAndProcess(RpcServer.java:1488)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener.doRead(RpcServer.java:790)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.doRunLoop(RpcServer.java:581)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.run(RpcServer.java:556)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-14 03:52:41,638 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13107,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47295","starttimems":1405335148530,"queuetimems":1,"class":"HRegionServer","responsesize":13248,"method":"Multi"}
2014-07-14 03:52:41,638 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 16735 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47295: output error
2014-07-14 03:52:41,639 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:52:41,731 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 16740 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47295: output error
2014-07-14 03:52:41,731 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:52:41,732 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 16739 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:47295: output error
2014-07-14 03:52:41,732 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 03:52:44,282 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8931, memsize=520.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/dd3259f7a306406db15df58a5d3fcc0a
2014-07-14 03:52:44,309 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/dd3259f7a306406db15df58a5d3fcc0a as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/dd3259f7a306406db15df58a5d3fcc0a
2014-07-14 03:52:44,328 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/dd3259f7a306406db15df58a5d3fcc0a, entries=1895400, sequenceid=8931, filesize=135.0m
2014-07-14 03:52:44,329 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~545.2m/571692160, currentsize=245.2m/257160240 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 30860ms, sequenceid=8931, compaction requested=true
2014-07-14 03:52:44,330 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:52:44,330 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 28 store files, 0 compacting, 28 eligible, 20 blocking
2014-07-14 03:52:44,330 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 93616ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:52:44,330 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 28 files from compaction candidates
2014-07-14 03:52:44,330 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:52:44,331 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:52:44,331 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 1.2g
2014-07-14 03:52:44,331 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:52:45,120 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:52:47,445 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:52:47,664 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:52:47,687 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335148080 with entries=88, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335167664
2014-07-14 03:52:50,964 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:52:51,255 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36381 synced till here 36373
2014-07-14 03:52:51,297 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335167664 with entries=123, filesize=88.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335170964
2014-07-14 03:52:52,689 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:52:52,728 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335170964 with entries=88, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335172689
2014-07-14 03:52:55,434 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:52:55,452 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36559 synced till here 36558
2014-07-14 03:52:55,477 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335172689 with entries=90, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335175434
2014-07-14 03:52:57,299 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:52:57,384 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36649 synced till here 36648
2014-07-14 03:52:57,405 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335175434 with entries=90, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335177300
2014-07-14 03:52:58,832 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:52:58,855 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36739 synced till here 36736
2014-07-14 03:52:58,907 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335177300 with entries=90, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335178833
2014-07-14 03:53:02,318 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:53:02,607 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8910, memsize=1.2g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/d76abf2b586b4a31bcc1a83d9d20b14d
2014-07-14 03:53:02,619 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/d76abf2b586b4a31bcc1a83d9d20b14d as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/d76abf2b586b4a31bcc1a83d9d20b14d
2014-07-14 03:53:03,090 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/d76abf2b586b4a31bcc1a83d9d20b14d, entries=4560880, sequenceid=8910, filesize=324.6m
2014-07-14 03:53:03,091 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.3g/1352598320, currentsize=520.7m/545998480 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 56097ms, sequenceid=8910, compaction requested=true
2014-07-14 03:53:03,091 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:53:03,091 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 20 blocking
2014-07-14 03:53:03,092 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. has too many store files; delaying flush up to 90000ms
2014-07-14 03:53:03,092 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-14 03:53:03,092 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-14 03:53:03,092 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:53:03,092 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:53:03,092 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:53:03,092 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 28 store files, 0 compacting, 28 eligible, 20 blocking
2014-07-14 03:53:03,092 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 28 files from compaction candidates
2014-07-14 03:53:03,092 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:53:03,092 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:53:03,092 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:53:03,108 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:53:03,108 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. has too many store files; delaying flush up to 90000ms
2014-07-14 03:53:03,109 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:53:03,109 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 20 blocking
2014-07-14 03:53:03,109 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-14 03:53:03,109 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:53:03,109 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:53:03,109 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:53:03,181 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335178833 with entries=124, filesize=86.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335182319
2014-07-14 03:53:03,182 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334983702
2014-07-14 03:53:03,182 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405334985733
2014-07-14 03:53:03,182 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335001088
2014-07-14 03:53:03,182 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335002672
2014-07-14 03:53:03,182 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335005024
2014-07-14 03:53:03,182 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335010888
2014-07-14 03:53:03,182 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335012307
2014-07-14 03:53:03,182 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335014670
2014-07-14 03:53:03,182 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335016970
2014-07-14 03:53:05,850 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:53:06,027 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36965 synced till here 36963
2014-07-14 03:53:06,054 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335182319 with entries=102, filesize=73.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335185851
2014-07-14 03:53:08,181 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:53:08,202 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37055 synced till here 37054
2014-07-14 03:53:08,215 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335185851 with entries=90, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335188181
2014-07-14 03:53:18,311 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:53:18,522 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37147 synced till here 37146
2014-07-14 03:53:18,545 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335188181 with entries=92, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335198312
2014-07-14 03:53:20,418 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90252ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:53:20,418 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 611.1m
2014-07-14 03:53:20,803 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:53:21,569 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:53:21,587 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37236 synced till here 37235
2014-07-14 03:53:21,618 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335198312 with entries=89, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335201570
2014-07-14 03:53:22,990 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:53:23,423 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37362 synced till here 37361
2014-07-14 03:53:23,625 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335201570 with entries=126, filesize=88.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335202990
2014-07-14 03:53:23,967 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b.
2014-07-14 03:53:24,831 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:53:24,868 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335202990 with entries=87, filesize=61.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335204832
2014-07-14 03:53:25,035 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9082, memsize=1.2g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/fd35dd29049346a19c371539af86f9f6
2014-07-14 03:53:25,050 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/fd35dd29049346a19c371539af86f9f6 as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/fd35dd29049346a19c371539af86f9f6
2014-07-14 03:53:25,216 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/fd35dd29049346a19c371539af86f9f6, entries=4395040, sequenceid=9082, filesize=312.8m
2014-07-14 03:53:25,216 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.2g/1314609760, currentsize=480.6m/503947920 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 40885ms, sequenceid=9082, compaction requested=true
2014-07-14 03:53:25,217 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:53:25,217 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 20 blocking
2014-07-14 03:53:25,217 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-14 03:53:25,217 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b., current region memstore size 265.1m
2014-07-14 03:53:25,217 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:53:25,217 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:53:25,218 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:53:25,222 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:53:25,423 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:53:27,657 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:53:27,721 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335204832 with entries=88, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335207659
2014-07-14 03:53:27,721 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335019559
2014-07-14 03:53:27,721 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335038664
2014-07-14 03:53:27,721 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335040728
2014-07-14 03:53:27,721 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335044307
2014-07-14 03:53:27,721 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335046766
2014-07-14 03:53:27,722 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335048097
2014-07-14 03:53:27,722 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335051668
2014-07-14 03:53:27,722 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335052958
2014-07-14 03:53:27,722 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335055838
2014-07-14 03:53:27,722 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335057725
2014-07-14 03:53:27,722 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335069025
2014-07-14 03:53:27,722 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335070060
2014-07-14 03:53:27,722 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335071487
2014-07-14 03:53:27,722 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335073121
2014-07-14 03:53:27,725 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335079655
2014-07-14 03:53:27,725 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335081754
2014-07-14 03:53:27,725 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335083353
2014-07-14 03:53:27,725 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335084704
2014-07-14 03:53:27,725 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335107125
2014-07-14 03:53:27,725 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335108740
2014-07-14 03:53:27,725 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335110085
2014-07-14 03:53:27,725 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335112254
2014-07-14 03:53:27,725 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335114296
2014-07-14 03:53:29,948 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:53:30,210 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335207659 with entries=103, filesize=73.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335209948
2014-07-14 03:53:32,945 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:53:32,995 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335209948 with entries=87, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335212946
2014-07-14 03:53:34,003 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9377, memsize=261.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/ea4db539443c4ea395ce5c34506f2283
2014-07-14 03:53:34,025 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/ea4db539443c4ea395ce5c34506f2283 as hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/ea4db539443c4ea395ce5c34506f2283
2014-07-14 03:53:34,039 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/ea4db539443c4ea395ce5c34506f2283, entries=953650, sequenceid=9377, filesize=67.9m
2014-07-14 03:53:34,040 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~265.1m/278019600, currentsize=35.4m/37131760 for region usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. in 8823ms, sequenceid=9377, compaction requested=true
2014-07-14 03:53:34,041 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:53:34,041 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 20 blocking
2014-07-14 03:53:34,041 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. has too many store files; delaying flush up to 90000ms
2014-07-14 03:53:34,042 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-14 03:53:34,042 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-14 03:53:34,042 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:53:34,042 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:53:34,042 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. because compaction request was cancelled
2014-07-14 03:53:34,042 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 20 blocking
2014-07-14 03:53:34,042 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-14 03:53:34,042 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:53:34,042 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:53:34,042 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:53:35,192 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:53:35,220 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335212946 with entries=87, filesize=61.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335215193
2014-07-14 03:53:35,220 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335115806
2014-07-14 03:53:35,220 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335117320
2014-07-14 03:53:35,220 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335118813
2014-07-14 03:53:35,220 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335121121
2014-07-14 03:53:40,334 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9326, memsize=600.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/e9fa4a75fc0c435982c01f64e2e468e5
2014-07-14 03:53:40,352 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/e9fa4a75fc0c435982c01f64e2e468e5 as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/e9fa4a75fc0c435982c01f64e2e468e5
2014-07-14 03:53:40,369 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/e9fa4a75fc0c435982c01f64e2e468e5, entries=2186260, sequenceid=9326, filesize=155.7m
2014-07-14 03:53:40,369 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~611.1m/640803120, currentsize=255.6m/268034240 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 19951ms, sequenceid=9326, compaction requested=true
2014-07-14 03:53:40,370 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:53:40,370 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 20 blocking
2014-07-14 03:53:40,370 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-14 03:53:40,371 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:53:40,371 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:53:40,371 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:53:52,068 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:53:52,068 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. has too many store files; delaying flush up to 90000ms
2014-07-14 03:53:52,068 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:53:52,068 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 20 blocking
2014-07-14 03:53:52,069 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-14 03:53:52,069 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:53:52,069 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:53:52,069 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:53:54,180 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:53:54,443 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37918 synced till here 37914
2014-07-14 03:53:54,462 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335215193 with entries=104, filesize=74.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335234181
2014-07-14 03:53:56,743 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:53:56,766 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335234181 with entries=87, filesize=60.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335236743
2014-07-14 03:53:57,914 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:53:57,945 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335236743 with entries=87, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335237915
2014-07-14 03:54:01,148 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:54:01,166 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38180 synced till here 38178
2014-07-14 03:54:01,184 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335237915 with entries=88, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335241148
2014-07-14 03:54:03,035 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:54:03,070 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38272 synced till here 38269
2014-07-14 03:54:03,090 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335241148 with entries=92, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335243035
2014-07-14 03:54:04,840 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:54:04,881 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335243035 with entries=88, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335244841
2014-07-14 03:54:06,995 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:54:07,013 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38449 synced till here 38448
2014-07-14 03:54:07,068 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335244841 with entries=89, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335246995
2014-07-14 03:54:08,734 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:54:08,775 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335246995 with entries=89, filesize=61.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335248734
2014-07-14 03:54:10,953 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:54:11,214 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335248734 with entries=89, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335250954
2014-07-14 03:54:11,214 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): cd10ec3e9e9086b0a4afff5b7f27579b
2014-07-14 03:54:17,923 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90478ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:54:17,923 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 582.9m
2014-07-14 03:54:18,216 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:54:26,463 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:54:26,493 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38717 synced till here 38715
2014-07-14 03:54:26,528 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335250954 with entries=90, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335266464
2014-07-14 03:54:26,528 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): cd10ec3e9e9086b0a4afff5b7f27579b
2014-07-14 03:54:29,199 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:54:29,877 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38860 synced till here 38858
2014-07-14 03:54:30,039 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335266464 with entries=143, filesize=101.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335269200
2014-07-14 03:54:30,040 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): cd10ec3e9e9086b0a4afff5b7f27579b
2014-07-14 03:54:31,535 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:54:31,790 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38973 synced till here 38970
2014-07-14 03:54:31,888 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335269200 with entries=113, filesize=80.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335271536
2014-07-14 03:54:31,892 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): cd10ec3e9e9086b0a4afff5b7f27579b
2014-07-14 03:54:33,439 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:54:33,454 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90346ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:54:33,454 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 1.3g
2014-07-14 03:54:33,465 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335271536 with entries=89, filesize=61.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335273439
2014-07-14 03:54:33,465 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): cd10ec3e9e9086b0a4afff5b7f27579b
2014-07-14 03:54:34,709 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:54:35,627 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:54:36,061 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39168 synced till here 39166
2014-07-14 03:54:36,079 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335273439 with entries=106, filesize=75.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335275628
2014-07-14 03:54:37,397 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9702, memsize=582.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/b8be7b172b284113b678b4e09d87f95f
2014-07-14 03:54:37,417 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/b8be7b172b284113b678b4e09d87f95f as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/b8be7b172b284113b678b4e09d87f95f
2014-07-14 03:54:37,432 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/b8be7b172b284113b678b4e09d87f95f, entries=2122190, sequenceid=9702, filesize=151.1m
2014-07-14 03:54:37,433 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~582.9m/611172240, currentsize=211.0m/221273840 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 19510ms, sequenceid=9702, compaction requested=true
2014-07-14 03:54:37,434 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:54:37,434 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. has too many store files; delaying flush up to 90000ms
2014-07-14 03:54:37,434 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 30 store files, 0 compacting, 30 eligible, 20 blocking
2014-07-14 03:54:37,434 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-14 03:54:37,434 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 30 files from compaction candidates
2014-07-14 03:54:37,434 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:54:37,435 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:54:37,435 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:54:37,435 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 20 blocking
2014-07-14 03:54:37,435 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-14 03:54:37,435 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:54:37,435 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:54:37,435 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:54:39,456 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:54:39,487 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39257 synced till here 39256
2014-07-14 03:54:39,503 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335275628 with entries=89, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335279457
2014-07-14 03:54:41,210 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:54:41,213 DEBUG [RpcServer.handler=41,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:54:41,214 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. has too many store files; delaying flush up to 90000ms
2014-07-14 03:54:41,215 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:54:41,216 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 30 store files, 0 compacting, 30 eligible, 20 blocking
2014-07-14 03:54:41,216 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 30 files from compaction candidates
2014-07-14 03:54:41,216 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:54:41,216 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:54:41,217 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:54:41,235 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39348 synced till here 39343
2014-07-14 03:54:41,290 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335279457 with entries=91, filesize=65.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335281211
2014-07-14 03:54:43,725 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:54:43,778 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335281211 with entries=90, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335283726
2014-07-14 03:54:52,377 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:54:52,414 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335283726 with entries=88, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335292378
2014-07-14 03:54:53,875 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:54:53,918 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39617 synced till here 39615
2014-07-14 03:54:54,071 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335292378 with entries=91, filesize=65.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335293875
2014-07-14 03:54:54,462 DEBUG [RpcServer.handler=38,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b.
2014-07-14 03:54:54,462 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b., current region memstore size 256.8m
2014-07-14 03:54:54,708 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:54:55,449 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:54:55,473 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39708 synced till here 39705
2014-07-14 03:54:55,510 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335293875 with entries=91, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335295450
2014-07-14 03:55:02,869 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9929, memsize=256.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/f369e05dc5054da3945f451388e74eb5
2014-07-14 03:55:02,887 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/f369e05dc5054da3945f451388e74eb5 as hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/f369e05dc5054da3945f451388e74eb5
2014-07-14 03:55:02,902 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/f369e05dc5054da3945f451388e74eb5, entries=933010, sequenceid=9929, filesize=66.4m
2014-07-14 03:55:02,902 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~258.2m/270732880, currentsize=6.8m/7130080 for region usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. in 8440ms, sequenceid=9929, compaction requested=true
2014-07-14 03:55:02,903 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:55:02,904 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 17 store files, 0 compacting, 17 eligible, 20 blocking
2014-07-14 03:55:02,904 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 97682ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:55:02,904 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 17 files from compaction candidates
2014-07-14 03:55:02,904 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:55:02,904 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:55:02,904 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 1.3g
2014-07-14 03:55:02,904 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. because compaction request was cancelled
2014-07-14 03:55:03,924 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:55:09,580 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:55:09,835 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335295450 with entries=88, filesize=61.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335309802
2014-07-14 03:55:10,474 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:55:10,494 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39885 synced till here 39882
2014-07-14 03:55:10,529 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335309802 with entries=89, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335310475
2014-07-14 03:55:12,605 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:55:12,641 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335310475 with entries=88, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335312606
2014-07-14 03:55:15,315 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:55:15,345 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335312606 with entries=86, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335315315
2014-07-14 03:55:17,467 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:55:17,486 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40149 synced till here 40148
2014-07-14 03:55:17,671 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335315315 with entries=90, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335317467
2014-07-14 03:55:18,952 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:55:18,966 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:55:18,982 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:55:18,997 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:55:19,014 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:55:19,052 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:55:20,462 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9798, memsize=1.3g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/7467a78e0d8a47b2ab21855856a0664e
2014-07-14 03:55:20,494 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/7467a78e0d8a47b2ab21855856a0664e as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/7467a78e0d8a47b2ab21855856a0664e
2014-07-14 03:55:20,531 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/7467a78e0d8a47b2ab21855856a0664e, entries=4977630, sequenceid=9798, filesize=354.3m
2014-07-14 03:55:20,532 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.3g/1445125440, currentsize=452.5m/474512800 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 47078ms, sequenceid=9798, compaction requested=true
2014-07-14 03:55:20,533 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:55:20,534 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 20 blocking
2014-07-14 03:55:20,534 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1483ms
2014-07-14 03:55:20,534 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:55:20,534 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-14 03:55:20,534 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1520ms
2014-07-14 03:55:20,534 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:55:20,534 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:55:20,534 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:55:20,534 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1537ms
2014-07-14 03:55:20,535 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:55:20,535 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:55:20,535 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1553ms
2014-07-14 03:55:20,535 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:55:20,549 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1583ms
2014-07-14 03:55:20,549 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:55:20,549 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1597ms
2014-07-14 03:55:20,549 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:55:20,640 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:55:20,701 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:55:20,728 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40237 synced till here 40235
2014-07-14 03:55:20,761 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335317467 with entries=88, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335320702
2014-07-14 03:55:20,762 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335124197
2014-07-14 03:55:20,762 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335133598
2014-07-14 03:55:20,762 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335135735
2014-07-14 03:55:20,762 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335137260
2014-07-14 03:55:20,762 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335142230
2014-07-14 03:55:20,762 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335143702
2014-07-14 03:55:20,762 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335145176
2014-07-14 03:55:22,334 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90266ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:55:22,335 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 612.0m
2014-07-14 03:55:22,705 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:55:32,648 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:55:32,966 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40353 synced till here 40350
2014-07-14 03:55:32,994 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335320702 with entries=116, filesize=81.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335332648
2014-07-14 03:55:34,221 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:55:34,521 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335332648 with entries=113, filesize=79.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335334221
2014-07-14 03:55:36,880 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:55:37,214 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40567 synced till here 40566
2014-07-14 03:55:37,223 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335334221 with entries=101, filesize=71.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335336881
2014-07-14 03:55:38,736 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:55:39,206 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40684 synced till here 40683
2014-07-14 03:55:39,375 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335336881 with entries=117, filesize=82.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335338736
2014-07-14 03:55:44,805 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10097, memsize=605.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/6d8840f53ec2434bae76150dcf78cc7c
2014-07-14 03:55:44,824 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/6d8840f53ec2434bae76150dcf78cc7c as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/6d8840f53ec2434bae76150dcf78cc7c
2014-07-14 03:55:44,845 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/6d8840f53ec2434bae76150dcf78cc7c, entries=2205320, sequenceid=10097, filesize=157.0m
2014-07-14 03:55:44,845 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~612.0m/641745040, currentsize=197.6m/207230640 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 22510ms, sequenceid=10097, compaction requested=true
2014-07-14 03:55:44,846 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:55:44,846 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 20 blocking
2014-07-14 03:55:44,846 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-14 03:55:44,846 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:55:44,846 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:55:44,846 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:55:44,972 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:55:44,988 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40772 synced till here 40768
2014-07-14 03:55:45,011 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335338736 with entries=88, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335344972
2014-07-14 03:55:46,198 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:55:46,451 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40872 synced till here 40870
2014-07-14 03:55:46,475 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335344972 with entries=100, filesize=70.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335346198
2014-07-14 03:55:47,040 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:55:47,040 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. has too many store files; delaying flush up to 90000ms
2014-07-14 03:55:47,040 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:55:47,040 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 20 blocking
2014-07-14 03:55:47,040 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-14 03:55:47,040 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:55:47,041 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:55:47,041 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:55:48,162 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:55:48,189 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40962 synced till here 40961
2014-07-14 03:55:48,211 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335346198 with entries=90, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335348163
2014-07-14 03:55:50,897 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:55:50,914 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41051 synced till here 41049
2014-07-14 03:55:50,938 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335348163 with entries=89, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335350898
2014-07-14 03:55:52,112 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9958, memsize=1.3g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/06425a249859428685b4798c7f1aad55
2014-07-14 03:55:52,134 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/06425a249859428685b4798c7f1aad55 as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/06425a249859428685b4798c7f1aad55
2014-07-14 03:55:52,200 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/06425a249859428685b4798c7f1aad55, entries=4930400, sequenceid=9958, filesize=350.9m
2014-07-14 03:55:52,200 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.3g/1426446880, currentsize=525.8m/551380880 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 49296ms, sequenceid=9958, compaction requested=true
2014-07-14 03:55:52,201 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:55:52,201 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 28 store files, 0 compacting, 28 eligible, 20 blocking
2014-07-14 03:55:52,201 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 28 files from compaction candidates
2014-07-14 03:55:52,201 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:55:52,201 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:55:52,202 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:55:54,992 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=12794, hits=5369, hitRatio=41.96%, , cachingAccesses=5373, cachingHits=5368, cachingHitsRatio=99.90%, evictions=0, evicted=3, evictedPerRun=Infinity
2014-07-14 03:56:04,036 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90571ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:56:04,037 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 781.6m
2014-07-14 03:56:04,389 DEBUG [RpcServer.handler=24,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:56:04,390 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. has too many store files; delaying flush up to 90000ms
2014-07-14 03:56:04,390 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:56:04,390 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 28 store files, 0 compacting, 28 eligible, 20 blocking
2014-07-14 03:56:04,390 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 28 files from compaction candidates
2014-07-14 03:56:04,390 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:56:04,390 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:56:04,390 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:56:04,569 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:56:06,665 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:56:06,779 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335350898 with entries=89, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335366666
2014-07-14 03:56:06,779 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335148080
2014-07-14 03:56:06,780 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335167664
2014-07-14 03:56:06,780 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335170964
2014-07-14 03:56:06,780 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335172689
2014-07-14 03:56:06,780 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335175434
2014-07-14 03:56:06,780 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335177300
2014-07-14 03:56:06,780 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335178833
2014-07-14 03:56:06,780 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335182319
2014-07-14 03:56:06,780 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335185851
2014-07-14 03:56:06,780 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335188181
2014-07-14 03:56:06,780 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335198312
2014-07-14 03:56:06,780 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335201570
2014-07-14 03:56:06,780 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335202990
2014-07-14 03:56:06,780 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335204832
2014-07-14 03:56:06,780 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335207659
2014-07-14 03:56:06,780 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335209948
2014-07-14 03:56:06,780 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335212946
2014-07-14 03:56:06,780 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335215193
2014-07-14 03:56:06,781 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335234181
2014-07-14 03:56:06,781 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335236743
2014-07-14 03:56:06,781 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335237915
2014-07-14 03:56:06,781 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335241148
2014-07-14 03:56:06,781 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335243035
2014-07-14 03:56:06,781 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335244841
2014-07-14 03:56:06,781 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335246995
2014-07-14 03:56:06,781 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335248734
2014-07-14 03:56:06,781 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335250954
2014-07-14 03:56:06,781 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335266464
2014-07-14 03:56:06,781 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335269200
2014-07-14 03:56:06,781 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335271536
2014-07-14 03:56:08,659 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:56:08,681 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41233 synced till here 41225
2014-07-14 03:56:08,772 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335366666 with entries=93, filesize=67.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335368659
2014-07-14 03:56:09,891 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:56:09,908 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41322 synced till here 41320
2014-07-14 03:56:09,927 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335368659 with entries=89, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335369892
2014-07-14 03:56:11,242 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90029ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:56:11,243 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 441.3m
2014-07-14 03:56:11,515 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:56:11,576 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:56:11,716 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41420 synced till here 41419
2014-07-14 03:56:11,735 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335369892 with entries=98, filesize=69.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335371515
2014-07-14 03:56:14,984 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:56:15,161 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41522 synced till here 41520
2014-07-14 03:56:15,479 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335371515 with entries=102, filesize=72.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335374985
2014-07-14 03:56:16,618 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:56:16,730 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335374985 with entries=90, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335376618
2014-07-14 03:56:20,795 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:56:20,844 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41702 synced till here 41701
2014-07-14 03:56:21,214 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335376618 with entries=90, filesize=64.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335380796
2014-07-14 03:56:22,832 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:56:23,581 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335380796 with entries=102, filesize=72.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335382832
2014-07-14 03:56:26,708 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10382, memsize=441.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/3edc72ee0b0441c794639a6c34fc00c0
2014-07-14 03:56:26,731 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/3edc72ee0b0441c794639a6c34fc00c0 as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/3edc72ee0b0441c794639a6c34fc00c0
2014-07-14 03:56:26,747 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/3edc72ee0b0441c794639a6c34fc00c0, entries=1606840, sequenceid=10382, filesize=114.4m
2014-07-14 03:56:26,747 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~441.3m/462758320, currentsize=185.2m/194209760 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 15504ms, sequenceid=10382, compaction requested=true
2014-07-14 03:56:26,748 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:56:26,748 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 32 store files, 0 compacting, 32 eligible, 20 blocking
2014-07-14 03:56:26,748 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 32 files from compaction candidates
2014-07-14 03:56:26,748 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:56:26,748 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:56:26,748 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:56:31,415 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10302, memsize=775.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/7d403fb14e7b4535864754c89005ed2a
2014-07-14 03:56:31,429 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/7d403fb14e7b4535864754c89005ed2a as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/7d403fb14e7b4535864754c89005ed2a
2014-07-14 03:56:31,440 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/7d403fb14e7b4535864754c89005ed2a, entries=2823340, sequenceid=10302, filesize=201.0m
2014-07-14 03:56:31,441 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~781.6m/819578720, currentsize=301.7m/316384800 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 27404ms, sequenceid=10302, compaction requested=true
2014-07-14 03:56:31,442 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:56:31,442 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 28 store files, 0 compacting, 28 eligible, 20 blocking
2014-07-14 03:56:31,442 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 28 files from compaction candidates
2014-07-14 03:56:31,442 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:56:31,442 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:56:31,443 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:56:43,339 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:56:43,339 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. has too many store files; delaying flush up to 90000ms
2014-07-14 03:56:43,340 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:56:43,340 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 28 store files, 0 compacting, 28 eligible, 20 blocking
2014-07-14 03:56:43,340 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 28 files from compaction candidates
2014-07-14 03:56:43,340 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:56:43,340 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:56:43,340 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:56:43,520 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b.
2014-07-14 03:56:43,521 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b., current region memstore size 256.4m
2014-07-14 03:56:43,702 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:56:44,412 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:56:44,624 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335382832 with entries=92, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335404412
2014-07-14 03:56:44,625 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335273439
2014-07-14 03:56:44,625 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335275628
2014-07-14 03:56:44,625 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335279457
2014-07-14 03:56:44,625 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335281211
2014-07-14 03:56:44,626 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335283726
2014-07-14 03:56:44,626 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335292378
2014-07-14 03:56:47,513 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:56:47,540 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41988 synced till here 41983
2014-07-14 03:56:47,613 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335404412 with entries=92, filesize=68.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335407514
2014-07-14 03:56:48,199 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:56:48,199 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. has too many store files; delaying flush up to 90000ms
2014-07-14 03:56:48,200 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:56:48,200 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 32 store files, 0 compacting, 32 eligible, 20 blocking
2014-07-14 03:56:48,200 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 32 files from compaction candidates
2014-07-14 03:56:48,200 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:56:48,200 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:56:48,200 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:56:49,601 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:56:49,630 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42081 synced till here 42080
2014-07-14 03:56:49,646 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335407514 with entries=93, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335409602
2014-07-14 03:56:51,152 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:56:51,172 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42172 synced till here 42171
2014-07-14 03:56:51,218 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335409602 with entries=91, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335411153
2014-07-14 03:56:52,966 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:56:53,315 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10479, memsize=256.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/cfca972d89404674bf14c1d534bf8376
2014-07-14 03:56:53,318 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335411153 with entries=108, filesize=75.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335412967
2014-07-14 03:56:53,332 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/cfca972d89404674bf14c1d534bf8376 as hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/cfca972d89404674bf14c1d534bf8376
2014-07-14 03:56:53,346 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/cfca972d89404674bf14c1d534bf8376, entries=933630, sequenceid=10479, filesize=66.5m
2014-07-14 03:56:53,346 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~256.4m/268859040, currentsize=48.7m/51071680 for region usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. in 9825ms, sequenceid=10479, compaction requested=true
2014-07-14 03:56:53,347 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:56:53,347 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 18 store files, 0 compacting, 18 eligible, 20 blocking
2014-07-14 03:56:53,347 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 18 files from compaction candidates
2014-07-14 03:56:53,347 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:56:53,347 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:56:53,347 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. because compaction request was cancelled
2014-07-14 03:56:54,757 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:56:54,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42368 synced till here 42366
2014-07-14 03:56:54,818 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335412967 with entries=88, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335414757
2014-07-14 03:56:54,819 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335293875
2014-07-14 03:56:56,791 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:56:57,273 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42509 synced till here 42508
2014-07-14 03:56:57,307 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335414757 with entries=141, filesize=100.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335416792
2014-07-14 03:56:59,236 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:56:59,422 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42612 synced till here 42609
2014-07-14 03:56:59,487 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335416792 with entries=103, filesize=72.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335419237
2014-07-14 03:57:00,991 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:57:01,091 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42706 synced till here 42703
2014-07-14 03:57:01,122 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335419237 with entries=94, filesize=67.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335421070
2014-07-14 03:57:03,072 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:57:03,343 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42812 synced till here 42811
2014-07-14 03:57:03,351 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335421070 with entries=106, filesize=73.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335423072
2014-07-14 03:57:15,241 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:57:15,464 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42913 synced till here 42912
2014-07-14 03:57:15,476 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335423072 with entries=101, filesize=72.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335435242
2014-07-14 03:57:15,478 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 71c27911c72dddd675be84dade80b522
2014-07-14 03:57:16,517 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:57:17,158 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90118ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:57:17,159 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 631.6m
2014-07-14 03:57:17,449 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335435242 with entries=124, filesize=87.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335436517
2014-07-14 03:57:17,449 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 71c27911c72dddd675be84dade80b522
2014-07-14 03:57:17,607 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:57:18,305 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:57:18,322 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43126 synced till here 43125
2014-07-14 03:57:18,349 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335436517 with entries=89, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335438306
2014-07-14 03:57:18,349 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 71c27911c72dddd675be84dade80b522
2014-07-14 03:57:19,771 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:57:19,798 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335438306 with entries=86, filesize=60.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335439771
2014-07-14 03:57:19,798 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 71c27911c72dddd675be84dade80b522
2014-07-14 03:57:21,601 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:57:21,628 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43301 synced till here 43300
2014-07-14 03:57:21,642 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335439771 with entries=89, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335441601
2014-07-14 03:57:21,643 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 71c27911c72dddd675be84dade80b522
2014-07-14 03:57:23,743 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:57:24,276 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335441601 with entries=104, filesize=73.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335443743
2014-07-14 03:57:24,277 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 71c27911c72dddd675be84dade80b522
2014-07-14 03:57:25,759 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:57:25,778 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43492 synced till here 43490
2014-07-14 03:57:26,267 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335443743 with entries=87, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335445760
2014-07-14 03:57:26,268 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 71c27911c72dddd675be84dade80b522
2014-07-14 03:57:27,967 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:57:28,351 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335445760 with entries=104, filesize=72.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335447967
2014-07-14 03:57:28,352 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 71c27911c72dddd675be84dade80b522
2014-07-14 03:57:35,288 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90899ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:57:35,288 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 1.5g
2014-07-14 03:57:36,343 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:57:37,500 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10792, memsize=594.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/ee9793a28e5e4c65a708d2c624732f98
2014-07-14 03:57:37,518 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/ee9793a28e5e4c65a708d2c624732f98 as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/ee9793a28e5e4c65a708d2c624732f98
2014-07-14 03:57:37,532 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/ee9793a28e5e4c65a708d2c624732f98, entries=2164140, sequenceid=10792, filesize=154.0m
2014-07-14 03:57:37,533 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~636.3m/667254400, currentsize=232.7m/244002320 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 20375ms, sequenceid=10792, compaction requested=true
2014-07-14 03:57:37,534 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:57:37,534 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 33 store files, 0 compacting, 33 eligible, 20 blocking
2014-07-14 03:57:37,534 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 33 files from compaction candidates
2014-07-14 03:57:37,534 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:57:37,534 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:57:37,534 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:57:38,949 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:57:38,950 DEBUG [RpcServer.handler=5,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:57:38,950 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. has too many store files; delaying flush up to 90000ms
2014-07-14 03:57:38,950 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:57:38,950 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 33 store files, 0 compacting, 33 eligible, 20 blocking
2014-07-14 03:57:38,950 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 33 files from compaction candidates
2014-07-14 03:57:38,951 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:57:38,951 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:57:38,951 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:57:38,988 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43687 synced till here 43684
2014-07-14 03:57:39,025 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335447967 with entries=91, filesize=65.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335458950
2014-07-14 03:57:40,245 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:57:40,276 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43778 synced till here 43776
2014-07-14 03:57:40,308 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335458950 with entries=91, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335460245
2014-07-14 03:57:41,635 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:57:41,656 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43866 synced till here 43865
2014-07-14 03:57:41,673 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335460245 with entries=88, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335461635
2014-07-14 03:57:44,361 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:57:44,379 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43954 synced till here 43953
2014-07-14 03:57:44,389 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335461635 with entries=88, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335464361
2014-07-14 03:57:45,262 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:57:45,280 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44043 synced till here 44042
2014-07-14 03:57:45,293 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335464361 with entries=89, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335465262
2014-07-14 03:57:45,517 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b.
2014-07-14 03:57:45,518 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b., current region memstore size 256.4m
2014-07-14 03:57:45,700 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:57:48,245 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:57:48,264 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44133 synced till here 44131
2014-07-14 03:57:48,294 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335465262 with entries=90, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335468245
2014-07-14 03:57:49,603 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:57:50,011 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335468245 with entries=103, filesize=71.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335469603
2014-07-14 03:57:53,685 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11028, memsize=243.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/7df29542f3f549f09afef0507f22aaa3
2014-07-14 03:57:53,714 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/7df29542f3f549f09afef0507f22aaa3 as hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/7df29542f3f549f09afef0507f22aaa3
2014-07-14 03:57:53,731 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/7df29542f3f549f09afef0507f22aaa3, entries=887370, sequenceid=11028, filesize=63.2m
2014-07-14 03:57:53,731 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~256.4m/268878080, currentsize=21.5m/22510400 for region usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. in 8213ms, sequenceid=11028, compaction requested=true
2014-07-14 03:57:53,732 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:57:53,732 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 19 store files, 0 compacting, 19 eligible, 20 blocking
2014-07-14 03:57:53,732 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 19 files from compaction candidates
2014-07-14 03:57:53,732 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 03:57:53,732 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:57:53,732 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. because compaction request was cancelled
2014-07-14 03:57:57,121 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:57:58,646 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1235ms
GC pool 'ParNew' had collection(s): count=1 time=1430ms
2014-07-14 03:57:58,647 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44329 synced till here 44326
2014-07-14 03:57:58,689 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335469603 with entries=93, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335477121
2014-07-14 03:57:59,664 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:57:59,684 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44419 synced till here 44418
2014-07-14 03:57:59,700 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335477121 with entries=90, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335479666
2014-07-14 03:58:07,310 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:58:08,063 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335479666 with entries=97, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335487311
2014-07-14 03:58:08,207 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 03:58:08,207 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. has too many store files, but is 1.3g vs best flushable region's 54.2m. Choosing the bigger.
2014-07-14 03:58:08,207 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. due to global heap pressure
2014-07-14 03:58:08,208 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 1.3g
2014-07-14 03:58:08,921 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:58:08,940 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44607 synced till here 44605
2014-07-14 03:58:08,968 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335487311 with entries=91, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335488921
2014-07-14 03:58:09,264 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:58:11,536 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:11,540 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:11,548 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:11,556 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:11,559 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:58:11,559 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:11,561 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:11,576 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44693 synced till here 44692
2014-07-14 03:58:11,584 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:11,596 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335488921 with entries=86, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335491559
2014-07-14 03:58:11,596 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:11,622 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:11,647 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:11,675 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:11,702 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:11,729 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:11,756 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:11,784 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:11,806 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:11,832 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:11,857 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:11,886 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:12,029 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:13,940 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:13,965 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:13,991 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:14,016 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:14,039 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:14,062 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:14,101 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:14,139 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:14,156 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:14,182 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:14,206 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:14,230 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:14,252 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:14,284 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:14,318 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:14,347 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:14,380 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:14,406 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:15,669 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:15,706 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:16,536 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:16,540 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:16,549 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:16,556 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:16,559 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:16,561 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:16,584 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:16,596 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:16,622 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:16,647 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:16,676 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:16,703 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:16,729 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:16,756 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:16,784 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:16,806 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:16,832 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:16,857 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:16,887 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:17,029 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:17,775 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:17,811 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:17,839 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:17,867 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:17,898 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:17,928 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:17,958 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:17,990 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:18,025 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:18,057 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:18,941 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:18,965 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:18,991 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:19,016 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:19,039 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:19,062 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:19,102 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:19,139 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:19,157 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:19,183 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:19,206 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:19,230 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:19,253 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:19,284 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:19,319 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:19,348 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:19,380 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:19,407 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:20,670 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:20,707 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:21,537 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:21,540 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:58:21,549 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:21,557 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:21,560 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:21,562 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:58:21,585 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:21,597 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:21,622 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:58:21,648 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:21,676 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:21,703 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:21,729 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:58:21,757 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:21,784 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:58:21,807 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:21,833 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:21,858 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:58:21,887 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:22,030 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:58:22,776 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:22,811 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:22,839 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:22,867 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:22,898 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:22,929 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:22,959 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:22,990 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:23,026 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:23,058 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:23,942 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 03:58:23,965 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:58:23,991 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:58:24,016 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:24,040 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:24,063 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:24,102 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:24,140 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:24,157 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:24,183 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:24,207 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:24,230 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:24,254 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 03:58:24,285 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:24,319 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:24,348 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:24,380 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:24,407 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:25,670 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:25,707 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:27,380 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15351ms
2014-07-14 03:58:27,381 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15495ms
2014-07-14 03:58:27,381 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15597ms
2014-07-14 03:58:27,381 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15575ms
2014-07-14 03:58:27,381 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15679ms
2014-07-14 03:58:27,381 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15549ms
2014-07-14 03:58:27,381 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15820ms
2014-07-14 03:58:27,381 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15825ms
2014-07-14 03:58:27,382 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15525ms
2014-07-14 03:58:27,382 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15846ms
2014-07-14 03:58:27,382 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15842ms
2014-07-14 03:58:27,382 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15834ms
2014-07-14 03:58:27,382 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15823ms
2014-07-14 03:58:27,382 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15798ms
2014-07-14 03:58:27,382 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15786ms
2014-07-14 03:58:27,382 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15760ms
2014-07-14 03:58:27,383 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15736ms
2014-07-14 03:58:27,383 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15708ms
2014-07-14 03:58:27,383 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15627ms
2014-07-14 03:58:27,383 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15654ms
2014-07-14 03:58:27,776 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:27,824 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10013ms
2014-07-14 03:58:27,839 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:27,868 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:27,898 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:58:27,929 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:27,959 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:27,991 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:28,026 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:28,058 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:28,739 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10928, memsize=1.4g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/eadec37583eb4d329433ff4f229338c4
2014-07-14 03:58:28,766 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/eadec37583eb4d329433ff4f229338c4 as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/eadec37583eb4d329433ff4f229338c4
2014-07-14 03:58:28,787 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/eadec37583eb4d329433ff4f229338c4, entries=5328300, sequenceid=10928, filesize=379.2m
2014-07-14 03:58:28,787 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.5g/1577815760, currentsize=425.3m/445949440 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 53499ms, sequenceid=10928, compaction requested=true
2014-07-14 03:58:28,788 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:58:28,788 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 20 blocking
2014-07-14 03:58:28,788 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10731ms
2014-07-14 03:58:28,788 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 100589ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:58:28,788 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,788 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-14 03:58:28,788 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 650.7m
2014-07-14 03:58:28,788 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:58:28,788 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:58:28,788 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:58:28,789 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10764ms
2014-07-14 03:58:28,789 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,793 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10803ms
2014-07-14 03:58:28,794 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,794 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10836ms
2014-07-14 03:58:28,794 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,794 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10866ms
2014-07-14 03:58:28,794 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,800 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10902ms
2014-07-14 03:58:28,800 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,800 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10933ms
2014-07-14 03:58:28,800 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,800 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10962ms
2014-07-14 03:58:28,801 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,801 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10990ms
2014-07-14 03:58:28,801 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,801 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11026ms
2014-07-14 03:58:28,801 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,805 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17076ms
2014-07-14 03:58:28,805 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,805 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17049ms
2014-07-14 03:58:28,805 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,806 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17130ms
2014-07-14 03:58:28,806 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,806 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17159ms
2014-07-14 03:58:28,806 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,806 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17184ms
2014-07-14 03:58:28,806 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,806 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17210ms
2014-07-14 03:58:28,806 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,806 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17222ms
2014-07-14 03:58:28,806 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,807 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17248ms
2014-07-14 03:58:28,807 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,808 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17259ms
2014-07-14 03:58:28,808 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,817 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17277ms
2014-07-14 03:58:28,817 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,819 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17283ms
2014-07-14 03:58:28,819 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,819 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16962ms
2014-07-14 03:58:28,819 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,820 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17263ms
2014-07-14 03:58:28,820 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,820 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17259ms
2014-07-14 03:58:28,820 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,820 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16988ms
2014-07-14 03:58:28,820 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,823 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17121ms
2014-07-14 03:58:28,823 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,823 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17017ms
2014-07-14 03:58:28,823 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,823 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17039ms
2014-07-14 03:58:28,824 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,824 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16938ms
2014-07-14 03:58:28,824 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,824 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16795ms
2014-07-14 03:58:28,824 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,828 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13122ms
2014-07-14 03:58:28,828 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,828 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13159ms
2014-07-14 03:58:28,828 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,828 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14422ms
2014-07-14 03:58:28,828 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,828 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14449ms
2014-07-14 03:58:28,828 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,828 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14481ms
2014-07-14 03:58:28,828 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,828 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14510ms
2014-07-14 03:58:28,829 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,829 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14545ms
2014-07-14 03:58:28,829 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,829 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14577ms
2014-07-14 03:58:28,829 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,829 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14600ms
2014-07-14 03:58:28,829 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,829 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14623ms
2014-07-14 03:58:28,829 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,829 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14647ms
2014-07-14 03:58:28,829 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,829 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14673ms
2014-07-14 03:58:28,830 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,830 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14691ms
2014-07-14 03:58:28,830 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,830 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14729ms
2014-07-14 03:58:28,830 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,830 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14768ms
2014-07-14 03:58:28,830 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,830 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14791ms
2014-07-14 03:58:28,830 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,830 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14815ms
2014-07-14 03:58:28,830 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,830 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14839ms
2014-07-14 03:58:28,830 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,831 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14866ms
2014-07-14 03:58:28,831 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,831 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14891ms
2014-07-14 03:58:28,831 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:28,850 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17486,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335491364,"queuetimems":0,"class":"HRegionServer","responsesize":13108,"method":"Multi"}
2014-07-14 03:58:28,850 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17443,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335491407,"queuetimems":0,"class":"HRegionServer","responsesize":12978,"method":"Multi"}
2014-07-14 03:58:28,887 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17557,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335491329,"queuetimems":0,"class":"HRegionServer","responsesize":13081,"method":"Multi"}
2014-07-14 03:58:29,012 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:58:29,116 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17670,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335491445,"queuetimems":0,"class":"HRegionServer","responsesize":12805,"method":"Multi"}
2014-07-14 03:58:29,201 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:58:30,489 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:58:30,541 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44795 synced till here 44768
2014-07-14 03:58:30,646 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19122,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335491523,"queuetimems":0,"class":"HRegionServer","responsesize":12977,"method":"Multi"}
2014-07-14 03:58:30,646 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19162,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335491483,"queuetimems":0,"class":"HRegionServer","responsesize":13025,"method":"Multi"}
2014-07-14 03:58:30,681 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12627,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335498054,"queuetimems":0,"class":"HRegionServer","responsesize":13199,"method":"Multi"}
2014-07-14 03:58:30,705 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335491559 with entries=102, filesize=80.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335510489
2014-07-14 03:58:30,705 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335295450
2014-07-14 03:58:30,705 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335309802
2014-07-14 03:58:30,705 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335310475
2014-07-14 03:58:30,705 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335312606
2014-07-14 03:58:30,705 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335315315
2014-07-14 03:58:30,705 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335317467
2014-07-14 03:58:30,705 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335320702
2014-07-14 03:58:30,705 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335332648
2014-07-14 03:58:30,705 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335334221
2014-07-14 03:58:30,706 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335336881
2014-07-14 03:58:30,706 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335338736
2014-07-14 03:58:30,712 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335344972
2014-07-14 03:58:30,712 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335346198
2014-07-14 03:58:30,712 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335348163
2014-07-14 03:58:30,718 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12695,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335498022,"queuetimems":0,"class":"HRegionServer","responsesize":12977,"method":"Multi"}
2014-07-14 03:58:31,306 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13535,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335497771,"queuetimems":0,"class":"HRegionServer","responsesize":13108,"method":"Multi"}
2014-07-14 03:58:31,309 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13387,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335497922,"queuetimems":0,"class":"HRegionServer","responsesize":13066,"method":"Multi"}
2014-07-14 03:58:31,310 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13475,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335497835,"queuetimems":0,"class":"HRegionServer","responsesize":13136,"method":"Multi"}
2014-07-14 03:58:31,312 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19586,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335491726,"queuetimems":0,"class":"HRegionServer","responsesize":13348,"method":"Multi"}
2014-07-14 03:58:32,316 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18035,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335494281,"queuetimems":0,"class":"HRegionServer","responsesize":12977,"method":"Multi"}
2014-07-14 03:58:32,317 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20487,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335491830,"queuetimems":0,"class":"HRegionServer","responsesize":12587,"method":"Multi"}
2014-07-14 03:58:32,322 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20727,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335491594,"queuetimems":0,"class":"HRegionServer","responsesize":12931,"method":"Multi"}
2014-07-14 03:58:32,322 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20437,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335491884,"queuetimems":0,"class":"HRegionServer","responsesize":13034,"method":"Multi"}
2014-07-14 03:58:32,322 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14367,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335497954,"queuetimems":0,"class":"HRegionServer","responsesize":12978,"method":"Multi"}
2014-07-14 03:58:32,324 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20704,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335491619,"queuetimems":0,"class":"HRegionServer","responsesize":13056,"method":"Multi"}
2014-07-14 03:58:32,399 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14504,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335497894,"queuetimems":1,"class":"HRegionServer","responsesize":13181,"method":"Multi"}
2014-07-14 03:58:32,399 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20699,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335491700,"queuetimems":0,"class":"HRegionServer","responsesize":13310,"method":"Multi"}
2014-07-14 03:58:32,399 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18436,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335493963,"queuetimems":0,"class":"HRegionServer","responsesize":13348,"method":"Multi"}
2014-07-14 03:58:32,399 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14413,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335497986,"queuetimems":1,"class":"HRegionServer","responsesize":12917,"method":"Multi"}
2014-07-14 03:58:32,399 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20595,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335491804,"queuetimems":0,"class":"HRegionServer","responsesize":12812,"method":"Multi"}
2014-07-14 03:58:32,400 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18196,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335494204,"queuetimems":0,"class":"HRegionServer","responsesize":13056,"method":"Multi"}
2014-07-14 03:58:32,399 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20372,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335492027,"queuetimems":1,"class":"HRegionServer","responsesize":12916,"method":"Multi"}
2014-07-14 03:58:32,400 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18245,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335494154,"queuetimems":0,"class":"HRegionServer","responsesize":12978,"method":"Multi"}
2014-07-14 03:58:32,400 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20645,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335491754,"queuetimems":1,"class":"HRegionServer","responsesize":13181,"method":"Multi"}
2014-07-14 03:58:32,401 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17997,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335494404,"queuetimems":0,"class":"HRegionServer","responsesize":12805,"method":"Multi"}
2014-07-14 03:58:32,402 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14596,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335497806,"queuetimems":0,"class":"HRegionServer","responsesize":13448,"method":"Multi"}
2014-07-14 03:58:32,409 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18229,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335494180,"queuetimems":0,"class":"HRegionServer","responsesize":13081,"method":"Multi"}
2014-07-14 03:58:32,415 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18321,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335494094,"queuetimems":0,"class":"HRegionServer","responsesize":13448,"method":"Multi"}
2014-07-14 03:58:32,417 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18356,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335494061,"queuetimems":1,"class":"HRegionServer","responsesize":13108,"method":"Multi"}
2014-07-14 03:58:32,418 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16717,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335495700,"queuetimems":1,"class":"HRegionServer","responsesize":12916,"method":"Multi"}
2014-07-14 03:58:32,417 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18190,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335494227,"queuetimems":0,"class":"HRegionServer","responsesize":13199,"method":"Multi"}
2014-07-14 03:58:32,420 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14556,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335497863,"queuetimems":0,"class":"HRegionServer","responsesize":12812,"method":"Multi"}
2014-07-14 03:58:32,420 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16754,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335495665,"queuetimems":1,"class":"HRegionServer","responsesize":13310,"method":"Multi"}
2014-07-14 03:58:32,420 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18481,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335493938,"queuetimems":0,"class":"HRegionServer","responsesize":12587,"method":"Multi"}
2014-07-14 03:58:32,421 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20863,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335491557,"queuetimems":0,"class":"HRegionServer","responsesize":13066,"method":"Multi"}
2014-07-14 03:58:32,421 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20565,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335491855,"queuetimems":0,"class":"HRegionServer","responsesize":12917,"method":"Multi"}
2014-07-14 03:58:32,421 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18384,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335494036,"queuetimems":1,"class":"HRegionServer","responsesize":13025,"method":"Multi"}
2014-07-14 03:58:32,631 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:58:32,645 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18256,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335494376,"queuetimems":0,"class":"HRegionServer","responsesize":13034,"method":"Multi"}
2014-07-14 03:58:32,683 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44939 synced till here 44905
2014-07-14 03:58:32,835 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18521,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335494314,"queuetimems":1,"class":"HRegionServer","responsesize":12812,"method":"Multi"}
2014-07-14 03:58:32,835 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18494,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335494341,"queuetimems":1,"class":"HRegionServer","responsesize":13181,"method":"Multi"}
2014-07-14 03:58:32,835 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18846,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335493989,"queuetimems":0,"class":"HRegionServer","responsesize":13136,"method":"Multi"}
2014-07-14 03:58:32,890 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21216,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335491673,"queuetimems":0,"class":"HRegionServer","responsesize":13448,"method":"Multi"}
2014-07-14 03:58:32,894 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18881,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335494012,"queuetimems":0,"class":"HRegionServer","responsesize":13066,"method":"Multi"}
2014-07-14 03:58:32,901 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18651,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335494250,"queuetimems":0,"class":"HRegionServer","responsesize":12917,"method":"Multi"}
2014-07-14 03:58:32,905 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18776,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335494129,"queuetimems":1,"class":"HRegionServer","responsesize":12931,"method":"Multi"}
2014-07-14 03:58:33,072 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335510489 with entries=144, filesize=94.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335512631
2014-07-14 03:58:34,386 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22606,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335491780,"queuetimems":1,"class":"HRegionServer","responsesize":13199,"method":"Multi"}
2014-07-14 03:58:34,394 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22748,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335491645,"queuetimems":0,"class":"HRegionServer","responsesize":13136,"method":"Multi"}
2014-07-14 03:58:34,620 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:58:34,636 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45046 synced till here 45013
2014-07-14 03:58:35,159 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335512631 with entries=107, filesize=88.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335514620
2014-07-14 03:58:37,235 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:58:37,347 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45173 synced till here 45160
2014-07-14 03:58:37,500 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335514620 with entries=127, filesize=77.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335517235
2014-07-14 03:58:39,173 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:58:39,416 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45291 synced till here 45251
2014-07-14 03:58:39,465 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335517235 with entries=118, filesize=93.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335519173
2014-07-14 03:58:40,769 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:58:41,254 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45457 synced till here 45434
2014-07-14 03:58:42,273 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335519173 with entries=166, filesize=109.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335520770
2014-07-14 03:58:43,118 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:58:43,217 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45576 synced till here 45538
2014-07-14 03:58:44,193 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335520770 with entries=119, filesize=76.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335523118
2014-07-14 03:58:45,529 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:58:46,603 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335523118 with entries=124, filesize=94.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335525529
2014-07-14 03:58:47,321 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:58:47,420 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45828 synced till here 45801
2014-07-14 03:58:47,618 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335525529 with entries=128, filesize=79.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335527322
2014-07-14 03:58:48,725 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:48,726 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:48,726 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:48,727 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:48,730 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:48,730 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:48,731 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:48,731 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:48,732 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:48,733 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:48,735 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:48,735 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:48,737 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:48,743 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:48,743 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:48,837 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:48,839 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:48,840 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:48,840 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:48,843 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:48,843 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,137 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:58:49,143 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,145 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,147 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,147 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,147 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,148 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,151 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,151 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,151 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,153 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,153 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,153 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,154 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,154 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,155 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,155 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,155 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,155 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,156 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,156 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,158 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,158 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,158 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,159 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,163 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,163 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,164 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,165 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,166 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 03:58:49,292 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335527322 with entries=85, filesize=68.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335529138
2014-07-14 03:58:53,727 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:53,727 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:53,728 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-14 03:58:53,728 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5003ms
2014-07-14 03:58:53,731 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:53,731 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:53,731 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:53,731 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:53,733 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:53,733 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-14 03:58:53,735 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:53,736 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:53,738 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:53,744 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:53,744 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:53,837 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:53,840 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:53,840 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:53,840 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:53,843 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:53,844 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:54,144 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:54,145 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:54,147 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:54,147 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:54,148 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:54,148 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:54,151 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:54,152 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:54,152 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5005ms
2014-07-14 03:58:54,153 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:54,153 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:54,155 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:54,155 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:54,155 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-14 03:58:54,156 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:54,156 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5003ms
2014-07-14 03:58:54,156 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:54,156 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:54,156 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5003ms
2014-07-14 03:58:54,157 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:54,158 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:54,158 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:54,158 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:54,160 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:54,163 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 03:58:54,164 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:54,165 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:54,165 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:54,166 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 03:58:58,727 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:58,728 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 03:58:58,729 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10004ms
2014-07-14 03:58:58,729 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10004ms
2014-07-14 03:58:58,731 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:58,732 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:58,732 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:58,732 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:58,733 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:58,733 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 03:58:58,736 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:58,736 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:58,739 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:58,744 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:58,744 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:58,838 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:58,840 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:58,840 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:58:58,841 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:58,844 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:58,844 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:59,145 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:59,146 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:59,148 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 03:58:59,148 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:59,149 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:58:59,150 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 03:58:59,152 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:59,152 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:59,153 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10006ms
2014-07-14 03:58:59,153 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:59,153 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:58:59,155 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 03:58:59,156 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:59,156 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-14 03:58:59,156 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 03:58:59,157 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-14 03:58:59,157 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10004ms
2014-07-14 03:58:59,157 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:59,157 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 03:58:59,157 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:59,158 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:58:59,159 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:59,160 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:59,160 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:59,163 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:59,164 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:59,165 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 03:58:59,165 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:58:59,167 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 03:58:59,652 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11212, memsize=632.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/0e9e9a1c9e5f4453a8d299aaba079fa0
2014-07-14 03:58:59,671 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/0e9e9a1c9e5f4453a8d299aaba079fa0 as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/0e9e9a1c9e5f4453a8d299aaba079fa0
2014-07-14 03:58:59,694 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/0e9e9a1c9e5f4453a8d299aaba079fa0, entries=2302260, sequenceid=11212, filesize=163.8m
2014-07-14 03:58:59,695 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~650.7m/682351280, currentsize=442.9m/464387520 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 30906ms, sequenceid=11212, compaction requested=true
2014-07-14 03:58:59,695 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:58:59,695 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 34 store files, 0 compacting, 34 eligible, 20 blocking
2014-07-14 03:58:59,695 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10529ms
2014-07-14 03:58:59,695 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 136356ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:58:59,695 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,695 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 34 files from compaction candidates
2014-07-14 03:58:59,696 DEBUG [MemStoreFlusher.1] regionserver.HRegion: NOT flushing memstore for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., flushing=true, writesEnabled=true
2014-07-14 03:58:59,696 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10530ms
2014-07-14 03:58:59,696 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,696 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:58:59,696 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10532ms
2014-07-14 03:58:59,696 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,696 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:58:59,696 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10533ms
2014-07-14 03:58:59,696 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:58:59,696 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,696 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. has too many store files; delaying flush up to 90000ms
2014-07-14 03:58:59,697 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 20 blocking
2014-07-14 03:58:59,697 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-14 03:58:59,697 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:58:59,697 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:58:59,697 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:58:59,697 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:58:59,698 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10536ms
2014-07-14 03:58:59,698 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,698 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10539ms
2014-07-14 03:58:59,698 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,698 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10540ms
2014-07-14 03:58:59,698 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,698 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10540ms
2014-07-14 03:58:59,698 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,698 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10540ms
2014-07-14 03:58:59,699 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,701 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10545ms
2014-07-14 03:58:59,701 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,701 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10546ms
2014-07-14 03:58:59,701 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,713 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10557ms
2014-07-14 03:58:59,713 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,713 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10560ms
2014-07-14 03:58:59,713 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,713 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10560ms
2014-07-14 03:58:59,713 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,727 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10573ms
2014-07-14 03:58:59,727 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,727 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10574ms
2014-07-14 03:58:59,727 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,727 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10573ms
2014-07-14 03:58:59,727 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,728 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10575ms
2014-07-14 03:58:59,728 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,731 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10578ms
2014-07-14 03:58:59,731 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,731 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10579ms
2014-07-14 03:58:59,731 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,731 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10584ms
2014-07-14 03:58:59,731 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,731 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10580ms
2014-07-14 03:58:59,731 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,732 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10581ms
2014-07-14 03:58:59,732 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,733 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10586ms
2014-07-14 03:58:59,733 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,733 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10585ms
2014-07-14 03:58:59,733 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,733 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10586ms
2014-07-14 03:58:59,733 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,735 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10589ms
2014-07-14 03:58:59,735 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,741 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10596ms
2014-07-14 03:58:59,741 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,749 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10606ms
2014-07-14 03:58:59,749 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,749 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10906ms
2014-07-14 03:58:59,749 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,749 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10906ms
2014-07-14 03:58:59,749 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,761 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10921ms
2014-07-14 03:58:59,761 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,761 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10921ms
2014-07-14 03:58:59,761 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,765 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10926ms
2014-07-14 03:58:59,765 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,769 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10932ms
2014-07-14 03:58:59,769 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,769 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11026ms
2014-07-14 03:58:59,769 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,769 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11026ms
2014-07-14 03:58:59,769 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,770 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11033ms
2014-07-14 03:58:59,771 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,773 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11038ms
2014-07-14 03:58:59,773 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,773 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11038ms
2014-07-14 03:58:59,773 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,773 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11042ms
2014-07-14 03:58:59,773 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,776 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11044ms
2014-07-14 03:58:59,776 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,776 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11045ms
2014-07-14 03:58:59,776 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,777 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11046ms
2014-07-14 03:58:59,777 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,777 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11047ms
2014-07-14 03:58:59,777 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,785 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11055ms
2014-07-14 03:58:59,785 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,785 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11060ms
2014-07-14 03:58:59,785 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,797 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11072ms
2014-07-14 03:58:59,797 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,805 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11079ms
2014-07-14 03:58:59,805 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,813 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11087ms
2014-07-14 03:58:59,813 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 03:58:59,839 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12640,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527199,"queuetimems":6014,"class":"HRegionServer","responsesize":12978,"method":"Multi"}
2014-07-14 03:58:59,878 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12691,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527186,"queuetimems":6383,"class":"HRegionServer","responsesize":12977,"method":"Multi"}
2014-07-14 03:58:59,878 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12755,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527123,"queuetimems":6483,"class":"HRegionServer","responsesize":13102,"method":"Multi"}
2014-07-14 03:58:59,878 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12885,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335526992,"queuetimems":6421,"class":"HRegionServer","responsesize":12925,"method":"Multi"}
2014-07-14 03:58:59,878 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12885,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335526992,"queuetimems":6388,"class":"HRegionServer","responsesize":13090,"method":"Multi"}
2014-07-14 03:58:59,878 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12688,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527190,"queuetimems":6298,"class":"HRegionServer","responsesize":13025,"method":"Multi"}
2014-07-14 03:59:00,731 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 03:59:00,731 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. has too many store files; delaying flush up to 90000ms
2014-07-14 03:59:00,731 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:59:00,731 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 34 store files, 0 compacting, 34 eligible, 20 blocking
2014-07-14 03:59:00,732 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 34 files from compaction candidates
2014-07-14 03:59:00,732 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:59:00,732 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:59:00,732 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 03:59:01,159 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13817,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527341,"queuetimems":4274,"class":"HRegionServer","responsesize":13310,"method":"Multi"}
2014-07-14 03:59:01,160 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13825,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527335,"queuetimems":4975,"class":"HRegionServer","responsesize":12956,"method":"Multi"}
2014-07-14 03:59:01,162 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13825,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527337,"queuetimems":4898,"class":"HRegionServer","responsesize":12944,"method":"Multi"}
2014-07-14 03:59:01,163 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13833,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527330,"queuetimems":5155,"class":"HRegionServer","responsesize":13066,"method":"Multi"}
2014-07-14 03:59:01,163 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13586,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527577,"queuetimems":2675,"class":"HRegionServer","responsesize":12992,"method":"Multi"}
2014-07-14 03:59:01,165 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13814,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527345,"queuetimems":3139,"class":"HRegionServer","responsesize":13113,"method":"Multi"}
2014-07-14 03:59:01,168 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13834,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527334,"queuetimems":5077,"class":"HRegionServer","responsesize":12931,"method":"Multi"}
2014-07-14 03:59:01,168 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13622,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527546,"queuetimems":2805,"class":"HRegionServer","responsesize":12970,"method":"Multi"}
2014-07-14 03:59:01,169 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13818,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527341,"queuetimems":4871,"class":"HRegionServer","responsesize":13381,"method":"Multi"}
2014-07-14 03:59:01,173 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13829,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527335,"queuetimems":5011,"class":"HRegionServer","responsesize":13448,"method":"Multi"}
2014-07-14 03:59:01,177 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13823,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527341,"queuetimems":4249,"class":"HRegionServer","responsesize":12916,"method":"Multi"}
2014-07-14 03:59:01,181 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13837,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527326,"queuetimems":6046,"class":"HRegionServer","responsesize":12917,"method":"Multi"}
2014-07-14 03:59:01,185 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13820,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527337,"queuetimems":4953,"class":"HRegionServer","responsesize":13217,"method":"Multi"}
2014-07-14 03:59:01,239 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14041,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527198,"queuetimems":6131,"class":"HRegionServer","responsesize":12805,"method":"Multi"}
2014-07-14 03:59:01,239 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13909,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527330,"queuetimems":5146,"class":"HRegionServer","responsesize":13199,"method":"Multi"}
2014-07-14 03:59:01,239 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13885,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527354,"queuetimems":3071,"class":"HRegionServer","responsesize":12947,"method":"Multi"}
2014-07-14 03:59:01,240 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13914,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527326,"queuetimems":5159,"class":"HRegionServer","responsesize":13348,"method":"Multi"}
2014-07-14 03:59:01,241 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14039,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527202,"queuetimems":5961,"class":"HRegionServer","responsesize":12812,"method":"Multi"}
2014-07-14 03:59:01,242 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13899,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527342,"queuetimems":3145,"class":"HRegionServer","responsesize":13119,"method":"Multi"}
2014-07-14 03:59:01,241 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13904,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527337,"queuetimems":4929,"class":"HRegionServer","responsesize":13160,"method":"Multi"}
2014-07-14 03:59:01,250 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12760,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335528490,"queuetimems":3147,"class":"HRegionServer","responsesize":12932,"method":"Multi"}
2014-07-14 03:59:01,256 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14075,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527180,"queuetimems":6512,"class":"HRegionServer","responsesize":12992,"method":"Multi"}
2014-07-14 03:59:01,258 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14061,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527197,"queuetimems":6260,"class":"HRegionServer","responsesize":13181,"method":"Multi"}
2014-07-14 03:59:01,264 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14078,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527186,"queuetimems":6475,"class":"HRegionServer","responsesize":12963,"method":"Multi"}
2014-07-14 03:59:01,269 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14083,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527186,"queuetimems":6343,"class":"HRegionServer","responsesize":13034,"method":"Multi"}
2014-07-14 03:59:01,277 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14079,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527198,"queuetimems":6218,"class":"HRegionServer","responsesize":13056,"method":"Multi"}
2014-07-14 03:59:01,281 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14083,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527198,"queuetimems":6188,"class":"HRegionServer","responsesize":12587,"method":"Multi"}
2014-07-14 03:59:01,281 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13935,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527346,"queuetimems":3096,"class":"HRegionServer","responsesize":12971,"method":"Multi"}
2014-07-14 03:59:01,282 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13949,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527333,"queuetimems":5099,"class":"HRegionServer","responsesize":13136,"method":"Multi"}
2014-07-14 03:59:01,282 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13948,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527334,"queuetimems":5043,"class":"HRegionServer","responsesize":13063,"method":"Multi"}
2014-07-14 03:59:01,289 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14102,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527186,"queuetimems":6437,"class":"HRegionServer","responsesize":13081,"method":"Multi"}
2014-07-14 03:59:01,329 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:59:01,331 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14000,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527330,"queuetimems":5127,"class":"HRegionServer","responsesize":13108,"method":"Multi"}
2014-07-14 03:59:01,331 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12837,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335528494,"queuetimems":3181,"class":"HRegionServer","responsesize":12944,"method":"Multi"}
2014-07-14 03:59:01,331 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12837,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335528493,"queuetimems":3120,"class":"HRegionServer","responsesize":12987,"method":"Multi"}
2014-07-14 03:59:01,434 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12952,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335528482,"queuetimems":3241,"class":"HRegionServer","responsesize":13099,"method":"Multi"}
2014-07-14 03:59:01,442 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13865,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527577,"queuetimems":2520,"class":"HRegionServer","responsesize":12700,"method":"Multi"}
2014-07-14 03:59:01,518 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46025 synced till here 46017
2014-07-14 03:59:01,525 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13942,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527582,"queuetimems":2446,"class":"HRegionServer","responsesize":13004,"method":"Multi"}
2014-07-14 03:59:01,531 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13044,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335528487,"queuetimems":3216,"class":"HRegionServer","responsesize":12925,"method":"Multi"}
2014-07-14 03:59:01,576 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335529138 with entries=112, filesize=69.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335541330
2014-07-14 03:59:01,865 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14287,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527577,"queuetimems":2560,"class":"HRegionServer","responsesize":13130,"method":"Multi"}
2014-07-14 03:59:01,865 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14283,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527582,"queuetimems":2485,"class":"HRegionServer","responsesize":13063,"method":"Multi"}
2014-07-14 03:59:01,866 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13030,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335528836,"queuetimems":3383,"class":"HRegionServer","responsesize":12963,"method":"Multi"}
2014-07-14 03:59:01,866 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13030,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335528836,"queuetimems":3419,"class":"HRegionServer","responsesize":13102,"method":"Multi"}
2014-07-14 03:59:01,866 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14229,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527637,"queuetimems":2430,"class":"HRegionServer","responsesize":13188,"method":"Multi"}
2014-07-14 03:59:01,866 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14281,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47303","starttimems":1405335527585,"queuetimems":2415,"class":"HRegionServer","responsesize":13217,"method":"Multi"}
2014-07-14 03:59:02,974 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:59:03,085 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46126 synced till here 46108
2014-07-14 03:59:03,208 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335541330 with entries=101, filesize=78.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335542974
2014-07-14 03:59:04,250 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:59:04,288 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46218 synced till here 46201
2014-07-14 03:59:04,406 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 03:59:04,407 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. has too many store files, but is 1.0g vs best flushable region's 234.6m. Choosing the bigger.
2014-07-14 03:59:04,407 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. due to global heap pressure
2014-07-14 03:59:04,407 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 1.0g
2014-07-14 03:59:04,413 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335542974 with entries=92, filesize=74.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335544251
2014-07-14 03:59:04,665 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b.
2014-07-14 03:59:04,977 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11173, memsize=1.3g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/10dd5e13d4b44de281ec6254012943e7
2014-07-14 03:59:05,021 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/10dd5e13d4b44de281ec6254012943e7 as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/10dd5e13d4b44de281ec6254012943e7
2014-07-14 03:59:05,048 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/10dd5e13d4b44de281ec6254012943e7, entries=4705620, sequenceid=11173, filesize=334.9m
2014-07-14 03:59:05,049 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.3g/1417435600, currentsize=655.8m/687607440 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 56841ms, sequenceid=11173, compaction requested=true
2014-07-14 03:59:05,050 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:59:05,050 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 20 blocking
2014-07-14 03:59:05,050 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-14 03:59:05,050 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b., current region memstore size 256.6m
2014-07-14 03:59:05,050 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:59:05,050 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:59:05,051 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:59:05,182 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 03:59:05,541 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:59:05,559 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 03:59:06,208 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:59:06,232 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46347 synced till here 46332
2014-07-14 03:59:06,353 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335544251 with entries=129, filesize=75.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335546209
2014-07-14 03:59:06,353 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335350898
2014-07-14 03:59:06,353 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335366666
2014-07-14 03:59:06,353 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335368659
2014-07-14 03:59:06,353 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335369892
2014-07-14 03:59:06,353 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335371515
2014-07-14 03:59:06,353 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335374985
2014-07-14 03:59:06,353 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335376618
2014-07-14 03:59:06,353 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335380796
2014-07-14 03:59:06,353 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335382832
2014-07-14 03:59:06,353 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335404412
2014-07-14 03:59:06,353 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335407514
2014-07-14 03:59:06,354 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335409602
2014-07-14 03:59:06,354 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335411153
2014-07-14 03:59:06,354 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335412967
2014-07-14 03:59:06,354 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335414757
2014-07-14 03:59:06,354 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335416792
2014-07-14 03:59:06,354 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335419237
2014-07-14 03:59:06,354 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335421070
2014-07-14 03:59:06,354 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335423072
2014-07-14 03:59:06,354 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335435242
2014-07-14 03:59:06,354 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335436517
2014-07-14 03:59:06,354 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335438306
2014-07-14 03:59:06,354 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335439771
2014-07-14 03:59:06,354 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335441601
2014-07-14 03:59:06,354 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335443743
2014-07-14 03:59:06,354 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335445760
2014-07-14 03:59:08,055 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:59:08,072 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46441 synced till here 46430
2014-07-14 03:59:08,162 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335546209 with entries=94, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335548055
2014-07-14 03:59:09,700 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:59:09,925 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335548055 with entries=103, filesize=68.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335549701
2014-07-14 03:59:10,013 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11581, memsize=88.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/4eda60b5ae414e36a7719cfea1f2de5c
2014-07-14 03:59:10,037 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/4eda60b5ae414e36a7719cfea1f2de5c as hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/4eda60b5ae414e36a7719cfea1f2de5c
2014-07-14 03:59:10,071 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/4eda60b5ae414e36a7719cfea1f2de5c, entries=323820, sequenceid=11581, filesize=23.1m
2014-07-14 03:59:10,072 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~256.6m/269016960, currentsize=32.2m/33718960 for region usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. in 5022ms, sequenceid=11581, compaction requested=true
2014-07-14 03:59:10,072 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:59:10,072 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 20 store files, 0 compacting, 20 eligible, 20 blocking
2014-07-14 03:59:10,072 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. has too many store files; delaying flush up to 90000ms
2014-07-14 03:59:10,072 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 20 files from compaction candidates
2014-07-14 03:59:10,073 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-14 03:59:10,073 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:59:10,073 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:59:10,073 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. because compaction request was cancelled
2014-07-14 03:59:10,073 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 20 blocking
2014-07-14 03:59:10,073 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-14 03:59:10,073 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:59:10,073 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:59:10,073 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 03:59:12,676 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:59:12,703 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46632 synced till here 46631
2014-07-14 03:59:12,725 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335549701 with entries=88, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335552677
2014-07-14 03:59:15,328 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:59:15,347 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46721 synced till here 46718
2014-07-14 03:59:15,660 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335552677 with entries=89, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335555328
2014-07-14 03:59:17,672 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:59:17,693 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46810 synced till here 46809
2014-07-14 03:59:17,718 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335555328 with entries=89, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335557673
2014-07-14 03:59:18,747 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:59:18,772 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335557673 with entries=88, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335558748
2014-07-14 03:59:20,075 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:59:20,097 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46988 synced till here 46987
2014-07-14 03:59:20,118 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335558748 with entries=90, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335560075
2014-07-14 03:59:21,694 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:59:21,771 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335560075 with entries=88, filesize=61.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335561694
2014-07-14 03:59:23,732 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:59:23,748 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47163 synced till here 47161
2014-07-14 03:59:23,766 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335561694 with entries=87, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335563732
2014-07-14 03:59:26,035 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11597, memsize=471.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/3d4528ad9dd141b49680caaca5e29617
2014-07-14 03:59:26,052 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/3d4528ad9dd141b49680caaca5e29617 as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/3d4528ad9dd141b49680caaca5e29617
2014-07-14 03:59:26,067 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/3d4528ad9dd141b49680caaca5e29617, entries=1717710, sequenceid=11597, filesize=122.4m
2014-07-14 03:59:26,068 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.0g/1080456720, currentsize=375.1m/393281200 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 21661ms, sequenceid=11597, compaction requested=true
2014-07-14 03:59:26,068 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:59:26,068 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 30 store files, 0 compacting, 30 eligible, 20 blocking
2014-07-14 03:59:26,069 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 30 files from compaction candidates
2014-07-14 03:59:26,069 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:59:26,069 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:59:26,069 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:59:26,078 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 03:59:26,078 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. has too many store files; delaying flush up to 90000ms
2014-07-14 03:59:26,078 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 03:59:26,078 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 30 store files, 0 compacting, 30 eligible, 20 blocking
2014-07-14 03:59:26,079 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 30 files from compaction candidates
2014-07-14 03:59:26,079 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 03:59:26,079 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 03:59:26,079 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 03:59:26,517 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:59:26,535 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47253 synced till here 47251
2014-07-14 03:59:26,572 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335563732 with entries=90, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335566517
2014-07-14 03:59:26,572 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335447967
2014-07-14 03:59:26,572 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335458950
2014-07-14 03:59:26,572 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335460245
2014-07-14 03:59:26,572 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335461635
2014-07-14 03:59:26,573 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335464361
2014-07-14 03:59:26,573 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335465262
2014-07-14 03:59:26,573 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335468245
2014-07-14 03:59:26,573 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335469603
2014-07-14 03:59:26,573 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335477121
2014-07-14 03:59:26,573 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335479666
2014-07-14 03:59:27,872 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:59:27,891 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47343 synced till here 47341
2014-07-14 03:59:27,903 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335566517 with entries=90, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335567872
2014-07-14 03:59:29,275 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:59:29,549 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47441 synced till here 47440
2014-07-14 03:59:29,568 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335567872 with entries=98, filesize=69.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335569276
2014-07-14 03:59:31,313 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:59:31,356 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335569276 with entries=88, filesize=61.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335571314
2014-07-14 03:59:33,508 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:59:33,554 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47620 synced till here 47617
2014-07-14 03:59:33,666 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335571314 with entries=91, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335573509
2014-07-14 03:59:35,114 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:59:35,156 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335573509 with entries=90, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335575114
2014-07-14 03:59:38,932 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:59:38,959 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47797 synced till here 47796
2014-07-14 03:59:38,991 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335575114 with entries=87, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335578932
2014-07-14 03:59:41,349 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:59:41,381 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47888 synced till here 47887
2014-07-14 03:59:41,471 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335578932 with entries=91, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335581349
2014-07-14 03:59:41,471 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): cd10ec3e9e9086b0a4afff5b7f27579b
2014-07-14 03:59:43,506 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:59:43,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47980 synced till here 47978
2014-07-14 03:59:43,566 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335581349 with entries=92, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335583507
2014-07-14 03:59:43,566 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): cd10ec3e9e9086b0a4afff5b7f27579b
2014-07-14 03:59:45,509 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:59:45,540 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335583507 with entries=87, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335585510
2014-07-14 03:59:45,540 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): cd10ec3e9e9086b0a4afff5b7f27579b
2014-07-14 03:59:47,159 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:59:47,174 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48155 synced till here 48154
2014-07-14 03:59:47,189 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335585510 with entries=88, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335587159
2014-07-14 03:59:47,189 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): cd10ec3e9e9086b0a4afff5b7f27579b
2014-07-14 03:59:48,749 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 03:59:48,763 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48242 synced till here 48241
2014-07-14 03:59:48,783 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335587159 with entries=87, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335588749
2014-07-14 03:59:48,783 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): cd10ec3e9e9086b0a4afff5b7f27579b
2014-07-14 03:59:49,646 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 03:59:49,646 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. has too many store files, but is 1.4g vs best flushable region's 235.8m. Choosing the bigger.
2014-07-14 03:59:49,646 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. due to global heap pressure
2014-07-14 03:59:49,647 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 1.4g
2014-07-14 03:59:50,572 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 04:00:00,970 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 04:00:00,970 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. has too many store files, but is 1.4g vs best flushable region's 236.7m. Choosing the bigger.
2014-07-14 04:00:00,970 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. due to global heap pressure
2014-07-14 04:00:00,971 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 1.4g
2014-07-14 04:00:02,081 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 04:00:04,222 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:00:04,241 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48331 synced till here 48330
2014-07-14 04:00:04,260 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335588749 with entries=89, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335604222
2014-07-14 04:00:06,163 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:00:06,182 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48420 synced till here 48418
2014-07-14 04:00:06,204 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335604222 with entries=89, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335606164
2014-07-14 04:00:06,438 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:06,485 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:07,076 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:07,105 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:07,140 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:07,166 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:07,192 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:07,220 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:07,248 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:07,273 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:07,892 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:07,941 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:07,974 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:08,014 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:08,083 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:08,110 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:08,153 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:08,187 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:08,260 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:08,468 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:08,507 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:08,548 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:08,582 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:08,622 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:08,655 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:09,492 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:09,537 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:09,580 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:09,638 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:09,689 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:09,732 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:09,766 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:10,623 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:10,658 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:10,687 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:10,724 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:10,759 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:10,791 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:10,828 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:10,857 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:10,895 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:10,926 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:10,954 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:10,983 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:11,013 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:11,052 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:11,078 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:11,106 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:11,136 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:11,174 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:00:11,438 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:00:11,486 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:00:12,077 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:00:12,106 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:00:12,140 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:00:12,166 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:00:12,193 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:00:12,220 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:00:12,249 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:00:12,274 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:00:13,794 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5139ms
2014-07-14 04:00:13,794 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5607ms
2014-07-14 04:00:13,795 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5820ms
2014-07-14 04:00:13,795 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5535ms
2014-07-14 04:00:13,795 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5288ms
2014-07-14 04:00:13,795 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5327ms
2014-07-14 04:00:13,796 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5247ms
2014-07-14 04:00:13,796 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5214ms
2014-07-14 04:00:13,796 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5174ms
2014-07-14 04:00:13,797 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5855ms
2014-07-14 04:00:13,797 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5905ms
2014-07-14 04:00:13,798 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5784ms
2014-07-14 04:00:13,798 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5688ms
2014-07-14 04:00:13,798 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5716ms
2014-07-14 04:00:13,799 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5645ms
2014-07-14 04:00:14,492 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:00:14,537 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:00:14,580 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:00:14,638 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:00:14,690 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:00:14,732 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:00:14,766 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:00:15,624 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:00:15,658 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:00:15,687 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:00:15,724 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:00:15,759 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:00:15,791 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:00:15,829 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:00:15,857 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:00:15,895 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:00:15,927 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:00:15,954 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:00:15,984 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:00:16,014 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:00:16,053 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:00:16,078 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:00:16,107 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:00:16,136 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:00:16,175 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:00:16,439 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:00:16,486 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:00:17,077 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:00:17,106 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:00:17,140 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:00:17,167 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:00:17,193 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:00:17,221 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:00:17,249 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:00:17,274 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:00:18,794 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10139ms
2014-07-14 04:00:18,794 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10607ms
2014-07-14 04:00:18,795 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10821ms
2014-07-14 04:00:18,795 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10535ms
2014-07-14 04:00:18,796 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10288ms
2014-07-14 04:00:18,796 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10328ms
2014-07-14 04:00:18,796 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10214ms
2014-07-14 04:00:18,796 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10248ms
2014-07-14 04:00:18,797 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10175ms
2014-07-14 04:00:18,797 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10856ms
2014-07-14 04:00:18,798 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10906ms
2014-07-14 04:00:18,798 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10784ms
2014-07-14 04:00:18,799 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10689ms
2014-07-14 04:00:18,799 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10717ms
2014-07-14 04:00:18,799 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10646ms
2014-07-14 04:00:19,493 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 04:00:19,538 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:00:19,581 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:00:19,638 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:00:19,690 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:00:19,733 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:00:19,767 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:00:20,745 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10021ms
2014-07-14 04:00:20,745 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10122ms
2014-07-14 04:00:20,745 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10058ms
2014-07-14 04:00:20,746 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10087ms
2014-07-14 04:00:20,759 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:00:20,791 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:00:20,829 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:00:20,857 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:00:20,895 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:00:20,927 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:00:20,954 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:00:20,984 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:00:21,014 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:00:21,053 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:00:21,079 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:00:21,107 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:00:21,136 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:00:21,175 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:00:21,235 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12106, memsize=790.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/dd83d178d98444bbbd70165d010841b9
2014-07-14 04:00:21,254 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/dd83d178d98444bbbd70165d010841b9 as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/dd83d178d98444bbbd70165d010841b9
2014-07-14 04:00:21,276 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/dd83d178d98444bbbd70165d010841b9, entries=2879280, sequenceid=12106, filesize=205.0m
2014-07-14 04:00:21,277 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.4g/1510103440, currentsize=62.3m/65276320 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 31629ms, sequenceid=12106, compaction requested=true
2014-07-14 04:00:21,277 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:00:21,277 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10103ms
2014-07-14 04:00:21,277 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,278 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10143ms
2014-07-14 04:00:21,278 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,278 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 30 store files, 0 compacting, 30 eligible, 20 blocking
2014-07-14 04:00:21,278 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10172ms
2014-07-14 04:00:21,278 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,281 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10203ms
2014-07-14 04:00:21,281 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,278 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 30 files from compaction candidates
2014-07-14 04:00:21,281 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:00:21,281 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:00:21,281 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 04:00:21,289 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10237ms
2014-07-14 04:00:21,289 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,289 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10276ms
2014-07-14 04:00:21,289 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,289 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10306ms
2014-07-14 04:00:21,289 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,289 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10336ms
2014-07-14 04:00:21,289 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,289 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10363ms
2014-07-14 04:00:21,290 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,290 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10396ms
2014-07-14 04:00:21,290 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,290 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10433ms
2014-07-14 04:00:21,290 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,291 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10463ms
2014-07-14 04:00:21,291 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,293 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10502ms
2014-07-14 04:00:21,293 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,294 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10536ms
2014-07-14 04:00:21,295 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,300 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10642ms
2014-07-14 04:00:21,300 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,300 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10613ms
2014-07-14 04:00:21,300 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,300 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10677ms
2014-07-14 04:00:21,300 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,301 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10578ms
2014-07-14 04:00:21,301 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,301 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11535ms
2014-07-14 04:00:21,301 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,305 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11573ms
2014-07-14 04:00:21,305 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,305 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11616ms
2014-07-14 04:00:21,305 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,305 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11668ms
2014-07-14 04:00:21,305 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,307 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11727ms
2014-07-14 04:00:21,307 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,307 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11770ms
2014-07-14 04:00:21,307 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,310 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11819ms
2014-07-14 04:00:21,310 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,310 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13157ms
2014-07-14 04:00:21,310 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,310 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13228ms
2014-07-14 04:00:21,310 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,311 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13201ms
2014-07-14 04:00:21,311 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,311 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13297ms
2014-07-14 04:00:21,312 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,312 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13420ms
2014-07-14 04:00:21,312 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,312 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13371ms
2014-07-14 04:00:21,312 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,313 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12691ms
2014-07-14 04:00:21,313 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,313 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12765ms
2014-07-14 04:00:21,313 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,313 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12731ms
2014-07-14 04:00:21,313 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,314 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12846ms
2014-07-14 04:00:21,315 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,315 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12808ms
2014-07-14 04:00:21,315 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,315 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13055ms
2014-07-14 04:00:21,315 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,315 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13341ms
2014-07-14 04:00:21,315 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,315 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13128ms
2014-07-14 04:00:21,315 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,315 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12660ms
2014-07-14 04:00:21,315 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,316 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14042ms
2014-07-14 04:00:21,316 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,316 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14068ms
2014-07-14 04:00:21,316 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,317 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14097ms
2014-07-14 04:00:21,317 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,317 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14125ms
2014-07-14 04:00:21,318 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,318 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14152ms
2014-07-14 04:00:21,318 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,319 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14180ms
2014-07-14 04:00:21,319 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,319 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14214ms
2014-07-14 04:00:21,319 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,319 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14243ms
2014-07-14 04:00:21,319 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,319 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14834ms
2014-07-14 04:00:21,319 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,320 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14882ms
2014-07-14 04:00:21,320 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:00:21,418 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15120,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335606297,"queuetimems":0,"class":"HRegionServer","responsesize":13111,"method":"Multi"}
2014-07-14 04:00:21,439 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15066,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335606372,"queuetimems":1,"class":"HRegionServer","responsesize":13061,"method":"Multi"}
2014-07-14 04:00:21,940 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:00:22,024 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48524 synced till here 48505
2014-07-14 04:00:22,172 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335606164 with entries=104, filesize=79.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335621941
2014-07-14 04:00:22,173 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335487311
2014-07-14 04:00:22,173 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335488921
2014-07-14 04:00:23,281 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1035ms
GC pool 'ParNew' had collection(s): count=1 time=1105ms
2014-07-14 04:00:23,644 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12540,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335611103,"queuetimems":0,"class":"HRegionServer","responsesize":12969,"method":"Multi"}
2014-07-14 04:00:23,786 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b.
2014-07-14 04:00:23,786 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b., current region memstore size 256.5m
2014-07-14 04:00:23,788 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12777,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335611010,"queuetimems":0,"class":"HRegionServer","responsesize":12848,"method":"Multi"}
2014-07-14 04:00:23,788 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12868,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335610920,"queuetimems":0,"class":"HRegionServer","responsesize":13136,"method":"Multi"}
2014-07-14 04:00:23,788 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12837,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335610950,"queuetimems":1,"class":"HRegionServer","responsesize":13168,"method":"Multi"}
2014-07-14 04:00:23,968 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 04:00:24,136 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:00:24,169 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48620 synced till here 48606
2014-07-14 04:00:24,220 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13336,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335610884,"queuetimems":0,"class":"HRegionServer","responsesize":12917,"method":"Multi"}
2014-07-14 04:00:24,221 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15674,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335608546,"queuetimems":0,"class":"HRegionServer","responsesize":13146,"method":"Multi"}
2014-07-14 04:00:24,222 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16037,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335608185,"queuetimems":0,"class":"HRegionServer","responsesize":12952,"method":"Multi"}
2014-07-14 04:00:24,222 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15602,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335608620,"queuetimems":0,"class":"HRegionServer","responsesize":13145,"method":"Multi"}
2014-07-14 04:00:24,225 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17152,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335607073,"queuetimems":0,"class":"HRegionServer","responsesize":12857,"method":"Multi"}
2014-07-14 04:00:24,231 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15765,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335608466,"queuetimems":0,"class":"HRegionServer","responsesize":13168,"method":"Multi"}
2014-07-14 04:00:24,233 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16153,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335608080,"queuetimems":0,"class":"HRegionServer","responsesize":12931,"method":"Multi"}
2014-07-14 04:00:24,233 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15975,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335608258,"queuetimems":0,"class":"HRegionServer","responsesize":12969,"method":"Multi"}
2014-07-14 04:00:24,263 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335621941 with entries=96, filesize=73.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335624136
2014-07-14 04:00:24,318 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16210,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335608107,"queuetimems":0,"class":"HRegionServer","responsesize":12927,"method":"Multi"}
2014-07-14 04:00:25,214 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17968,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335607246,"queuetimems":0,"class":"HRegionServer","responsesize":12877,"method":"Multi"}
2014-07-14 04:00:25,214 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18078,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335607136,"queuetimems":0,"class":"HRegionServer","responsesize":13136,"method":"Multi"}
2014-07-14 04:00:25,217 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15641,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335609576,"queuetimems":0,"class":"HRegionServer","responsesize":12934,"method":"Multi"}
2014-07-14 04:00:25,219 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16565,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335608653,"queuetimems":1,"class":"HRegionServer","responsesize":12855,"method":"Multi"}
2014-07-14 04:00:25,324 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16818,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335608505,"queuetimems":0,"class":"HRegionServer","responsesize":12934,"method":"Multi"}
2014-07-14 04:00:25,324 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14677,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335610647,"queuetimems":1,"class":"HRegionServer","responsesize":13283,"method":"Multi"}
2014-07-14 04:00:25,324 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15639,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335609685,"queuetimems":1,"class":"HRegionServer","responsesize":13417,"method":"Multi"}
2014-07-14 04:00:25,324 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14538,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335610786,"queuetimems":1,"class":"HRegionServer","responsesize":13162,"method":"Multi"}
2014-07-14 04:00:25,324 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14603,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335610721,"queuetimems":1,"class":"HRegionServer","responsesize":13310,"method":"Multi"}
2014-07-14 04:00:25,324 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14344,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335610980,"queuetimems":1,"class":"HRegionServer","responsesize":12952,"method":"Multi"}
2014-07-14 04:00:25,325 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17353,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335607972,"queuetimems":0,"class":"HRegionServer","responsesize":13053,"method":"Multi"}
2014-07-14 04:00:25,325 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17436,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335607889,"queuetimems":1,"class":"HRegionServer","responsesize":12848,"method":"Multi"}
2014-07-14 04:00:25,325 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15561,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335609763,"queuetimems":1,"class":"HRegionServer","responsesize":13061,"method":"Multi"}
2014-07-14 04:00:25,325 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18135,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335607190,"queuetimems":0,"class":"HRegionServer","responsesize":13310,"method":"Multi"}
2014-07-14 04:00:25,325 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15601,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335609724,"queuetimems":1,"class":"HRegionServer","responsesize":12994,"method":"Multi"}
2014-07-14 04:00:25,326 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18222,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335607103,"queuetimems":0,"class":"HRegionServer","responsesize":13030,"method":"Multi"}
2014-07-14 04:00:25,326 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14709,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335610616,"queuetimems":1,"class":"HRegionServer","responsesize":12931,"method":"Multi"}
2014-07-14 04:00:25,326 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17387,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335607939,"queuetimems":0,"class":"HRegionServer","responsesize":13222,"method":"Multi"}
2014-07-14 04:00:25,333 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18115,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335607218,"queuetimems":0,"class":"HRegionServer","responsesize":12918,"method":"Multi"}
2014-07-14 04:00:25,333 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14259,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335611074,"queuetimems":0,"class":"HRegionServer","responsesize":12927,"method":"Multi"}
2014-07-14 04:00:25,334 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15847,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335609487,"queuetimems":1,"class":"HRegionServer","responsesize":12855,"method":"Multi"}
2014-07-14 04:00:25,334 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15701,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335609633,"queuetimems":0,"class":"HRegionServer","responsesize":13111,"method":"Multi"}
2014-07-14 04:00:25,334 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14583,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335610751,"queuetimems":0,"class":"HRegionServer","responsesize":13216,"method":"Multi"}
2014-07-14 04:00:25,333 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18170,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335607163,"queuetimems":0,"class":"HRegionServer","responsesize":13283,"method":"Multi"}
2014-07-14 04:00:25,335 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14168,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335611166,"queuetimems":1,"class":"HRegionServer","responsesize":12918,"method":"Multi"}
2014-07-14 04:00:25,335 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14510,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335610825,"queuetimems":0,"class":"HRegionServer","responsesize":12877,"method":"Multi"}
2014-07-14 04:00:25,336 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18064,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335607271,"queuetimems":0,"class":"HRegionServer","responsesize":13446,"method":"Multi"}
2014-07-14 04:00:25,336 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14482,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335610854,"queuetimems":0,"class":"HRegionServer","responsesize":13145,"method":"Multi"}
2014-07-14 04:00:25,337 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15803,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335609534,"queuetimems":1,"class":"HRegionServer","responsesize":12945,"method":"Multi"}
2014-07-14 04:00:25,345 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16766,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335608579,"queuetimems":1,"class":"HRegionServer","responsesize":13162,"method":"Multi"}
2014-07-14 04:00:25,333 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14201,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335611132,"queuetimems":1,"class":"HRegionServer","responsesize":12857,"method":"Multi"}
2014-07-14 04:00:25,345 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17333,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335608012,"queuetimems":1,"class":"HRegionServer","responsesize":12917,"method":"Multi"}
2014-07-14 04:00:25,336 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14289,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335611046,"queuetimems":0,"class":"HRegionServer","responsesize":13053,"method":"Multi"}
2014-07-14 04:00:25,469 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17318,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335608151,"queuetimems":0,"class":"HRegionServer","responsesize":13216,"method":"Multi"}
2014-07-14 04:00:25,469 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14786,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335610683,"queuetimems":0,"class":"HRegionServer","responsesize":13146,"method":"Multi"}
2014-07-14 04:00:26,239 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:00:27,372 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48746 synced till here 48742
2014-07-14 04:00:27,442 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335624136 with entries=126, filesize=82.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335626239
2014-07-14 04:00:29,366 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:00:29,382 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48860 synced till here 48829
2014-07-14 04:00:29,863 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335626239 with entries=114, filesize=88.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335629366
2014-07-14 04:00:31,501 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1162ms
GC pool 'ParNew' had collection(s): count=1 time=1162ms
2014-07-14 04:00:31,773 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:00:31,774 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 04:00:31,920 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48982 synced till here 48979
2014-07-14 04:00:31,977 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335629366 with entries=122, filesize=74.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335631773
2014-07-14 04:00:33,795 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1286ms
GC pool 'ParNew' had collection(s): count=1 time=1278ms
2014-07-14 04:00:34,045 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:00:34,135 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49062 synced till here 49061
2014-07-14 04:00:34,220 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335631773 with entries=80, filesize=64.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335634046
2014-07-14 04:00:36,327 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:00:36,593 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49231 synced till here 49211
2014-07-14 04:00:36,948 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335634046 with entries=169, filesize=114.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335636327
2014-07-14 04:00:39,163 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:00:39,369 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49373 synced till here 49357
2014-07-14 04:00:39,468 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335636327 with entries=142, filesize=96.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335639164
2014-07-14 04:00:41,576 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:00:41,620 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49499 synced till here 49461
2014-07-14 04:00:42,907 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335639164 with entries=126, filesize=93.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335641576
2014-07-14 04:00:43,127 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12131, memsize=243.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/64b462e9b83449468c236a36e8306cac
2014-07-14 04:00:43,146 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/64b462e9b83449468c236a36e8306cac as hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/64b462e9b83449468c236a36e8306cac
2014-07-14 04:00:43,159 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/64b462e9b83449468c236a36e8306cac, entries=885800, sequenceid=12131, filesize=63.1m
2014-07-14 04:00:43,160 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~257.0m/269452640, currentsize=111.1m/116521440 for region usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. in 19374ms, sequenceid=12131, compaction requested=true
2014-07-14 04:00:43,160 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:00:43,161 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 20 blocking
2014-07-14 04:00:43,161 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 97980ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 04:00:43,161 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-14 04:00:43,161 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:00:43,161 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:00:43,161 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 500.7m
2014-07-14 04:00:43,161 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. because compaction request was cancelled
2014-07-14 04:00:43,763 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:00:43,833 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49643 synced till here 49608
2014-07-14 04:00:43,976 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 04:00:44,161 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335641576 with entries=144, filesize=100.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335643764
2014-07-14 04:00:45,121 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12116, memsize=747.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/ebf471e28e3a448baab4cf85233b2093
2014-07-14 04:00:45,136 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/ebf471e28e3a448baab4cf85233b2093 as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/ebf471e28e3a448baab4cf85233b2093
2014-07-14 04:00:45,150 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/ebf471e28e3a448baab4cf85233b2093, entries=2721740, sequenceid=12116, filesize=193.8m
2014-07-14 04:00:45,150 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.4g/1462617120, currentsize=504.1m/528544560 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 44179ms, sequenceid=12116, compaction requested=true
2014-07-14 04:00:45,151 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:00:45,151 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 35 store files, 0 compacting, 35 eligible, 20 blocking
2014-07-14 04:00:45,151 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. has too many store files; delaying flush up to 90000ms
2014-07-14 04:00:45,151 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 35 files from compaction candidates
2014-07-14 04:00:45,152 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-14 04:00:45,152 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:00:45,152 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:00:45,152 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 04:00:45,152 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 30 store files, 0 compacting, 30 eligible, 20 blocking
2014-07-14 04:00:45,152 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 30 files from compaction candidates
2014-07-14 04:00:45,152 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:00:45,152 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:00:45,152 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 04:00:45,233 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 04:00:45,233 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. has too many store files; delaying flush up to 90000ms
2014-07-14 04:00:45,234 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 35 store files, 0 compacting, 35 eligible, 20 blocking
2014-07-14 04:00:45,234 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 35 files from compaction candidates
2014-07-14 04:00:45,234 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:00:45,235 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:00:45,235 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 04:00:45,235 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:00:45,771 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:00:45,787 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49763 synced till here 49733
2014-07-14 04:00:46,221 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335643764 with entries=120, filesize=93.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335645772
2014-07-14 04:00:46,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335491559
2014-07-14 04:00:46,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335510489
2014-07-14 04:00:46,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335512631
2014-07-14 04:00:46,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335514620
2014-07-14 04:00:46,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335517235
2014-07-14 04:00:46,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335519173
2014-07-14 04:00:46,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335520770
2014-07-14 04:00:46,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335523118
2014-07-14 04:00:46,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335525529
2014-07-14 04:00:46,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335527322
2014-07-14 04:00:46,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335529138
2014-07-14 04:00:46,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335541330
2014-07-14 04:00:46,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335542974
2014-07-14 04:00:46,331 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 71c27911c72dddd675be84dade80b522
2014-07-14 04:00:47,687 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:00:47,704 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49885 synced till here 49865
2014-07-14 04:00:47,841 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335645772 with entries=122, filesize=74.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335647688
2014-07-14 04:00:47,841 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 71c27911c72dddd675be84dade80b522
2014-07-14 04:00:49,971 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:00:50,197 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50000 synced till here 49976
2014-07-14 04:00:50,469 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335647688 with entries=115, filesize=90.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335649972
2014-07-14 04:00:50,469 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 71c27911c72dddd675be84dade80b522
2014-07-14 04:00:52,413 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:00:52,510 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50148 synced till here 50125
2014-07-14 04:00:52,712 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335649972 with entries=148, filesize=96.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335652414
2014-07-14 04:00:52,712 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 71c27911c72dddd675be84dade80b522
2014-07-14 04:00:53,972 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:00:54,076 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50261 synced till here 50235
2014-07-14 04:00:54,255 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335652414 with entries=113, filesize=84.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335653973
2014-07-14 04:00:54,255 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 71c27911c72dddd675be84dade80b522
2014-07-14 04:00:54,992 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=12794, hits=5369, hitRatio=41.96%, , cachingAccesses=5373, cachingHits=5368, cachingHitsRatio=99.90%, evictions=0, evicted=3, evictedPerRun=Infinity
2014-07-14 04:00:55,265 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12444, memsize=144.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/bbbb26b97cf34c30b98249f6ed12e9b0
2014-07-14 04:00:55,285 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/bbbb26b97cf34c30b98249f6ed12e9b0 as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/bbbb26b97cf34c30b98249f6ed12e9b0
2014-07-14 04:00:55,301 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/bbbb26b97cf34c30b98249f6ed12e9b0, entries=527020, sequenceid=12444, filesize=37.6m
2014-07-14 04:00:55,302 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~519.1m/544338320, currentsize=282.8m/296560640 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 12140ms, sequenceid=12444, compaction requested=true
2014-07-14 04:00:55,302 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:00:55,302 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 20 blocking
2014-07-14 04:00:55,302 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-14 04:00:55,302 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:00:55,302 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:00:55,303 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 04:00:56,555 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90477ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 04:00:56,556 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 1.5g
2014-07-14 04:00:57,397 DEBUG [RpcServer.handler=5,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 04:00:57,397 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. has too many store files; delaying flush up to 90000ms
2014-07-14 04:00:57,397 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:00:57,397 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 20 blocking
2014-07-14 04:00:57,397 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-14 04:00:57,398 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:00:57,398 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:00:57,398 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 04:00:57,506 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:00:57,528 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50366 synced till here 50365
2014-07-14 04:00:57,546 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335653973 with entries=105, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335657506
2014-07-14 04:00:57,923 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 04:01:00,339 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:01:00,360 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50454 synced till here 50453
2014-07-14 04:01:00,384 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335657506 with entries=88, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335660339
2014-07-14 04:01:01,883 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:01:02,187 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50555 synced till here 50538
2014-07-14 04:01:02,270 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335660339 with entries=101, filesize=74.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335661883
2014-07-14 04:01:03,957 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:01:04,318 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b.
2014-07-14 04:01:04,318 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. has too many store files; delaying flush up to 90000ms
2014-07-14 04:01:04,319 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 20 blocking
2014-07-14 04:01:04,319 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-14 04:01:04,319 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:01:04,319 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:01:04,319 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. because compaction request was cancelled
2014-07-14 04:01:04,319 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:01:04,449 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50697 synced till here 50694
2014-07-14 04:01:04,480 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335661883 with entries=142, filesize=98.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335663957
2014-07-14 04:01:05,872 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:01:05,985 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50793 synced till here 50788
2014-07-14 04:01:06,042 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335663957 with entries=96, filesize=67.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335665872
2014-07-14 04:01:07,688 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:01:07,742 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335665872 with entries=88, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335667688
2014-07-14 04:01:09,880 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:01:09,903 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50969 synced till here 50968
2014-07-14 04:01:09,925 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335667688 with entries=88, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335669880
2014-07-14 04:01:11,775 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:01:12,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51062 synced till here 51058
2014-07-14 04:01:12,636 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335669880 with entries=93, filesize=65.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335671776
2014-07-14 04:01:13,231 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 04:01:13,231 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. has too many store files, but is 1.1g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 04:01:13,231 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. due to global heap pressure
2014-07-14 04:01:13,232 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 1.1g
2014-07-14 04:01:13,764 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:01:13,793 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51156 synced till here 51149
2014-07-14 04:01:14,001 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335671776 with entries=94, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335673765
2014-07-14 04:01:15,284 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 04:01:15,657 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:01:15,696 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51252 synced till here 51245
2014-07-14 04:01:15,805 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335673765 with entries=96, filesize=68.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335675657
2014-07-14 04:01:15,970 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:16,009 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:16,024 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:16,026 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:16,050 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:16,050 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:16,050 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:16,409 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:16,463 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:16,508 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:16,633 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:16,633 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:17,488 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:17,500 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:17,511 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:17,557 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:17,602 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:17,642 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:17,686 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:17,721 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:17,756 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:17,783 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:17,811 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:17,836 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:17,864 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:17,895 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:17,918 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:17,956 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:17,983 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:18,011 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:18,038 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:18,074 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:18,541 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:18,574 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:18,625 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:18,657 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:18,701 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:18,744 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:18,773 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:18,954 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:18,990 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:19,033 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:19,070 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:19,114 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:19,163 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:19,210 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:19,246 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:19,280 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:19,360 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:19,458 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:20,971 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:01:21,009 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:21,024 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:21,026 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:21,050 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:21,050 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:21,051 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:01:22,556 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5044ms
2014-07-14 04:01:22,556 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1071ms
GC pool 'ParNew' had collection(s): count=1 time=1240ms
2014-07-14 04:01:22,556 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6093ms
2014-07-14 04:01:22,557 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6049ms
2014-07-14 04:01:22,557 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5943ms
2014-07-14 04:01:22,557 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5995ms
2014-07-14 04:01:22,557 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5069ms
2014-07-14 04:01:22,557 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5058ms
2014-07-14 04:01:22,558 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:01:22,558 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6149ms
2014-07-14 04:01:22,602 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:22,642 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:01:22,687 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:22,722 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:22,757 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:01:22,784 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:22,811 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:22,837 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:22,865 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:01:22,895 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:22,918 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:22,956 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:01:22,984 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:23,011 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:23,038 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:23,075 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:01:23,541 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:23,574 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:23,625 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:23,657 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:23,702 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:01:23,744 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:23,773 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:23,954 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:23,990 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:24,034 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:01:24,071 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:01:24,114 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:24,163 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:24,210 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:24,246 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:24,280 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:24,360 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:24,458 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:01:25,972 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:01:26,010 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:01:26,025 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:01:26,026 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:01:26,051 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:01:26,051 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:01:26,051 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:01:27,916 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10021ms
2014-07-14 04:01:27,917 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10315ms
2014-07-14 04:01:27,918 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10277ms
2014-07-14 04:01:27,918 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10232ms
2014-07-14 04:01:27,918 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10197ms
2014-07-14 04:01:27,918 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10162ms
2014-07-14 04:01:27,918 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10135ms
2014-07-14 04:01:27,919 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10107ms
2014-07-14 04:01:27,919 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10083ms
2014-07-14 04:01:27,920 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10055ms
2014-07-14 04:01:27,920 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 04:01:27,920 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11457ms
2014-07-14 04:01:27,921 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10409ms
2014-07-14 04:01:27,921 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11413ms
2014-07-14 04:01:27,921 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11307ms
2014-07-14 04:01:27,922 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11359ms
2014-07-14 04:01:27,923 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10434ms
2014-07-14 04:01:27,923 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10424ms
2014-07-14 04:01:27,924 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10367ms
2014-07-14 04:01:27,924 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11515ms
2014-07-14 04:01:27,957 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 04:01:27,984 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:01:28,012 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:01:28,039 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:01:28,075 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:01:28,542 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:01:28,575 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:01:28,626 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:01:28,658 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:01:28,702 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:01:28,744 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:01:28,773 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:01:28,955 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:01:28,992 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:01:29,034 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:01:29,071 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:01:29,116 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:01:29,163 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:01:29,210 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:01:29,247 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:01:29,281 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:01:29,361 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:01:29,459 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:01:30,972 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 04:01:31,011 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 04:01:31,025 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 04:01:31,026 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 04:01:31,051 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 04:01:31,052 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 04:01:31,052 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 04:01:32,297 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12821, memsize=327.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/d74e22e52e844e89b892ec94ff46a882
2014-07-14 04:01:32,319 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/d74e22e52e844e89b892ec94ff46a882 as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/d74e22e52e844e89b892ec94ff46a882
2014-07-14 04:01:32,338 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/d74e22e52e844e89b892ec94ff46a882, entries=1191800, sequenceid=12821, filesize=84.9m
2014-07-14 04:01:32,338 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.1g/1145534960, currentsize=59.5m/62421840 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 19106ms, sequenceid=12821, compaction requested=true
2014-07-14 04:01:32,339 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:01:32,339 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 36 store files, 0 compacting, 36 eligible, 20 blocking
2014-07-14 04:01:32,339 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 36 files from compaction candidates
2014-07-14 04:01:32,339 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16289ms
2014-07-14 04:01:32,339 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,339 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:01:32,339 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16289ms
2014-07-14 04:01:32,339 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,339 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:01:32,339 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16289ms
2014-07-14 04:01:32,339 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,339 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 04:01:32,340 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16314ms
2014-07-14 04:01:32,340 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,340 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16316ms
2014-07-14 04:01:32,340 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,340 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16331ms
2014-07-14 04:01:32,340 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,341 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16371ms
2014-07-14 04:01:32,341 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,341 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12883ms
2014-07-14 04:01:32,341 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,341 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12981ms
2014-07-14 04:01:32,341 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,341 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13061ms
2014-07-14 04:01:32,341 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,345 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13099ms
2014-07-14 04:01:32,345 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,345 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13135ms
2014-07-14 04:01:32,345 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,347 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13184ms
2014-07-14 04:01:32,347 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,348 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13234ms
2014-07-14 04:01:32,348 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,349 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13279ms
2014-07-14 04:01:32,349 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,357 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13324ms
2014-07-14 04:01:32,357 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,357 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13367ms
2014-07-14 04:01:32,357 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,357 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13403ms
2014-07-14 04:01:32,357 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,365 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13592ms
2014-07-14 04:01:32,366 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,366 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13622ms
2014-07-14 04:01:32,366 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,366 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13665ms
2014-07-14 04:01:32,366 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,366 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13709ms
2014-07-14 04:01:32,366 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,372 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13746ms
2014-07-14 04:01:32,372 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,372 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13798ms
2014-07-14 04:01:32,372 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,373 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13832ms
2014-07-14 04:01:32,373 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,374 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14299ms
2014-07-14 04:01:32,374 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,375 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14337ms
2014-07-14 04:01:32,375 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,375 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14364ms
2014-07-14 04:01:32,375 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,377 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14394ms
2014-07-14 04:01:32,377 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,380 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14425ms
2014-07-14 04:01:32,380 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,381 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16832,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335675548,"queuetimems":1,"class":"HRegionServer","responsesize":13433,"method":"Multi"}
2014-07-14 04:01:32,385 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15976ms
2014-07-14 04:01:32,385 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,385 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14828ms
2014-07-14 04:01:32,385 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,385 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14886ms
2014-07-14 04:01:32,385 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,385 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14897ms
2014-07-14 04:01:32,386 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,386 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15824ms
2014-07-14 04:01:32,386 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,386 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15772ms
2014-07-14 04:01:32,386 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,386 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15878ms
2014-07-14 04:01:32,386 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,388 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14875ms
2014-07-14 04:01:32,388 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,388 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16813,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335675575,"queuetimems":0,"class":"HRegionServer","responsesize":12981,"method":"Multi"}
2014-07-14 04:01:32,394 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15930ms
2014-07-14 04:01:32,394 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:32,394 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14476ms
2014-07-14 04:01:32,395 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:33,472 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15636ms
2014-07-14 04:01:33,472 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:33,472 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1052ms
GC pool 'ParNew' had collection(s): count=1 time=1070ms
2014-07-14 04:01:33,493 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15629ms
2014-07-14 04:01:33,493 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:33,493 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15682ms
2014-07-14 04:01:33,493 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:33,495 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15711ms
2014-07-14 04:01:33,495 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:33,500 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15744ms
2014-07-14 04:01:33,501 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:33,501 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15780ms
2014-07-14 04:01:33,502 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:33,504 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15818ms
2014-07-14 04:01:33,505 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:33,506 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15865ms
2014-07-14 04:01:33,506 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:33,506 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15904ms
2014-07-14 04:01:33,506 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:33,508 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15613ms
2014-07-14 04:01:33,508 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:33,558 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17899,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335675659,"queuetimems":1,"class":"HRegionServer","responsesize":12962,"method":"Multi"}
2014-07-14 04:01:33,561 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17928,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335675633,"queuetimems":0,"class":"HRegionServer","responsesize":13376,"method":"Multi"}
2014-07-14 04:01:33,581 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17974,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335675607,"queuetimems":0,"class":"HRegionServer","responsesize":12872,"method":"Multi"}
2014-07-14 04:01:33,593 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17906,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335675687,"queuetimems":0,"class":"HRegionServer","responsesize":12931,"method":"Multi"}
2014-07-14 04:01:33,593 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17873,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335675720,"queuetimems":0,"class":"HRegionServer","responsesize":13271,"method":"Multi"}
2014-07-14 04:01:34,245 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:01:34,406 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14949,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335679456,"queuetimems":1,"class":"HRegionServer","responsesize":13163,"method":"Multi"}
2014-07-14 04:01:34,449 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15093,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335679356,"queuetimems":0,"class":"HRegionServer","responsesize":12867,"method":"Multi"}
2014-07-14 04:01:34,469 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15193,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335679275,"queuetimems":0,"class":"HRegionServer","responsesize":13160,"method":"Multi"}
2014-07-14 04:01:34,474 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15231,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335679242,"queuetimems":0,"class":"HRegionServer","responsesize":12936,"method":"Multi"}
2014-07-14 04:01:34,486 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15280,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335679205,"queuetimems":0,"class":"HRegionServer","responsesize":12872,"method":"Multi"}
2014-07-14 04:01:35,992 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51401 synced till here 51366
2014-07-14 04:01:36,276 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335675657 with entries=149, filesize=114.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335694245
2014-07-14 04:01:36,882 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19373,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335677508,"queuetimems":1,"class":"HRegionServer","responsesize":13249,"method":"Multi"}
2014-07-14 04:01:36,882 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19197,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335677684,"queuetimems":0,"class":"HRegionServer","responsesize":13163,"method":"Multi"}
2014-07-14 04:01:36,977 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18446,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335678530,"queuetimems":2,"class":"HRegionServer","responsesize":12931,"method":"Multi"}
2014-07-14 04:01:36,977 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18027,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335678949,"queuetimems":0,"class":"HRegionServer","responsesize":13063,"method":"Multi"}
2014-07-14 04:01:36,977 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17909,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335679067,"queuetimems":0,"class":"HRegionServer","responsesize":13069,"method":"Multi"}
2014-07-14 04:01:36,977 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18360,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335678616,"queuetimems":0,"class":"HRegionServer","responsesize":13433,"method":"Multi"}
2014-07-14 04:01:36,977 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20520,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335676456,"queuetimems":0,"class":"HRegionServer","responsesize":13492,"method":"Multi"}
2014-07-14 04:01:36,982 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17822,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335679159,"queuetimems":0,"class":"HRegionServer","responsesize":12981,"method":"Multi"}
2014-07-14 04:01:36,983 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19091,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335677891,"queuetimems":2,"class":"HRegionServer","responsesize":13347,"method":"Multi"}
2014-07-14 04:01:36,988 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18953,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335678035,"queuetimems":0,"class":"HRegionServer","responsesize":12877,"method":"Multi"}
2014-07-14 04:01:36,993 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19015,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335677978,"queuetimems":0,"class":"HRegionServer","responsesize":13345,"method":"Multi"}
2014-07-14 04:01:36,994 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19440,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335677554,"queuetimems":1,"class":"HRegionServer","responsesize":12902,"method":"Multi"}
2014-07-14 04:01:36,996 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20438,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335676558,"queuetimems":0,"class":"HRegionServer","responsesize":13056,"method":"Multi"}
2014-07-14 04:01:36,996 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19082,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335677914,"queuetimems":0,"class":"HRegionServer","responsesize":12733,"method":"Multi"}
2014-07-14 04:01:36,998 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19242,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335677754,"queuetimems":1,"class":"HRegionServer","responsesize":13214,"method":"Multi"}
2014-07-14 04:01:38,215 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:01:38,217 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19480,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335678736,"queuetimems":0,"class":"HRegionServer","responsesize":12962,"method":"Multi"}
2014-07-14 04:01:38,220 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19108,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335679111,"queuetimems":0,"class":"HRegionServer","responsesize":13166,"method":"Multi"}
2014-07-14 04:01:38,220 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20152,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335678068,"queuetimems":0,"class":"HRegionServer","responsesize":12869,"method":"Multi"}
2014-07-14 04:01:38,221 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19649,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335678571,"queuetimems":1,"class":"HRegionServer","responsesize":13271,"method":"Multi"}
2014-07-14 04:01:38,220 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20270,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335677950,"queuetimems":0,"class":"HRegionServer","responsesize":12790,"method":"Multi"}
2014-07-14 04:01:38,220 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21613,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335676607,"queuetimems":1,"class":"HRegionServer","responsesize":12734,"method":"Multi"}
2014-07-14 04:01:38,221 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20386,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335677834,"queuetimems":0,"class":"HRegionServer","responsesize":12934,"method":"Multi"}
2014-07-14 04:01:38,222 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19570,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335678652,"queuetimems":1,"class":"HRegionServer","responsesize":13376,"method":"Multi"}
2014-07-14 04:01:38,221 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19235,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335678985,"queuetimems":0,"class":"HRegionServer","responsesize":13040,"method":"Multi"}
2014-07-14 04:01:38,222 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19192,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335679030,"queuetimems":0,"class":"HRegionServer","responsesize":12606,"method":"Multi"}
2014-07-14 04:01:38,222 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20504,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335677717,"queuetimems":0,"class":"HRegionServer","responsesize":13045,"method":"Multi"}
2014-07-14 04:01:38,223 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19531,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335678692,"queuetimems":0,"class":"HRegionServer","responsesize":13061,"method":"Multi"}
2014-07-14 04:01:38,223 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20361,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335677862,"queuetimems":0,"class":"HRegionServer","responsesize":13326,"method":"Multi"}
2014-07-14 04:01:38,224 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20727,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335677496,"queuetimems":0,"class":"HRegionServer","responsesize":13084,"method":"Multi"}
2014-07-14 04:01:38,224 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20739,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335677485,"queuetimems":2,"class":"HRegionServer","responsesize":12807,"method":"Multi"}
2014-07-14 04:01:38,225 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20625,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335677600,"queuetimems":1,"class":"HRegionServer","responsesize":13145,"method":"Multi"}
2014-07-14 04:01:38,225 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19457,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335678768,"queuetimems":0,"class":"HRegionServer","responsesize":13047,"method":"Multi"}
2014-07-14 04:01:38,224 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20216,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335678007,"queuetimems":0,"class":"HRegionServer","responsesize":12946,"method":"Multi"}
2014-07-14 04:01:38,225 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21821,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335676404,"queuetimems":0,"class":"HRegionServer","responsesize":12954,"method":"Multi"}
2014-07-14 04:01:38,226 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20586,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335677639,"queuetimems":0,"class":"HRegionServer","responsesize":13026,"method":"Multi"}
2014-07-14 04:01:38,234 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20446,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335677779,"queuetimems":0,"class":"HRegionServer","responsesize":12437,"method":"Multi"}
2014-07-14 04:01:38,346 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51530 synced till here 51505
2014-07-14 04:01:38,372 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21864,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335676504,"queuetimems":1,"class":"HRegionServer","responsesize":13025,"method":"Multi"}
2014-07-14 04:01:38,455 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20645,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335677809,"queuetimems":1,"class":"HRegionServer","responsesize":12720,"method":"Multi"}
2014-07-14 04:01:38,489 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335694245 with entries=129, filesize=80.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335698216
2014-07-14 04:01:40,211 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1002ms
GC pool 'ParNew' had collection(s): count=1 time=1074ms
2014-07-14 04:01:40,678 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:01:40,782 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51651 synced till here 51621
2014-07-14 04:01:40,999 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335698216 with entries=121, filesize=91.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335700679
2014-07-14 04:01:43,906 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:01:43,929 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 04:01:43,931 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. has too many store files; delaying flush up to 90000ms
2014-07-14 04:01:43,945 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:01:43,945 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 36 store files, 0 compacting, 36 eligible, 20 blocking
2014-07-14 04:01:43,945 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 36 files from compaction candidates
2014-07-14 04:01:43,945 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:01:43,945 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:01:43,945 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 04:01:44,017 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51797 synced till here 51753
2014-07-14 04:01:44,767 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335700679 with entries=146, filesize=94.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335703906
2014-07-14 04:01:46,567 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:01:46,658 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51908 synced till here 51894
2014-07-14 04:01:46,793 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335703906 with entries=111, filesize=75.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335706568
2014-07-14 04:01:48,173 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:01:48,505 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 04:01:48,506 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. has too many store files, but is 992.4m vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 04:01:48,506 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. due to global heap pressure
2014-07-14 04:01:48,506 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 992.4m
2014-07-14 04:01:48,520 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52023 synced till here 52012
2014-07-14 04:01:48,601 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335706568 with entries=115, filesize=76.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335708174
2014-07-14 04:01:50,644 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:01:50,679 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52168 synced till here 52133
2014-07-14 04:01:51,046 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,049 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,050 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,051 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,051 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,052 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,052 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,053 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,054 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,054 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,056 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,058 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,058 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,060 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,060 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,061 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,061 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,062 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,065 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,066 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,066 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,071 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,071 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,072 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,072 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,073 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,077 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,078 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,078 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335708174 with entries=145, filesize=98.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335710645
2014-07-14 04:01:51,115 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,121 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,130 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,137 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,139 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,155 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,163 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,203 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,204 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,205 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,205 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,205 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,210 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,214 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,854 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,854 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,855 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,855 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,855 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,856 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,857 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,857 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:01:51,965 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 04:01:53,152 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12620, memsize=882.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/95e08705241b438b9d61e8536b8ce629
2014-07-14 04:01:53,166 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/95e08705241b438b9d61e8536b8ce629 as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/95e08705241b438b9d61e8536b8ce629
2014-07-14 04:01:53,179 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/95e08705241b438b9d61e8536b8ce629, entries=3214000, sequenceid=12620, filesize=228.8m
2014-07-14 04:01:53,179 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.5g/1660434480, currentsize=505.5m/530069760 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 56623ms, sequenceid=12620, compaction requested=true
2014-07-14 04:01:53,180 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:01:53,180 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 20 blocking
2014-07-14 04:01:53,180 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1323ms
2014-07-14 04:01:53,180 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,180 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-14 04:01:53,180 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1323ms
2014-07-14 04:01:53,180 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:01:53,180 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,180 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:01:53,180 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 04:01:53,185 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1329ms
2014-07-14 04:01:53,185 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,185 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1330ms
2014-07-14 04:01:53,185 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,185 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1331ms
2014-07-14 04:01:53,185 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,185 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1331ms
2014-07-14 04:01:53,185 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,185 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1969ms
2014-07-14 04:01:53,185 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,185 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1331ms
2014-07-14 04:01:53,186 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,186 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1972ms
2014-07-14 04:01:53,186 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,186 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1976ms
2014-07-14 04:01:53,186 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,189 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1986ms
2014-07-14 04:01:53,189 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,190 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1987ms
2014-07-14 04:01:53,190 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,191 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1988ms
2014-07-14 04:01:53,191 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,191 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1988ms
2014-07-14 04:01:53,191 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,191 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1988ms
2014-07-14 04:01:53,191 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,191 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2028ms
2014-07-14 04:01:53,191 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,205 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2050ms
2014-07-14 04:01:53,205 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,205 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2066ms
2014-07-14 04:01:53,205 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,205 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2069ms
2014-07-14 04:01:53,205 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,205 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2076ms
2014-07-14 04:01:53,205 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,205 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2084ms
2014-07-14 04:01:53,206 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,206 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2091ms
2014-07-14 04:01:53,206 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,206 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2128ms
2014-07-14 04:01:53,206 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,206 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2129ms
2014-07-14 04:01:53,206 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,209 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2136ms
2014-07-14 04:01:53,209 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,209 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2137ms
2014-07-14 04:01:53,209 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,213 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2142ms
2014-07-14 04:01:53,213 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,213 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2142ms
2014-07-14 04:01:53,213 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,213 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2142ms
2014-07-14 04:01:53,213 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,213 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2160ms
2014-07-14 04:01:53,214 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,221 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2170ms
2014-07-14 04:01:53,221 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,229 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2164ms
2014-07-14 04:01:53,229 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,229 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2167ms
2014-07-14 04:01:53,229 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,229 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2168ms
2014-07-14 04:01:53,229 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,237 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2176ms
2014-07-14 04:01:53,237 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,242 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2182ms
2014-07-14 04:01:53,242 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,242 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2182ms
2014-07-14 04:01:53,242 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,242 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2184ms
2014-07-14 04:01:53,242 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,242 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2184ms
2014-07-14 04:01:53,242 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,242 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2186ms
2014-07-14 04:01:53,243 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,249 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2195ms
2014-07-14 04:01:53,249 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,249 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2196ms
2014-07-14 04:01:53,249 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,249 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2196ms
2014-07-14 04:01:53,250 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,250 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2198ms
2014-07-14 04:01:53,250 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,250 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2199ms
2014-07-14 04:01:53,250 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,250 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2199ms
2014-07-14 04:01:53,250 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,253 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2202ms
2014-07-14 04:01:53,253 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,253 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2203ms
2014-07-14 04:01:53,253 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,255 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2206ms
2014-07-14 04:01:53,255 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,255 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2209ms
2014-07-14 04:01:53,255 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:01:53,548 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 04:01:53,548 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. has too many store files; delaying flush up to 90000ms
2014-07-14 04:01:53,549 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:01:53,549 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 20 blocking
2014-07-14 04:01:53,549 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-14 04:01:53,549 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:01:53,549 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:01:53,549 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 04:01:54,009 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:01:54,096 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52308 synced till here 52290
2014-07-14 04:01:54,241 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335710645 with entries=140, filesize=95.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335714009
2014-07-14 04:01:54,241 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335544251
2014-07-14 04:01:54,241 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335546209
2014-07-14 04:01:54,241 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335548055
2014-07-14 04:01:54,241 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335549701
2014-07-14 04:01:54,241 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335552677
2014-07-14 04:01:54,241 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335555328
2014-07-14 04:01:54,241 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335557673
2014-07-14 04:01:54,241 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335558748
2014-07-14 04:01:54,241 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335560075
2014-07-14 04:01:54,241 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335561694
2014-07-14 04:01:54,241 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335563732
2014-07-14 04:01:54,241 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335566517
2014-07-14 04:01:54,241 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335567872
2014-07-14 04:01:54,242 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335569276
2014-07-14 04:01:54,242 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335571314
2014-07-14 04:01:54,242 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335573509
2014-07-14 04:01:54,242 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335575114
2014-07-14 04:01:54,242 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335578932
2014-07-14 04:01:54,242 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335581349
2014-07-14 04:01:54,242 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335583507
2014-07-14 04:01:54,242 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335585510
2014-07-14 04:01:54,242 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335587159
2014-07-14 04:01:54,242 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335588749
2014-07-14 04:01:54,242 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335604222
2014-07-14 04:01:54,242 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335606164
2014-07-14 04:01:55,522 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:01:55,637 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52426 synced till here 52424
2014-07-14 04:01:55,683 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335714009 with entries=118, filesize=82.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335715523
2014-07-14 04:01:55,683 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:01:57,577 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:01:57,678 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52559 synced till here 52514
2014-07-14 04:01:58,121 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335715523 with entries=133, filesize=92.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335717577
2014-07-14 04:01:58,122 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:01:59,779 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:01:59,877 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52677 synced till here 52667
2014-07-14 04:01:59,966 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335717577 with entries=118, filesize=82.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335719780
2014-07-14 04:01:59,966 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:02:00,802 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:02:00,871 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335719780 with entries=91, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335720802
2014-07-14 04:02:00,871 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:02:02,133 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:02:02,187 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52877 synced till here 52863
2014-07-14 04:02:02,256 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335720802 with entries=109, filesize=72.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335722134
2014-07-14 04:02:02,256 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:02:02,469 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90695ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 04:02:02,470 DEBUG [MemStoreFlusher.0] regionserver.HRegion: NOT flushing memstore for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., flushing=true, writesEnabled=true
2014-07-14 04:02:03,575 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:02:03,607 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52972 synced till here 52965
2014-07-14 04:02:03,669 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335722134 with entries=95, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335723575
2014-07-14 04:02:03,670 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:02:05,534 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:02:05,558 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335723575 with entries=92, filesize=61.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335725535
2014-07-14 04:02:05,558 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:02:08,595 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:02:09,531 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335725535 with entries=90, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335728596
2014-07-14 04:02:09,532 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:02:10,782 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:02:10,802 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53241 synced till here 53240
2014-07-14 04:02:10,821 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335728596 with entries=87, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335730782
2014-07-14 04:02:10,822 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:02:10,871 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 04:02:10,871 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. has too many store files, but is 906.8m vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 04:02:10,871 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. due to global heap pressure
2014-07-14 04:02:10,871 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 906.8m
2014-07-14 04:02:12,129 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 04:02:12,604 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:02:12,729 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53334 synced till here 53333
2014-07-14 04:02:12,744 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335730782 with entries=93, filesize=65.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335732605
2014-07-14 04:02:12,744 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:02:14,087 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,088 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,103 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,124 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,126 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,126 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,135 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,177 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,187 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,187 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,187 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,191 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,191 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,193 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,195 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,218 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,262 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,287 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,324 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,402 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,448 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,475 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,498 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,523 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,549 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,576 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,603 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,627 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,651 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:14,744 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:15,143 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13099, memsize=390.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/21b730d68c4149d7bdc018bc71359d94
2014-07-14 04:02:15,158 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:15,170 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/21b730d68c4149d7bdc018bc71359d94 as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/21b730d68c4149d7bdc018bc71359d94
2014-07-14 04:02:15,190 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/21b730d68c4149d7bdc018bc71359d94, entries=1423390, sequenceid=13099, filesize=101.4m
2014-07-14 04:02:15,190 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1010.9m/1059992800, currentsize=530.3m/556013840 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 26684ms, sequenceid=13099, compaction requested=true
2014-07-14 04:02:15,191 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:02:15,191 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 32 store files, 0 compacting, 32 eligible, 20 blocking
2014-07-14 04:02:15,191 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33ms
2014-07-14 04:02:15,191 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 32 files from compaction candidates
2014-07-14 04:02:15,191 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,191 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:02:15,191 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 447ms
2014-07-14 04:02:15,191 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:02:15,191 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,191 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 04:02:15,191 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 540ms
2014-07-14 04:02:15,192 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,192 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 565ms
2014-07-14 04:02:15,192 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,192 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 590ms
2014-07-14 04:02:15,192 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,193 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 617ms
2014-07-14 04:02:15,193 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,193 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 644ms
2014-07-14 04:02:15,193 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,195 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 672ms
2014-07-14 04:02:15,195 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,200 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 702ms
2014-07-14 04:02:15,201 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,201 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 726ms
2014-07-14 04:02:15,201 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,201 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 753ms
2014-07-14 04:02:15,201 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,203 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 801ms
2014-07-14 04:02:15,203 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,203 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 879ms
2014-07-14 04:02:15,203 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,203 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 916ms
2014-07-14 04:02:15,203 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,203 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 941ms
2014-07-14 04:02:15,204 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,204 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 986ms
2014-07-14 04:02:15,204 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,209 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1014ms
2014-07-14 04:02:15,209 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,209 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1016ms
2014-07-14 04:02:15,209 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,209 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1018ms
2014-07-14 04:02:15,209 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,209 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1018ms
2014-07-14 04:02:15,209 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,209 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1022ms
2014-07-14 04:02:15,210 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,210 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1023ms
2014-07-14 04:02:15,210 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,210 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1023ms
2014-07-14 04:02:15,210 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,213 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1036ms
2014-07-14 04:02:15,213 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,220 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1085ms
2014-07-14 04:02:15,220 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,229 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1103ms
2014-07-14 04:02:15,229 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,229 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1103ms
2014-07-14 04:02:15,229 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,229 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1105ms
2014-07-14 04:02:15,229 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,229 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1126ms
2014-07-14 04:02:15,229 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,229 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1142ms
2014-07-14 04:02:15,230 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,236 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1149ms
2014-07-14 04:02:15,236 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:15,315 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:02:15,339 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 04:02:15,339 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. has too many store files; delaying flush up to 90000ms
2014-07-14 04:02:15,340 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:02:15,340 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 32 store files, 0 compacting, 32 eligible, 20 blocking
2014-07-14 04:02:15,340 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 32 files from compaction candidates
2014-07-14 04:02:15,340 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:02:15,340 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:02:15,340 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 04:02:15,375 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53442 synced till here 53419
2014-07-14 04:02:15,586 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335732605 with entries=108, filesize=80.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335735315
2014-07-14 04:02:15,589 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:02:17,294 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:02:17,310 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53534 synced till here 53533
2014-07-14 04:02:17,327 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335735315 with entries=92, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335737295
2014-07-14 04:02:17,327 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=44, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:02:18,999 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:02:19,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53625 synced till here 53623
2014-07-14 04:02:19,036 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335737295 with entries=91, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335738999
2014-07-14 04:02:19,037 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=45, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:02:21,410 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:02:21,427 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53712 synced till here 53711
2014-07-14 04:02:21,446 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335738999 with entries=87, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335741410
2014-07-14 04:02:21,446 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=46, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:02:23,160 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:02:23,189 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335741410 with entries=89, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335743161
2014-07-14 04:02:23,189 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=47, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:02:25,314 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:02:25,345 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335743161 with entries=90, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335745315
2014-07-14 04:02:25,346 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=48, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:02:26,856 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:02:26,931 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53985 synced till here 53977
2014-07-14 04:02:27,015 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335745315 with entries=94, filesize=69.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335746856
2014-07-14 04:02:27,017 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=49, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:02:28,014 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90617ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 04:02:28,014 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 763.9m
2014-07-14 04:02:28,522 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:02:28,616 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 04:02:28,685 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54089 synced till here 54087
2014-07-14 04:02:28,713 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335746856 with entries=104, filesize=73.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335748523
2014-07-14 04:02:28,713 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=50, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:02:30,109 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:02:30,169 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54184 synced till here 54177
2014-07-14 04:02:30,236 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335748523 with entries=95, filesize=66.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335750109
2014-07-14 04:02:30,236 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=51, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:02:30,289 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:30,291 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:30,308 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:30,319 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:30,326 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:30,333 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:30,339 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:30,361 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:30,362 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:30,403 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:30,437 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:30,478 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:30,517 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:30,568 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:30,632 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:30,811 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:30,855 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:30,892 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:30,915 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:31,355 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:31,395 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:31,881 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:32,456 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:32,500 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:32,540 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:32,578 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:32,624 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:33,389 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:33,404 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:33,413 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:33,428 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:33,455 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:33,490 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:33,534 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:33,591 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:33,621 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:33,644 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:33,668 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:33,691 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:33,715 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:33,772 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:33,806 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:33,854 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:33,911 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:33,960 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:33,992 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:34,043 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:34,077 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:34,120 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:34,173 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:34,479 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13209, memsize=471.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/9d46553b693b4a59b75c3345058141a5
2014-07-14 04:02:34,494 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/9d46553b693b4a59b75c3345058141a5 as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/9d46553b693b4a59b75c3345058141a5
2014-07-14 04:02:34,507 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/9d46553b693b4a59b75c3345058141a5, entries=1717260, sequenceid=13209, filesize=122.4m
2014-07-14 04:02:34,507 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~909.9m/954109760, currentsize=368.7m/386639760 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 23636ms, sequenceid=13209, compaction requested=true
2014-07-14 04:02:34,508 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:02:34,508 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 32 store files, 0 compacting, 32 eligible, 20 blocking
2014-07-14 04:02:34,508 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 335ms
2014-07-14 04:02:34,508 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 32 files from compaction candidates
2014-07-14 04:02:34,508 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,508 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:02:34,508 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 388ms
2014-07-14 04:02:34,508 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:02:34,508 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,508 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 04:02:34,509 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 431ms
2014-07-14 04:02:34,509 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,509 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 466ms
2014-07-14 04:02:34,509 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,509 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 517ms
2014-07-14 04:02:34,509 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,509 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 549ms
2014-07-14 04:02:34,509 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,517 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 606ms
2014-07-14 04:02:34,517 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,518 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 664ms
2014-07-14 04:02:34,518 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,518 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 712ms
2014-07-14 04:02:34,518 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,519 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 747ms
2014-07-14 04:02:34,519 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,520 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 805ms
2014-07-14 04:02:34,520 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,521 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 829ms
2014-07-14 04:02:34,524 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,525 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 857ms
2014-07-14 04:02:34,525 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,525 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 881ms
2014-07-14 04:02:34,525 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,526 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 905ms
2014-07-14 04:02:34,526 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,526 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 935ms
2014-07-14 04:02:34,526 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,532 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 998ms
2014-07-14 04:02:34,532 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,532 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1042ms
2014-07-14 04:02:34,532 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,532 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1077ms
2014-07-14 04:02:34,532 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,533 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1105ms
2014-07-14 04:02:34,533 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,533 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1120ms
2014-07-14 04:02:34,533 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,536 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1132ms
2014-07-14 04:02:34,536 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,536 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1147ms
2014-07-14 04:02:34,536 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,543 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1919ms
2014-07-14 04:02:34,543 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,543 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1965ms
2014-07-14 04:02:34,544 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,544 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2004ms
2014-07-14 04:02:34,544 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,544 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2044ms
2014-07-14 04:02:34,544 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,547 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2091ms
2014-07-14 04:02:34,547 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,547 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2666ms
2014-07-14 04:02:34,547 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,547 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3152ms
2014-07-14 04:02:34,548 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,548 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3193ms
2014-07-14 04:02:34,548 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,553 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3638ms
2014-07-14 04:02:34,553 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,554 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3662ms
2014-07-14 04:02:34,554 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,554 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3699ms
2014-07-14 04:02:34,555 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,555 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3744ms
2014-07-14 04:02:34,555 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,557 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3925ms
2014-07-14 04:02:34,557 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,558 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3989ms
2014-07-14 04:02:34,558 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,558 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4041ms
2014-07-14 04:02:34,558 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,558 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4080ms
2014-07-14 04:02:34,558 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,559 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4121ms
2014-07-14 04:02:34,559 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,559 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4156ms
2014-07-14 04:02:34,559 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,560 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4198ms
2014-07-14 04:02:34,560 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,561 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4199ms
2014-07-14 04:02:34,561 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,561 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4222ms
2014-07-14 04:02:34,561 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,562 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4228ms
2014-07-14 04:02:34,562 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,562 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4236ms
2014-07-14 04:02:34,562 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,562 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4243ms
2014-07-14 04:02:34,562 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,562 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4254ms
2014-07-14 04:02:34,562 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,565 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4274ms
2014-07-14 04:02:34,565 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,566 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4277ms
2014-07-14 04:02:34,566 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:34,645 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 04:02:34,645 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. has too many store files; delaying flush up to 90000ms
2014-07-14 04:02:34,645 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 32 store files, 0 compacting, 32 eligible, 20 blocking
2014-07-14 04:02:34,646 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 32 files from compaction candidates
2014-07-14 04:02:34,646 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:02:34,646 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:02:34,646 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 04:02:34,646 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:02:36,508 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:02:36,530 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54306 synced till here 54266
2014-07-14 04:02:36,806 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335750109 with entries=122, filesize=98.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335756509
2014-07-14 04:02:36,807 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=52, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:02:38,873 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:02:38,934 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54427 synced till here 54418
2014-07-14 04:02:39,031 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335756509 with entries=121, filesize=73.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335758873
2014-07-14 04:02:39,031 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=53, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:02:40,996 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:02:41,009 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54544 synced till here 54513
2014-07-14 04:02:41,349 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335758873 with entries=117, filesize=97.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335760997
2014-07-14 04:02:41,349 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=54, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:02:43,607 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:02:44,919 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1012ms
GC pool 'ParNew' had collection(s): count=1 time=1115ms
2014-07-14 04:02:44,970 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54724 synced till here 54698
2014-07-14 04:02:45,213 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335760997 with entries=180, filesize=120.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335763608
2014-07-14 04:02:45,214 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=55, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:02:45,504 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 04:02:45,504 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. has too many store files, but is 1.4g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 04:02:45,504 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. due to global heap pressure
2014-07-14 04:02:45,504 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 1.4g
2014-07-14 04:02:47,417 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:02:47,463 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54850 synced till here 54831
2014-07-14 04:02:47,590 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335763608 with entries=126, filesize=92.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335767417
2014-07-14 04:02:47,591 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=56, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:02:47,985 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:47,986 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:47,986 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:47,992 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:47,992 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:47,994 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:47,995 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:47,995 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:47,996 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:47,997 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:47,998 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:47,999 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,000 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,001 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,001 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,003 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,006 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,006 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,009 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,012 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,012 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,168 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,170 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,170 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,171 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,171 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,171 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,172 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,173 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,173 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,174 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,176 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,177 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,177 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,177 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,180 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,181 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,261 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,262 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,262 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,262 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,263 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:48,263 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:49,311 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:49,312 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:49,312 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:49,312 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:49,313 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:49,314 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:49,315 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:02:49,572 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 04:02:52,439 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13594, memsize=387.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/7dba07cda1044ea595eddd8641ad829a
2014-07-14 04:02:52,455 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/7dba07cda1044ea595eddd8641ad829a as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/7dba07cda1044ea595eddd8641ad829a
2014-07-14 04:02:52,470 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/7dba07cda1044ea595eddd8641ad829a, entries=1411820, sequenceid=13594, filesize=100.5m
2014-07-14 04:02:52,470 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~763.9m/801027360, currentsize=375.9m/394112080 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 24456ms, sequenceid=13594, compaction requested=true
2014-07-14 04:02:52,471 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:02:52,471 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 33 store files, 0 compacting, 33 eligible, 20 blocking
2014-07-14 04:02:52,472 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 33 files from compaction candidates
2014-07-14 04:02:52,472 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3156ms
2014-07-14 04:02:52,472 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,472 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:02:52,472 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:02:52,472 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3158ms
2014-07-14 04:02:52,472 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 04:02:52,472 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,472 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3160ms
2014-07-14 04:02:52,472 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,473 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3161ms
2014-07-14 04:02:52,473 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,473 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3161ms
2014-07-14 04:02:52,473 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,473 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3161ms
2014-07-14 04:02:52,473 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,473 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3162ms
2014-07-14 04:02:52,473 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,473 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4210ms
2014-07-14 04:02:52,474 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,474 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4212ms
2014-07-14 04:02:52,474 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,481 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4219ms
2014-07-14 04:02:52,481 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,481 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4219ms
2014-07-14 04:02:52,481 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,485 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4224ms
2014-07-14 04:02:52,485 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,485 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4224ms
2014-07-14 04:02:52,485 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,485 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4304ms
2014-07-14 04:02:52,485 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,486 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4306ms
2014-07-14 04:02:52,486 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,486 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4309ms
2014-07-14 04:02:52,487 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,493 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4316ms
2014-07-14 04:02:52,493 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,493 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4317ms
2014-07-14 04:02:52,493 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,493 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4317ms
2014-07-14 04:02:52,493 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,493 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4319ms
2014-07-14 04:02:52,493 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,495 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4321ms
2014-07-14 04:02:52,495 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,495 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4322ms
2014-07-14 04:02:52,495 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,496 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4324ms
2014-07-14 04:02:52,496 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,497 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4326ms
2014-07-14 04:02:52,497 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,501 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4330ms
2014-07-14 04:02:52,501 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,501 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4330ms
2014-07-14 04:02:52,501 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,502 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4331ms
2014-07-14 04:02:52,502 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,502 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4332ms
2014-07-14 04:02:52,502 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,509 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4341ms
2014-07-14 04:02:52,510 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,517 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4505ms
2014-07-14 04:02:52,517 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,525 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4513ms
2014-07-14 04:02:52,525 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,533 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4524ms
2014-07-14 04:02:52,533 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,533 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4527ms
2014-07-14 04:02:52,533 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,546 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4539ms
2014-07-14 04:02:52,546 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,547 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4543ms
2014-07-14 04:02:52,547 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,548 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4549ms
2014-07-14 04:02:52,548 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,553 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4552ms
2014-07-14 04:02:52,553 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,561 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4561ms
2014-07-14 04:02:52,561 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,566 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4566ms
2014-07-14 04:02:52,566 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,568 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4570ms
2014-07-14 04:02:52,568 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,568 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4571ms
2014-07-14 04:02:52,568 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,573 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4574ms
2014-07-14 04:02:52,573 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,574 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4579ms
2014-07-14 04:02:52,574 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,578 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4579ms
2014-07-14 04:02:52,579 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,581 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4587ms
2014-07-14 04:02:52,582 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,582 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. has too many store files; delaying flush up to 90000ms
2014-07-14 04:02:52,582 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 33 store files, 0 compacting, 33 eligible, 20 blocking
2014-07-14 04:02:52,583 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 33 files from compaction candidates
2014-07-14 04:02:52,583 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:02:52,583 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:02:52,583 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 04:02:52,583 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:02:52,583 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 04:02:52,585 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4593ms
2014-07-14 04:02:52,585 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,589 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4597ms
2014-07-14 04:02:52,589 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,589 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4603ms
2014-07-14 04:02:52,589 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,593 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4607ms
2014-07-14 04:02:52,593 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:52,597 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4613ms
2014-07-14 04:02:52,597 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:02:54,193 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1381ms
GC pool 'ParNew' had collection(s): count=1 time=1412ms
2014-07-14 04:02:54,219 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:02:54,237 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54983 synced till here 54948
2014-07-14 04:02:54,506 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335767417 with entries=133, filesize=86.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335774220
2014-07-14 04:02:54,507 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=57, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:02:56,348 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:02:56,448 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55115 synced till here 55087
2014-07-14 04:02:56,654 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335774220 with entries=132, filesize=92.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335776348
2014-07-14 04:02:56,654 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=58, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:02:57,420 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:02:58,349 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55222 synced till here 55205
2014-07-14 04:02:58,582 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335776348 with entries=107, filesize=77.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335777421
2014-07-14 04:02:58,582 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=59, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:02:59,326 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:02:59,364 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55336 synced till here 55313
2014-07-14 04:02:59,589 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 04:02:59,589 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. has too many store files, but is 816.4m vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 04:02:59,589 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. due to global heap pressure
2014-07-14 04:02:59,589 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 816.4m
2014-07-14 04:02:59,760 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335777421 with entries=114, filesize=81.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335779327
2014-07-14 04:02:59,761 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=60, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:03:01,377 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:03:01,450 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55447 synced till here 55434
2014-07-14 04:03:01,839 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335779327 with entries=111, filesize=78.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335781377
2014-07-14 04:03:01,840 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=61, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:03:02,016 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 04:03:03,258 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1163ms
GC pool 'ParNew' had collection(s): count=1 time=1182ms
2014-07-14 04:03:03,452 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,454 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,455 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,456 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,456 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,456 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,460 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,463 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,466 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,467 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,467 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,467 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,467 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,468 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,469 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,481 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,482 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,538 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,544 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,546 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,547 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,601 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,602 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,603 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,606 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,607 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,678 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,679 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,679 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,679 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,682 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,682 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,683 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,683 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,685 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,685 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,686 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,688 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,736 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:03:03,738 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,738 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,738 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,739 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,740 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,740 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,741 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,741 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,741 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,741 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,741 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,742 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:03,768 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335781377 with entries=85, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335783736
2014-07-14 04:03:03,769 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=62, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:03:08,453 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:03:08,454 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:08,455 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:08,456 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:03:08,456 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:08,457 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:03:08,460 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:08,463 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:08,467 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:03:08,467 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:08,467 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5012ms
2014-07-14 04:03:08,468 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:03:08,468 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-14 04:03:08,468 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5012ms
2014-07-14 04:03:08,470 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:03:08,481 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:08,482 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:08,539 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:03:08,544 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:08,547 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:03:08,547 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:08,601 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:08,602 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:08,603 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:08,607 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:03:08,608 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:08,679 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:03:08,679 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:03:08,679 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:03:08,680 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:03:08,682 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:08,682 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:08,684 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:08,684 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:03:08,685 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:03:08,685 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:08,686 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:08,688 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:03:08,738 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:08,739 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:08,739 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:03:08,740 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:08,740 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:08,741 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:08,741 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5003ms
2014-07-14 04:03:08,741 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5003ms
2014-07-14 04:03:08,742 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5004ms
2014-07-14 04:03:08,742 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:03:08,742 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:03:08,743 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-14 04:03:13,453 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:03:13,455 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:03:13,456 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:03:13,456 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:03:13,457 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:03:13,457 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:03:13,460 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:03:13,463 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:03:13,467 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:03:13,468 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:03:13,469 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 04:03:13,469 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 04:03:13,469 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10014ms
2014-07-14 04:03:13,470 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10014ms
2014-07-14 04:03:13,471 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:03:13,482 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:03:13,483 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:03:13,539 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:03:13,545 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:03:13,547 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:03:13,548 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:03:13,602 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:03:13,602 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:03:13,603 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:03:13,607 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:03:13,608 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:03:13,679 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:03:13,679 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:03:13,680 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 04:03:13,681 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:03:13,682 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:03:13,683 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:03:13,684 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:03:13,684 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:03:13,685 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:03:13,685 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:03:13,687 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:03:13,689 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:03:13,739 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:03:13,739 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:03:13,740 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:03:13,740 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:03:13,741 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:03:13,742 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:03:13,742 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10004ms
2014-07-14 04:03:13,743 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10005ms
2014-07-14 04:03:13,744 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10005ms
2014-07-14 04:03:13,744 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-14 04:03:13,745 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10004ms
2014-07-14 04:03:13,745 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10004ms
2014-07-14 04:03:18,548 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 04:03:18,549 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15083ms
2014-07-14 04:03:18,550 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15094ms
2014-07-14 04:03:18,550 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15083ms
2014-07-14 04:03:18,550 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15084ms
2014-07-14 04:03:18,550 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15083ms
2014-07-14 04:03:18,551 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15096ms
2014-07-14 04:03:18,551 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15095ms
2014-07-14 04:03:18,551 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15082ms
2014-07-14 04:03:18,552 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15071ms
2014-07-14 04:03:18,552 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15070ms
2014-07-14 04:03:18,552 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15005ms
2014-07-14 04:03:18,553 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15014ms
2014-07-14 04:03:18,554 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15009ms
2014-07-14 04:03:18,554 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15102ms
2014-07-14 04:03:18,554 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15100ms
2014-07-14 04:03:18,554 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15099ms
2014-07-14 04:03:18,554 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15098ms
2014-07-14 04:03:18,555 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15099ms
2014-07-14 04:03:18,555 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15095ms
2014-07-14 04:03:18,555 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15092ms
2014-07-14 04:03:18,602 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 04:03:18,603 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 04:03:18,603 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 04:03:18,607 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 04:03:18,609 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 04:03:18,679 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 04:03:18,680 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 04:03:18,681 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15003ms
2014-07-14 04:03:18,681 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 04:03:18,683 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 04:03:18,683 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 04:03:18,685 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 04:03:18,685 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 04:03:18,685 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 04:03:18,686 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 04:03:18,687 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 04:03:18,689 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 04:03:18,739 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 04:03:18,740 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 04:03:18,740 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 04:03:18,742 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 04:03:18,743 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15003ms
2014-07-14 04:03:18,743 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15003ms
2014-07-14 04:03:18,743 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15005ms
2014-07-14 04:03:18,743 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15005ms
2014-07-14 04:03:18,744 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15006ms
2014-07-14 04:03:18,745 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15004ms
2014-07-14 04:03:18,745 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15004ms
2014-07-14 04:03:18,746 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15005ms
2014-07-14 04:03:18,893 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13747, memsize=392.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/6bec7d862171443e8773a057425f3fdb
2014-07-14 04:03:18,906 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/6bec7d862171443e8773a057425f3fdb as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/6bec7d862171443e8773a057425f3fdb
2014-07-14 04:03:18,915 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/6bec7d862171443e8773a057425f3fdb, entries=1427270, sequenceid=13747, filesize=101.6m
2014-07-14 04:03:18,916 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~833.5m/873965520, currentsize=62.1m/65138960 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 19327ms, sequenceid=13747, compaction requested=true
2014-07-14 04:03:18,916 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:03:18,917 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 33 store files, 0 compacting, 33 eligible, 20 blocking
2014-07-14 04:03:18,917 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 33 files from compaction candidates
2014-07-14 04:03:18,917 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15176ms
2014-07-14 04:03:18,917 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:03:18,917 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,917 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:03:18,917 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15176ms
2014-07-14 04:03:18,917 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,917 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 04:03:18,917 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15176ms
2014-07-14 04:03:18,917 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,918 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15179ms
2014-07-14 04:03:18,918 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,921 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15183ms
2014-07-14 04:03:18,921 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,921 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15183ms
2014-07-14 04:03:18,921 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,923 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15183ms
2014-07-14 04:03:18,923 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,923 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15183ms
2014-07-14 04:03:18,923 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,923 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15184ms
2014-07-14 04:03:18,924 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,924 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15186ms
2014-07-14 04:03:18,925 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,928 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15190ms
2014-07-14 04:03:18,928 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,928 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15190ms
2014-07-14 04:03:18,928 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,931 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15241ms
2014-07-14 04:03:18,931 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,931 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15245ms
2014-07-14 04:03:18,931 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,931 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15246ms
2014-07-14 04:03:18,931 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,931 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15247ms
2014-07-14 04:03:18,932 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,933 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15249ms
2014-07-14 04:03:18,933 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,933 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15250ms
2014-07-14 04:03:18,933 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,936 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15254ms
2014-07-14 04:03:18,936 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,936 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15254ms
2014-07-14 04:03:18,937 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,937 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15258ms
2014-07-14 04:03:18,937 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,938 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15259ms
2014-07-14 04:03:18,938 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,938 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15260ms
2014-07-14 04:03:18,938 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,945 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15267ms
2014-07-14 04:03:18,945 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,945 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15338ms
2014-07-14 04:03:18,945 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,953 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15347ms
2014-07-14 04:03:18,953 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,953 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15350ms
2014-07-14 04:03:18,953 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,953 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15351ms
2014-07-14 04:03:18,953 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,953 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15352ms
2014-07-14 04:03:18,954 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,957 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15494ms
2014-07-14 04:03:18,957 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,958 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15497ms
2014-07-14 04:03:18,958 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,959 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15502ms
2014-07-14 04:03:18,959 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,959 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15503ms
2014-07-14 04:03:18,959 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,959 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15504ms
2014-07-14 04:03:18,960 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,960 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15506ms
2014-07-14 04:03:18,960 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,960 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15508ms
2014-07-14 04:03:18,960 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,963 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15419ms
2014-07-14 04:03:18,963 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,965 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15427ms
2014-07-14 04:03:18,965 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,966 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15418ms
2014-07-14 04:03:18,966 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,969 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15487ms
2014-07-14 04:03:18,969 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,969 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15488ms
2014-07-14 04:03:18,969 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,969 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15500ms
2014-07-14 04:03:18,969 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:18,973 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15517ms
2014-07-14 04:03:18,973 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:19,022 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15518ms
2014-07-14 04:03:19,023 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:19,023 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15556ms
2014-07-14 04:03:19,023 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:19,037 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15571ms
2014-07-14 04:03:19,038 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:19,038 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15571ms
2014-07-14 04:03:19,039 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:19,045 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15590ms
2014-07-14 04:03:19,045 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:19,046 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15580ms
2014-07-14 04:03:19,046 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:19,046 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15500ms
2014-07-14 04:03:19,046 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:03:19,106 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17904,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335781202,"queuetimems":6646,"class":"HRegionServer","responsesize":13067,"method":"Multi"}
2014-07-14 04:03:19,113 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18120,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335780993,"queuetimems":6783,"class":"HRegionServer","responsesize":12961,"method":"Multi"}
2014-07-14 04:03:19,121 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19776,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335779345,"queuetimems":6951,"class":"HRegionServer","responsesize":12960,"method":"Multi"}
2014-07-14 04:03:19,121 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17919,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335781202,"queuetimems":6590,"class":"HRegionServer","responsesize":13031,"method":"Multi"}
2014-07-14 04:03:19,122 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18132,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335780989,"queuetimems":6788,"class":"HRegionServer","responsesize":13195,"method":"Multi"}
2014-07-14 04:03:19,122 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17920,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335781202,"queuetimems":6741,"class":"HRegionServer","responsesize":13062,"method":"Multi"}
2014-07-14 04:03:19,122 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18117,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335781005,"queuetimems":6605,"class":"HRegionServer","responsesize":13139,"method":"Multi"}
2014-07-14 04:03:19,121 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18147,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335780974,"queuetimems":8497,"class":"HRegionServer","responsesize":12913,"method":"Multi"}
2014-07-14 04:03:19,122 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18121,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335781001,"queuetimems":6628,"class":"HRegionServer","responsesize":12822,"method":"Multi"}
2014-07-14 04:03:19,122 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18149,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335780973,"queuetimems":8523,"class":"HRegionServer","responsesize":12970,"method":"Multi"}
2014-07-14 04:03:19,122 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18127,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335780994,"queuetimems":6660,"class":"HRegionServer","responsesize":13001,"method":"Multi"}
2014-07-14 04:03:19,380 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13772, memsize=560.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/41eb77fb6e674228ac6747ae814f77ee
2014-07-14 04:03:19,387 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20049,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335779337,"queuetimems":6970,"class":"HRegionServer","responsesize":13241,"method":"Multi"}
2014-07-14 04:03:19,387 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20054,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335779333,"queuetimems":7022,"class":"HRegionServer","responsesize":13021,"method":"Multi"}
2014-07-14 04:03:19,387 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20057,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335779330,"queuetimems":7946,"class":"HRegionServer","responsesize":13068,"method":"Multi"}
2014-07-14 04:03:19,393 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20057,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335779336,"queuetimems":6996,"class":"HRegionServer","responsesize":12884,"method":"Multi"}
2014-07-14 04:03:19,418 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/41eb77fb6e674228ac6747ae814f77ee as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/41eb77fb6e674228ac6747ae814f77ee
2014-07-14 04:03:19,430 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/41eb77fb6e674228ac6747ae814f77ee, entries=2041610, sequenceid=13772, filesize=145.4m
2014-07-14 04:03:19,430 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.4g/1540597360, currentsize=308.0m/322944000 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 33926ms, sequenceid=13772, compaction requested=true
2014-07-14 04:03:19,431 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:03:19,431 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 37 store files, 0 compacting, 37 eligible, 20 blocking
2014-07-14 04:03:19,431 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 37 files from compaction candidates
2014-07-14 04:03:19,431 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:03:19,431 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:03:19,432 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 04:03:19,589 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 04:03:19,589 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. has too many store files; delaying flush up to 90000ms
2014-07-14 04:03:19,590 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:03:19,590 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 37 store files, 0 compacting, 37 eligible, 20 blocking
2014-07-14 04:03:19,591 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 37 files from compaction candidates
2014-07-14 04:03:19,591 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:03:19,591 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:03:19,591 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 04:03:19,717 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18304,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335781411,"queuetimems":6261,"class":"HRegionServer","responsesize":13397,"method":"Multi"}
2014-07-14 04:03:19,724 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18330,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335781394,"queuetimems":6489,"class":"HRegionServer","responsesize":12746,"method":"Multi"}
2014-07-14 04:03:19,724 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18298,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335781426,"queuetimems":5153,"class":"HRegionServer","responsesize":12961,"method":"Multi"}
2014-07-14 04:03:19,724 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18334,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335781390,"queuetimems":6561,"class":"HRegionServer","responsesize":13274,"method":"Multi"}
2014-07-14 04:03:19,795 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18409,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335781385,"queuetimems":6677,"class":"HRegionServer","responsesize":13198,"method":"Multi"}
2014-07-14 04:03:19,801 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18598,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335781203,"queuetimems":6559,"class":"HRegionServer","responsesize":13159,"method":"Multi"}
2014-07-14 04:03:19,809 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18164,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335781645,"queuetimems":5347,"class":"HRegionServer","responsesize":13227,"method":"Multi"}
2014-07-14 04:03:19,809 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18604,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335781205,"queuetimems":6524,"class":"HRegionServer","responsesize":12803,"method":"Multi"}
2014-07-14 04:03:19,897 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:03:19,898 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18495,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335781402,"queuetimems":6369,"class":"HRegionServer","responsesize":13150,"method":"Multi"}
2014-07-14 04:03:19,898 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18500,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335781397,"queuetimems":6400,"class":"HRegionServer","responsesize":13071,"method":"Multi"}
2014-07-14 04:03:19,898 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16457,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335783441,"queuetimems":6928,"class":"HRegionServer","responsesize":12988,"method":"Multi"}
2014-07-14 04:03:19,898 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18488,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335781410,"queuetimems":6285,"class":"HRegionServer","responsesize":13064,"method":"Multi"}
2014-07-14 04:03:19,899 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18495,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335781403,"queuetimems":6348,"class":"HRegionServer","responsesize":12856,"method":"Multi"}
2014-07-14 04:03:19,899 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16482,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335783416,"queuetimems":6973,"class":"HRegionServer","responsesize":13199,"method":"Multi"}
2014-07-14 04:03:19,899 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18484,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335781413,"queuetimems":6222,"class":"HRegionServer","responsesize":12801,"method":"Multi"}
2014-07-14 04:03:19,899 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16483,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335783416,"queuetimems":7069,"class":"HRegionServer","responsesize":13068,"method":"Multi"}
2014-07-14 04:03:19,898 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18696,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335781201,"queuetimems":6767,"class":"HRegionServer","responsesize":12881,"method":"Multi"}
2014-07-14 04:03:19,900 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16483,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335783416,"queuetimems":7092,"class":"HRegionServer","responsesize":13163,"method":"Multi"}
2014-07-14 04:03:19,898 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18512,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335781385,"queuetimems":6633,"class":"HRegionServer","responsesize":12980,"method":"Multi"}
2014-07-14 04:03:19,912 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18526,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335781386,"queuetimems":6602,"class":"HRegionServer","responsesize":12972,"method":"Multi"}
2014-07-14 04:03:19,912 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18495,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335781417,"queuetimems":6193,"class":"HRegionServer","responsesize":13148,"method":"Multi"}
2014-07-14 04:03:19,912 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16470,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335783442,"queuetimems":6905,"class":"HRegionServer","responsesize":13115,"method":"Multi"}
2014-07-14 04:03:19,912 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16482,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335783430,"queuetimems":6946,"class":"HRegionServer","responsesize":13119,"method":"Multi"}
2014-07-14 04:03:19,913 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18515,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335781397,"queuetimems":6442,"class":"HRegionServer","responsesize":12853,"method":"Multi"}
2014-07-14 04:03:19,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55653 synced till here 55638
2014-07-14 04:03:20,105 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335783736 with entries=121, filesize=80.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335799897
2014-07-14 04:03:20,106 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=63, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:03:21,645 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:03:21,647 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17967,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335783679,"queuetimems":6804,"class":"HRegionServer","responsesize":13177,"method":"Multi"}
2014-07-14 04:03:21,647 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18183,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335783463,"queuetimems":6872,"class":"HRegionServer","responsesize":12919,"method":"Multi"}
2014-07-14 04:03:21,647 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18042,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335783604,"queuetimems":6756,"class":"HRegionServer","responsesize":12884,"method":"Multi"}
2014-07-14 04:03:21,687 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55766 synced till here 55735
2014-07-14 04:03:21,692 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18229,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335783463,"queuetimems":6840,"class":"HRegionServer","responsesize":12818,"method":"Multi"}
2014-07-14 04:03:21,704 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335799897 with entries=113, filesize=80.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335801645
2014-07-14 04:03:21,705 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=64, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:03:21,726 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18122,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335783603,"queuetimems":6787,"class":"HRegionServer","responsesize":12843,"method":"Multi"}
2014-07-14 04:03:21,737 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18258,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335783479,"queuetimems":6791,"class":"HRegionServer","responsesize":13198,"method":"Multi"}
2014-07-14 04:03:21,745 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18148,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335783597,"queuetimems":6835,"class":"HRegionServer","responsesize":12922,"method":"Multi"}
2014-07-14 04:03:21,757 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18160,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335783597,"queuetimems":6806,"class":"HRegionServer","responsesize":12706,"method":"Multi"}
2014-07-14 04:03:21,770 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18228,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335783542,"queuetimems":6829,"class":"HRegionServer","responsesize":13133,"method":"Multi"}
2014-07-14 04:03:21,770 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18306,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335783464,"queuetimems":6807,"class":"HRegionServer","responsesize":13185,"method":"Multi"}
2014-07-14 04:03:21,789 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18192,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335783597,"queuetimems":6859,"class":"HRegionServer","responsesize":13157,"method":"Multi"}
2014-07-14 04:03:23,018 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:03:23,078 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55884 synced till here 55857
2014-07-14 04:03:23,312 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335801645 with entries=118, filesize=87.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335803018
2014-07-14 04:03:23,315 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=65, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:03:23,963 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:03:23,999 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55995 synced till here 55980
2014-07-14 04:03:24,557 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335803018 with entries=111, filesize=72.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335803964
2014-07-14 04:03:24,558 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=66, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:03:26,118 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 04:03:26,118 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. has too many store files; delaying flush up to 90000ms
2014-07-14 04:03:26,119 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:03:26,119 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 33 store files, 0 compacting, 33 eligible, 20 blocking
2014-07-14 04:03:26,119 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 33 files from compaction candidates
2014-07-14 04:03:26,120 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:03:26,120 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:03:26,120 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 04:03:26,361 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:03:26,385 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56088 synced till here 56086
2014-07-14 04:03:26,416 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335803964 with entries=93, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335806361
2014-07-14 04:03:26,417 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=67, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:03:28,047 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:03:28,065 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56179 synced till here 56176
2014-07-14 04:03:28,155 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335806361 with entries=91, filesize=65.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335808048
2014-07-14 04:03:28,156 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=68, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:03:30,163 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:03:30,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56277 synced till here 56276
2014-07-14 04:03:30,453 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335808048 with entries=98, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335810163
2014-07-14 04:03:30,453 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=69, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:03:32,972 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:03:33,309 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56379 synced till here 56378
2014-07-14 04:03:33,335 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335810163 with entries=102, filesize=72.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335812973
2014-07-14 04:03:33,336 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=70, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:03:35,533 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:03:35,566 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335812973 with entries=92, filesize=61.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335815533
2014-07-14 04:03:35,567 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=71, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:03:37,417 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:03:37,595 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56578 synced till here 56570
2014-07-14 04:03:37,653 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335815533 with entries=107, filesize=78.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335817417
2014-07-14 04:03:37,659 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=72, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:03:38,968 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:03:39,030 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56677 synced till here 56670
2014-07-14 04:03:39,105 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335817417 with entries=99, filesize=68.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335818969
2014-07-14 04:03:39,107 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=73, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:03:40,451 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:03:40,849 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335818969 with entries=89, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335820452
2014-07-14 04:03:40,849 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=74, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:03:42,366 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:03:42,393 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335820452 with entries=87, filesize=61.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335822367
2014-07-14 04:03:42,394 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=75, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:03:44,384 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:03:44,410 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56948 synced till here 56946
2014-07-14 04:03:44,491 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335822367 with entries=95, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335824385
2014-07-14 04:03:44,491 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=76, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:03:46,996 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:03:47,016 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57041 synced till here 57039
2014-07-14 04:03:47,036 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335824385 with entries=93, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335826996
2014-07-14 04:03:47,037 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=77, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:03:47,998 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:03:48,587 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57137 synced till here 57133
2014-07-14 04:03:48,619 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335826996 with entries=96, filesize=66.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335827998
2014-07-14 04:03:48,626 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=78, maxlogs=32; forcing flush of 1 regions(s): 331f54c5b3020b63e58b94456232194b
2014-07-14 04:03:49,416 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 04:03:49,416 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. has too many store files, but is 1.2g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 04:03:49,416 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. due to global heap pressure
2014-07-14 04:03:49,416 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 1.2g
2014-07-14 04:03:49,561 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 04:03:49,562 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. has too many store files, but is 1.0g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 04:03:49,562 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. due to global heap pressure
2014-07-14 04:03:49,562 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b., current region memstore size 1.0g
2014-07-14 04:03:50,295 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 04:03:50,679 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 04:03:50,866 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:03:50,897 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335827998 with entries=90, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335830867
2014-07-14 04:03:52,494 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:03:52,513 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:52,518 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335830867 with entries=87, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335832495
2014-07-14 04:03:52,535 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:52,543 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:52,576 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:52,581 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:52,680 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:52,883 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:52,924 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:53,043 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:53,232 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:54,119 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:54,289 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:54,331 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:54,489 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:54,526 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:54,633 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:54,698 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:54,822 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:54,868 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:55,642 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:55,648 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:55,658 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:55,674 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:55,703 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:55,729 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:55,755 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:55,783 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:55,835 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:56,013 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:56,060 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:56,102 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:56,281 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:56,609 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:56,662 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:56,703 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:56,733 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:56,761 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:56,802 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:56,834 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:56,864 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:56,895 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:56,929 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:56,974 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:57,002 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:57,086 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:57,514 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:57,533 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:57,536 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:03:57,544 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:57,576 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:03:57,581 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:57,681 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:03:57,883 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:57,925 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:03:57,951 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:58,044 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:58,232 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:58,754 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:58,784 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:58,817 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:03:59,120 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:03:59,289 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:59,331 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:03:59,489 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:59,526 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:59,633 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:59,698 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:59,822 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:03:59,868 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:04:00,643 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:04:00,649 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:04:00,658 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:04:00,674 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:04:00,703 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:04:00,729 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:04:00,755 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:04:00,783 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:04:00,835 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:04:02,033 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5030ms
2014-07-14 04:04:02,033 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5272ms
2014-07-14 04:04:02,034 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5932ms
2014-07-14 04:04:02,034 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5753ms
2014-07-14 04:04:02,034 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5425ms
2014-07-14 04:04:02,034 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5372ms
2014-07-14 04:04:02,034 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5301ms
2014-07-14 04:04:02,036 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5332ms
2014-07-14 04:04:02,036 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5234ms
2014-07-14 04:04:02,036 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5202ms
2014-07-14 04:04:02,036 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5062ms
2014-07-14 04:04:02,037 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5173ms
2014-07-14 04:04:02,037 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5108ms
2014-07-14 04:04:02,038 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5143ms
2014-07-14 04:04:02,038 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5978ms
2014-07-14 04:04:02,039 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6025ms
2014-07-14 04:04:02,087 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:04:02,514 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:04:02,533 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:04:02,536 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:04:02,544 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:04:02,576 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:04:02,581 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:04:02,681 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:04:02,883 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:04:02,925 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:04:02,951 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:04:03,044 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:04:03,232 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:04:03,754 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:04:03,784 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:04:03,817 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:04:04,120 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:04:04,289 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:04:04,331 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:04:04,489 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:04:04,526 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:04:04,633 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:04:04,698 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:04:04,823 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:04:04,868 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:04:05,644 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:04:05,649 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:04:05,658 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:04:05,675 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:04:05,703 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:04:05,730 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:04:05,756 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:04:05,784 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:04:05,836 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:04:07,033 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10031ms
2014-07-14 04:04:07,034 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10932ms
2014-07-14 04:04:07,035 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10372ms
2014-07-14 04:04:07,035 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10274ms
2014-07-14 04:04:07,035 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10302ms
2014-07-14 04:04:07,035 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10426ms
2014-07-14 04:04:07,035 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10754ms
2014-07-14 04:04:07,036 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10333ms
2014-07-14 04:04:07,037 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10234ms
2014-07-14 04:04:07,037 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10203ms
2014-07-14 04:04:07,038 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10173ms
2014-07-14 04:04:07,038 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10064ms
2014-07-14 04:04:07,039 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10143ms
2014-07-14 04:04:07,039 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10110ms
2014-07-14 04:04:07,039 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10979ms
2014-07-14 04:04:07,039 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11026ms
2014-07-14 04:04:07,087 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:04:07,111 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14348, memsize=350.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/c2fd68cc8ee24c809c88504551096ce8
2014-07-14 04:04:07,133 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/.tmp/c2fd68cc8ee24c809c88504551096ce8 as hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/c2fd68cc8ee24c809c88504551096ce8
2014-07-14 04:04:07,147 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/331f54c5b3020b63e58b94456232194b/family/c2fd68cc8ee24c809c88504551096ce8, entries=1276640, sequenceid=14348, filesize=90.9m
2014-07-14 04:04:07,147 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.0g/1084406240, currentsize=17.6m/18433280 for region usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. in 17585ms, sequenceid=14348, compaction requested=true
2014-07-14 04:04:07,148 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:04:07,148 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 20 blocking
2014-07-14 04:04:07,148 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-14 04:04:07,148 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10062ms
2014-07-14 04:04:07,148 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:04:07,148 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,148 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:04:07,148 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 92503ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 04:04:07,148 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. because compaction request was cancelled
2014-07-14 04:04:07,148 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11135ms
2014-07-14 04:04:07,148 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,149 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 738.8m
2014-07-14 04:04:07,149 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11089ms
2014-07-14 04:04:07,149 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,149 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10220ms
2014-07-14 04:04:07,149 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,149 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10254ms
2014-07-14 04:04:07,149 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,149 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10175ms
2014-07-14 04:04:07,149 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,149 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10285ms
2014-07-14 04:04:07,149 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,157 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10323ms
2014-07-14 04:04:07,157 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,165 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10363ms
2014-07-14 04:04:07,165 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,165 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10462ms
2014-07-14 04:04:07,165 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,165 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10884ms
2014-07-14 04:04:07,165 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,165 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10556ms
2014-07-14 04:04:07,166 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,166 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10433ms
2014-07-14 04:04:07,166 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,166 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10405ms
2014-07-14 04:04:07,166 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,166 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10504ms
2014-07-14 04:04:07,166 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,169 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11067ms
2014-07-14 04:04:07,169 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,177 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10175ms
2014-07-14 04:04:07,178 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,178 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11343ms
2014-07-14 04:04:07,178 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,178 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11395ms
2014-07-14 04:04:07,178 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,178 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11423ms
2014-07-14 04:04:07,178 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,178 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11449ms
2014-07-14 04:04:07,178 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,178 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11476ms
2014-07-14 04:04:07,179 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,184 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11510ms
2014-07-14 04:04:07,184 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,184 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11526ms
2014-07-14 04:04:07,184 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,184 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11536ms
2014-07-14 04:04:07,185 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,185 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11543ms
2014-07-14 04:04:07,185 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,186 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12317ms
2014-07-14 04:04:07,186 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,186 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12364ms
2014-07-14 04:04:07,186 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,192 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12494ms
2014-07-14 04:04:07,192 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,193 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12560ms
2014-07-14 04:04:07,193 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,193 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12667ms
2014-07-14 04:04:07,194 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,194 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12705ms
2014-07-14 04:04:07,194 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,201 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12871ms
2014-07-14 04:04:07,202 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,202 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12913ms
2014-07-14 04:04:07,202 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,202 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13083ms
2014-07-14 04:04:07,202 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,202 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8385ms
2014-07-14 04:04:07,202 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,202 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8419ms
2014-07-14 04:04:07,203 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,203 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8449ms
2014-07-14 04:04:07,203 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,203 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13971ms
2014-07-14 04:04:07,203 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,204 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14160ms
2014-07-14 04:04:07,204 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,204 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9253ms
2014-07-14 04:04:07,204 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,204 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14280ms
2014-07-14 04:04:07,204 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,205 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14321ms
2014-07-14 04:04:07,205 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,205 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14525ms
2014-07-14 04:04:07,205 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,205 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14624ms
2014-07-14 04:04:07,205 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,206 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14631ms
2014-07-14 04:04:07,206 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,213 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14670ms
2014-07-14 04:04:07,213 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,215 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14680ms
2014-07-14 04:04:07,216 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,217 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9683ms
2014-07-14 04:04:07,217 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,225 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14712ms
2014-07-14 04:04:07,225 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:04:07,235 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14888,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335832347,"queuetimems":0,"class":"HRegionServer","responsesize":12986,"method":"Multi"}
2014-07-14 04:04:07,466 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15007,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335832458,"queuetimems":0,"class":"HRegionServer","responsesize":13159,"method":"Multi"}
2014-07-14 04:04:08,664 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1127ms
GC pool 'ParNew' had collection(s): count=1 time=1092ms
2014-07-14 04:04:08,711 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16316,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335832395,"queuetimems":0,"class":"HRegionServer","responsesize":12731,"method":"Multi"}
2014-07-14 04:04:08,804 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16316,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335832487,"queuetimems":1,"class":"HRegionServer","responsesize":13366,"method":"Multi"}
2014-07-14 04:04:09,131 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 04:04:09,957 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:04:09,979 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12896,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335837082,"queuetimems":0,"class":"HRegionServer","responsesize":13004,"method":"Multi"}
2014-07-14 04:04:10,107 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57435 synced till here 57434
2014-07-14 04:04:10,110 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14100,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335836010,"queuetimems":0,"class":"HRegionServer","responsesize":12996,"method":"Multi"}
2014-07-14 04:04:10,150 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335832495 with entries=121, filesize=94.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335849958
2014-07-14 04:04:10,150 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335621941
2014-07-14 04:04:10,150 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335624136
2014-07-14 04:04:10,150 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335626239
2014-07-14 04:04:10,150 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335629366
2014-07-14 04:04:10,150 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335631773
2014-07-14 04:04:10,150 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335634046
2014-07-14 04:04:10,150 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335636327
2014-07-14 04:04:10,150 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335639164
2014-07-14 04:04:10,150 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335641576
2014-07-14 04:04:10,150 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335643764
2014-07-14 04:04:10,150 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335645772
2014-07-14 04:04:10,150 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335647688
2014-07-14 04:04:10,150 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335649972
2014-07-14 04:04:10,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335652414
2014-07-14 04:04:10,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335653973
2014-07-14 04:04:10,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335657506
2014-07-14 04:04:10,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335660339
2014-07-14 04:04:10,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335661883
2014-07-14 04:04:10,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335663957
2014-07-14 04:04:10,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335665872
2014-07-14 04:04:10,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335667688
2014-07-14 04:04:10,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335669880
2014-07-14 04:04:10,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335671776
2014-07-14 04:04:10,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335673765
2014-07-14 04:04:10,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335675657
2014-07-14 04:04:10,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335694245
2014-07-14 04:04:10,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335698216
2014-07-14 04:04:10,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335700679
2014-07-14 04:04:10,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335703906
2014-07-14 04:04:10,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335706568
2014-07-14 04:04:10,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335708174
2014-07-14 04:04:10,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335710645
2014-07-14 04:04:10,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335714009
2014-07-14 04:04:10,152 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335715523
2014-07-14 04:04:10,152 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335717577
2014-07-14 04:04:10,152 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335719780
2014-07-14 04:04:10,152 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335720802
2014-07-14 04:04:10,152 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335722134
2014-07-14 04:04:10,152 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335723575
2014-07-14 04:04:10,152 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335725535
2014-07-14 04:04:10,152 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335728596
2014-07-14 04:04:10,152 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335730782
2014-07-14 04:04:10,152 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335732605
2014-07-14 04:04:10,156 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335735315
2014-07-14 04:04:10,156 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335737295
2014-07-14 04:04:10,156 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335738999
2014-07-14 04:04:10,156 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335741410
2014-07-14 04:04:10,156 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335743161
2014-07-14 04:04:10,156 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335745315
2014-07-14 04:04:11,178 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14347,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335836830,"queuetimems":0,"class":"HRegionServer","responsesize":12809,"method":"Multi"}
2014-07-14 04:04:11,764 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14837,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335836926,"queuetimems":1,"class":"HRegionServer","responsesize":13024,"method":"Multi"}
2014-07-14 04:04:11,764 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16946,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335834817,"queuetimems":0,"class":"HRegionServer","responsesize":13366,"method":"Multi"}
2014-07-14 04:04:11,764 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15034,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335836729,"queuetimems":1,"class":"HRegionServer","responsesize":12739,"method":"Multi"}
2014-07-14 04:04:11,765 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16064,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335835700,"queuetimems":0,"class":"HRegionServer","responsesize":12946,"method":"Multi"}
2014-07-14 04:04:11,765 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17137,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335834627,"queuetimems":0,"class":"HRegionServer","responsesize":12918,"method":"Multi"}
2014-07-14 04:04:11,765 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15706,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335836058,"queuetimems":0,"class":"HRegionServer","responsesize":12980,"method":"Multi"}
2014-07-14 04:04:11,767 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17077,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335834690,"queuetimems":0,"class":"HRegionServer","responsesize":12986,"method":"Multi"}
2014-07-14 04:04:11,767 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12955,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335838812,"queuetimems":0,"class":"HRegionServer","responsesize":12996,"method":"Multi"}
2014-07-14 04:04:11,768 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16128,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335835639,"queuetimems":0,"class":"HRegionServer","responsesize":12731,"method":"Multi"}
2014-07-14 04:04:11,767 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19089,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335832678,"queuetimems":0,"class":"HRegionServer","responsesize":13119,"method":"Multi"}
2014-07-14 04:04:11,781 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18552,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335833229,"queuetimems":1,"class":"HRegionServer","responsesize":13212,"method":"Multi"}
2014-07-14 04:04:11,782 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16126,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335835655,"queuetimems":0,"class":"HRegionServer","responsesize":13082,"method":"Multi"}
2014-07-14 04:04:11,791 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17505,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335834286,"queuetimems":0,"class":"HRegionServer","responsesize":12739,"method":"Multi"}
2014-07-14 04:04:11,788 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13840,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335837948,"queuetimems":1,"class":"HRegionServer","responsesize":13147,"method":"Multi"}
2014-07-14 04:04:11,787 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15957,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335835830,"queuetimems":0,"class":"HRegionServer","responsesize":12615,"method":"Multi"}
2014-07-14 04:04:11,787 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15509,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335836278,"queuetimems":1,"class":"HRegionServer","responsesize":13046,"method":"Multi"}
2014-07-14 04:04:11,787 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17670,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335834117,"queuetimems":0,"class":"HRegionServer","responsesize":13038,"method":"Multi"}
2014-07-14 04:04:11,787 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15136,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335836651,"queuetimems":0,"class":"HRegionServer","responsesize":12947,"method":"Multi"}
2014-07-14 04:04:11,787 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16115,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335835672,"queuetimems":0,"class":"HRegionServer","responsesize":12823,"method":"Multi"}
2014-07-14 04:04:11,786 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14819,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335836967,"queuetimems":1,"class":"HRegionServer","responsesize":13048,"method":"Multi"}
2014-07-14 04:04:11,786 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17263,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335834523,"queuetimems":0,"class":"HRegionServer","responsesize":13063,"method":"Multi"}
2014-07-14 04:04:11,786 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16007,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335835778,"queuetimems":0,"class":"HRegionServer","responsesize":12855,"method":"Multi"}
2014-07-14 04:04:11,786 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18863,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335832922,"queuetimems":0,"class":"HRegionServer","responsesize":12921,"method":"Multi"}
2014-07-14 04:04:11,785 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15093,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335836692,"queuetimems":0,"class":"HRegionServer","responsesize":13127,"method":"Multi"}
2014-07-14 04:04:11,783 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14785,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335836998,"queuetimems":0,"class":"HRegionServer","responsesize":12948,"method":"Multi"}
2014-07-14 04:04:11,783 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14252,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335837530,"queuetimems":0,"class":"HRegionServer","responsesize":13018,"method":"Multi"}
2014-07-14 04:04:11,783 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16917,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335834865,"queuetimems":0,"class":"HRegionServer","responsesize":13159,"method":"Multi"}
2014-07-14 04:04:11,783 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16029,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335835753,"queuetimems":1,"class":"HRegionServer","responsesize":13304,"method":"Multi"}
2014-07-14 04:04:11,782 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15682,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335836100,"queuetimems":0,"class":"HRegionServer","responsesize":13113,"method":"Multi"}
2014-07-14 04:04:11,782 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19249,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335832533,"queuetimems":0,"class":"HRegionServer","responsesize":13048,"method":"Multi"}
2014-07-14 04:04:11,782 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16056,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335835726,"queuetimems":0,"class":"HRegionServer","responsesize":13147,"method":"Multi"}
2014-07-14 04:04:11,887 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13142,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335838744,"queuetimems":0,"class":"HRegionServer","responsesize":12918,"method":"Multi"}
2014-07-14 04:04:11,887 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15128,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335836758,"queuetimems":0,"class":"HRegionServer","responsesize":13212,"method":"Multi"}
2014-07-14 04:04:11,891 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15029,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335836861,"queuetimems":0,"class":"HRegionServer","responsesize":13063,"method":"Multi"}
2014-07-14 04:04:11,892 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17407,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335834485,"queuetimems":1,"class":"HRegionServer","responsesize":13127,"method":"Multi"}
2014-07-14 04:04:11,897 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16250,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335835647,"queuetimems":0,"class":"HRegionServer","responsesize":12834,"method":"Multi"}
2014-07-14 04:04:11,899 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15006,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335836892,"queuetimems":1,"class":"HRegionServer","responsesize":12921,"method":"Multi"}
2014-07-14 04:04:11,905 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15113,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335836792,"queuetimems":1,"class":"HRegionServer","responsesize":13119,"method":"Multi"}
2014-07-14 04:04:11,913 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:04:11,897 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19018,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335832879,"queuetimems":0,"class":"HRegionServer","responsesize":13024,"method":"Multi"}
2014-07-14 04:04:11,995 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57555 synced till here 57547
2014-07-14 04:04:12,048 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13271,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335838777,"queuetimems":0,"class":"HRegionServer","responsesize":13082,"method":"Multi"}
2014-07-14 04:04:12,049 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15442,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335836606,"queuetimems":1,"class":"HRegionServer","responsesize":13038,"method":"Multi"}
2014-07-14 04:04:12,049 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17720,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335834328,"queuetimems":0,"class":"HRegionServer","responsesize":12809,"method":"Multi"}
2014-07-14 04:04:12,049 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19012,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335833036,"queuetimems":0,"class":"HRegionServer","responsesize":12947,"method":"Multi"}
2014-07-14 04:04:12,061 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335849958 with entries=120, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335851913
2014-07-14 04:04:13,991 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:04:14,020 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57659 synced till here 57634
2014-07-14 04:04:14,402 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335851913 with entries=104, filesize=80.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335853991
2014-07-14 04:04:16,205 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:04:16,561 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57823 synced till here 57782
2014-07-14 04:04:16,791 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335853991 with entries=164, filesize=106.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335856206
2014-07-14 04:04:18,087 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14377, memsize=474.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/f2cdf93b68b44e58b062fde79ac601e7
2014-07-14 04:04:18,100 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/f2cdf93b68b44e58b062fde79ac601e7 as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/f2cdf93b68b44e58b062fde79ac601e7
2014-07-14 04:04:18,108 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/f2cdf93b68b44e58b062fde79ac601e7, entries=1728380, sequenceid=14377, filesize=123.1m
2014-07-14 04:04:18,108 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.2g/1267715360, currentsize=303.0m/317723920 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 28692ms, sequenceid=14377, compaction requested=true
2014-07-14 04:04:18,109 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:04:18,110 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 34 store files, 0 compacting, 34 eligible, 20 blocking
2014-07-14 04:04:18,110 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 34 files from compaction candidates
2014-07-14 04:04:18,110 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:04:18,110 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:04:18,110 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 04:04:18,166 DEBUG [RpcServer.handler=19,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b.
2014-07-14 04:04:18,199 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. has too many store files; delaying flush up to 90000ms
2014-07-14 04:04:18,199 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:04:18,200 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 34 store files, 0 compacting, 34 eligible, 20 blocking
2014-07-14 04:04:18,200 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 34 files from compaction candidates
2014-07-14 04:04:18,200 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:04:18,200 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:04:18,200 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 04:04:18,767 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:04:18,799 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57956 synced till here 57927
2014-07-14 04:04:18,947 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335856206 with entries=133, filesize=97.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335858768
2014-07-14 04:04:18,948 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335746856
2014-07-14 04:04:18,948 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335748523
2014-07-14 04:04:18,948 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335750109
2014-07-14 04:04:18,948 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335756509
2014-07-14 04:04:18,948 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335758873
2014-07-14 04:04:18,948 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335760997
2014-07-14 04:04:20,223 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:04:20,554 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58091 synced till here 58069
2014-07-14 04:04:20,747 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335858768 with entries=135, filesize=96.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335860224
2014-07-14 04:04:22,315 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:04:22,534 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58231 synced till here 58224
2014-07-14 04:04:22,569 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335860224 with entries=140, filesize=88.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335862316
2014-07-14 04:04:23,681 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:04:23,997 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335862316 with entries=100, filesize=66.5m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335863682
2014-07-14 04:04:23,997 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): cb0b7b4d7da9eff851ee6768c30cfe8c
2014-07-14 04:04:25,929 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:04:25,957 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335863682 with entries=87, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335865930
2014-07-14 04:04:25,958 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): cb0b7b4d7da9eff851ee6768c30cfe8c
2014-07-14 04:04:26,844 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:04:27,317 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58515 synced till here 58513
2014-07-14 04:04:27,346 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335865930 with entries=97, filesize=69.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335866844
2014-07-14 04:04:27,346 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): cb0b7b4d7da9eff851ee6768c30cfe8c
2014-07-14 04:04:28,232 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:04:28,477 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335866844 with entries=103, filesize=69.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335868232
2014-07-14 04:04:28,478 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): cb0b7b4d7da9eff851ee6768c30cfe8c
2014-07-14 04:04:31,005 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:04:31,032 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335868232 with entries=89, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335871006
2014-07-14 04:04:31,033 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): cb0b7b4d7da9eff851ee6768c30cfe8c
2014-07-14 04:04:32,636 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:04:32,654 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58798 synced till here 58797
2014-07-14 04:04:32,670 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335871006 with entries=91, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335872636
2014-07-14 04:04:32,670 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): cb0b7b4d7da9eff851ee6768c30cfe8c
2014-07-14 04:04:34,315 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14227, memsize=478.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/54e0dc9fcfb04f389c5cb123204f02d7
2014-07-14 04:04:34,327 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/54e0dc9fcfb04f389c5cb123204f02d7 as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/54e0dc9fcfb04f389c5cb123204f02d7
2014-07-14 04:04:34,338 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/54e0dc9fcfb04f389c5cb123204f02d7, entries=1743750, sequenceid=14227, filesize=124.2m
2014-07-14 04:04:34,338 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~738.8m/774738160, currentsize=581.1m/609331920 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 27190ms, sequenceid=14227, compaction requested=true
2014-07-14 04:04:34,339 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:04:34,339 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 34 store files, 0 compacting, 34 eligible, 20 blocking
2014-07-14 04:04:34,339 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 34 files from compaction candidates
2014-07-14 04:04:34,339 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:04:34,339 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:04:34,340 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 04:04:34,372 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 04:04:34,373 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. has too many store files; delaying flush up to 90000ms
2014-07-14 04:04:34,373 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:04:34,373 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 34 store files, 0 compacting, 34 eligible, 20 blocking
2014-07-14 04:04:34,373 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 34 files from compaction candidates
2014-07-14 04:04:34,373 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:04:34,373 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:04:34,373 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
2014-07-14 04:04:34,687 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:04:34,703 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58886 synced till here 58885
2014-07-14 04:04:34,721 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335872636 with entries=88, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335874687
2014-07-14 04:04:34,721 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): cb0b7b4d7da9eff851ee6768c30cfe8c
2014-07-14 04:04:36,479 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:04:36,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58976 synced till here 58975
2014-07-14 04:04:36,521 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335874687 with entries=90, filesize=62.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335876479
2014-07-14 04:04:36,521 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): cb0b7b4d7da9eff851ee6768c30cfe8c
2014-07-14 04:04:38,690 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:04:38,729 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335876479 with entries=86, filesize=61.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335878690
2014-07-14 04:04:38,729 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): cb0b7b4d7da9eff851ee6768c30cfe8c
2014-07-14 04:04:40,828 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:04:40,879 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335878690 with entries=87, filesize=61.3m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335880829
2014-07-14 04:04:40,879 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): cb0b7b4d7da9eff851ee6768c30cfe8c
2014-07-14 04:04:43,410 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:04:43,448 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335880829 with entries=91, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335883411
2014-07-14 04:04:43,448 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): cb0b7b4d7da9eff851ee6768c30cfe8c
2014-07-14 04:04:45,485 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:04:45,522 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335883411 with entries=90, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335885485
2014-07-14 04:04:45,523 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=44, maxlogs=32; forcing flush of 1 regions(s): cb0b7b4d7da9eff851ee6768c30cfe8c
2014-07-14 04:04:47,411 DEBUG [RpcServer.handler=38,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b.
2014-07-14 04:04:47,412 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. has too many store files; delaying flush up to 90000ms
2014-07-14 04:04:47,412 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:04:47,412 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 20 blocking
2014-07-14 04:04:47,412 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-14 04:04:47,412 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:04:47,412 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:04:47,412 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user9,1405333934035.331f54c5b3020b63e58b94456232194b. because compaction request was cancelled
2014-07-14 04:04:47,679 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:04:47,710 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335885485 with entries=86, filesize=61.1m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335887679
2014-07-14 04:04:47,711 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=45, maxlogs=32; forcing flush of 1 regions(s): cb0b7b4d7da9eff851ee6768c30cfe8c
2014-07-14 04:04:50,004 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90415ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c.
2014-07-14 04:04:50,004 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c., current region memstore size 1.8g
2014-07-14 04:04:50,379 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:04:50,395 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59509 synced till here 59508
2014-07-14 04:04:50,405 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335887679 with entries=93, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335890380
2014-07-14 04:04:51,471 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 04:04:51,472 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. has too many store files, but is 903.9m vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 04:04:51,472 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. due to global heap pressure
2014-07-14 04:04:51,472 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b., current region memstore size 903.9m
2014-07-14 04:04:51,523 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 04:04:52,051 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 04:04:53,584 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:04:53,617 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335890380 with entries=87, filesize=61.2m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335893584
2014-07-14 04:04:57,947 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 04:04:57,978 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335893584 with entries=92, filesize=61.4m; new WAL /hbase/WALs/slave1,60020,1405333554898/slave1%2C60020%2C1405333554898.1405335897948
2014-07-14 04:04:58,667 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:05:00,640 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:05:02,861 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:05:03,667 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:05:05,178 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:05:05,640 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:05:07,723 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:05:07,862 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 04:05:08,668 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 04:05:10,178 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 04:05:10,641 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 04:05:10,754 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405333554898: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 04:05:10,915 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14975, memsize=555.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/a72d32c816f447c1a93d12a173d889eb
2014-07-14 04:05:10,934 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/.tmp/a72d32c816f447c1a93d12a173d889eb as hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/a72d32c816f447c1a93d12a173d889eb
2014-07-14 04:05:10,952 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cd10ec3e9e9086b0a4afff5b7f27579b/family/a72d32c816f447c1a93d12a173d889eb, entries=2021850, sequenceid=14975, filesize=144.0m
2014-07-14 04:05:10,953 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~903.9m/947823040, currentsize=61.4m/64371840 for region usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. in 19481ms, sequenceid=14975, compaction requested=true
2014-07-14 04:05:10,953 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:05:10,954 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 35 store files, 0 compacting, 35 eligible, 20 blocking
2014-07-14 04:05:10,954 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 104836ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522.
2014-07-14 04:05:10,954 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 35 files from compaction candidates
2014-07-14 04:05:10,954 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 200ms
2014-07-14 04:05:10,954 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:05:10,954 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:05:10,954 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522., current region memstore size 900.3m
2014-07-14 04:05:10,954 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:05:10,954 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user3,1405333934035.cd10ec3e9e9086b0a4afff5b7f27579b. because compaction request was cancelled
2014-07-14 04:05:10,954 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10314ms
2014-07-14 04:05:10,954 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:05:10,954 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5776ms
2014-07-14 04:05:10,954 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:05:10,955 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12288ms
2014-07-14 04:05:10,955 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:05:10,955 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8094ms
2014-07-14 04:05:10,955 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:05:10,955 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3232ms
2014-07-14 04:05:10,955 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405333554898
2014-07-14 04:05:11,036 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12512,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335898523,"queuetimems":0,"class":"HRegionServer","responsesize":13247,"method":"Multi"}
2014-07-14 04:05:11,207 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10570,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47304","starttimems":1405335900636,"queuetimems":0,"class":"HRegionServer","responsesize":13247,"method":"Multi"}
2014-07-14 04:05:11,587 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 04:05:25,547 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14962, memsize=955.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/f8cf411e4d384291a92e91d8a5309edc
2014-07-14 04:05:25,567 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/.tmp/f8cf411e4d384291a92e91d8a5309edc as hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/f8cf411e4d384291a92e91d8a5309edc
2014-07-14 04:05:25,583 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cb0b7b4d7da9eff851ee6768c30cfe8c/family/f8cf411e4d384291a92e91d8a5309edc, entries=3479270, sequenceid=14962, filesize=247.7m
2014-07-14 04:05:25,583 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.8g/1911449760, currentsize=89.5m/93850320 for region usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. in 35579ms, sequenceid=14962, compaction requested=true
2014-07-14 04:05:25,584 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:05:25,584 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 38 store files, 0 compacting, 38 eligible, 20 blocking
2014-07-14 04:05:25,584 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 38 files from compaction candidates
2014-07-14 04:05:25,584 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:05:25,584 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:05:25,585 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user5,1405333934035.cb0b7b4d7da9eff851ee6768c30cfe8c. because compaction request was cancelled
2014-07-14 04:05:30,166 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14825, memsize=559.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/0a09449a9f4944918634f6b4df15a434
2014-07-14 04:05:30,195 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/.tmp/0a09449a9f4944918634f6b4df15a434 as hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/0a09449a9f4944918634f6b4df15a434
2014-07-14 04:05:30,214 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71c27911c72dddd675be84dade80b522/family/0a09449a9f4944918634f6b4df15a434, entries=2036200, sequenceid=14825, filesize=145.0m
2014-07-14 04:05:30,214 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~900.3m/944030320, currentsize=11.4m/11949040 for region usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. in 19260ms, sequenceid=14825, compaction requested=true
2014-07-14 04:05:30,215 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 04:05:30,215 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 35 store files, 0 compacting, 35 eligible, 20 blocking
2014-07-14 04:05:30,215 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 35 files from compaction candidates
2014-07-14 04:05:30,215 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 04:05:30,215 DEBUG [regionserver60020-smallCompactions-1405333594100] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 04:05:30,215 DEBUG [regionserver60020-smallCompactions-1405333594100] regionserver.CompactSplitThread: Not compacting usertable,user4,1405333934035.71c27911c72dddd675be84dade80b522. because compaction request was cancelled
