Tue Jul 22 11:06:47 PDT 2014 Starting regionserver on sceplus-vm48
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 128203
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 32768
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 128203
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
2014-07-22 11:06:47,591 INFO  [main] util.VersionInfo: HBase 0.98.3-hadoop1
2014-07-22 11:06:47,591 INFO  [main] util.VersionInfo: Subversion git://acer/usr/src/Hadoop/hbase -r d5e65a9144e315bb0a964e7730871af32f5018d5
2014-07-22 11:06:47,591 INFO  [main] util.VersionInfo: Compiled by apurtell on Sat May 31 19:34:57 PDT 2014
2014-07-22 11:06:47,817 INFO  [main] util.ServerCommandLine: env:JAVA_HOME=/usr/lib/jvm/java-1.6.0-openjdk-amd64/
2014-07-22 11:06:47,817 INFO  [main] util.ServerCommandLine: env:SHLVL=3
2014-07-22 11:06:47,817 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_DIR=/home/hadoop/hbase/bin/../logs
2014-07-22 11:06:47,817 INFO  [main] util.ServerCommandLine: env:HBASE_HOME=/home/hadoop/hbase/bin/..
2014-07-22 11:06:47,818 INFO  [main] util.ServerCommandLine: env:HBASE_OPTS=-XX:+UseConcMarkSweepGC   -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm48.log -Dhbase.home.dir=/home/hadoop/hbase/bin/.. -Dhbase.id.str=hadoop -Dhbase.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64 -Dhbase.security.logger=INFO,RFAS
2014-07-22 11:06:47,818 INFO  [main] util.ServerCommandLine: env:HBASE_ENV_INIT=true
2014-07-22 11:06:47,818 INFO  [main] util.ServerCommandLine: env:SSH_CLIENT=9.1.143.58 41893 22
2014-07-22 11:06:47,818 INFO  [main] util.ServerCommandLine: env:HBASE_HEAPSIZE=10240
2014-07-22 11:06:47,818 INFO  [main] util.ServerCommandLine: env:MAIL=/var/mail/hadoop
2014-07-22 11:06:47,818 INFO  [main] util.ServerCommandLine: env:HBASE_ZNODE_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.znode
2014-07-22 11:06:47,818 INFO  [main] util.ServerCommandLine: env:PWD=/home/hadoop/hbase
2014-07-22 11:06:47,818 INFO  [main] util.ServerCommandLine: env:LOGNAME=hadoop
2014-07-22 11:06:47,818 INFO  [main] util.ServerCommandLine: env:HBASE_MANAGES_ZK=true
2014-07-22 11:06:47,819 INFO  [main] util.ServerCommandLine: env:HBASE_NICENESS=0
2014-07-22 11:06:47,819 INFO  [main] util.ServerCommandLine: env:HBASE_REST_OPTS=
2014-07-22 11:06:47,819 INFO  [main] util.ServerCommandLine: env:LD_LIBRARY_PATH=/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64/server:/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64:/usr/lib/jvm/java-6-openjdk-amd64/jre/../lib/amd64::/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-07-22 11:06:47,819 INFO  [main] util.ServerCommandLine: env:MALLOC_ARENA_MAX=4
2014-07-22 11:06:47,819 INFO  [main] util.ServerCommandLine: env:SSH_CONNECTION=9.1.143.58 41893 9.1.143.58 22
2014-07-22 11:06:47,819 INFO  [main] util.ServerCommandLine: env:SHELL=/bin/bash
2014-07-22 11:06:47,819 INFO  [main] util.ServerCommandLine: env:HBASE_PID_DIR=/var/hadoop/pids
2014-07-22 11:06:47,819 INFO  [main] util.ServerCommandLine: env:HBASE_ROOT_LOGGER=INFO,RFA
2014-07-22 11:06:47,822 INFO  [main] util.ServerCommandLine: env:CLASSPATH=/home/hadoop/hbase/bin/../conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-07-22 11:06:47,822 INFO  [main] util.ServerCommandLine: env:HBASE_THRIFT_OPTS=
2014-07-22 11:06:47,822 INFO  [main] util.ServerCommandLine: env:PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games
2014-07-22 11:06:47,822 INFO  [main] util.ServerCommandLine: env:USER=hadoop
2014-07-22 11:06:47,822 INFO  [main] util.ServerCommandLine: env:HBASE_SECURITY_LOGGER=INFO,RFAS
2014-07-22 11:06:47,822 INFO  [main] util.ServerCommandLine: env:HOME=/home/hadoop
2014-07-22 11:06:47,822 INFO  [main] util.ServerCommandLine: env:HBASE_LIBRARY_PATH=/home/hadoop/hbase/lib/native/Linux-amd64-64
2014-07-22 11:06:47,822 INFO  [main] util.ServerCommandLine: env:HBASE_START_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.autorestart
2014-07-22 11:06:47,822 INFO  [main] util.ServerCommandLine: env:XDG_SESSION_ID=649
2014-07-22 11:06:47,823 INFO  [main] util.ServerCommandLine: env:HBASE_LOGFILE=hbase-hadoop-regionserver-sceplus-vm48.log
2014-07-22 11:06:47,823 INFO  [main] util.ServerCommandLine: env:XDG_RUNTIME_DIR=/run/user/1001
2014-07-22 11:06:47,823 INFO  [main] util.ServerCommandLine: env:LANG=en_US.UTF-8
2014-07-22 11:06:47,823 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_PREFIX=hbase-hadoop-regionserver-sceplus-vm48
2014-07-22 11:06:47,823 INFO  [main] util.ServerCommandLine: env:HBASE_IDENT_STRING=hadoop
2014-07-22 11:06:47,825 INFO  [main] util.ServerCommandLine: vmName=OpenJDK 64-Bit Server VM, vmVendor=Sun Microsystems Inc., vmVersion=23.25-b01
2014-07-22 11:06:47,825 INFO  [main] util.ServerCommandLine: vmInputArguments=[-Dproc_regionserver, -XX:OnOutOfMemoryError=kill -9 %p, -Xmx10240m, -XX:+UseConcMarkSweepGC, -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs, -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm48.log, -Dhbase.home.dir=/home/hadoop/hbase/bin/.., -Dhbase.id.str=hadoop, -Dhbase.root.logger=INFO,RFA, -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64, -Dhbase.security.logger=INFO,RFAS]
2014-07-22 11:06:48,039 DEBUG [main] regionserver.HRegionServer: regionserver/sceplus-vm48.almaden.ibm.com/9.1.143.58:60020 HConnection server-to-server retries=350
2014-07-22 11:06:48,398 INFO  [main] ipc.RpcServer: regionserver/sceplus-vm48.almaden.ibm.com/9.1.143.58:60020: started 10 reader(s).
2014-07-22 11:06:48,477 INFO  [main] impl.MetricsConfig: loaded properties from hadoop-metrics2-hbase.properties
2014-07-22 11:06:48,489 INFO  [main] impl.MetricsSinkAdapter: Sink file-all started
2014-07-22 11:06:48,555 INFO  [main] impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.
2014-07-22 11:06:48,557 INFO  [main] impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2014-07-22 11:06:48,557 INFO  [main] impl.MetricsSystemImpl: HBase metrics system started
2014-07-22 11:06:48,562 INFO  [main] impl.MetricsSourceAdapter: MBean for source jvm registered.
2014-07-22 11:06:48,566 INFO  [main] impl.MetricsSourceAdapter: MBean for source IPC,sub=IPC registered.
2014-07-22 11:06:48,651 INFO  [main] impl.MetricsSourceAdapter: MBean for source ugi registered.
2014-07-22 11:06:48,651 WARN  [main] impl.MetricsSystemImpl: Source name ugi already exists!
2014-07-22 11:06:48,656 DEBUG [main] util.DirectMemoryUtils: Failed to retrieve nio.BufferPool direct MemoryUsed attribute.
javax.management.InstanceNotFoundException: java.nio:type=BufferPool,name=direct
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1117)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:678)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:682)
	at org.apache.hadoop.hbase.util.DirectMemoryUtils.<clinit>(DirectMemoryUtils.java:72)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.instantiateBlockCache(CacheConfig.java:396)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.<init>(CacheConfig.java:179)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:621)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:534)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2393)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:61)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:85)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2410)
2014-07-22 11:06:48,659 INFO  [main] hfile.CacheConfig: Allocating LruBlockCache with maximum size 4.0g
2014-07-22 11:06:48,752 INFO  [main] mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2014-07-22 11:06:48,814 INFO  [main] http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
2014-07-22 11:06:48,825 INFO  [main] http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 60030
2014-07-22 11:06:48,827 INFO  [main] http.HttpServer: listener.getLocalPort() returned 60030 webServer.getConnectors()[0].getLocalPort() returned 60030
2014-07-22 11:06:48,827 INFO  [main] http.HttpServer: Jetty bound to port 60030
2014-07-22 11:06:48,827 INFO  [main] mortbay.log: jetty-6.1.26
2014-07-22 11:06:49,153 INFO  [main] mortbay.log: Started SelectChannelConnector@0.0.0.0:60030
2014-07-22 11:06:49,200 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2014-07-22 11:06:49,200 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:host.name=sceplus-vm48.almaden.ibm.com
2014-07-22 11:06:49,200 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.version=1.6.0_31
2014-07-22 11:06:49,200 INFO  [main] regionserver.ShutdownHook: Installed shutdown hook thread: Shutdownhook:regionserver60020
2014-07-22 11:06:49,200 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.vendor=Sun Microsystems Inc.
2014-07-22 11:06:49,201 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-6-openjdk-amd64/jre
2014-07-22 11:06:49,201 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.class.path=/home/hadoop/hbase/bin/../conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-07-22 11:06:49,201 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-07-22 11:06:49,201 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
2014-07-22 11:06:49,201 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
2014-07-22 11:06:49,201 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.name=Linux
2014-07-22 11:06:49,201 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.arch=amd64
2014-07-22 11:06:49,201 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.version=3.13.0-24-generic
2014-07-22 11:06:49,201 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.name=hadoop
2014-07-22 11:06:49,201 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.home=/home/hadoop
2014-07-22 11:06:49,201 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.dir=/home/hadoop/hbase-0.98.3-hadoop1
2014-07-22 11:06:49,202 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=regionserver:60020, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-22 11:06:49,224 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=regionserver:60020 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-22 11:06:49,227 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Opening socket connection to server slave1/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-22 11:06:49,231 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Socket connection established to slave1/9.1.143.59:2181, initiating session
2014-07-22 11:06:49,904 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x0, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-22 11:06:50,017 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/master
2014-07-22 11:06:50,017 INFO  [regionserver60020] util.RetryCounter: Sleeping 1000ms before retry #0...
2014-07-22 11:06:50,161 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-22 11:06:50,161 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-07-22 11:06:50,162 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x0, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-22 11:06:52,252 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Opening socket connection to server slave1/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-22 11:06:52,253 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Socket connection established to slave1/9.1.143.59:2181, initiating session
2014-07-22 11:06:52,258 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Session establishment complete on server slave1/9.1.143.59:2181, sessionid = 0x1475f411c9d0002, negotiated timeout = 90000
2014-07-22 11:07:22,532 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x3913e70e, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-22 11:07:22,533 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x3913e70e connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-22 11:07:22,534 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-22 11:07:22,534 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-07-22 11:07:22,545 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x475f4123560000, negotiated timeout = 90000
2014-07-22 11:07:22,797 DEBUG [regionserver60020] catalog.CatalogTracker: Starting catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@256c949f
2014-07-22 11:07:22,802 INFO  [regionserver60020] regionserver.HRegionServer: ClusterId : 3b61b992-e8ee-43f8-b0c6-14cd23a8afbe
2014-07-22 11:07:22,808 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initializing
2014-07-22 11:07:22,816 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Node /hbase/online-snapshot/acquired already exists and this is not a retry
2014-07-22 11:07:22,849 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initialized
2014-07-22 11:07:22,855 INFO  [regionserver60020] regionserver.MemStoreFlusher: globalMemStoreLimit=4.0g, globalMemStoreLimitLowMark=3.8g, maxHeap=9.9g
2014-07-22 11:07:22,860 INFO  [regionserver60020] regionserver.HRegionServer: CompactionChecker runs every 10sec
2014-07-22 11:07:22,875 INFO  [regionserver60020] regionserver.HRegionServer: reportForDuty to master=sceplus-vm48.almaden.ibm.com,60000,1406052406724 with port=60020, startcode=1406052408577
2014-07-22 11:07:23,254 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: hbase.rootdir=hdfs://master:54310/hbase
2014-07-22 11:07:23,254 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: fs.default.name=hdfs://master:54310
2014-07-22 11:07:23,280 INFO  [regionserver60020] fs.HFileSystem: Added intercepting call to namenode#getBlockLocations so can do block reordering using class class org.apache.hadoop.hbase.fs.HFileSystem$ReorderWALBlocks
2014-07-22 11:07:23,288 DEBUG [regionserver60020] regionserver.HRegionServer: logdir=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:07:23,323 DEBUG [regionserver60020] regionserver.Replication: ReplicationStatisticsThread 300
2014-07-22 11:07:23,338 INFO  [regionserver60020] wal.FSHLog: WAL/HLog configuration: blocksize=64 MB, rollsize=60.8 MB, enabled=true
2014-07-22 11:07:23,451 INFO  [regionserver60020] wal.FSHLog: New WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052443345
2014-07-22 11:07:23,463 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=WAL registered.
2014-07-22 11:07:23,469 INFO  [regionserver60020] regionserver.MetricsRegionServerWrapperImpl: Computing regionserver metrics every 5000 milliseconds
2014-07-22 11:07:23,473 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Server registered.
2014-07-22 11:07:23,477 INFO  [regionserver60020] trace.SpanReceiverHost: SpanReceiver org.cloudera.htrace.impl.LocalFileSpanReceiver was loaded successfully.
2014-07-22 11:07:23,480 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_REGION-sceplus-vm48:60020, corePoolSize=3, maxPoolSize=3
2014-07-22 11:07:23,480 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_META-sceplus-vm48:60020, corePoolSize=1, maxPoolSize=1
2014-07-22 11:07:23,480 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_REGION-sceplus-vm48:60020, corePoolSize=3, maxPoolSize=3
2014-07-22 11:07:23,480 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_META-sceplus-vm48:60020, corePoolSize=1, maxPoolSize=1
2014-07-22 11:07:23,480 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_LOG_REPLAY_OPS-sceplus-vm48:60020, corePoolSize=2, maxPoolSize=2
2014-07-22 11:07:23,487 INFO  [regionserver60020] regionserver.ReplicationSourceManager: Current list of replicators: [slave1,60020,1406052407090, sceplus-vm48.almaden.ibm.com,60020,1406052408577] other RSs: [slave1,60020,1406052407090, sceplus-vm48.almaden.ibm.com,60020,1406052408577]
2014-07-22 11:07:23,511 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Replication registered.
2014-07-22 11:07:23,513 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x507ca72d, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-22 11:07:23,514 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x507ca72d connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-22 11:07:23,514 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-22 11:07:23,515 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-07-22 11:07:23,519 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x475f4123560001, negotiated timeout = 90000
2014-07-22 11:07:23,526 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: starting
2014-07-22 11:07:23,526 INFO  [RpcServer.listener,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: starting
2014-07-22 11:07:23,570 INFO  [regionserver60020] regionserver.HRegionServer: Serving as sceplus-vm48.almaden.ibm.com,60020,1406052408577, RpcServer on sceplus-vm48.almaden.ibm.com/9.1.143.58:60020, sessionid=0x1475f411c9d0002
2014-07-22 11:07:23,570 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1406052408577] regionserver.SplitLogWorker: SplitLogWorker sceplus-vm48.almaden.ibm.com,60020,1406052408577 starting
2014-07-22 11:07:23,570 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is starting
2014-07-22 11:07:23,570 DEBUG [regionserver60020] snapshot.RegionServerSnapshotManager: Start Snapshot Manager sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:07:23,570 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Starting procedure member 'sceplus-vm48.almaden.ibm.com,60020,1406052408577'
2014-07-22 11:07:23,570 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Checking for aborted procedures on node: '/hbase/online-snapshot/abort'
2014-07-22 11:07:23,571 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Looking for new procedures under znode:'/hbase/online-snapshot/acquired'
2014-07-22 11:07:23,572 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is started
2014-07-22 11:07:27,348 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open hbase:meta,,1.1588230740
2014-07-22 11:07:27,461 DEBUG [RS_OPEN_META-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 1588230740 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 11:07:27,482 DEBUG [RS_OPEN_META-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 1588230740 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 11:07:27,483 DEBUG [RS_OPEN_META-sceplus-vm48:60020-0] regionserver.HRegionServer: logdir=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:07:27,484 INFO  [RS_OPEN_META-sceplus-vm48:60020-0] wal.FSHLog: WAL/HLog configuration: blocksize=64 MB, rollsize=60.8 MB, enabled=true
2014-07-22 11:07:27,527 INFO  [RS_OPEN_META-sceplus-vm48:60020-0] wal.FSHLog: New WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta
2014-07-22 11:07:27,550 DEBUG [RS_OPEN_META-sceplus-vm48:60020-0] regionserver.HRegion: Opening region: {ENCODED => 1588230740, NAME => 'hbase:meta,,1', STARTKEY => '', ENDKEY => ''}
2014-07-22 11:07:27,572 DEBUG [RS_OPEN_META-sceplus-vm48:60020-0] coprocessor.CoprocessorHost: Loading coprocessor class org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint with path null and priority 536870911
2014-07-22 11:07:27,577 DEBUG [RS_OPEN_META-sceplus-vm48:60020-0] regionserver.HRegion: Registered coprocessor service: region=hbase:meta,,1 service=MultiRowMutationService
2014-07-22 11:07:27,580 INFO  [RS_OPEN_META-sceplus-vm48:60020-0] regionserver.RegionCoprocessorHost: Loaded coprocessor org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint from HTD of hbase:meta successfully.
2014-07-22 11:07:27,585 INFO  [RS_OPEN_META-sceplus-vm48:60020-0] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Regions registered.
2014-07-22 11:07:27,586 DEBUG [RS_OPEN_META-sceplus-vm48:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table meta 1588230740
2014-07-22 11:07:27,586 DEBUG [RS_OPEN_META-sceplus-vm48:60020-0] regionserver.HRegion: Instantiated hbase:meta,,1.1588230740
2014-07-22 11:07:27,661 INFO  [StoreOpener-1588230740-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 604800000, major jitter 0.500000
2014-07-22 11:07:27,707 INFO  [StoreFileOpenerThread-info-1] util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc32
2014-07-22 11:07:27,756 DEBUG [StoreOpener-1588230740-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/hbase/meta/1588230740/info/bee87cfb1d6448cbbc7a3f615af88c32, isReference=false, isBulkLoadResult=false, seqid=7559, majorCompaction=true
2014-07-22 11:07:27,794 DEBUG [RS_OPEN_META-sceplus-vm48:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/hbase/meta/1588230740
2014-07-22 11:07:27,802 INFO  [RS_OPEN_META-sceplus-vm48:60020-0] regionserver.HRegion: Onlined 1588230740; next sequenceid=7560
2014-07-22 11:07:27,802 DEBUG [RS_OPEN_META-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 1588230740
2014-07-22 11:07:27,806 INFO  [PostOpenDeployTasks:1588230740] regionserver.HRegionServer: Post open deploy tasks for region=hbase:meta,,1.1588230740
2014-07-22 11:07:27,808 INFO  [PostOpenDeployTasks:1588230740] zookeeper.ZooKeeperNodeTracker: Setting hbase:meta region location in ZooKeeper as sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:07:27,818 INFO  [PostOpenDeployTasks:1588230740] regionserver.HRegionServer: Finished post open deploy task for hbase:meta,,1.1588230740
2014-07-22 11:07:27,819 DEBUG [RS_OPEN_META-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 1588230740 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 11:07:27,827 DEBUG [RS_OPEN_META-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 1588230740 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 11:07:27,827 DEBUG [RS_OPEN_META-sceplus-vm48:60020-0] handler.OpenRegionHandler: Transitioned 1588230740 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:07:27,827 DEBUG [RS_OPEN_META-sceplus-vm48:60020-0] handler.OpenRegionHandler: Opened hbase:meta,,1.1588230740 on sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:07:59,481 INFO  [Priority.RpcServer.handler=7,port=60020] regionserver.HRegionServer: Compacting hbase:meta,,1.1588230740
2014-07-22 11:07:59,483 DEBUG [Priority.RpcServer.handler=7,port=60020] compactions.RatioBasedCompactionPolicy: Selecting compaction from 1 store files, 0 compacting, 1 eligible, 2000 blocking
2014-07-22 11:07:59,485 DEBUG [Priority.RpcServer.handler=7,port=60020] regionserver.HStore: 1588230740 - info: Initiating major compaction
2014-07-22 11:07:59,487 DEBUG [Priority.RpcServer.handler=7,port=60020] regionserver.CompactSplitThread: Small Compaction requested: org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext@67642bf5; Because: User-triggered major compaction; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 11:07:59,488 INFO  [regionserver60020-smallCompactions-1406052479487] regionserver.HRegion: Starting compaction on info in region hbase:meta,,1.1588230740
2014-07-22 11:07:59,489 INFO  [regionserver60020-smallCompactions-1406052479487] regionserver.HStore: Starting compaction of 1 file(s) in info of hbase:meta,,1.1588230740 into tmpdir=hdfs://master:54310/hbase/data/hbase/meta/1588230740/.tmp, totalSize=10.6k
2014-07-22 11:07:59,491 DEBUG [regionserver60020-smallCompactions-1406052479487] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/hbase/meta/1588230740/info/bee87cfb1d6448cbbc7a3f615af88c32, keycount=86, bloomtype=NONE, size=10.6k, encoding=NONE, seqNum=7559, earliestPutTs=1402645258588
2014-07-22 11:07:59,500 DEBUG [regionserver60020-smallCompactions-1406052479487] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:07:59,564 DEBUG [regionserver60020-smallCompactions-1406052479487] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/hbase/meta/1588230740/.tmp/ed4ab6c9a9d949e08a065cc830929193 as hdfs://master:54310/hbase/data/hbase/meta/1588230740/info/ed4ab6c9a9d949e08a065cc830929193
2014-07-22 11:07:59,583 DEBUG [regionserver60020-smallCompactions-1406052479487] regionserver.HStore: Removing store files after compaction...
2014-07-22 11:07:59,598 DEBUG [regionserver60020-smallCompactions-1406052479487] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/hbase/meta/1588230740/info/bee87cfb1d6448cbbc7a3f615af88c32, to hdfs://master:54310/hbase/archive/data/hbase/meta/1588230740/info/bee87cfb1d6448cbbc7a3f615af88c32
2014-07-22 11:07:59,598 INFO  [regionserver60020-smallCompactions-1406052479487] regionserver.HStore: Completed major compaction of 1 file(s) in info of hbase:meta,,1.1588230740 into ed4ab6c9a9d949e08a065cc830929193(size=10.6k), total size for store is 10.6k. This selection was in queue for 0sec, and took 0sec to execute.
2014-07-22 11:07:59,601 INFO  [regionserver60020-smallCompactions-1406052479487] regionserver.CompactSplitThread: Completed compaction: Request = regionName=hbase:meta,,1.1588230740, storeName=info, fileCount=1, fileSize=10.6k, priority=1, time=149470877361963; duration=0sec
2014-07-22 11:07:59,602 DEBUG [regionserver60020-smallCompactions-1406052479487] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 11:11:48,669 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=0, accesses=53, hits=51, hitRatio=96.22%, , cachingAccesses=51, cachingHits=49, cachingHitsRatio=96.07%, evictions=0, evicted=2, evictedPerRun=Infinity
2014-07-22 11:13:07,072 INFO  [Priority.RpcServer.handler=5,port=60020] regionserver.HRegionServer: Open usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.
2014-07-22 11:13:07,092 INFO  [Priority.RpcServer.handler=5,port=60020] regionserver.HRegionServer: Open usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.
2014-07-22 11:13:07,092 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 4264c18b25708212810563a172f7f7f7 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 11:13:07,093 INFO  [Priority.RpcServer.handler=5,port=60020] regionserver.HRegionServer: Open usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.
2014-07-22 11:13:07,093 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 4d714c493433d51e92af518b12e607b2 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 11:13:07,094 INFO  [Priority.RpcServer.handler=5,port=60020] regionserver.HRegionServer: Open usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f.
2014-07-22 11:13:07,094 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 20f69fa9e49e2ee9c1b670c938523b8e from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 11:13:07,094 INFO  [Priority.RpcServer.handler=5,port=60020] regionserver.HRegionServer: Open usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.
2014-07-22 11:13:07,098 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 4264c18b25708212810563a172f7f7f7 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 11:13:07,098 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 4d714c493433d51e92af518b12e607b2 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 11:13:07,098 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Opening region: {ENCODED => 4264c18b25708212810563a172f7f7f7, NAME => 'usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.', STARTKEY => 'user7', ENDKEY => 'user8'}
2014-07-22 11:13:07,098 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Opening region: {ENCODED => 4d714c493433d51e92af518b12e607b2, NAME => 'usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.', STARTKEY => 'user5', ENDKEY => 'user6'}
2014-07-22 11:13:07,100 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 4264c18b25708212810563a172f7f7f7
2014-07-22 11:13:07,100 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 4d714c493433d51e92af518b12e607b2
2014-07-22 11:13:07,100 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Instantiated usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.
2014-07-22 11:13:07,100 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Instantiated usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.
2014-07-22 11:13:07,107 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 20f69fa9e49e2ee9c1b670c938523b8e from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 11:13:07,107 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Opening region: {ENCODED => 20f69fa9e49e2ee9c1b670c938523b8e, NAME => 'usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.', STARTKEY => 'user2', ENDKEY => 'user3'}
2014-07-22 11:13:07,108 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 20f69fa9e49e2ee9c1b670c938523b8e
2014-07-22 11:13:07,108 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Instantiated usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.
2014-07-22 11:13:07,110 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] util.NativeCodeLoader: Loaded the native-hadoop library
2014-07-22 11:13:07,112 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
2014-07-22 11:13:07,115 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] compress.CodecPool: Got brand-new compressor
2014-07-22 11:13:07,115 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] compress.CodecPool: Got brand-new compressor
2014-07-22 11:13:07,116 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] compress.CodecPool: Got brand-new compressor
2014-07-22 11:13:07,129 INFO  [StoreOpener-4d714c493433d51e92af518b12e607b2-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 11:13:07,131 INFO  [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 11:13:07,134 INFO  [StoreOpener-4264c18b25708212810563a172f7f7f7-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 11:13:07,137 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2
2014-07-22 11:13:07,140 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e
2014-07-22 11:13:07,140 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7
2014-07-22 11:13:07,140 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Onlined 4d714c493433d51e92af518b12e607b2; next sequenceid=1
2014-07-22 11:13:07,140 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 4d714c493433d51e92af518b12e607b2
2014-07-22 11:13:07,142 INFO  [PostOpenDeployTasks:4d714c493433d51e92af518b12e607b2] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.
2014-07-22 11:13:07,142 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Onlined 20f69fa9e49e2ee9c1b670c938523b8e; next sequenceid=1
2014-07-22 11:13:07,142 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 20f69fa9e49e2ee9c1b670c938523b8e
2014-07-22 11:13:07,144 INFO  [PostOpenDeployTasks:20f69fa9e49e2ee9c1b670c938523b8e] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.
2014-07-22 11:13:07,180 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Onlined 4264c18b25708212810563a172f7f7f7; next sequenceid=1
2014-07-22 11:13:07,180 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 4264c18b25708212810563a172f7f7f7
2014-07-22 11:13:07,183 INFO  [PostOpenDeployTasks:4264c18b25708212810563a172f7f7f7] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.
2014-07-22 11:13:07,215 INFO  [PostOpenDeployTasks:20f69fa9e49e2ee9c1b670c938523b8e] catalog.MetaEditor: Updated row usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e. with server=sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:13:07,215 INFO  [PostOpenDeployTasks:20f69fa9e49e2ee9c1b670c938523b8e] regionserver.HRegionServer: Finished post open deploy task for usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.
2014-07-22 11:13:07,216 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 20f69fa9e49e2ee9c1b670c938523b8e from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 11:13:07,216 INFO  [PostOpenDeployTasks:4d714c493433d51e92af518b12e607b2] catalog.MetaEditor: Updated row usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2. with server=sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:13:07,217 INFO  [PostOpenDeployTasks:4d714c493433d51e92af518b12e607b2] regionserver.HRegionServer: Finished post open deploy task for usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.
2014-07-22 11:13:07,218 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 4d714c493433d51e92af518b12e607b2 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 11:13:07,217 INFO  [PostOpenDeployTasks:4264c18b25708212810563a172f7f7f7] catalog.MetaEditor: Updated row usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7. with server=sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:13:07,218 INFO  [PostOpenDeployTasks:4264c18b25708212810563a172f7f7f7] regionserver.HRegionServer: Finished post open deploy task for usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.
2014-07-22 11:13:07,219 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 4264c18b25708212810563a172f7f7f7 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 11:13:07,222 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 20f69fa9e49e2ee9c1b670c938523b8e from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 11:13:07,222 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] handler.OpenRegionHandler: Transitioned 20f69fa9e49e2ee9c1b670c938523b8e to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:13:07,223 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] handler.OpenRegionHandler: Opened usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e. on sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:13:07,223 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning b4227bd82d287303700b1960a94f313f from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 11:13:07,223 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 4d714c493433d51e92af518b12e607b2 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 11:13:07,223 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] handler.OpenRegionHandler: Transitioned 4d714c493433d51e92af518b12e607b2 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:13:07,224 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] handler.OpenRegionHandler: Opened usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2. on sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:13:07,224 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 92ba9ea7e04274b0bbdc5cbfefd7393e from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 11:13:07,224 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 4264c18b25708212810563a172f7f7f7 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 11:13:07,224 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Transitioned 4264c18b25708212810563a172f7f7f7 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:13:07,224 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Opened usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7. on sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:13:07,229 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node b4227bd82d287303700b1960a94f313f from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 11:13:07,229 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Opening region: {ENCODED => b4227bd82d287303700b1960a94f313f, NAME => 'usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f.', STARTKEY => 'user9', ENDKEY => ''}
2014-07-22 11:13:07,230 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 92ba9ea7e04274b0bbdc5cbfefd7393e from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 11:13:07,230 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable b4227bd82d287303700b1960a94f313f
2014-07-22 11:13:07,230 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Opening region: {ENCODED => 92ba9ea7e04274b0bbdc5cbfefd7393e, NAME => 'usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.', STARTKEY => 'user8', ENDKEY => 'user9'}
2014-07-22 11:13:07,230 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Instantiated usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f.
2014-07-22 11:13:07,231 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 92ba9ea7e04274b0bbdc5cbfefd7393e
2014-07-22 11:13:07,231 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Instantiated usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.
2014-07-22 11:13:07,238 INFO  [StoreOpener-b4227bd82d287303700b1960a94f313f-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 11:13:07,239 INFO  [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 11:13:07,242 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f
2014-07-22 11:13:07,243 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e
2014-07-22 11:13:07,245 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Onlined b4227bd82d287303700b1960a94f313f; next sequenceid=1
2014-07-22 11:13:07,246 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node b4227bd82d287303700b1960a94f313f
2014-07-22 11:13:07,246 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Onlined 92ba9ea7e04274b0bbdc5cbfefd7393e; next sequenceid=1
2014-07-22 11:13:07,246 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 92ba9ea7e04274b0bbdc5cbfefd7393e
2014-07-22 11:13:07,248 INFO  [PostOpenDeployTasks:b4227bd82d287303700b1960a94f313f] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f.
2014-07-22 11:13:07,249 INFO  [PostOpenDeployTasks:92ba9ea7e04274b0bbdc5cbfefd7393e] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.
2014-07-22 11:13:07,257 INFO  [PostOpenDeployTasks:b4227bd82d287303700b1960a94f313f] catalog.MetaEditor: Updated row usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f. with server=sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:13:07,257 INFO  [PostOpenDeployTasks:b4227bd82d287303700b1960a94f313f] regionserver.HRegionServer: Finished post open deploy task for usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f.
2014-07-22 11:13:07,257 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning b4227bd82d287303700b1960a94f313f from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 11:13:07,263 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node b4227bd82d287303700b1960a94f313f from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 11:13:07,263 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] handler.OpenRegionHandler: Transitioned b4227bd82d287303700b1960a94f313f to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:13:07,263 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] handler.OpenRegionHandler: Opened usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f. on sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:13:07,296 INFO  [PostOpenDeployTasks:92ba9ea7e04274b0bbdc5cbfefd7393e] catalog.MetaEditor: Updated row usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e. with server=sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:13:07,296 INFO  [PostOpenDeployTasks:92ba9ea7e04274b0bbdc5cbfefd7393e] regionserver.HRegionServer: Finished post open deploy task for usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.
2014-07-22 11:13:07,298 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 92ba9ea7e04274b0bbdc5cbfefd7393e from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 11:13:07,304 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 92ba9ea7e04274b0bbdc5cbfefd7393e from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 11:13:07,304 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] handler.OpenRegionHandler: Transitioned 92ba9ea7e04274b0bbdc5cbfefd7393e to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:13:07,304 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] handler.OpenRegionHandler: Opened usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e. on sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:13:25,865 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:13:25,885 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 95 synced till here 81
2014-07-22 11:13:25,950 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052443345 with entries=95, filesize=74.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052805865
2014-07-22 11:13:28,480 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:13:28,636 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 193 synced till here 187
2014-07-22 11:13:28,718 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052805865 with entries=98, filesize=66.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052808481
2014-07-22 11:13:31,034 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:13:31,058 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 290 synced till here 285
2014-07-22 11:13:31,174 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052808481 with entries=97, filesize=66.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052811035
2014-07-22 11:13:32,692 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:13:32,744 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052811035 with entries=119, filesize=62.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052812692
2014-07-22 11:13:43,769 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:13:43,789 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 668 synced till here 667
2014-07-22 11:13:43,806 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052812692 with entries=259, filesize=63.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052823769
2014-07-22 11:13:46,225 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:13:46,246 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 754 synced till here 751
2014-07-22 11:13:46,284 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052823769 with entries=86, filesize=64.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052826225
2014-07-22 11:14:08,381 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:14:08,520 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1000 synced till here 997
2014-07-22 11:14:08,582 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052826225 with entries=246, filesize=65.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052848382
2014-07-22 11:14:10,637 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:14:10,655 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1135 synced till here 1132
2014-07-22 11:14:10,699 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052848382 with entries=135, filesize=64.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052850638
2014-07-22 11:14:12,505 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:14:12,522 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1218 synced till here 1216
2014-07-22 11:14:12,540 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052850638 with entries=83, filesize=61.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052852506
2014-07-22 11:14:14,955 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.
2014-07-22 11:14:14,957 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7., current region memstore size 256.1m
2014-07-22 11:14:14,965 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.
2014-07-22 11:14:14,965 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e., current region memstore size 256.1m
2014-07-22 11:14:15,099 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:14:15,155 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:14:15,160 INFO  [MemStoreFlusher.0] compress.CodecPool: Got brand-new compressor
2014-07-22 11:14:18,398 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.
2014-07-22 11:14:18,841 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.
2014-07-22 11:14:19,316 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:14:19,456 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052852506 with entries=237, filesize=61.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052859317
2014-07-22 11:14:21,774 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:14:23,254 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1619 synced till here 1614
2014-07-22 11:14:23,529 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052859317 with entries=164, filesize=97.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052861774
2014-07-22 11:14:25,732 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=351, memsize=256.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/.tmp/594d902bd0f54938ba261a96b2f4962b
2014-07-22 11:14:25,742 INFO  [MemStoreFlusher.0] compress.CodecPool: Got brand-new decompressor
2014-07-22 11:14:25,747 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/.tmp/594d902bd0f54938ba261a96b2f4962b as hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/594d902bd0f54938ba261a96b2f4962b
2014-07-22 11:14:25,757 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/594d902bd0f54938ba261a96b2f4962b, entries=932470, sequenceid=351, filesize=66.4m
2014-07-22 11:14:25,757 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~256.1m/268545280, currentsize=86.2m/90346160 for region usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e. in 10792ms, sequenceid=351, compaction requested=false
2014-07-22 11:14:25,759 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e., current region memstore size 316.9m
2014-07-22 11:14:25,947 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:14:26,169 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=350, memsize=256.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/.tmp/df4659cb871847ad962fda2f67fa2613
2014-07-22 11:14:26,201 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/.tmp/df4659cb871847ad962fda2f67fa2613 as hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/df4659cb871847ad962fda2f67fa2613
2014-07-22 11:14:26,218 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/df4659cb871847ad962fda2f67fa2613, entries=932320, sequenceid=350, filesize=66.4m
2014-07-22 11:14:26,218 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~256.1m/268503440, currentsize=86.3m/90531040 for region usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7. in 11262ms, sequenceid=350, compaction requested=false
2014-07-22 11:14:26,218 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2., current region memstore size 316.8m
2014-07-22 11:14:26,419 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:14:35,253 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=161, memsize=316.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/.tmp/b5b5d030592d4350b1c50987defd26a0
2014-07-22 11:14:35,268 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/.tmp/b5b5d030592d4350b1c50987defd26a0 as hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/b5b5d030592d4350b1c50987defd26a0
2014-07-22 11:14:35,281 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/b5b5d030592d4350b1c50987defd26a0, entries=1154010, sequenceid=161, filesize=82.2m
2014-07-22 11:14:35,281 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~316.9m/332344720, currentsize=0.0/0 for region usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e. in 9522ms, sequenceid=161, compaction requested=false
2014-07-22 11:14:35,494 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=161, memsize=316.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/.tmp/938a37a7348e42c2b27136f7e0ed1195
2014-07-22 11:14:35,510 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/.tmp/938a37a7348e42c2b27136f7e0ed1195 as hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/938a37a7348e42c2b27136f7e0ed1195
2014-07-22 11:14:35,520 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/938a37a7348e42c2b27136f7e0ed1195, entries=1153450, sequenceid=161, filesize=82.1m
2014-07-22 11:14:35,520 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~316.8m/332184080, currentsize=0.0/0 for region usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2. in 9302ms, sequenceid=161, compaction requested=false
2014-07-22 11:14:58,732 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:14:58,757 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1867 synced till here 1865
2014-07-22 11:14:58,776 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052861774 with entries=248, filesize=63.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052898733
2014-07-22 11:15:01,577 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:15:01,761 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1946 synced till here 1943
2014-07-22 11:15:01,813 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052898733 with entries=79, filesize=65.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052901578
2014-07-22 11:15:03,730 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1194ms
GC pool 'ParNew' had collection(s): count=2 time=1496ms
2014-07-22 11:15:05,849 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:15:05,978 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2032 synced till here 2023
2014-07-22 11:15:06,069 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052901578 with entries=86, filesize=72.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052905849
2014-07-22 11:15:07,741 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:15:07,839 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2120 synced till here 2115
2014-07-22 11:15:07,877 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052905849 with entries=88, filesize=67.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052907741
2014-07-22 11:15:10,347 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:15:10,367 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2209 synced till here 2203
2014-07-22 11:15:10,559 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052907741 with entries=89, filesize=66.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052910347
2014-07-22 11:15:12,071 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:15:12,421 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2321 synced till here 2320
2014-07-22 11:15:12,455 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052910347 with entries=112, filesize=72.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052912072
2014-07-22 11:15:13,586 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.
2014-07-22 11:15:13,587 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7., current region memstore size 256.8m
2014-07-22 11:15:13,796 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.
2014-07-22 11:15:13,800 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e., current region memstore size 256.6m
2014-07-22 11:15:13,946 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:15:14,132 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:15:14,560 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:15:14,689 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2414 synced till here 2406
2014-07-22 11:15:14,735 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052912072 with entries=93, filesize=66.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052914560
2014-07-22 11:15:16,205 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:15:17,721 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052914560 with entries=136, filesize=91.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052916205
2014-07-22 11:15:22,273 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:15:22,287 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2733 synced till here 2732
2014-07-22 11:15:22,350 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052916205 with entries=183, filesize=63.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052922274
2014-07-22 11:15:23,011 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.
2014-07-22 11:15:23,155 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.
2014-07-22 11:15:24,205 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=633, memsize=256.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/.tmp/2ff955ba92db4f73b81a3a56c2fd204a
2014-07-22 11:15:24,218 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/.tmp/2ff955ba92db4f73b81a3a56c2fd204a as hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/2ff955ba92db4f73b81a3a56c2fd204a
2014-07-22 11:15:24,228 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/2ff955ba92db4f73b81a3a56c2fd204a, entries=934420, sequenceid=633, filesize=66.6m
2014-07-22 11:15:24,229 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~256.6m/269105440, currentsize=99.6m/104485520 for region usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e. in 10429ms, sequenceid=633, compaction requested=false
2014-07-22 11:15:24,230 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e., current region memstore size 256.8m
2014-07-22 11:15:24,246 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=636, memsize=256.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/.tmp/4b9b5298f80141809d0a250c99a13bb6
2014-07-22 11:15:24,257 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/.tmp/4b9b5298f80141809d0a250c99a13bb6 as hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/4b9b5298f80141809d0a250c99a13bb6
2014-07-22 11:15:24,267 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/4b9b5298f80141809d0a250c99a13bb6, entries=935060, sequenceid=636, filesize=66.6m
2014-07-22 11:15:24,268 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~256.8m/269290640, currentsize=99.6m/104440960 for region usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7. in 10681ms, sequenceid=636, compaction requested=false
2014-07-22 11:15:24,268 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2., current region memstore size 256.9m
2014-07-22 11:15:24,406 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:15:24,453 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:15:25,579 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:15:25,607 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052922274 with entries=169, filesize=61.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052925579
2014-07-22 11:15:28,003 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:15:28,759 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3034 synced till here 3032
2014-07-22 11:15:30,643 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052925579 with entries=132, filesize=89.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052928004
2014-07-22 11:15:32,225 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:15:32,386 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3120 synced till here 3104
2014-07-22 11:15:32,585 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052928004 with entries=86, filesize=72.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052932226
2014-07-22 11:15:34,010 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:15:34,029 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3194 synced till here 3193
2014-07-22 11:15:34,078 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052932226 with entries=74, filesize=64.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052934011
2014-07-22 11:15:36,583 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:15:36,601 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3279 synced till here 3269
2014-07-22 11:15:36,807 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052934011 with entries=85, filesize=71.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052936584
2014-07-22 11:15:36,993 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=303, memsize=256.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/.tmp/2e0b706d67674ffc90596a002621506c
2014-07-22 11:15:37,090 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/.tmp/2e0b706d67674ffc90596a002621506c as hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/2e0b706d67674ffc90596a002621506c
2014-07-22 11:15:37,109 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/2e0b706d67674ffc90596a002621506c, entries=935260, sequenceid=303, filesize=66.6m
2014-07-22 11:15:37,109 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~256.9m/269347840, currentsize=152.9m/160376480 for region usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2. in 12841ms, sequenceid=303, compaction requested=false
2014-07-22 11:15:37,733 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=303, memsize=256.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/.tmp/ed505024c9b3478ba712ad831e764ea9
2014-07-22 11:15:37,746 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/.tmp/ed505024c9b3478ba712ad831e764ea9 as hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/ed505024c9b3478ba712ad831e764ea9
2014-07-22 11:15:37,771 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/ed505024c9b3478ba712ad831e764ea9, entries=935100, sequenceid=303, filesize=66.6m
2014-07-22 11:15:37,771 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~256.8m/269301360, currentsize=175.1m/183594880 for region usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e. in 13541ms, sequenceid=303, compaction requested=false
2014-07-22 11:15:38,994 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:15:39,183 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3369 synced till here 3367
2014-07-22 11:15:39,202 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052936584 with entries=90, filesize=68.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052938995
2014-07-22 11:15:39,233 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.
2014-07-22 11:15:39,234 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7., current region memstore size 257.8m
2014-07-22 11:15:39,710 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:15:39,721 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.
2014-07-22 11:15:39,722 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e., current region memstore size 257.8m
2014-07-22 11:15:40,386 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:15:40,824 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:15:40,846 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3442 synced till here 3440
2014-07-22 11:15:41,004 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052938995 with entries=73, filesize=64.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052940825
2014-07-22 11:15:43,592 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:15:43,661 DEBUG [RpcServer.handler=37,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.
2014-07-22 11:15:44,010 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3558 synced till here 3550
2014-07-22 11:15:44,071 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.
2014-07-22 11:15:44,083 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052940825 with entries=116, filesize=103.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052943593
2014-07-22 11:15:45,801 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f.
2014-07-22 11:15:46,136 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:15:46,160 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3630 synced till here 3627
2014-07-22 11:15:46,187 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052943593 with entries=72, filesize=65.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052946137
2014-07-22 11:15:48,170 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:15:49,444 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3739 synced till here 3736
2014-07-22 11:15:49,674 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052946137 with entries=109, filesize=91.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052948170
2014-07-22 11:15:51,306 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:15:51,412 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3825 synced till here 3814
2014-07-22 11:15:51,489 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052948170 with entries=86, filesize=71.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052951308
2014-07-22 11:15:53,036 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:15:53,058 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3936 synced till here 3932
2014-07-22 11:15:53,094 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052951308 with entries=111, filesize=63.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052953036
2014-07-22 11:15:56,010 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=885, memsize=262.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/.tmp/bcf3d23daea84497b55f2b3cae3e2e74
2014-07-22 11:15:56,021 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/.tmp/bcf3d23daea84497b55f2b3cae3e2e74 as hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/bcf3d23daea84497b55f2b3cae3e2e74
2014-07-22 11:15:56,031 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/bcf3d23daea84497b55f2b3cae3e2e74, entries=956290, sequenceid=885, filesize=68.1m
2014-07-22 11:15:56,032 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~262.6m/275402880, currentsize=195.4m/204867760 for region usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e. in 16310ms, sequenceid=885, compaction requested=true
2014-07-22 11:15:56,032 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 11:15:56,033 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e., current region memstore size 381.4m
2014-07-22 11:15:56,033 DEBUG [regionserver60020-smallCompactions-1406052479487] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 2000 blocking
2014-07-22 11:15:56,036 DEBUG [regionserver60020-smallCompactions-1406052479487] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 210894147 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-07-22 11:15:56,037 DEBUG [regionserver60020-smallCompactions-1406052479487] regionserver.HStore: 92ba9ea7e04274b0bbdc5cbfefd7393e - family: Initiating major compaction
2014-07-22 11:15:56,037 INFO  [regionserver60020-smallCompactions-1406052479487] regionserver.HRegion: Starting compaction on family in region usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.
2014-07-22 11:15:56,037 INFO  [regionserver60020-smallCompactions-1406052479487] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/.tmp, totalSize=201.1m
2014-07-22 11:15:56,038 DEBUG [regionserver60020-smallCompactions-1406052479487] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/594d902bd0f54938ba261a96b2f4962b, keycount=93247, bloomtype=ROW, size=66.4m, encoding=NONE, seqNum=351, earliestPutTs=1406052804739
2014-07-22 11:15:56,038 DEBUG [regionserver60020-smallCompactions-1406052479487] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/2ff955ba92db4f73b81a3a56c2fd204a, keycount=93442, bloomtype=ROW, size=66.6m, encoding=NONE, seqNum=633, earliestPutTs=1406052854980
2014-07-22 11:15:56,038 DEBUG [regionserver60020-smallCompactions-1406052479487] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/bcf3d23daea84497b55f2b3cae3e2e74, keycount=95629, bloomtype=ROW, size=68.1m, encoding=NONE, seqNum=885, earliestPutTs=1406052913853
2014-07-22 11:15:56,064 DEBUG [regionserver60020-smallCompactions-1406052479487] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:15:56,104 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=886, memsize=258.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/.tmp/81630f3817204ce2ae724724fcbfc019
2014-07-22 11:15:56,117 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/.tmp/81630f3817204ce2ae724724fcbfc019 as hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/81630f3817204ce2ae724724fcbfc019
2014-07-22 11:15:56,128 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/81630f3817204ce2ae724724fcbfc019, entries=942720, sequenceid=886, filesize=67.2m
2014-07-22 11:15:56,129 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~258.9m/271496160, currentsize=198.6m/208242480 for region usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7. in 16894ms, sequenceid=886, compaction requested=true
2014-07-22 11:15:56,129 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-22 11:15:56,129 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2., current region memstore size 380.4m
2014-07-22 11:15:56,297 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:15:56,382 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:15:56,385 INFO  [MemStoreFlusher.0] compress.CodecPool: Got brand-new compressor
2014-07-22 11:15:56,385 INFO  [MemStoreFlusher.0] compress.CodecPool: Got brand-new compressor
2014-07-22 11:16:08,617 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=516, memsize=380.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/.tmp/ceb5d461a1f047bf8e6ba2a7be251a66
2014-07-22 11:16:08,627 INFO  [MemStoreFlusher.0] compress.CodecPool: Got brand-new decompressor
2014-07-22 11:16:08,629 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/.tmp/ceb5d461a1f047bf8e6ba2a7be251a66 as hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/ceb5d461a1f047bf8e6ba2a7be251a66
2014-07-22 11:16:08,632 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=516, memsize=381.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/.tmp/423463ddf3a541cbb0435d3b592d2d03
2014-07-22 11:16:08,640 INFO  [MemStoreFlusher.0] compress.CodecPool: Got brand-new decompressor
2014-07-22 11:16:08,640 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/ceb5d461a1f047bf8e6ba2a7be251a66, entries=1385070, sequenceid=516, filesize=98.6m
2014-07-22 11:16:08,640 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~380.4m/398888480, currentsize=0.0/0 for region usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2. in 12511ms, sequenceid=516, compaction requested=true
2014-07-22 11:16:08,641 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:2), split_queue=0, merge_queue=0
2014-07-22 11:16:08,641 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f., current region memstore size 297.7m
2014-07-22 11:16:08,642 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/.tmp/423463ddf3a541cbb0435d3b592d2d03 as hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/423463ddf3a541cbb0435d3b592d2d03
2014-07-22 11:16:08,651 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/423463ddf3a541cbb0435d3b592d2d03, entries=1388570, sequenceid=516, filesize=98.8m
2014-07-22 11:16:08,652 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~381.4m/399897360, currentsize=0.0/0 for region usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e. in 12618ms, sequenceid=516, compaction requested=true
2014-07-22 11:16:08,652 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:3), split_queue=0, merge_queue=0
2014-07-22 11:16:08,834 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:16:10,589 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:16:10,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4262 synced till here 4261
2014-07-22 11:16:10,737 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052953036 with entries=326, filesize=63.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052970590
2014-07-22 11:16:12,812 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:16:12,831 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4336 synced till here 4326
2014-07-22 11:16:13,053 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052970590 with entries=74, filesize=68.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052972813
2014-07-22 11:16:13,226 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.
2014-07-22 11:16:13,226 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7., current region memstore size 256.4m
2014-07-22 11:16:13,578 DEBUG [RpcServer.handler=39,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.
2014-07-22 11:16:13,763 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:16:14,560 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:16:14,956 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4483 synced till here 4476
2014-07-22 11:16:15,043 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052972813 with entries=147, filesize=74.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052974560
2014-07-22 11:16:21,076 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1020, memsize=297.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/.tmp/88cbda18a3de4b6d898a6312425584d1
2014-07-22 11:16:21,088 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/.tmp/88cbda18a3de4b6d898a6312425584d1 as hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/88cbda18a3de4b6d898a6312425584d1
2014-07-22 11:16:21,098 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/88cbda18a3de4b6d898a6312425584d1, entries=1083850, sequenceid=1020, filesize=77.2m
2014-07-22 11:16:21,098 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~297.7m/312119840, currentsize=27.0m/28267600 for region usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f. in 12457ms, sequenceid=1020, compaction requested=false
2014-07-22 11:16:21,099 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e., current region memstore size 302.9m
2014-07-22 11:16:21,287 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:16:22,180 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:16:22,232 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4777 synced till here 4776
2014-07-22 11:16:22,259 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052974560 with entries=294, filesize=64.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052982181
2014-07-22 11:16:22,260 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052443345
2014-07-22 11:16:22,260 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052805865
2014-07-22 11:16:22,260 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052808481
2014-07-22 11:16:22,260 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052811035
2014-07-22 11:16:22,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052812692
2014-07-22 11:16:22,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052823769
2014-07-22 11:16:22,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052826225
2014-07-22 11:16:22,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052848382
2014-07-22 11:16:22,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052850638
2014-07-22 11:16:22,262 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052852506
2014-07-22 11:16:22,262 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052859317
2014-07-22 11:16:22,262 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052861774
2014-07-22 11:16:22,262 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052898733
2014-07-22 11:16:22,262 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052901578
2014-07-22 11:16:22,262 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052905849
2014-07-22 11:16:22,262 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052907741
2014-07-22 11:16:22,262 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052910347
2014-07-22 11:16:22,262 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052912072
2014-07-22 11:16:22,262 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052914560
2014-07-22 11:16:22,263 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052916205
2014-07-22 11:16:22,263 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052922274
2014-07-22 11:16:22,263 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052925579
2014-07-22 11:16:22,263 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052928004
2014-07-22 11:16:22,263 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052932226
2014-07-22 11:16:22,263 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052934011
2014-07-22 11:16:22,263 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052936584
2014-07-22 11:16:24,270 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:16:24,364 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4869 synced till here 4865
2014-07-22 11:16:24,400 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052982181 with entries=92, filesize=65.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052984271
2014-07-22 11:16:25,279 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1127, memsize=257.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/.tmp/0064041da4124f198b81d9fc3801698f
2014-07-22 11:16:25,299 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/.tmp/0064041da4124f198b81d9fc3801698f as hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/0064041da4124f198b81d9fc3801698f
2014-07-22 11:16:25,311 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/0064041da4124f198b81d9fc3801698f, entries=936260, sequenceid=1127, filesize=66.7m
2014-07-22 11:16:25,311 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~257.1m/269633280, currentsize=89.8m/94204000 for region usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7. in 12085ms, sequenceid=1127, compaction requested=true
2014-07-22 11:16:25,311 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:4), split_queue=0, merge_queue=0
2014-07-22 11:16:26,296 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:16:27,005 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4989 synced till here 4988
2014-07-22 11:16:27,335 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052984271 with entries=120, filesize=95.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052986296
2014-07-22 11:16:29,391 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:16:29,473 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5068 synced till here 5061
2014-07-22 11:16:29,901 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052986296 with entries=79, filesize=68.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052989392
2014-07-22 11:16:31,626 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:16:31,719 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5150 synced till here 5133
2014-07-22 11:16:31,933 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052989392 with entries=82, filesize=78.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052991627
2014-07-22 11:16:32,609 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.
2014-07-22 11:16:32,669 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e., current region memstore size 261.1m
2014-07-22 11:16:32,887 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.
2014-07-22 11:16:33,140 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:16:33,660 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:16:34,612 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5267 synced till here 5265
2014-07-22 11:16:34,798 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052991627 with entries=117, filesize=82.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052993660
2014-07-22 11:16:35,166 DEBUG [regionserver60020-smallCompactions-1406052479487] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/.tmp/0598ac603b8d47fe9ba4b16af92bed9a as hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/0598ac603b8d47fe9ba4b16af92bed9a
2014-07-22 11:16:35,441 DEBUG [regionserver60020-smallCompactions-1406052479487] regionserver.HStore: Removing store files after compaction...
2014-07-22 11:16:35,447 DEBUG [regionserver60020-smallCompactions-1406052479487] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/594d902bd0f54938ba261a96b2f4962b, to hdfs://master:54310/hbase/archive/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/594d902bd0f54938ba261a96b2f4962b
2014-07-22 11:16:35,449 DEBUG [regionserver60020-smallCompactions-1406052479487] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/2ff955ba92db4f73b81a3a56c2fd204a, to hdfs://master:54310/hbase/archive/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/2ff955ba92db4f73b81a3a56c2fd204a
2014-07-22 11:16:35,450 DEBUG [regionserver60020-smallCompactions-1406052479487] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/bcf3d23daea84497b55f2b3cae3e2e74, to hdfs://master:54310/hbase/archive/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/bcf3d23daea84497b55f2b3cae3e2e74
2014-07-22 11:16:35,451 INFO  [regionserver60020-smallCompactions-1406052479487] regionserver.HStore: Completed major compaction of 3 file(s) in family of usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e. into 0598ac603b8d47fe9ba4b16af92bed9a(size=201.0m), total size for store is 201.0m. This selection was in queue for 0sec, and took 39sec to execute.
2014-07-22 11:16:35,452 INFO  [regionserver60020-smallCompactions-1406052479487] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e., storeName=family, fileCount=3, fileSize=201.1m, priority=1997, time=149947429058296; duration=39sec
2014-07-22 11:16:35,452 DEBUG [regionserver60020-smallCompactions-1406052479487] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:4), split_queue=0, merge_queue=0
2014-07-22 11:16:35,452 DEBUG [regionserver60020-smallCompactions-1406052479487] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 2000 blocking
2014-07-22 11:16:35,452 DEBUG [regionserver60020-smallCompactions-1406052479487] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 4 files of size 279816165 starting at candidate #0 after considering 3 permutations with 3 in ratio
2014-07-22 11:16:35,452 DEBUG [regionserver60020-smallCompactions-1406052479487] regionserver.HStore: 4264c18b25708212810563a172f7f7f7 - family: Initiating major compaction
2014-07-22 11:16:35,453 INFO  [regionserver60020-smallCompactions-1406052479487] regionserver.HRegion: Starting compaction on family in region usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.
2014-07-22 11:16:35,453 INFO  [regionserver60020-smallCompactions-1406052479487] regionserver.HStore: Starting compaction of 4 file(s) in family of usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/.tmp, totalSize=266.9m
2014-07-22 11:16:35,453 DEBUG [regionserver60020-smallCompactions-1406052479487] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/df4659cb871847ad962fda2f67fa2613, keycount=93232, bloomtype=ROW, size=66.4m, encoding=NONE, seqNum=350, earliestPutTs=1406052804671
2014-07-22 11:16:35,453 DEBUG [regionserver60020-smallCompactions-1406052479487] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/4b9b5298f80141809d0a250c99a13bb6, keycount=93506, bloomtype=ROW, size=66.6m, encoding=NONE, seqNum=636, earliestPutTs=1406052854974
2014-07-22 11:16:35,453 DEBUG [regionserver60020-smallCompactions-1406052479487] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/81630f3817204ce2ae724724fcbfc019, keycount=94272, bloomtype=ROW, size=67.2m, encoding=NONE, seqNum=886, earliestPutTs=1406052913811
2014-07-22 11:16:35,453 DEBUG [regionserver60020-smallCompactions-1406052479487] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/0064041da4124f198b81d9fc3801698f, keycount=93626, bloomtype=ROW, size=66.7m, encoding=NONE, seqNum=1127, earliestPutTs=1406052939336
2014-07-22 11:16:35,590 DEBUG [regionserver60020-smallCompactions-1406052479487] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:16:36,500 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:16:36,596 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5345 synced till here 5330
2014-07-22 11:16:36,927 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052993660 with entries=78, filesize=75.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052996500
2014-07-22 11:16:37,666 DEBUG [RpcServer.handler=39,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.
2014-07-22 11:16:38,953 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:16:39,054 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052996500 with entries=85, filesize=61.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052998954
2014-07-22 11:16:40,062 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1240, memsize=302.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/.tmp/7c46a3941f9d44b99aaf9dbaf9c4d9a7
2014-07-22 11:16:40,080 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/.tmp/7c46a3941f9d44b99aaf9dbaf9c4d9a7 as hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/7c46a3941f9d44b99aaf9dbaf9c4d9a7
2014-07-22 11:16:40,090 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/7c46a3941f9d44b99aaf9dbaf9c4d9a7, entries=1102720, sequenceid=1240, filesize=78.5m
2014-07-22 11:16:40,100 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~302.9m/317572720, currentsize=227.0m/238028400 for region usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e. in 18992ms, sequenceid=1240, compaction requested=false
2014-07-22 11:16:40,101 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2., current region memstore size 346.6m
2014-07-22 11:16:40,836 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:16:40,885 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:16:41,080 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5515 synced till here 5514
2014-07-22 11:16:41,310 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052998954 with entries=85, filesize=68.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053000837
2014-07-22 11:16:41,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052938995
2014-07-22 11:16:41,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052940825
2014-07-22 11:16:41,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052943593
2014-07-22 11:16:41,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052946137
2014-07-22 11:16:41,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052948170
2014-07-22 11:16:41,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052951308
2014-07-22 11:16:42,030 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.
2014-07-22 11:16:42,922 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:16:43,221 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5647 synced till here 5644
2014-07-22 11:16:43,243 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053000837 with entries=132, filesize=73.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053002922
2014-07-22 11:16:46,610 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=648, memsize=269.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/.tmp/c142103f1eb845a69785bc97218beba3
2014-07-22 11:16:46,622 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/.tmp/c142103f1eb845a69785bc97218beba3 as hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/c142103f1eb845a69785bc97218beba3
2014-07-22 11:16:46,630 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/c142103f1eb845a69785bc97218beba3, entries=979810, sequenceid=648, filesize=69.8m
2014-07-22 11:16:46,631 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~269.1m/282177680, currentsize=121.0m/126880400 for region usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e. in 13961ms, sequenceid=648, compaction requested=true
2014-07-22 11:16:46,631 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:4), split_queue=0, merge_queue=0
2014-07-22 11:16:46,631 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7., current region memstore size 338.2m
2014-07-22 11:16:46,839 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:16:48,667 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.17 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=10048, hits=1167, hitRatio=11.61%, , cachingAccesses=1169, cachingHits=1165, cachingHitsRatio=99.65%, evictions=0, evicted=2, evictedPerRun=Infinity
2014-07-22 11:16:53,436 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=690, memsize=350.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/.tmp/1f2296f0ee664f28854c35aa847d551e
2014-07-22 11:16:53,454 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/.tmp/1f2296f0ee664f28854c35aa847d551e as hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/1f2296f0ee664f28854c35aa847d551e
2014-07-22 11:16:53,476 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/1f2296f0ee664f28854c35aa847d551e, entries=1276970, sequenceid=690, filesize=91.0m
2014-07-22 11:16:53,476 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~350.7m/367758240, currentsize=40.9m/42910640 for region usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2. in 13375ms, sequenceid=690, compaction requested=true
2014-07-22 11:16:53,477 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:5), split_queue=0, merge_queue=0
2014-07-22 11:16:53,477 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e., current region memstore size 289.7m
2014-07-22 11:16:53,735 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:16:57,698 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1463, memsize=338.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/.tmp/2676e2031f334c0fa8ad5ed4a67ffff4
2014-07-22 11:16:57,736 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/.tmp/2676e2031f334c0fa8ad5ed4a67ffff4 as hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/2676e2031f334c0fa8ad5ed4a67ffff4
2014-07-22 11:16:57,901 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/2676e2031f334c0fa8ad5ed4a67ffff4, entries=1231340, sequenceid=1463, filesize=87.7m
2014-07-22 11:16:57,902 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~338.2m/354615200, currentsize=0.0/0 for region usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7. in 11271ms, sequenceid=1463, compaction requested=false
2014-07-22 11:17:02,883 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1467, memsize=289.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/.tmp/d2338ca383644f5ab416f2fdb26b92f8
2014-07-22 11:17:02,908 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/.tmp/d2338ca383644f5ab416f2fdb26b92f8 as hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/d2338ca383644f5ab416f2fdb26b92f8
2014-07-22 11:17:02,923 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/d2338ca383644f5ab416f2fdb26b92f8, entries=1054780, sequenceid=1467, filesize=75.1m
2014-07-22 11:17:02,923 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~289.7m/303768320, currentsize=1.8m/1938160 for region usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e. in 9446ms, sequenceid=1467, compaction requested=true
2014-07-22 11:17:02,923 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:6), split_queue=0, merge_queue=0
2014-07-22 11:17:04,743 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:17:04,773 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053002922 with entries=224, filesize=61.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053024744
2014-07-22 11:17:16,181 DEBUG [regionserver60020-smallCompactions-1406052479487] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/.tmp/8551416e90ae403ea576c9e4f1a99379 as hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/8551416e90ae403ea576c9e4f1a99379
2014-07-22 11:17:16,208 DEBUG [regionserver60020-smallCompactions-1406052479487] regionserver.HStore: Removing store files after compaction...
2014-07-22 11:17:16,216 DEBUG [regionserver60020-smallCompactions-1406052479487] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/df4659cb871847ad962fda2f67fa2613, to hdfs://master:54310/hbase/archive/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/df4659cb871847ad962fda2f67fa2613
2014-07-22 11:17:16,251 DEBUG [regionserver60020-smallCompactions-1406052479487] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/4b9b5298f80141809d0a250c99a13bb6, to hdfs://master:54310/hbase/archive/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/4b9b5298f80141809d0a250c99a13bb6
2014-07-22 11:17:16,255 DEBUG [regionserver60020-smallCompactions-1406052479487] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/81630f3817204ce2ae724724fcbfc019, to hdfs://master:54310/hbase/archive/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/81630f3817204ce2ae724724fcbfc019
2014-07-22 11:17:16,257 DEBUG [regionserver60020-smallCompactions-1406052479487] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/0064041da4124f198b81d9fc3801698f, to hdfs://master:54310/hbase/archive/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/0064041da4124f198b81d9fc3801698f
2014-07-22 11:17:16,258 INFO  [regionserver60020-smallCompactions-1406052479487] regionserver.HStore: Completed major compaction of 4 file(s) in family of usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7. into 8551416e90ae403ea576c9e4f1a99379(size=266.6m), total size for store is 354.3m. This selection was in queue for 0sec, and took 40sec to execute.
2014-07-22 11:17:16,258 INFO  [regionserver60020-smallCompactions-1406052479487] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7., storeName=family, fileCount=4, fileSize=266.9m, priority=1996, time=149986844952445; duration=40sec
2014-07-22 11:17:16,258 DEBUG [regionserver60020-smallCompactions-1406052479487] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:6), split_queue=0, merge_queue=0
2014-07-22 11:17:16,258 DEBUG [regionserver60020-smallCompactions-1406052479487] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 2000 blocking
2014-07-22 11:17:16,259 DEBUG [regionserver60020-smallCompactions-1406052479487] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 4 files of size 332869133 starting at candidate #0 after considering 3 permutations with 3 in ratio
2014-07-22 11:17:16,259 DEBUG [regionserver60020-smallCompactions-1406052479487] regionserver.HStore: 20f69fa9e49e2ee9c1b670c938523b8e - family: Initiating major compaction
2014-07-22 11:17:16,259 INFO  [regionserver60020-smallCompactions-1406052479487] regionserver.HRegion: Starting compaction on family in region usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.
2014-07-22 11:17:16,259 INFO  [regionserver60020-smallCompactions-1406052479487] regionserver.HStore: Starting compaction of 4 file(s) in family of usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/.tmp, totalSize=317.4m
2014-07-22 11:17:16,259 DEBUG [regionserver60020-smallCompactions-1406052479487] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/b5b5d030592d4350b1c50987defd26a0, keycount=115401, bloomtype=ROW, size=82.2m, encoding=NONE, seqNum=161, earliestPutTs=1406052802864
2014-07-22 11:17:16,260 DEBUG [regionserver60020-smallCompactions-1406052479487] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/ed505024c9b3478ba712ad831e764ea9, keycount=93510, bloomtype=ROW, size=66.6m, encoding=NONE, seqNum=303, earliestPutTs=1406052881777
2014-07-22 11:17:16,260 DEBUG [regionserver60020-smallCompactions-1406052479487] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/423463ddf3a541cbb0435d3b592d2d03, keycount=138857, bloomtype=ROW, size=98.8m, encoding=NONE, seqNum=516, earliestPutTs=1406052924770
2014-07-22 11:17:16,260 DEBUG [regionserver60020-smallCompactions-1406052479487] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/c142103f1eb845a69785bc97218beba3, keycount=97981, bloomtype=ROW, size=69.8m, encoding=NONE, seqNum=648, earliestPutTs=1406052969184
2014-07-22 11:17:16,286 DEBUG [regionserver60020-smallCompactions-1406052479487] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:17:19,040 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:17:19,062 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5990 synced till here 5985
2014-07-22 11:17:19,194 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053024744 with entries=119, filesize=64.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053039040
2014-07-22 11:17:20,748 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:17:20,849 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053039040 with entries=84, filesize=62.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053040748
2014-07-22 11:17:23,067 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:17:23,174 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6151 synced till here 6146
2014-07-22 11:17:23,224 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053040748 with entries=77, filesize=67.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053043068
2014-07-22 11:17:25,018 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:17:25,018 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.
2014-07-22 11:17:25,019 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e., current region memstore size 257.4m
2014-07-22 11:17:25,489 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:17:25,631 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6270 synced till here 6266
2014-07-22 11:17:26,245 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053043068 with entries=119, filesize=77.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053045019
2014-07-22 11:17:28,032 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:17:28,093 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6365 synced till here 6355
2014-07-22 11:17:28,164 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053045019 with entries=95, filesize=78.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053048032
2014-07-22 11:17:30,196 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:17:30,218 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6458 synced till here 6450
2014-07-22 11:17:30,381 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053048032 with entries=93, filesize=69.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053050196
2014-07-22 11:17:31,645 DEBUG [RpcServer.handler=41,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.
2014-07-22 11:17:31,647 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2., current region memstore size 256.5m
2014-07-22 11:17:32,234 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:17:32,368 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6559 synced till here 6538
2014-07-22 11:17:32,419 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:17:32,843 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053050196 with entries=101, filesize=80.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053052235
2014-07-22 11:17:34,443 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:17:34,466 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6633 synced till here 6625
2014-07-22 11:17:34,494 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.
2014-07-22 11:17:34,709 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053052235 with entries=74, filesize=66.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053054444
2014-07-22 11:17:35,596 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.
2014-07-22 11:17:36,878 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:17:36,986 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6718 synced till here 6704
2014-07-22 11:17:37,338 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053054444 with entries=85, filesize=76.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053056878
2014-07-22 11:17:39,348 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:17:39,517 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6802 synced till here 6799
2014-07-22 11:17:39,545 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053056878 with entries=84, filesize=63.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053059348
2014-07-22 11:17:42,066 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=792, memsize=257.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/.tmp/2d1fd8fc6fe047d69f876ec24ce4503f
2014-07-22 11:17:42,083 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/.tmp/2d1fd8fc6fe047d69f876ec24ce4503f as hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/2d1fd8fc6fe047d69f876ec24ce4503f
2014-07-22 11:17:42,102 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/2d1fd8fc6fe047d69f876ec24ce4503f, entries=937260, sequenceid=792, filesize=66.7m
2014-07-22 11:17:42,102 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~257.4m/269922960, currentsize=182.3m/191186080 for region usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e. in 17083ms, sequenceid=792, compaction requested=false
2014-07-22 11:17:42,103 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7., current region memstore size 328.3m
2014-07-22 11:17:42,316 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:17:45,357 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=840, memsize=265.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/.tmp/6b415b0567c140a88fa3a9f591d5494d
2014-07-22 11:17:45,380 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/.tmp/6b415b0567c140a88fa3a9f591d5494d as hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/6b415b0567c140a88fa3a9f591d5494d
2014-07-22 11:17:45,399 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/6b415b0567c140a88fa3a9f591d5494d, entries=964740, sequenceid=840, filesize=68.7m
2014-07-22 11:17:45,399 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~265.0m/277836480, currentsize=94.1m/98686400 for region usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2. in 13752ms, sequenceid=840, compaction requested=true
2014-07-22 11:17:45,400 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:6), split_queue=0, merge_queue=0
2014-07-22 11:17:45,400 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e., current region memstore size 330.8m
2014-07-22 11:17:45,731 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:17:52,609 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1733, memsize=328.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/.tmp/5c1d551df4bb4d15a1c7a6c4e8546068
2014-07-22 11:17:52,620 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/.tmp/5c1d551df4bb4d15a1c7a6c4e8546068 as hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/5c1d551df4bb4d15a1c7a6c4e8546068
2014-07-22 11:17:52,634 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/5c1d551df4bb4d15a1c7a6c4e8546068, entries=1195480, sequenceid=1733, filesize=85.2m
2014-07-22 11:17:52,635 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~328.3m/344292960, currentsize=7.1m/7447520 for region usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7. in 10532ms, sequenceid=1733, compaction requested=true
2014-07-22 11:17:52,635 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:7), split_queue=0, merge_queue=0
2014-07-22 11:17:56,292 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1747, memsize=330.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/.tmp/81cb8e6e81024396b9616e415b3e0b7b
2014-07-22 11:17:56,304 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/.tmp/81cb8e6e81024396b9616e415b3e0b7b as hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/81cb8e6e81024396b9616e415b3e0b7b
2014-07-22 11:17:56,328 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/81cb8e6e81024396b9616e415b3e0b7b, entries=1204310, sequenceid=1747, filesize=85.8m
2014-07-22 11:17:56,328 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~330.8m/346833520, currentsize=10.7m/11237600 for region usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e. in 10928ms, sequenceid=1747, compaction requested=true
2014-07-22 11:17:56,328 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:8), split_queue=0, merge_queue=0
2014-07-22 11:17:58,432 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:17:58,467 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7165 synced till here 7163
2014-07-22 11:17:58,600 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053059348 with entries=363, filesize=63.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053078433
2014-07-22 11:18:00,668 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:18:00,701 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7258 synced till here 7257
2014-07-22 11:18:01,077 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053078433 with entries=93, filesize=64.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053080669
2014-07-22 11:18:05,495 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:18:05,536 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7351 synced till here 7334
2014-07-22 11:18:05,538 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.
2014-07-22 11:18:05,539 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e., current region memstore size 258.0m
2014-07-22 11:18:05,756 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053080669 with entries=93, filesize=76.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053085496
2014-07-22 11:18:06,533 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:18:07,736 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:18:07,775 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7448 synced till here 7440
2014-07-22 11:18:07,951 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053085496 with entries=97, filesize=69.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053087736
2014-07-22 11:18:10,557 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:18:10,696 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7540 synced till here 7529
2014-07-22 11:18:10,790 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053087736 with entries=92, filesize=74.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053090557
2014-07-22 11:18:11,423 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f.
2014-07-22 11:18:11,424 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f., current region memstore size 256.6m
2014-07-22 11:18:11,768 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:18:18,602 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=932, memsize=267.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/.tmp/6b42bd1423f24b43a0366bc77fe45b9e
2014-07-22 11:18:18,620 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/.tmp/6b42bd1423f24b43a0366bc77fe45b9e as hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/6b42bd1423f24b43a0366bc77fe45b9e
2014-07-22 11:18:18,628 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/6b42bd1423f24b43a0366bc77fe45b9e, entries=975450, sequenceid=932, filesize=69.5m
2014-07-22 11:18:18,629 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~267.9m/280921040, currentsize=75.1m/78700000 for region usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e. in 13091ms, sequenceid=932, compaction requested=false
2014-07-22 11:18:20,339 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1809, memsize=256.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/.tmp/fb69618522a545d98be00c7b345feb32
2014-07-22 11:18:20,357 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/.tmp/fb69618522a545d98be00c7b345feb32 as hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/fb69618522a545d98be00c7b345feb32
2014-07-22 11:18:20,369 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/fb69618522a545d98be00c7b345feb32, entries=934180, sequenceid=1809, filesize=66.5m
2014-07-22 11:18:20,369 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~256.6m/269019280, currentsize=5.3m/5603920 for region usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f. in 8945ms, sequenceid=1809, compaction requested=false
2014-07-22 11:18:20,414 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:18:20,427 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.
2014-07-22 11:18:20,428 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2., current region memstore size 259.1m
2014-07-22 11:18:20,504 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7811 synced till here 7810
2014-07-22 11:18:20,525 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053090557 with entries=271, filesize=63.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053100415
2014-07-22 11:18:20,525 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052953036
2014-07-22 11:18:20,525 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052970590
2014-07-22 11:18:20,525 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052972813
2014-07-22 11:18:20,525 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052974560
2014-07-22 11:18:20,526 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052982181
2014-07-22 11:18:20,526 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052984271
2014-07-22 11:18:20,526 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052986296
2014-07-22 11:18:20,526 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052989392
2014-07-22 11:18:20,526 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052991627
2014-07-22 11:18:20,526 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052993660
2014-07-22 11:18:20,526 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052996500
2014-07-22 11:18:20,526 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052998954
2014-07-22 11:18:20,526 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053000837
2014-07-22 11:18:20,526 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053002922
2014-07-22 11:18:20,526 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053024744
2014-07-22 11:18:20,526 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053039040
2014-07-22 11:18:20,526 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053040748
2014-07-22 11:18:20,527 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053043068
2014-07-22 11:18:20,527 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053045019
2014-07-22 11:18:20,527 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053048032
2014-07-22 11:18:20,752 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:18:21,875 DEBUG [regionserver60020-smallCompactions-1406052479487] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/.tmp/d36788c3b0224727ba1e9c3f6b188b52 as hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/d36788c3b0224727ba1e9c3f6b188b52
2014-07-22 11:18:22,058 DEBUG [regionserver60020-smallCompactions-1406052479487] regionserver.HStore: Removing store files after compaction...
2014-07-22 11:18:22,068 DEBUG [regionserver60020-smallCompactions-1406052479487] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/b5b5d030592d4350b1c50987defd26a0, to hdfs://master:54310/hbase/archive/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/b5b5d030592d4350b1c50987defd26a0
2014-07-22 11:18:22,071 DEBUG [regionserver60020-smallCompactions-1406052479487] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/ed505024c9b3478ba712ad831e764ea9, to hdfs://master:54310/hbase/archive/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/ed505024c9b3478ba712ad831e764ea9
2014-07-22 11:18:22,156 DEBUG [regionserver60020-smallCompactions-1406052479487] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/423463ddf3a541cbb0435d3b592d2d03, to hdfs://master:54310/hbase/archive/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/423463ddf3a541cbb0435d3b592d2d03
2014-07-22 11:18:22,167 DEBUG [regionserver60020-smallCompactions-1406052479487] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/c142103f1eb845a69785bc97218beba3, to hdfs://master:54310/hbase/archive/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/c142103f1eb845a69785bc97218beba3
2014-07-22 11:18:22,168 INFO  [regionserver60020-smallCompactions-1406052479487] regionserver.HStore: Completed major compaction of 4 file(s) in family of usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e. into d36788c3b0224727ba1e9c3f6b188b52(size=317.2m), total size for store is 453.5m. This selection was in queue for 0sec, and took 1mins, 5sec to execute.
2014-07-22 11:18:22,168 INFO  [regionserver60020-smallCompactions-1406052479487] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e., storeName=family, fileCount=4, fileSize=317.4m, priority=1996, time=150027651246193; duration=1mins, 5sec
2014-07-22 11:18:22,168 DEBUG [regionserver60020-smallCompactions-1406052479487] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:8), split_queue=0, merge_queue=0
2014-07-22 11:18:22,168 DEBUG [regionserver60020-smallCompactions-1406052479487] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 2000 blocking
2014-07-22 11:18:22,169 DEBUG [regionserver60020-smallCompactions-1406052479487] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 5 files of size 426820378 starting at candidate #0 after considering 6 permutations with 6 in ratio
2014-07-22 11:18:22,169 DEBUG [regionserver60020-smallCompactions-1406052479487] regionserver.HStore: 4d714c493433d51e92af518b12e607b2 - family: Initiating major compaction
2014-07-22 11:18:22,169 INFO  [regionserver60020-smallCompactions-1406052479487] regionserver.HRegion: Starting compaction on family in region usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.
2014-07-22 11:18:22,170 INFO  [regionserver60020-smallCompactions-1406052479487] regionserver.HStore: Starting compaction of 5 file(s) in family of usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/.tmp, totalSize=407.0m
2014-07-22 11:18:22,170 DEBUG [regionserver60020-smallCompactions-1406052479487] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/938a37a7348e42c2b27136f7e0ed1195, keycount=115345, bloomtype=ROW, size=82.1m, encoding=NONE, seqNum=161, earliestPutTs=1406052804204
2014-07-22 11:18:22,170 DEBUG [regionserver60020-smallCompactions-1406052479487] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/2e0b706d67674ffc90596a002621506c, keycount=93526, bloomtype=ROW, size=66.6m, encoding=NONE, seqNum=303, earliestPutTs=1406052881865
2014-07-22 11:18:22,170 DEBUG [regionserver60020-smallCompactions-1406052479487] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/ceb5d461a1f047bf8e6ba2a7be251a66, keycount=138507, bloomtype=ROW, size=98.6m, encoding=NONE, seqNum=516, earliestPutTs=1406052924809
2014-07-22 11:18:22,170 DEBUG [regionserver60020-smallCompactions-1406052479487] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/1f2296f0ee664f28854c35aa847d551e, keycount=127697, bloomtype=ROW, size=91.0m, encoding=NONE, seqNum=690, earliestPutTs=1406052969224
2014-07-22 11:18:22,170 DEBUG [regionserver60020-smallCompactions-1406052479487] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/6b415b0567c140a88fa3a9f591d5494d, keycount=96474, bloomtype=ROW, size=68.7m, encoding=NONE, seqNum=840, earliestPutTs=1406053000217
2014-07-22 11:18:22,217 DEBUG [regionserver60020-smallCompactions-1406052479487] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:18:22,383 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:18:22,495 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7886 synced till here 7879
2014-07-22 11:18:22,703 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053100415 with entries=75, filesize=68.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053102384
2014-07-22 11:18:24,512 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:18:24,670 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7980 synced till here 7965
2014-07-22 11:18:24,890 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053102384 with entries=94, filesize=76.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053104512
2014-07-22 11:18:26,900 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:18:26,920 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.
2014-07-22 11:18:26,921 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7., current region memstore size 256.5m
2014-07-22 11:18:27,095 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8073 synced till here 8064
2014-07-22 11:18:27,565 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:18:27,615 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053104512 with entries=93, filesize=74.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053106901
2014-07-22 11:18:29,290 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.
2014-07-22 11:18:29,643 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:18:29,983 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8186 synced till here 8143
2014-07-22 11:18:30,748 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053106901 with entries=113, filesize=94.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053109643
2014-07-22 11:18:31,770 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.
2014-07-22 11:18:32,266 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:18:32,285 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8314 synced till here 8292
2014-07-22 11:18:32,582 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053109643 with entries=128, filesize=79.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053112266
2014-07-22 11:18:34,662 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:18:34,835 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8404 synced till here 8396
2014-07-22 11:18:35,202 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053112266 with entries=90, filesize=71.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053114662
2014-07-22 11:18:36,816 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:18:36,931 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8488 synced till here 8477
2014-07-22 11:18:37,308 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053114662 with entries=84, filesize=71.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053116817
2014-07-22 11:18:38,882 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=974, memsize=260.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/.tmp/dae83432f6db4867af23fe606214cbf5
2014-07-22 11:18:38,905 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/.tmp/dae83432f6db4867af23fe606214cbf5 as hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/dae83432f6db4867af23fe606214cbf5
2014-07-22 11:18:39,041 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/dae83432f6db4867af23fe606214cbf5, entries=949210, sequenceid=974, filesize=67.7m
2014-07-22 11:18:39,042 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~260.7m/273365920, currentsize=255.2m/267635920 for region usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2. in 18614ms, sequenceid=974, compaction requested=false
2014-07-22 11:18:39,042 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e., current region memstore size 401.4m
2014-07-22 11:18:39,076 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:18:39,078 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.
2014-07-22 11:18:39,304 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8572 synced till here 8556
2014-07-22 11:18:39,769 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053116817 with entries=84, filesize=75.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053119077
2014-07-22 11:18:39,769 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053050196
2014-07-22 11:18:39,769 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053052235
2014-07-22 11:18:39,769 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053054444
2014-07-22 11:18:39,770 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053056878
2014-07-22 11:18:40,353 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:18:42,684 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:18:42,926 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8687 synced till here 8646
2014-07-22 11:18:43,434 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053119077 with entries=115, filesize=89.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053122684
2014-07-22 11:18:45,267 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:18:45,326 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8793 synced till here 8790
2014-07-22 11:18:45,492 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053122684 with entries=106, filesize=66.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053125267
2014-07-22 11:18:47,256 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2063, memsize=257.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/.tmp/4d3a25b91e5f49b9a967b8cb68c45c2c
2014-07-22 11:18:47,272 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/.tmp/4d3a25b91e5f49b9a967b8cb68c45c2c as hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/4d3a25b91e5f49b9a967b8cb68c45c2c
2014-07-22 11:18:47,430 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/4d3a25b91e5f49b9a967b8cb68c45c2c, entries=937100, sequenceid=2063, filesize=66.8m
2014-07-22 11:18:47,430 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~257.4m/269876160, currentsize=243.1m/254874720 for region usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7. in 20509ms, sequenceid=2063, compaction requested=true
2014-07-22 11:18:47,431 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:8), split_queue=0, merge_queue=0
2014-07-22 11:18:47,432 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e., current region memstore size 447.5m
2014-07-22 11:18:47,782 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:18:47,819 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8866 synced till here 8855
2014-07-22 11:18:47,978 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053125267 with entries=73, filesize=75.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053127786
2014-07-22 11:18:48,286 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.
2014-07-22 11:18:49,097 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:18:50,033 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:18:50,308 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8948 synced till here 8946
2014-07-22 11:18:50,324 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053127786 with entries=82, filesize=69.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053130034
2014-07-22 11:18:52,487 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:18:52,524 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9030 synced till here 9026
2014-07-22 11:18:53,013 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053130034 with entries=82, filesize=65.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053132488
2014-07-22 11:18:55,344 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:18:55,496 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9136 synced till here 9119
2014-07-22 11:18:55,830 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053132488 with entries=106, filesize=83.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053135345
2014-07-22 11:18:57,396 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:18:57,573 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9238 synced till here 9213
2014-07-22 11:18:57,787 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053135345 with entries=102, filesize=75.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053137396
2014-07-22 11:18:59,584 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:18:59,736 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9327 synced till here 9321
2014-07-22 11:18:59,930 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053137396 with entries=89, filesize=67.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053139585
2014-07-22 11:19:02,857 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:19:03,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9450 synced till here 9410
2014-07-22 11:19:04,132 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053139585 with entries=123, filesize=108.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053142857
2014-07-22 11:19:18,602 WARN  [regionserver60020] util.Sleeper: We slept 15443ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-22 11:19:18,607 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 12249ms
No GCs detected
2014-07-22 11:19:19,855 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:19:19,971 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9572 synced till here 9548
2014-07-22 11:19:21,409 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17086,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053142792,"queuetimems":1,"class":"HRegionServer","responsesize":15861,"method":"Multi"}
2014-07-22 11:19:21,409 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15791,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053144113,"queuetimems":31,"class":"HRegionServer","responsesize":9297,"method":"Multi"}
2014-07-22 11:19:21,409 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15784,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053144101,"queuetimems":501,"class":"HRegionServer","responsesize":2851,"method":"Multi"}
2014-07-22 11:19:21,410 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15802,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053144117,"queuetimems":13,"class":"HRegionServer","responsesize":7845,"method":"Multi"}
2014-07-22 11:19:21,410 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15756,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053144116,"queuetimems":27,"class":"HRegionServer","responsesize":3070,"method":"Multi"}
2014-07-22 11:19:21,422 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16169,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053143699,"queuetimems":290,"class":"HRegionServer","responsesize":4549,"method":"Multi"}
2014-07-22 11:19:21,427 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15767,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053144101,"queuetimems":500,"class":"HRegionServer","responsesize":1368,"method":"Multi"}
2014-07-22 11:19:21,867 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16939,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053144925,"queuetimems":308,"class":"HRegionServer","responsesize":506,"method":"Multi"}
2014-07-22 11:19:21,867 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18147,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053143714,"queuetimems":244,"class":"HRegionServer","responsesize":9294,"method":"Multi"}
2014-07-22 11:19:21,868 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16934,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053144930,"queuetimems":312,"class":"HRegionServer","responsesize":907,"method":"Multi"}
2014-07-22 11:19:21,868 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16926,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053144937,"queuetimems":317,"class":"HRegionServer","responsesize":1696,"method":"Multi"}
2014-07-22 11:19:21,869 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17071,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053144792,"queuetimems":462,"class":"HRegionServer","responsesize":3386,"method":"Multi"}
2014-07-22 11:19:21,882 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17063,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053144800,"queuetimems":390,"class":"HRegionServer","responsesize":4907,"method":"Multi"}
2014-07-22 11:19:21,884 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17074,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053144788,"queuetimems":524,"class":"HRegionServer","responsesize":5653,"method":"Multi"}
2014-07-22 11:19:22,056 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053142857 with entries=122, filesize=89.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053159856
2014-07-22 11:19:25,009 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20219,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053144789,"queuetimems":522,"class":"HRegionServer","responsesize":2927,"method":"Multi"}
2014-07-22 11:19:25,018 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20217,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053144792,"queuetimems":478,"class":"HRegionServer","responsesize":11196,"method":"Multi"}
2014-07-22 11:19:25,018 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20221,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053144788,"queuetimems":668,"class":"HRegionServer","responsesize":1147,"method":"Multi"}
2014-07-22 11:19:25,019 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20889,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053144119,"queuetimems":0,"class":"HRegionServer","responsesize":7537,"method":"Multi"}
2014-07-22 11:19:25,021 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20896,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053144113,"queuetimems":154,"class":"HRegionServer","responsesize":19584,"method":"Multi"}
2014-07-22 11:19:25,064 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21826,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053143182,"queuetimems":0,"class":"HRegionServer","responsesize":18330,"method":"Multi"}
2014-07-22 11:19:25,065 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20901,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053144108,"queuetimems":233,"class":"HRegionServer","responsesize":16798,"method":"Multi"}
2014-07-22 11:19:25,064 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19769,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053145270,"queuetimems":192,"class":"HRegionServer","responsesize":8863,"method":"Multi"}
2014-07-22 11:19:25,064 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19779,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053145260,"queuetimems":286,"class":"HRegionServer","responsesize":9434,"method":"Multi"}
2014-07-22 11:19:25,243 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19763,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053145275,"queuetimems":161,"class":"HRegionServer","responsesize":12529,"method":"Multi"}
2014-07-22 11:19:25,237 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19779,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053145258,"queuetimems":545,"class":"HRegionServer","responsesize":2927,"method":"Multi"}
2014-07-22 11:19:25,220 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19750,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053145259,"queuetimems":544,"class":"HRegionServer","responsesize":2113,"method":"Multi"}
2014-07-22 11:19:26,169 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20494,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053145674,"queuetimems":202,"class":"HRegionServer","responsesize":10841,"method":"Multi"}
2014-07-22 11:19:26,175 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20473,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053145702,"queuetimems":183,"class":"HRegionServer","responsesize":6166,"method":"Multi"}
2014-07-22 11:19:26,181 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20461,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053145703,"queuetimems":28,"class":"HRegionServer","responsesize":5170,"method":"Multi"}
2014-07-22 11:19:26,195 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20324,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053145870,"queuetimems":0,"class":"HRegionServer","responsesize":8060,"method":"Multi"}
2014-07-22 11:19:26,225 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20522,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053145702,"queuetimems":55,"class":"HRegionServer","responsesize":11272,"method":"Multi"}
2014-07-22 11:19:26,225 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20523,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053145702,"queuetimems":39,"class":"HRegionServer","responsesize":5551,"method":"Multi"}
2014-07-22 11:19:26,225 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20367,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053145857,"queuetimems":0,"class":"HRegionServer","responsesize":7999,"method":"Multi"}
2014-07-22 11:19:26,231 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20369,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053145862,"queuetimems":0,"class":"HRegionServer","responsesize":855,"method":"Multi"}
2014-07-22 11:19:26,231 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20530,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053145701,"queuetimems":204,"class":"HRegionServer","responsesize":5825,"method":"Multi"}
2014-07-22 11:19:26,232 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20370,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053145861,"queuetimems":0,"class":"HRegionServer","responsesize":5200,"method":"Multi"}
2014-07-22 11:19:26,225 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20523,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053145701,"queuetimems":219,"class":"HRegionServer","responsesize":8551,"method":"Multi"}
2014-07-22 11:19:26,280 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20607,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053145673,"queuetimems":528,"class":"HRegionServer","responsesize":9235,"method":"Multi"}
2014-07-22 11:19:28,218 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:19:28,221 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25558,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053142662,"queuetimems":0,"class":"HRegionServer","responsesize":16708,"method":"Multi"}
2014-07-22 11:19:28,271 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25258,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053142969,"queuetimems":7,"class":"HRegionServer","responsesize":16921,"method":"Multi"}
2014-07-22 11:19:28,273 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23351,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053144921,"queuetimems":304,"class":"HRegionServer","responsesize":18788,"method":"Multi"}
2014-07-22 11:19:28,273 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23463,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053144809,"queuetimems":255,"class":"HRegionServer","responsesize":21336,"method":"Multi"}
2014-07-22 11:19:28,361 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22686,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053145674,"queuetimems":387,"class":"HRegionServer","responsesize":21190,"method":"Multi"}
2014-07-22 11:19:28,361 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23556,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053144804,"queuetimems":353,"class":"HRegionServer","responsesize":19904,"method":"Multi"}
2014-07-22 11:19:28,385 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23101,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053145259,"queuetimems":535,"class":"HRegionServer","responsesize":7143,"method":"Multi"}
2014-07-22 11:19:28,412 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23596,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053144816,"queuetimems":236,"class":"HRegionServer","responsesize":14539,"method":"Multi"}
2014-07-22 11:19:28,527 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9680 synced till here 9649
2014-07-22 11:19:29,418 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25709,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053143707,"queuetimems":265,"class":"HRegionServer","responsesize":16941,"method":"Multi"}
2014-07-22 11:19:29,445 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26445,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053142991,"queuetimems":0,"class":"HRegionServer","responsesize":14846,"method":"Multi"}
2014-07-22 11:19:29,459 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25767,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053143691,"queuetimems":293,"class":"HRegionServer","responsesize":16920,"method":"Multi"}
2014-07-22 11:19:29,503 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25315,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053144104,"queuetimems":448,"class":"HRegionServer","responsesize":16713,"method":"Multi"}
2014-07-22 11:19:29,780 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053159856 with entries=108, filesize=84.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053168219
2014-07-22 11:19:31,768 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10346,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053161422,"queuetimems":58,"class":"HRegionServer","responsesize":20022,"method":"Multi"}
2014-07-22 11:19:32,382 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10593,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053161788,"queuetimems":0,"class":"HRegionServer","responsesize":18941,"method":"Multi"}
2014-07-22 11:19:32,383 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10703,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053161678,"queuetimems":0,"class":"HRegionServer","responsesize":22140,"method":"Multi"}
2014-07-22 11:19:32,382 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10958,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053161423,"queuetimems":159,"class":"HRegionServer","responsesize":19775,"method":"Multi"}
2014-07-22 11:19:33,559 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:19:34,106 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9814 synced till here 9808
2014-07-22 11:19:34,575 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12174,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053162070,"queuetimems":4,"class":"HRegionServer","responsesize":17110,"method":"Multi"}
2014-07-22 11:19:35,102 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053168219 with entries=134, filesize=75.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053173758
2014-07-22 11:19:39,186 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:19:39,733 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9928 synced till here 9895
2014-07-22 11:19:41,441 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2162, memsize=408.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/.tmp/9da9c1ff868e4a25a84ba3faf5547771
2014-07-22 11:19:41,456 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/.tmp/9da9c1ff868e4a25a84ba3faf5547771 as hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/9da9c1ff868e4a25a84ba3faf5547771
2014-07-22 11:19:41,484 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/9da9c1ff868e4a25a84ba3faf5547771, entries=1487120, sequenceid=2162, filesize=105.9m
2014-07-22 11:19:41,486 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~408.4m/428278240, currentsize=390.9m/409934480 for region usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e. in 62444ms, sequenceid=2162, compaction requested=true
2014-07-22 11:19:41,486 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:9), split_queue=0, merge_queue=0
2014-07-22 11:19:41,487 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2., current region memstore size 724.2m
2014-07-22 11:19:42,298 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.
2014-07-22 11:19:43,632 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053173758 with entries=114, filesize=106.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053179186
2014-07-22 11:19:43,632 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053059348
2014-07-22 11:19:43,632 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053078433
2014-07-22 11:19:43,632 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053080669
2014-07-22 11:19:45,054 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11272,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053173781,"queuetimems":1553,"class":"HRegionServer","responsesize":8878,"method":"Multi"}
2014-07-22 11:19:45,054 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11263,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053173790,"queuetimems":260,"class":"HRegionServer","responsesize":18477,"method":"Multi"}
2014-07-22 11:19:45,066 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11263,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053173790,"queuetimems":22,"class":"HRegionServer","responsesize":15838,"method":"Multi"}
2014-07-22 11:19:45,067 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12796,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053172270,"queuetimems":3724,"class":"HRegionServer","responsesize":14922,"method":"Multi"}
2014-07-22 11:19:45,082 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11245,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053173836,"queuetimems":1,"class":"HRegionServer","responsesize":17768,"method":"Multi"}
2014-07-22 11:19:45,082 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11322,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053173759,"queuetimems":1584,"class":"HRegionServer","responsesize":21733,"method":"Multi"}
2014-07-22 11:19:45,106 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10931,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053174174,"queuetimems":1,"class":"HRegionServer","responsesize":17273,"method":"Multi"}
2014-07-22 11:19:45,106 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12669,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053172397,"queuetimems":1502,"class":"HRegionServer","responsesize":22550,"method":"Multi"}
2014-07-22 11:19:45,116 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11343,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053173772,"queuetimems":1584,"class":"HRegionServer","responsesize":6775,"method":"Multi"}
2014-07-22 11:19:45,122 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11330,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053173790,"queuetimems":301,"class":"HRegionServer","responsesize":22614,"method":"Multi"}
2014-07-22 11:19:45,126 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10344,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053174781,"queuetimems":0,"class":"HRegionServer","responsesize":13072,"method":"Multi"}
2014-07-22 11:19:49,642 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11121,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053178520,"queuetimems":920,"class":"HRegionServer","responsesize":5159,"method":"Multi"}
2014-07-22 11:19:49,662 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11149,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053178512,"queuetimems":1162,"class":"HRegionServer","responsesize":2068,"method":"Multi"}
2014-07-22 11:19:49,662 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12715,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053176946,"queuetimems":247,"class":"HRegionServer","responsesize":12359,"method":"Multi"}
2014-07-22 11:19:49,663 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12718,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053176944,"queuetimems":283,"class":"HRegionServer","responsesize":17148,"method":"Multi"}
2014-07-22 11:19:49,672 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11148,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053178520,"queuetimems":1125,"class":"HRegionServer","responsesize":14322,"method":"Multi"}
2014-07-22 11:19:49,677 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12688,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053176988,"queuetimems":18,"class":"HRegionServer","responsesize":12566,"method":"Multi"}
2014-07-22 11:19:49,684 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13366,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053176317,"queuetimems":1,"class":"HRegionServer","responsesize":19372,"method":"Multi"}
2014-07-22 11:19:49,685 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17305,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053172379,"queuetimems":2018,"class":"HRegionServer","responsesize":16851,"method":"Multi"}
2014-07-22 11:19:49,690 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17316,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053172372,"queuetimems":2393,"class":"HRegionServer","responsesize":16993,"method":"Multi"}
2014-07-22 11:19:49,691 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10502,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053179189,"queuetimems":1567,"class":"HRegionServer","responsesize":5015,"method":"Multi"}
2014-07-22 11:19:49,697 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10394,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053179303,"queuetimems":1415,"class":"HRegionServer","responsesize":3212,"method":"Multi"}
2014-07-22 11:19:51,832 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:19:51,942 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10079 synced till here 10037
2014-07-22 11:19:52,916 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 11:19:55,048 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15797,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053179250,"queuetimems":1409,"class":"HRegionServer","responsesize":6449,"method":"Multi"}
2014-07-22 11:19:55,049 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15854,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053179193,"queuetimems":1365,"class":"HRegionServer","responsesize":5244,"method":"Multi"}
2014-07-22 11:19:55,276 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10146,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053185130,"queuetimems":496,"class":"HRegionServer","responsesize":1472,"method":"Multi"}
2014-07-22 11:19:55,277 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10161,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053185116,"queuetimems":1762,"class":"HRegionServer","responsesize":12681,"method":"Multi"}
2014-07-22 11:19:55,278 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10194,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053185083,"queuetimems":5209,"class":"HRegionServer","responsesize":2769,"method":"Multi"}
2014-07-22 11:19:55,280 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10213,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053185067,"queuetimems":5201,"class":"HRegionServer","responsesize":2514,"method":"Multi"}
2014-07-22 11:19:55,277 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10208,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053185068,"queuetimems":5199,"class":"HRegionServer","responsesize":1744,"method":"Multi"}
2014-07-22 11:19:55,298 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10192,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053185106,"queuetimems":5228,"class":"HRegionServer","responsesize":1126,"method":"Multi"}
2014-07-22 11:19:55,307 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10184,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053185122,"queuetimems":1682,"class":"HRegionServer","responsesize":8101,"method":"Multi"}
2014-07-22 11:19:55,308 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10241,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053185067,"queuetimems":5211,"class":"HRegionServer","responsesize":1119,"method":"Multi"}
2014-07-22 11:19:55,316 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10187,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053185129,"queuetimems":775,"class":"HRegionServer","responsesize":2447,"method":"Multi"}
2014-07-22 11:19:55,297 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10295,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053185002,"queuetimems":5233,"class":"HRegionServer","responsesize":4234,"method":"Multi"}
2014-07-22 11:19:55,324 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10198,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053185126,"queuetimems":1571,"class":"HRegionServer","responsesize":4432,"method":"Multi"}
2014-07-22 11:19:55,298 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10164,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053185134,"queuetimems":24,"class":"HRegionServer","responsesize":3730,"method":"Multi"}
2014-07-22 11:19:55,330 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10208,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053185121,"queuetimems":1705,"class":"HRegionServer","responsesize":7860,"method":"Multi"}
2014-07-22 11:19:55,331 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10205,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053185126,"queuetimems":867,"class":"HRegionServer","responsesize":4816,"method":"Multi"}
2014-07-22 11:19:57,381 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10583,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053186797,"queuetimems":4,"class":"HRegionServer","responsesize":8008,"method":"Multi"}
2014-07-22 11:19:57,395 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10823,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053186571,"queuetimems":0,"class":"HRegionServer","responsesize":6405,"method":"Multi"}
2014-07-22 11:19:57,399 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053179186 with entries=151, filesize=100.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053191832
2014-07-22 11:19:59,210 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14089,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053185120,"queuetimems":1758,"class":"HRegionServer","responsesize":8808,"method":"Multi"}
2014-07-22 11:19:59,210 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15826,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053183383,"queuetimems":3628,"class":"HRegionServer","responsesize":19862,"method":"Multi"}
2014-07-22 11:20:00,661 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:20:00,861 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14013,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053186847,"queuetimems":1,"class":"HRegionServer","responsesize":3202,"method":"Multi"}
2014-07-22 11:20:00,867 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14257,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053186610,"queuetimems":1,"class":"HRegionServer","responsesize":2812,"method":"Multi"}
2014-07-22 11:20:00,902 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11248,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053189644,"queuetimems":1985,"class":"HRegionServer","responsesize":7762,"method":"Multi"}
2014-07-22 11:20:00,902 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15781,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053185107,"queuetimems":2601,"class":"HRegionServer","responsesize":21968,"method":"Multi"}
2014-07-22 11:20:00,902 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13243,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053187637,"queuetimems":0,"class":"HRegionServer","responsesize":8521,"method":"Multi"}
2014-07-22 11:20:00,902 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14120,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053186769,"queuetimems":1,"class":"HRegionServer","responsesize":3348,"method":"Multi"}
2014-07-22 11:20:00,903 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15879,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053185009,"queuetimems":5157,"class":"HRegionServer","responsesize":21445,"method":"Multi"}
2014-07-22 11:20:00,902 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14051,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053186837,"queuetimems":1,"class":"HRegionServer","responsesize":7297,"method":"Multi"}
2014-07-22 11:20:00,903 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14166,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053186722,"queuetimems":9,"class":"HRegionServer","responsesize":6974,"method":"Multi"}
2014-07-22 11:20:00,902 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14707,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053186181,"queuetimems":4,"class":"HRegionServer","responsesize":20635,"method":"Multi"}
2014-07-22 11:20:01,544 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10174 synced till here 10155
2014-07-22 11:20:02,378 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12713,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053189664,"queuetimems":1328,"class":"HRegionServer","responsesize":8390,"method":"Multi"}
2014-07-22 11:20:02,378 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12715,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053189662,"queuetimems":1915,"class":"HRegionServer","responsesize":17963,"method":"Multi"}
2014-07-22 11:20:02,378 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23185,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053179192,"queuetimems":1373,"class":"HRegionServer","responsesize":19099,"method":"Multi"}
2014-07-22 11:20:02,378 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12691,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053189686,"queuetimems":397,"class":"HRegionServer","responsesize":19701,"method":"Multi"}
2014-07-22 11:20:02,938 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053191832 with entries=95, filesize=77.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053200662
2014-07-22 11:20:03,972 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14295,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053189677,"queuetimems":470,"class":"HRegionServer","responsesize":14331,"method":"Multi"}
2014-07-22 11:20:04,072 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14403,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053189669,"queuetimems":1259,"class":"HRegionServer","responsesize":21121,"method":"Multi"}
2014-07-22 11:20:04,478 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14802,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053189675,"queuetimems":1079,"class":"HRegionServer","responsesize":18777,"method":"Multi"}
2014-07-22 11:20:05,375 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14267,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053191104,"queuetimems":9,"class":"HRegionServer","responsesize":19005,"method":"Multi"}
2014-07-22 11:20:05,376 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10071,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053195300,"queuetimems":2952,"class":"HRegionServer","responsesize":13209,"method":"Multi"}
2014-07-22 11:20:05,376 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10043,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053195328,"queuetimems":2206,"class":"HRegionServer","responsesize":16323,"method":"Multi"}
2014-07-22 11:20:05,376 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10063,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053195308,"queuetimems":2945,"class":"HRegionServer","responsesize":6740,"method":"Multi"}
2014-07-22 11:20:06,194 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10870,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053195324,"queuetimems":2258,"class":"HRegionServer","responsesize":17353,"method":"Multi"}
2014-07-22 11:20:06,612 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 11:20:06,636 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11315,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053195321,"queuetimems":2297,"class":"HRegionServer","responsesize":21887,"method":"Multi"}
2014-07-22 11:20:07,224 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11894,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053195330,"queuetimems":1703,"class":"HRegionServer","responsesize":21486,"method":"Multi"}
2014-07-22 11:20:07,225 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17552,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053189672,"queuetimems":1184,"class":"HRegionServer","responsesize":22854,"method":"Multi"}
2014-07-22 11:20:07,236 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16959,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053189677,"queuetimems":571,"class":"HRegionServer","responsesize":22134,"method":"Multi"}
2014-07-22 11:20:08,610 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10295 synced till here 10253
2014-07-22 11:21:26,144 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 91491ms for sessionid 0x475f4123560001, closing socket connection and attempting reconnect
2014-07-22 11:21:26,145 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 97141ms for sessionid 0x1475f411c9d0002, closing socket connection and attempting reconnect
2014-07-22 11:21:26,154 WARN  [ResponseProcessor for block blk_7017692894291479755_269923] hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block blk_7017692894291479755_269923java.net.SocketTimeoutException: 66000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/9.1.143.58:44660 remote=/9.1.143.58:50010]
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)
	at java.io.DataInputStream.readFully(DataInputStream.java:195)
	at java.io.DataInputStream.readLong(DataInputStream.java:416)
	at org.apache.hadoop.hdfs.protocol.DataTransferProtocol$PipelineAck.readFields(DataTransferProtocol.java:124)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:3161)

2014-07-22 11:21:26,154 WARN  [DataStreamer for file /hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/.tmp/b0d23469b2c84d4a9123950c3add0b82 block blk_7017692894291479755_269923] hdfs.DFSClient: Error Recovery for blk_7017692894291479755_269923 bad datanode[0] 9.1.143.58:50010
2014-07-22 11:21:26,160 WARN  [regionserver60020] util.Sleeper: We slept 80406ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-22 11:21:26,161 WARN  [ResponseProcessor for block blk_1861640772741817804_269940] hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block blk_1861640772741817804_269940java.net.SocketTimeoutException: 66000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/9.1.143.58:44686 remote=/9.1.143.58:50010]
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)
	at java.io.DataInputStream.readFully(DataInputStream.java:195)
	at java.io.DataInputStream.readLong(DataInputStream.java:416)
	at org.apache.hadoop.hdfs.protocol.DataTransferProtocol$PipelineAck.readFields(DataTransferProtocol.java:124)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:3161)

2014-07-22 11:21:26,154 WARN  [regionserver60020.compactionChecker] util.Sleeper: We slept 87152ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-22 11:21:26,161 WARN  [ResponseProcessor for block blk_606387359051991351_269935] hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block blk_606387359051991351_269935java.net.SocketTimeoutException: 66000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/9.1.143.58:44678 remote=/9.1.143.58:50010]
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)
	at java.io.DataInputStream.readFully(DataInputStream.java:195)
	at java.io.DataInputStream.readLong(DataInputStream.java:416)
	at org.apache.hadoop.hdfs.protocol.DataTransferProtocol$PipelineAck.readFields(DataTransferProtocol.java:124)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:3161)

2014-07-22 11:21:26,161 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 block blk_1861640772741817804_269940] hdfs.DFSClient: Error Recovery for blk_1861640772741817804_269940 bad datanode[0] 9.1.143.58:50010
2014-07-22 11:21:26,162 WARN  [DataStreamer for file /hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/.tmp/57945128ea204482937c3c0e7a47f397 block blk_606387359051991351_269935] hdfs.DFSClient: Error Recovery for blk_606387359051991351_269935 bad datanode[0] 9.1.143.58:50010
2014-07-22 11:21:26,162 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 97147ms for sessionid 0x475f4123560000, closing socket connection and attempting reconnect
2014-07-22 11:21:26,163 WARN  [DataStreamer for file /hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/.tmp/b0d23469b2c84d4a9123950c3add0b82 block blk_7017692894291479755_269923] hdfs.DFSClient: Error Recovery for block blk_7017692894291479755_269923 in pipeline 9.1.143.58:50010, 9.1.143.59:50010: bad datanode 9.1.143.58:50010
2014-07-22 11:21:26,163 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 77087ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=77385ms
2014-07-22 11:21:26,163 WARN  [ResponseProcessor for block blk_797617860964781395_269622] hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block blk_797617860964781395_269622java.net.SocketTimeoutException: 66000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/9.1.143.58:43964 remote=/9.1.143.58:50010]
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)
	at java.io.DataInputStream.readFully(DataInputStream.java:195)
	at java.io.DataInputStream.readLong(DataInputStream.java:416)
	at org.apache.hadoop.hdfs.protocol.DataTransferProtocol$PipelineAck.readFields(DataTransferProtocol.java:124)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:3161)

2014-07-22 11:21:26,163 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Error Recovery for blk_797617860964781395_269622 bad datanode[0] 9.1.143.58:50010
2014-07-22 11:21:26,164 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Error Recovery for block blk_797617860964781395_269622 in pipeline 9.1.143.58:50010, 9.1.143.59:50010: bad datanode 9.1.143.58:50010
2014-07-22 11:21:26,163 WARN  [DataStreamer for file /hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/.tmp/57945128ea204482937c3c0e7a47f397 block blk_606387359051991351_269935] hdfs.DFSClient: Error Recovery for block blk_606387359051991351_269935 in pipeline 9.1.143.58:50010, 9.1.143.59:50010: bad datanode 9.1.143.58:50010
2014-07-22 11:21:26,159 WARN  [regionserver60020.periodicFlusher] util.Sleeper: We slept 87152ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-22 11:21:26,163 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 block blk_1861640772741817804_269940] hdfs.DFSClient: Error Recovery for block blk_1861640772741817804_269940 in pipeline 9.1.143.58:50010, 9.1.143.59:50010: bad datanode 9.1.143.58:50010
2014-07-22 11:21:26,246 FATAL [regionserver60020] regionserver.HRegionServer: ABORTING region server sceplus-vm48.almaden.ibm.com,60020,1406052408577: org.apache.hadoop.hbase.YouAreDeadException: Server REPORT rejected; currently processing sceplus-vm48.almaden.ibm.com,60020,1406052408577 as dead server
org.apache.hadoop.hbase.YouAreDeadException: org.apache.hadoop.hbase.YouAreDeadException: Server REPORT rejected; currently processing sceplus-vm48.almaden.ibm.com,60020,1406052408577 as dead server
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:534)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:95)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:79)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.getRemoteException(ProtobufUtil.java:285)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.tryRegionServerReport(HRegionServer.java:1065)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:901)
	at java.lang.Thread.run(Thread.java:701)
Caused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException: org.apache.hadoop.hbase.YouAreDeadException: Server REPORT rejected; currently processing sceplus-vm48.almaden.ibm.com,60020,1406052408577 as dead server
	at org.apache.hadoop.hbase.master.ServerManager.checkIsDead(ServerManager.java:369)
	at org.apache.hadoop.hbase.master.ServerManager.regionServerReport(ServerManager.java:274)
	at org.apache.hadoop.hbase.master.HMaster.regionServerReport(HMaster.java:1357)
	at org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionServerStatusService$2.callBlockingMethod(RegionServerStatusProtos.java:5087)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2012)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:98)
	at org.apache.hadoop.hbase.ipc.FifoRpcScheduler$1.run(FifoRpcScheduler.java:73)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)

	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1453)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1657)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1715)
	at org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionServerStatusService$BlockingStub.regionServerReport(RegionServerStatusProtos.java:5414)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.tryRegionServerReport(HRegionServer.java:1063)
	... 2 more
2014-07-22 11:21:26,247 FATAL [regionserver60020] regionserver.HRegionServer: RegionServer abort: loaded coprocessors are: [org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint]
2014-07-22 11:21:26,297 INFO  [regionserver60020] regionserver.HRegionServer: STOPPED: org.apache.hadoop.hbase.YouAreDeadException: Server REPORT rejected; currently processing sceplus-vm48.almaden.ibm.com,60020,1406052408577 as dead server
2014-07-22 11:21:26,297 INFO  [regionserver60020] ipc.RpcServer: Stopping server on 60020
2014-07-22 11:21:26,298 INFO  [RpcServer.listener,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: stopping
2014-07-22 11:21:26,305 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: stopped
2014-07-22 11:21:26,305 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: stopping
2014-07-22 11:21:26,306 INFO  [regionserver60020] regionserver.SplitLogWorker: Sending interrupt to stop the worker thread
2014-07-22 11:21:26,306 INFO  [regionserver60020] regionserver.HRegionServer: Stopping infoServer
2014-07-22 11:21:27,286 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Failed recovery attempt #0 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_797617860964781395_269622 has out of date GS 269622 found 269942, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-22 11:21:27,324 INFO  [regionserver60020] mortbay.log: Stopped SelectChannelConnector@0.0.0.0:60030
2014-07-22 11:21:27,318 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-22 11:21:27,287 DEBUG [RpcServer.handler=10,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,286 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1406052408577] regionserver.SplitLogWorker: SplitLogWorker interrupted while waiting for task, exiting: java.lang.InterruptedException
2014-07-22 11:21:27,325 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Error Recovery for block blk_797617860964781395_269622 failed  because recovery from primary datanode 9.1.143.59:50010 failed 1 times.  Pipeline was 9.1.143.58:50010, 9.1.143.59:50010. Will retry...
2014-07-22 11:21:27,325 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1406052408577] regionserver.SplitLogWorker: SplitLogWorker sceplus-vm48.almaden.ibm.com,60020,1406052408577 exiting
2014-07-22 11:21:27,325 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-07-22 11:21:27,340 DEBUG [RpcServer.handler=12,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,340 DEBUG [RpcServer.handler=0,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,341 DEBUG [RpcServer.handler=45,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,341 DEBUG [RpcServer.handler=33,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,341 DEBUG [RpcServer.handler=28,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,341 DEBUG [RpcServer.handler=23,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,342 DEBUG [RpcServer.handler=19,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,342 DEBUG [RpcServer.handler=47,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,342 DEBUG [RpcServer.handler=8,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,342 DEBUG [RpcServer.handler=31,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,343 DEBUG [RpcServer.handler=32,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,343 DEBUG [RpcServer.handler=16,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,343 DEBUG [RpcServer.handler=39,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,344 DEBUG [RpcServer.handler=44,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,344 DEBUG [RpcServer.handler=38,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,378 DEBUG [RpcServer.handler=17,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,378 DEBUG [RpcServer.handler=49,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,379 DEBUG [RpcServer.handler=41,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,381 DEBUG [RpcServer.handler=42,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,381 DEBUG [RpcServer.handler=35,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,381 DEBUG [RpcServer.handler=13,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,381 DEBUG [RpcServer.handler=7,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,382 DEBUG [RpcServer.handler=37,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,382 DEBUG [RpcServer.handler=4,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,382 DEBUG [RpcServer.handler=26,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,388 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x1475f411c9d0002 has expired, closing socket connection
2014-07-22 11:21:27,388 FATAL [regionserver60020-EventThread] regionserver.HRegionServer: ABORTING region server sceplus-vm48.almaden.ibm.com,60020,1406052408577: regionserver:60020-0x1475f411c9d0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase regionserver:60020-0x1475f411c9d0002 received expired from ZooKeeper, aborting
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:401)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:319)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
2014-07-22 11:21:27,432 FATAL [regionserver60020-EventThread] regionserver.HRegionServer: RegionServer abort: loaded coprocessors are: [org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint]
2014-07-22 11:21:27,432 DEBUG [RpcServer.handler=11,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,434 DEBUG [RpcServer.handler=34,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,436 DEBUG [RpcServer.handler=48,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,437 DEBUG [RpcServer.handler=3,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,437 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Opening socket connection to server slave1/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-22 11:21:27,438 DEBUG [RpcServer.handler=43,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,438 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Opening socket connection to server slave1/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-22 11:21:27,438 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Socket connection established to slave1/9.1.143.59:2181, initiating session
2014-07-22 11:21:27,439 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Socket connection established to slave1/9.1.143.59:2181, initiating session
2014-07-22 11:21:27,439 INFO  [regionserver60020-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-07-22 11:21:27,439 DEBUG [RpcServer.handler=40,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,439 DEBUG [RpcServer.handler=36,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,439 DEBUG [RpcServer.handler=25,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,440 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x475f4123560001 has expired, closing socket connection
2014-07-22 11:21:27,456 WARN  [regionserver60020-EventThread] client.HConnectionManager$HConnectionImplementation: This client just lost it's session with ZooKeeper, closing it. It will be recreated next time someone needs it
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:401)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:319)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
2014-07-22 11:21:27,498 INFO  [regionserver60020-EventThread] client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x475f4123560000
2014-07-22 11:21:27,498 INFO  [regionserver60020-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-07-22 11:21:27,456 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x475f4123560000 has expired, closing socket connection
2014-07-22 11:21:27,440 WARN  [regionserver60020-EventThread] client.HConnectionManager$HConnectionImplementation: This client just lost it's session with ZooKeeper, closing it. It will be recreated next time someone needs it
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:401)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:319)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
2014-07-22 11:21:27,499 INFO  [regionserver60020-EventThread] client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x475f4123560001
2014-07-22 11:21:27,440 DEBUG [RpcServer.handler=14,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,499 DEBUG [RpcServer.handler=21,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,499 INFO  [regionserver60020-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-07-22 11:21:27,500 DEBUG [RpcServer.handler=46,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,500 DEBUG [RpcServer.handler=20,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,500 DEBUG [RpcServer.handler=22,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,500 DEBUG [RpcServer.handler=2,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,501 DEBUG [RpcServer.handler=29,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,501 DEBUG [RpcServer.handler=9,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:27,945 WARN  [regionserver60020-WAL.AsyncSyncer0] hdfs.DFSClient: Unable to persist blocks in hflush for /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053200662
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053200662 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.fsync(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.fsync(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.fsync(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3930)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:27,945 WARN  [regionserver60020-WAL.AsyncSyncer0] hdfs.DFSClient: Error while syncing
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053200662 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.fsync(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.fsync(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.fsync(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3930)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:27,951 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053200662 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.fsync(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.fsync(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.fsync(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3930)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:27,951 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053200662 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:27,952 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:28,012 ERROR [regionserver60020.logRoller] wal.FSHLog: Failed close of HLog writer
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:28,024 WARN  [regionserver60020.logRoller] wal.FSHLog: Riding over HLog close failure! error count=1
2014-07-22 11:21:28,045 DEBUG [RpcServer.handler=13,port=60020] regionserver.HRegion: rollbackMemstore rolled back 30 keyvalues from start:0 to end:3
2014-07-22 11:21:28,053 DEBUG [RpcServer.handler=5,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:28,056 DEBUG [RpcServer.handler=18,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:28,079 FATAL [regionserver60020.logRoller] regionserver.HRegionServer: ABORTING region server sceplus-vm48.almaden.ibm.com,60020,1406052408577: IOE in log roller
java.io.FileNotFoundException: File does not exist: hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053200662
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:558)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog.rollWriter(FSHLog.java:576)
	at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:97)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:28,086 FATAL [regionserver60020.logRoller] regionserver.HRegionServer: RegionServer abort: loaded coprocessors are: [org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint]
2014-07-22 11:21:28,099 INFO  [regionserver60020.logRoller] regionserver.LogRoller: LogRoller exiting.
2014-07-22 11:21:28,580 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: rollbackMemstore rolled back 80 keyvalues from start:0 to end:8
2014-07-22 11:21:28,582 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: rollbackMemstore rolled back 170 keyvalues from start:0 to end:17
2014-07-22 11:21:28,647 DEBUG [RpcServer.handler=30,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-22 11:21:28,659 DEBUG [RpcServer.handler=38,port=60020] regionserver.HRegion: rollbackMemstore rolled back 400 keyvalues from start:0 to end:40
2014-07-22 11:21:28,661 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: rollbackMemstore rolled back 400 keyvalues from start:0 to end:40
2014-07-22 11:21:28,663 DEBUG [RpcServer.handler=41,port=60020] regionserver.HRegion: rollbackMemstore rolled back 110 keyvalues from start:0 to end:11
2014-07-22 11:21:28,701 DEBUG [RpcServer.handler=46,port=60020] regionserver.HRegion: rollbackMemstore rolled back 290 keyvalues from start:0 to end:29
2014-07-22 11:21:28,777 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: rollbackMemstore rolled back 1310 keyvalues from start:0 to end:131
2014-07-22 11:21:28,781 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: rollbackMemstore rolled back 790 keyvalues from start:0 to end:79
2014-07-22 11:21:28,788 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: rollbackMemstore rolled back 270 keyvalues from start:0 to end:27
2014-07-22 11:21:28,793 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: rollbackMemstore rolled back 570 keyvalues from start:0 to end:57
2014-07-22 11:21:28,819 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: rollbackMemstore rolled back 880 keyvalues from start:0 to end:88
2014-07-22 11:21:28,827 DEBUG [RpcServer.handler=7,port=60020] regionserver.HRegion: rollbackMemstore rolled back 690 keyvalues from start:0 to end:69
2014-07-22 11:21:28,830 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":86113,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053202717,"queuetimems":1144,"class":"HRegionServer","responsesize":44290,"method":"Multi"}
2014-07-22 11:21:28,832 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11221 service: ClientService methodName: Multi size: 804.4k connection: 9.1.143.53:38310: output error
2014-07-22 11:21:28,848 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:28,848 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: rollbackMemstore rolled back 1050 keyvalues from start:0 to end:105
2014-07-22 11:21:28,886 WARN  [regionserver60020-WAL.AsyncSyncer0] hdfs.DFSClient: Unable to persist blocks in hflush for /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.fsync(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.fsync(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.fsync(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3930)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:28,887 WARN  [regionserver60020-WAL.AsyncSyncer0] hdfs.DFSClient: Error while syncing
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.fsync(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.fsync(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.fsync(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3930)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:28,886 WARN  [regionserver60020-WAL.AsyncSyncer1] wal.FSHLog: HDFS pipeline error detected. Found 1 replicas but expecting no less than 2 replicas.  Requesting close of hlog.
2014-07-22 11:21:28,889 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.fsync(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.fsync(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.fsync(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3930)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:28,910 WARN  [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: HDFS pipeline error detected. Found 1 replicas but expecting no less than 2 replicas.  Requesting close of hlog.
2014-07-22 11:21:28,922 DEBUG [RpcServer.handler=23,port=60020] regionserver.HRegion: rollbackMemstore rolled back 1300 keyvalues from start:0 to end:130
2014-07-22 11:21:28,926 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: rollbackMemstore rolled back 2560 keyvalues from start:0 to end:256
2014-07-22 11:21:28,963 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: rollbackMemstore rolled back 690 keyvalues from start:0 to end:69
2014-07-22 11:21:28,980 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: rollbackMemstore rolled back 1420 keyvalues from start:0 to end:142
2014-07-22 11:21:29,008 DEBUG [RpcServer.handler=37,port=60020] regionserver.HRegion: rollbackMemstore rolled back 690 keyvalues from start:0 to end:69
2014-07-22 11:21:29,058 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: rollbackMemstore rolled back 2300 keyvalues from start:0 to end:230
2014-07-22 11:21:29,139 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5600 keyvalues from start:0 to end:560
2014-07-22 11:21:29,185 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: rollbackMemstore rolled back 2490 keyvalues from start:0 to end:249
2014-07-22 11:21:29,189 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: rollbackMemstore rolled back 7180 keyvalues from start:0 to end:718
2014-07-22 11:21:29,230 DEBUG [RpcServer.handler=39,port=60020] regionserver.HRegion: rollbackMemstore rolled back 4430 keyvalues from start:0 to end:443
2014-07-22 11:21:29,255 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6100 keyvalues from start:0 to end:610
2014-07-22 11:21:29,308 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6660 keyvalues from start:0 to end:666
2014-07-22 11:21:29,311 DEBUG [RpcServer.handler=19,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5730 keyvalues from start:0 to end:573
2014-07-22 11:21:29,355 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: rollbackMemstore rolled back 3390 keyvalues from start:0 to end:339
2014-07-22 11:21:29,370 INFO  [RpcServer.handler=6,port=60020] regionserver.HRegion: Interrupted while waiting for a lock
2014-07-22 11:21:29,382 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: rollbackMemstore rolled back 3460 keyvalues from start:0 to end:346
2014-07-22 11:21:29,417 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6550 keyvalues from start:0 to end:655
2014-07-22 11:21:29,428 INFO  [RpcServer.handler=15,port=60020] regionserver.HRegion: Interrupted while waiting for a lock
2014-07-22 11:21:29,428 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: rollbackMemstore rolled back 7930 keyvalues from start:0 to end:793
2014-07-22 11:21:29,470 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5950 keyvalues from start:0 to end:595
2014-07-22 11:21:29,662 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6710 keyvalues from start:0 to end:671
2014-07-22 11:21:29,681 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5510 keyvalues from start:0 to end:551
2014-07-22 11:21:29,695 INFO  [RpcServer.handler=24,port=60020] regionserver.HRegion: Interrupted while waiting for a lock
2014-07-22 11:21:30,009 INFO  [RpcServer.handler=1,port=60020] regionserver.HRegion: Interrupted while waiting for a lock
2014-07-22 11:21:30,026 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: rollbackMemstore rolled back 9310 keyvalues from start:0 to end:931
2014-07-22 11:21:30,050 INFO  [RpcServer.handler=27,port=60020] regionserver.HRegion: Interrupted while waiting for a lock
2014-07-22 11:21:30,117 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: rollbackMemstore rolled back 8010 keyvalues from start:0 to end:801
2014-07-22 11:21:30,164 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: rollbackMemstore rolled back 13330 keyvalues from start:0 to end:1333
2014-07-22 11:21:30,196 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: rollbackMemstore rolled back 12700 keyvalues from start:0 to end:1270
2014-07-22 11:21:30,254 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:30,364 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:30,365 WARN  [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: HDFS pipeline error detected. Found 1 replicas but expecting no less than 2 replicas.  Requesting close of hlog.
2014-07-22 11:21:30,371 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":85349,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053205021,"queuetimems":0,"class":"HRegionServer","responsesize":185912,"method":"Multi"}
2014-07-22 11:21:30,372 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:30,383 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:30,384 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":85344,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053205026,"queuetimems":0,"class":"HRegionServer","responsesize":282486,"method":"Multi"}
2014-07-22 11:21:30,384 WARN  [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: HDFS pipeline error detected. Found 1 replicas but expecting no less than 2 replicas.  Requesting close of hlog.
2014-07-22 11:21:30,385 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11269 service: ClientService methodName: Multi size: 397.3k connection: 9.1.143.53:38310: output error
2014-07-22 11:21:30,385 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:30,434 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:30,435 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:30,435 WARN  [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: HDFS pipeline error detected. Found 1 replicas but expecting no less than 2 replicas.  Requesting close of hlog.
2014-07-22 11:21:30,438 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11264 service: ClientService methodName: Multi size: 252.4k connection: 9.1.143.53:38310: output error
2014-07-22 11:21:30,438 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:30,451 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:30,456 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:30,456 WARN  [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Too many consecutive RollWriter requests, it's a sign of the total number of live datanodes is lower than the tolerable replicas.
2014-07-22 11:21:30,473 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":85909,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053204562,"queuetimems":0,"class":"HRegionServer","responsesize":422942,"method":"Multi"}
2014-07-22 11:21:30,520 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:30,520 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:30,522 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: rollbackMemstore rolled back 11570 keyvalues from start:0 to end:1157
2014-07-22 11:21:30,525 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:30,525 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:30,527 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:30,529 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:30,527 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":85500,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053205024,"queuetimems":0,"class":"HRegionServer","responsesize":664985,"method":"Multi"}
2014-07-22 11:21:30,547 DEBUG [RpcServer.handler=30,port=60020] regionserver.HRegion: rollbackMemstore rolled back 1660 keyvalues from start:0 to end:166
2014-07-22 11:21:30,548 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: rollbackMemstore rolled back 13100 keyvalues from start:0 to end:1310
2014-07-22 11:21:30,571 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: rollbackMemstore rolled back 1960 keyvalues from start:0 to end:196
2014-07-22 11:21:30,646 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":86090,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053204535,"queuetimems":8,"class":"HRegionServer","responsesize":1082747,"method":"Multi"}
2014-07-22 11:21:30,646 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11243 service: ClientService methodName: Multi size: 608.3k connection: 9.1.143.53:38310: output error
2014-07-22 11:21:30,646 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:30,661 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11265 service: ClientService methodName: Multi size: 925.5k connection: 9.1.143.53:38310: output error
2014-07-22 11:21:30,661 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:30,668 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11240 service: ClientService methodName: Multi size: 1.5m connection: 9.1.143.53:38310: output error
2014-07-22 11:21:30,668 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:30,744 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: rollbackMemstore rolled back 13480 keyvalues from start:0 to end:1348
2014-07-22 11:21:30,802 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":86665,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053204081,"queuetimems":719,"class":"HRegionServer","responsesize":83821,"method":"Multi"}
2014-07-22 11:21:30,817 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11233 service: ClientService methodName: Multi size: 1.5m connection: 9.1.143.53:38310: output error
2014-07-22 11:21:30,817 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:30,876 DEBUG [RpcServer.handler=5,port=60020] regionserver.HRegion: rollbackMemstore rolled back 2760 keyvalues from start:0 to end:276
2014-07-22 11:21:31,076 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":87075,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053203973,"queuetimems":1340,"class":"HRegionServer","responsesize":140189,"method":"Multi"}
2014-07-22 11:21:31,089 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:31,089 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:31,090 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11225 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:38310: output error
2014-07-22 11:21:31,090 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:31,125 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: rollbackMemstore rolled back 9060 keyvalues from start:0 to end:906
2014-07-22 11:21:31,194 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":88644,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053202378,"queuetimems":917,"class":"HRegionServer","responsesize":425307,"method":"Multi"}
2014-07-22 11:21:31,233 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11218 service: ClientService methodName: Multi size: 2.0m connection: 9.1.143.53:38310: output error
2014-07-22 11:21:31,233 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:31,344 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:31,351 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:31,383 DEBUG [RpcServer.handler=39,port=60020] regionserver.HRegion: rollbackMemstore rolled back 4480 keyvalues from start:0 to end:448
2014-07-22 11:21:31,526 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:31,534 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:31,565 DEBUG [RpcServer.handler=46,port=60020] regionserver.HRegion: rollbackMemstore rolled back 300 keyvalues from start:0 to end:30
2014-07-22 11:21:31,580 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":84309,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053207271,"queuetimems":764,"class":"HRegionServer","responsesize":1380,"method":"Multi"}
2014-07-22 11:21:31,580 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11328 service: ClientService methodName: Multi size: 3.8k connection: 9.1.143.53:38310: output error
2014-07-22 11:21:31,580 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:32,010 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,011 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,040 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":87000,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053205038,"queuetimems":4,"class":"HRegionServer","responsesize":277804,"method":"Multi"}
2014-07-22 11:21:32,038 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,116 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,127 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11263 service: ClientService methodName: Multi size: 1.4m connection: 9.1.143.53:38310: output error
2014-07-22 11:21:32,127 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:32,147 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":87555,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053204566,"queuetimems":0,"class":"HRegionServer","responsesize":409808,"method":"Multi"}
2014-07-22 11:21:32,148 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":88050,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053204076,"queuetimems":888,"class":"HRegionServer","responsesize":267552,"method":"Multi"}
2014-07-22 11:21:32,150 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11245 service: ClientService methodName: Multi size: 1.1m connection: 9.1.143.53:38310: output error
2014-07-22 11:21:32,150 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:32,177 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11236 service: ClientService methodName: Multi size: 1.4m connection: 9.1.143.53:38310: output error
2014-07-22 11:21:32,177 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:32,198 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: rollbackMemstore rolled back 420 keyvalues from start:0 to end:42
2014-07-22 11:21:32,255 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,256 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,260 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,260 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,261 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,261 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,262 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,285 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,324 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: rollbackMemstore rolled back 350 keyvalues from start:0 to end:35
2014-07-22 11:21:32,352 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: rollbackMemstore rolled back 150 keyvalues from start:0 to end:15
2014-07-22 11:21:32,355 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: rollbackMemstore rolled back 570 keyvalues from start:0 to end:57
2014-07-22 11:21:32,383 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,383 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,389 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,394 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: rollbackMemstore rolled back 2570 keyvalues from start:0 to end:257
2014-07-22 11:21:32,405 DEBUG [RpcServer.handler=37,port=60020] regionserver.HRegion: rollbackMemstore rolled back 630 keyvalues from start:0 to end:63
2014-07-22 11:21:32,468 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: rollbackMemstore rolled back 790 keyvalues from start:0 to end:79
2014-07-22 11:21:32,478 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,515 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: rollbackMemstore rolled back 750 keyvalues from start:0 to end:75
2014-07-22 11:21:32,629 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,643 DEBUG [RpcServer.handler=23,port=60020] regionserver.HRegion: rollbackMemstore rolled back 1290 keyvalues from start:0 to end:129
2014-07-22 11:21:32,641 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: rollbackMemstore rolled back 1280 keyvalues from start:0 to end:128
2014-07-22 11:21:32,629 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,703 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,703 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,707 DEBUG [RpcServer.handler=38,port=60020] regionserver.HRegion: rollbackMemstore rolled back 220 keyvalues from start:0 to end:22
2014-07-22 11:21:32,719 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,728 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,729 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,729 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,727 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: rollbackMemstore rolled back 730 keyvalues from start:0 to end:73
2014-07-22 11:21:32,734 DEBUG [RpcServer.handler=7,port=60020] regionserver.HRegion: rollbackMemstore rolled back 750 keyvalues from start:0 to end:75
2014-07-22 11:21:32,749 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,754 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: rollbackMemstore rolled back 940 keyvalues from start:0 to end:94
2014-07-22 11:21:32,768 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,759 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: rollbackMemstore rolled back 1320 keyvalues from start:0 to end:132
2014-07-22 11:21:32,779 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: rollbackMemstore rolled back 2100 keyvalues from start:0 to end:210
2014-07-22 11:21:32,782 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,782 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,858 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: rollbackMemstore rolled back 4170 keyvalues from start:0 to end:417
2014-07-22 11:21:32,902 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,902 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: rollbackMemstore rolled back 3200 keyvalues from start:0 to end:320
2014-07-22 11:21:32,902 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: rollbackMemstore rolled back 8910 keyvalues from start:0 to end:891
2014-07-22 11:21:32,919 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,933 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,935 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: rollbackMemstore rolled back 2790 keyvalues from start:0 to end:279
2014-07-22 11:21:32,950 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,950 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,951 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,961 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,961 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:32,999 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":90599,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053202379,"queuetimems":823,"class":"HRegionServer","responsesize":784669,"method":"Multi"}
2014-07-22 11:21:33,016 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":85701,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053207236,"queuetimems":730,"class":"HRegionServer","responsesize":279149,"method":"Multi"}
2014-07-22 11:21:33,018 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11297 service: ClientService methodName: Multi size: 758.2k connection: 9.1.143.53:38310: output error
2014-07-22 11:21:33,018 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:33,038 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: rollbackMemstore rolled back 7870 keyvalues from start:0 to end:787
2014-07-22 11:21:33,046 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11219 service: ClientService methodName: Multi size: 3.6m connection: 9.1.143.53:38310: output error
2014-07-22 11:21:33,046 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:33,077 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":86440,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053206636,"queuetimems":174,"class":"HRegionServer","responsesize":58851,"method":"Multi"}
2014-07-22 11:21:33,077 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11312 service: ClientService methodName: Multi size: 77.5k connection: 9.1.143.53:38310: output error
2014-07-22 11:21:33,077 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:33,081 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":85830,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053207251,"queuetimems":744,"class":"HRegionServer","responsesize":169776,"method":"Multi"}
2014-07-22 11:21:33,081 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":86622,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053206459,"queuetimems":0,"class":"HRegionServer","responsesize":81846,"method":"Multi"}
2014-07-22 11:21:33,082 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11295 service: ClientService methodName: Multi size: 180.0k connection: 9.1.143.53:38310: output error
2014-07-22 11:21:33,082 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:33,090 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11274 service: ClientService methodName: Multi size: 222.4k connection: 9.1.143.53:38310: output error
2014-07-22 11:21:33,090 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:33,093 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":87274,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053205807,"queuetimems":0,"class":"HRegionServer","responsesize":452751,"method":"Multi"}
2014-07-22 11:21:33,095 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":86636,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053206458,"queuetimems":0,"class":"HRegionServer","responsesize":119096,"method":"Multi"}
2014-07-22 11:21:33,099 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5760 keyvalues from start:0 to end:576
2014-07-22 11:21:33,307 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":86082,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053207225,"queuetimems":727,"class":"HRegionServer","responsesize":72639,"method":"Multi"}
2014-07-22 11:21:33,307 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":86690,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053206617,"queuetimems":157,"class":"HRegionServer","responsesize":131109,"method":"Multi"}
2014-07-22 11:21:33,309 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":87144,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053206165,"queuetimems":328,"class":"HRegionServer","responsesize":60675,"method":"Multi"}
2014-07-22 11:21:33,313 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:33,314 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:33,314 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":87470,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053205837,"queuetimems":1,"class":"HRegionServer","responsesize":119103,"method":"Multi"}
2014-07-22 11:21:33,343 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11278 service: ClientService methodName: Multi size: 323.6k connection: 9.1.143.53:38310: output error
2014-07-22 11:21:33,343 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:33,350 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":86117,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053207233,"queuetimems":735,"class":"HRegionServer","responsesize":14694,"method":"Multi"}
2014-07-22 11:21:33,355 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":86724,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053206626,"queuetimems":165,"class":"HRegionServer","responsesize":202363,"method":"Multi"}
2014-07-22 11:21:33,380 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11291 service: ClientService methodName: Multi size: 323.6k connection: 9.1.143.53:38310: output error
2014-07-22 11:21:33,380 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:33,380 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11288 service: ClientService methodName: Multi size: 165.0k connection: 9.1.143.53:38310: output error
2014-07-22 11:21:33,381 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:33,381 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":86755,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053206626,"queuetimems":165,"class":"HRegionServer","responsesize":3707,"method":"Multi"}
2014-07-22 11:21:33,384 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11248 service: ClientService methodName: Multi size: 342.3k connection: 9.1.143.53:38310: output error
2014-07-22 11:21:33,384 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:33,384 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":86120,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053207225,"queuetimems":729,"class":"HRegionServer","responsesize":5132,"method":"Multi"}
2014-07-22 11:21:33,385 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11272 service: ClientService methodName: Multi size: 142.5k connection: 9.1.143.53:38310: output error
2014-07-22 11:21:33,385 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:33,386 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11300 service: ClientService methodName: Multi size: 197.4k connection: 9.1.143.53:38310: output error
2014-07-22 11:21:33,386 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11304 service: ClientService methodName: Multi size: 30.1k connection: 9.1.143.53:38310: output error
2014-07-22 11:21:33,386 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:33,386 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:33,393 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11315 service: ClientService methodName: Multi size: 16.4k connection: 9.1.143.53:38310: output error
2014-07-22 11:21:33,393 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:33,396 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11270 service: ClientService methodName: Multi size: 549.6k connection: 9.1.143.53:38310: output error
2014-07-22 11:21:33,399 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:33,399 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11298 service: ClientService methodName: Multi size: 40.1k connection: 9.1.143.53:38310: output error
2014-07-22 11:21:33,400 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:33,402 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":87564,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053205837,"queuetimems":0,"class":"HRegionServer","responsesize":27103,"method":"Multi"}
2014-07-22 11:21:33,402 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11290 service: ClientService methodName: Multi size: 73.8k connection: 9.1.143.53:38310: output error
2014-07-22 11:21:33,402 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:33,408 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":86183,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053207225,"queuetimems":729,"class":"HRegionServer","responsesize":95673,"method":"Multi"}
2014-07-22 11:21:33,408 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":86772,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053206636,"queuetimems":174,"class":"HRegionServer","responsesize":28493,"method":"Multi"}
2014-07-22 11:21:33,409 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11302 service: ClientService methodName: Multi size: 102.5k connection: 9.1.143.53:38310: output error
2014-07-22 11:21:33,409 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:33,409 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11310 service: ClientService methodName: Multi size: 77.6k connection: 9.1.143.53:38310: output error
2014-07-22 11:21:33,413 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6170 keyvalues from start:0 to end:617
2014-07-22 11:21:33,417 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":88033,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053205383,"queuetimems":0,"class":"HRegionServer","responsesize":586709,"method":"Multi"}
2014-07-22 11:21:33,419 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:33,437 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11252 service: ClientService methodName: Multi size: 659.5k connection: 9.1.143.53:38310: output error
2014-07-22 11:21:33,437 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:34,244 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":89184,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053205059,"queuetimems":0,"class":"HRegionServer","responsesize":726758,"method":"Multi"}
2014-07-22 11:21:34,245 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":88869,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053205375,"queuetimems":0,"class":"HRegionServer","responsesize":564378,"method":"Multi"}
2014-07-22 11:21:34,246 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11260 service: ClientService methodName: Multi size: 1.9m connection: 9.1.143.53:38310: output error
2014-07-22 11:21:34,246 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:34,246 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11254 service: ClientService methodName: Multi size: 1.5m connection: 9.1.143.53:38310: output error
2014-07-22 11:21:34,246 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:34,263 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:34,263 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:34,264 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:34,264 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:34,305 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":87654,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053206650,"queuetimems":154,"class":"HRegionServer","responsesize":303102,"method":"Multi"}
2014-07-22 11:21:34,305 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11306 service: ClientService methodName: Multi size: 823.2k connection: 9.1.143.53:38310: output error
2014-07-22 11:21:34,305 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:34,325 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:34,326 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:34,335 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:34,335 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:34,340 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: rollbackMemstore rolled back 1800 keyvalues from start:0 to end:180
2014-07-22 11:21:34,345 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6980 keyvalues from start:0 to end:698
2014-07-22 11:21:34,353 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":87715,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053206637,"queuetimems":149,"class":"HRegionServer","responsesize":605283,"method":"Multi"}
2014-07-22 11:21:34,354 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11308 service: ClientService methodName: Multi size: 1.6m connection: 9.1.143.53:38310: output error
2014-07-22 11:21:34,354 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:34,412 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:34,412 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:34,423 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6920 keyvalues from start:0 to end:692
2014-07-22 11:21:34,424 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":88589,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053205834,"queuetimems":7,"class":"HRegionServer","responsesize":257500,"method":"Multi"}
2014-07-22 11:21:34,424 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11292 service: ClientService methodName: Multi size: 1.4m connection: 9.1.143.53:38310: output error
2014-07-22 11:21:34,424 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:34,453 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":87995,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053206457,"queuetimems":0,"class":"HRegionServer","responsesize":619578,"method":"Multi"}
2014-07-22 11:21:34,454 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11280 service: ClientService methodName: Multi size: 1.6m connection: 9.1.143.53:38310: output error
2014-07-22 11:21:34,454 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:34,454 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: rollbackMemstore rolled back 3460 keyvalues from start:0 to end:346
2014-07-22 11:21:34,484 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:34,484 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:34,484 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: rollbackMemstore rolled back 9360 keyvalues from start:0 to end:936
2014-07-22 11:21:34,492 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: rollbackMemstore rolled back 1980 keyvalues from start:0 to end:198
2014-07-22 11:21:34,504 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:34,505 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:34,544 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":88727,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053205791,"queuetimems":1,"class":"HRegionServer","responsesize":720740,"method":"Multi"}
2014-07-22 11:21:34,547 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11251 service: ClientService methodName: Multi size: 1.9m connection: 9.1.143.53:38310: output error
2014-07-22 11:21:34,547 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:34,565 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:34,566 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:34,567 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:34,568 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:34,658 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: rollbackMemstore rolled back 1990 keyvalues from start:0 to end:199
2014-07-22 11:21:34,679 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":88456,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053206222,"queuetimems":1,"class":"HRegionServer","responsesize":402356,"method":"Multi"}
2014-07-22 11:21:34,679 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:34,680 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:34,680 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11281 service: ClientService methodName: Multi size: 1.8m connection: 9.1.143.53:38310: output error
2014-07-22 11:21:34,680 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:34,700 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:34,701 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:34,703 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:34,704 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:34,757 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: rollbackMemstore rolled back 2760 keyvalues from start:0 to end:276
2014-07-22 11:21:34,770 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: rollbackMemstore rolled back 7940 keyvalues from start:0 to end:794
2014-07-22 11:21:34,774 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":89752,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053205020,"queuetimems":0,"class":"HRegionServer","responsesize":985703,"method":"Multi"}
2014-07-22 11:21:34,784 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11247 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.53:38310: output error
2014-07-22 11:21:34,784 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:34,827 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:34,828 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:34,835 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":89028,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053205805,"queuetimems":0,"class":"HRegionServer","responsesize":1839832,"method":"Multi"}
2014-07-22 11:21:34,845 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11276 service: ClientService methodName: Multi size: 1.9m connection: 9.1.143.53:38310: output error
2014-07-22 11:21:34,845 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:34,850 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: rollbackMemstore rolled back 9140 keyvalues from start:0 to end:914
2014-07-22 11:21:34,862 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":89520,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053205341,"queuetimems":8,"class":"HRegionServer","responsesize":1114428,"method":"Multi"}
2014-07-22 11:21:34,864 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11256 service: ClientService methodName: Multi size: 864.4k connection: 9.1.143.53:38310: output error
2014-07-22 11:21:34,877 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:34,894 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":89584,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053205308,"queuetimems":0,"class":"HRegionServer","responsesize":837157,"method":"Multi"}
2014-07-22 11:21:34,896 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11257 service: ClientService methodName: Multi size: 2.2m connection: 9.1.143.53:38310: output error
2014-07-22 11:21:34,896 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:34,900 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: rollbackMemstore rolled back 9320 keyvalues from start:0 to end:932
2014-07-22 11:21:35,134 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: rollbackMemstore rolled back 12190 keyvalues from start:0 to end:1219
2014-07-22 11:21:35,197 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: rollbackMemstore rolled back 12980 keyvalues from start:0 to end:1298
2014-07-22 11:21:35,225 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: rollbackMemstore rolled back 13230 keyvalues from start:0 to end:1323
2014-07-22 11:21:35,287 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1220, memsize=457.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/.tmp/b0d23469b2c84d4a9123950c3add0b82
2014-07-22 11:21:35,290 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: rollbackMemstore rolled back 12860 keyvalues from start:0 to end:1286
2014-07-22 11:21:35,331 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/.tmp/b0d23469b2c84d4a9123950c3add0b82 as hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/b0d23469b2c84d4a9123950c3add0b82
2014-07-22 11:21:35,366 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/b0d23469b2c84d4a9123950c3add0b82, entries=1664460, sequenceid=1220, filesize=118.6m
2014-07-22 11:21:35,366 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~457.1m/479351360, currentsize=467.5m/490206160 for region usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e. in 167934ms, sequenceid=1220, compaction requested=true
2014-07-22 11:21:35,367 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: MemStoreFlusher.1 exiting
2014-07-22 11:21:35,376 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: rollbackMemstore rolled back 13470 keyvalues from start:0 to end:1347
2014-07-22 11:21:35,399 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:35,400 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:35,423 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:35,423 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:35,427 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:35,427 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:35,485 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:35,485 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:35,488 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: rollbackMemstore rolled back 2630 keyvalues from start:0 to end:263
2014-07-22 11:21:35,489 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:35,489 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:35,502 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: rollbackMemstore rolled back 4130 keyvalues from start:0 to end:413
2014-07-22 11:21:35,510 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: rollbackMemstore rolled back 3500 keyvalues from start:0 to end:350
2014-07-22 11:21:35,537 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":91443,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053204073,"queuetimems":1143,"class":"HRegionServer","responsesize":1389587,"method":"Multi"}
2014-07-22 11:21:35,540 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11231 service: ClientService methodName: Multi size: 3.7m connection: 9.1.143.53:38310: output error
2014-07-22 11:21:35,541 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:35,548 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":91446,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053204092,"queuetimems":0,"class":"HRegionServer","responsesize":1253880,"method":"Multi"}
2014-07-22 11:21:35,554 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11239 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:38310: output error
2014-07-22 11:21:35,554 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:35,556 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":91466,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053204073,"queuetimems":1103,"class":"HRegionServer","responsesize":964362,"method":"Multi"}
2014-07-22 11:21:35,559 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11230 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:38310: output error
2014-07-22 11:21:35,559 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:35,606 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: rollbackMemstore rolled back 4070 keyvalues from start:0 to end:407
2014-07-22 11:21:35,642 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: rollbackMemstore rolled back 4140 keyvalues from start:0 to end:414
2014-07-22 11:21:35,659 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":91578,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053204073,"queuetimems":900,"class":"HRegionServer","responsesize":1419947,"method":"Multi"}
2014-07-22 11:21:35,664 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11229 service: ClientService methodName: Multi size: 3.8m connection: 9.1.143.53:38310: output error
2014-07-22 11:21:35,664 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:35,750 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:35,751 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:35,756 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":91655,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053204080,"queuetimems":730,"class":"HRegionServer","responsesize":1419031,"method":"Multi"}
2014-07-22 11:21:35,760 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11235 service: ClientService methodName: Multi size: 3.8m connection: 9.1.143.53:38310: output error
2014-07-22 11:21:35,760 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:36,154 INFO  [RS_OPEN_META-sceplus-vm48:60020-0-MetaLogRoller] regionserver.LogRoller: LogRoller exiting.
2014-07-22 11:21:36,157 INFO  [regionserver60020.leaseChecker] regionserver.Leases: regionserver60020.leaseChecker closing leases
2014-07-22 11:21:36,158 INFO  [regionserver60020.leaseChecker] regionserver.Leases: regionserver60020.leaseChecker closed leases
2014-07-22 11:21:36,166 INFO  [regionserver60020.compactionChecker] regionserver.HRegionServer$CompactionChecker: regionserver60020.compactionChecker exiting
2014-07-22 11:21:36,171 INFO  [regionserver60020.periodicFlusher] regionserver.HRegionServer$PeriodicMemstoreFlusher: regionserver60020.periodicFlusher exiting
2014-07-22 11:21:36,419 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: rollbackMemstore rolled back 13170 keyvalues from start:0 to end:1317
2014-07-22 11:21:36,590 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:36,591 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:21:36,700 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: rollbackMemstore rolled back 3630 keyvalues from start:0 to end:363
2014-07-22 11:21:36,738 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":90530,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:38310","starttimems":1406053206201,"queuetimems":0,"class":"HRegionServer","responsesize":1430529,"method":"Multi"}
2014-07-22 11:21:36,743 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11285 service: ClientService methodName: Multi size: 3.8m connection: 9.1.143.53:38310: output error
2014-07-22 11:21:36,743 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 11:21:57,644 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Error Recovery for blk_797617860964781395_269622 bad datanode[0] 9.1.143.58:50010
2014-07-22 11:21:57,645 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Error Recovery for block blk_797617860964781395_269622 in pipeline 9.1.143.58:50010, 9.1.143.59:50010: bad datanode 9.1.143.58:50010
2014-07-22 11:21:57,660 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Failed recovery attempt #1 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_797617860964781395_269622 has out of date GS 269622 found 269942, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-22 11:21:57,660 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Error Recovery for block blk_797617860964781395_269622 failed  because recovery from primary datanode 9.1.143.59:50010 failed 2 times.  Pipeline was 9.1.143.58:50010, 9.1.143.59:50010. Will retry...
2014-07-22 11:21:59,145 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Error Recovery for blk_797617860964781395_269622 bad datanode[0] 9.1.143.58:50010
2014-07-22 11:21:59,145 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Error Recovery for block blk_797617860964781395_269622 in pipeline 9.1.143.58:50010, 9.1.143.59:50010: bad datanode 9.1.143.58:50010
2014-07-22 11:21:59,164 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Failed recovery attempt #2 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_797617860964781395_269622 has out of date GS 269622 found 269942, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-22 11:21:59,164 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Error Recovery for block blk_797617860964781395_269622 failed  because recovery from primary datanode 9.1.143.59:50010 failed 3 times.  Pipeline was 9.1.143.58:50010, 9.1.143.59:50010. Will retry...
2014-07-22 11:21:59,860 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1524, memsize=738.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/.tmp/85c0e6f8fe2b4b70a80ad499c79ac5eb
2014-07-22 11:21:59,871 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/.tmp/85c0e6f8fe2b4b70a80ad499c79ac5eb as hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/85c0e6f8fe2b4b70a80ad499c79ac5eb
2014-07-22 11:21:59,963 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/85c0e6f8fe2b4b70a80ad499c79ac5eb, entries=2687110, sequenceid=1524, filesize=191.3m
2014-07-22 11:21:59,963 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~738.0m/773864640, currentsize=106.2m/111309200 for region usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2. in 138476ms, sequenceid=1524, compaction requested=false
2014-07-22 11:21:59,964 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: MemStoreFlusher.0 exiting
2014-07-22 11:21:59,964 INFO  [regionserver60020] snapshot.RegionServerSnapshotManager: Stopping RegionServerSnapshotManager abruptly.
2014-07-22 11:21:59,965 INFO  [regionserver60020.nonceCleaner] regionserver.ServerNonceManager$1: regionserver60020.nonceCleaner exiting
2014-07-22 11:21:59,967 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] handler.CloseRegionHandler: Processing close of usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f.
2014-07-22 11:21:59,968 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Closing usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f.: disabling compactions & flushes
2014-07-22 11:21:59,968 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Updates disabled for region usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f.
2014-07-22 11:21:59,968 INFO  [regionserver60020] regionserver.HRegionServer: aborting server sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:21:59,968 DEBUG [regionserver60020] catalog.CatalogTracker: Stopping catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@256c949f
2014-07-22 11:21:59,969 INFO  [regionserver60020] regionserver.CompactSplitThread: Waiting for Split Thread to finish...
2014-07-22 11:21:59,969 INFO  [regionserver60020] regionserver.CompactSplitThread: Waiting for Merge Thread to finish...
2014-07-22 11:21:59,969 INFO  [regionserver60020] regionserver.CompactSplitThread: Waiting for Large Compaction Thread to finish...
2014-07-22 11:21:59,968 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] handler.CloseRegionHandler: Processing close of usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.
2014-07-22 11:21:59,969 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] handler.CloseRegionHandler: Processing close of usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.
2014-07-22 11:21:59,969 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Closing usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.: disabling compactions & flushes
2014-07-22 11:21:59,969 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Closing usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.: disabling compactions & flushes
2014-07-22 11:21:59,969 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Updates disabled for region usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.
2014-07-22 11:21:59,970 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HRegion: waiting for 1 compactions to complete for region usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.
2014-07-22 11:21:59,980 INFO  [StoreCloserThread-usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f.-1] regionserver.HStore: Closed family
2014-07-22 11:21:59,981 ERROR [RS_CLOSE_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Memstore size is 248949920
2014-07-22 11:21:59,981 INFO  [StoreCloserThread-usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.-1] regionserver.HStore: Closed family
2014-07-22 11:21:59,982 ERROR [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Memstore size is 490206160
2014-07-22 11:21:59,991 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Closed usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f.
2014-07-22 11:21:59,991 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Closed usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.
2014-07-22 11:21:59,991 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] handler.CloseRegionHandler: Closed usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f.
2014-07-22 11:21:59,991 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] handler.CloseRegionHandler: Closed usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.
2014-07-22 11:21:59,991 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] handler.CloseRegionHandler: Processing close of usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.
2014-07-22 11:21:59,991 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] handler.CloseRegionHandler: Processing close of usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.
2014-07-22 11:21:59,992 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Closing usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.: disabling compactions & flushes
2014-07-22 11:21:59,992 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Closing usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.: disabling compactions & flushes
2014-07-22 11:21:59,992 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Updates disabled for region usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.
2014-07-22 11:21:59,992 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Updates disabled for region usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.
2014-07-22 11:21:59,997 INFO  [StoreCloserThread-usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.-1] regionserver.HStore: Closed family
2014-07-22 11:21:59,998 ERROR [RS_CLOSE_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Memstore size is 720611840
2014-07-22 11:21:59,998 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Closed usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.
2014-07-22 11:21:59,998 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] handler.CloseRegionHandler: Closed usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.
2014-07-22 11:21:59,998 INFO  [StoreCloserThread-usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.-1] regionserver.HStore: Closed family
2014-07-22 11:21:59,999 ERROR [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Memstore size is 554918640
2014-07-22 11:21:59,999 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Closed usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.
2014-07-22 11:22:00,000 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] handler.CloseRegionHandler: Closed usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.
2014-07-22 11:22:00,001 INFO  [regionserver60020-smallCompactions-1406052479487] regionserver.HRegion: compaction interrupted
java.io.InterruptedIOException: Aborting compaction of store family in region usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2. because it was interrupted.
	at org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.compact(DefaultCompactor.java:81)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext.compact(DefaultStoreEngine.java:109)
	at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:1086)
	at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1481)
	at org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.run(CompactSplitThread.java:475)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:22:00,001 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Updates disabled for region usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.
2014-07-22 11:22:00,001 INFO  [regionserver60020-smallCompactions-1406052479487] regionserver.CompactSplitThread: Aborted compaction: Request = regionName=usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2., storeName=family, fileCount=5, fileSize=407.0m (82.1m, 66.6m, 98.6m, 91.0m, 68.7m), priority=1995, time=150093561457114; duration=3mins, 37sec
2014-07-22 11:22:00,001 DEBUG [regionserver60020-smallCompactions-1406052479487] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:9), split_queue=0, merge_queue=0
2014-07-22 11:22:00,002 INFO  [regionserver60020] regionserver.CompactSplitThread: Waiting for Small Compaction Thread to finish...
2014-07-22 11:22:00,002 INFO  [regionserver60020] regionserver.HRegionServer: Waiting on 2 regions to close
2014-07-22 11:22:00,002 DEBUG [regionserver60020] regionserver.HRegionServer: {4d714c493433d51e92af518b12e607b2=usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2., 1588230740=hbase:meta,,1.1588230740}
2014-07-22 11:22:00,003 DEBUG [RS_CLOSE_META-sceplus-vm48:60020-0] handler.CloseRegionHandler: Processing close of hbase:meta,,1.1588230740
2014-07-22 11:22:00,003 DEBUG [RS_CLOSE_META-sceplus-vm48:60020-0] regionserver.HRegion: Closing hbase:meta,,1.1588230740: disabling compactions & flushes
2014-07-22 11:22:00,003 DEBUG [RS_CLOSE_META-sceplus-vm48:60020-0] regionserver.HRegion: Updates disabled for region hbase:meta,,1.1588230740
2014-07-22 11:22:00,004 INFO  [StoreCloserThread-hbase:meta,,1.1588230740-1] regionserver.HStore: Closed info
2014-07-22 11:22:00,005 ERROR [RS_CLOSE_META-sceplus-vm48:60020-0] regionserver.HRegion: Memstore size is 20128
2014-07-22 11:22:00,005 DEBUG [RS_CLOSE_META-sceplus-vm48:60020-0] coprocessor.CoprocessorHost: Stop coprocessor org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint
2014-07-22 11:22:00,005 INFO  [RS_CLOSE_META-sceplus-vm48:60020-0] regionserver.HRegion: Closed hbase:meta,,1.1588230740
2014-07-22 11:22:00,006 DEBUG [RS_CLOSE_META-sceplus-vm48:60020-0] handler.CloseRegionHandler: Closed hbase:meta,,1.1588230740
2014-07-22 11:22:00,006 INFO  [StoreCloserThread-usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.-1] regionserver.HStore: Closed family
2014-07-22 11:22:00,007 ERROR [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Memstore size is 111309200
2014-07-22 11:22:00,007 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Closed usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.
2014-07-22 11:22:00,007 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] handler.CloseRegionHandler: Closed usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.
2014-07-22 11:22:00,165 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Error Recovery for blk_797617860964781395_269622 bad datanode[0] 9.1.143.58:50010
2014-07-22 11:22:00,165 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Error Recovery for block blk_797617860964781395_269622 in pipeline 9.1.143.58:50010, 9.1.143.59:50010: bad datanode 9.1.143.58:50010
2014-07-22 11:22:00,183 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Failed recovery attempt #3 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_797617860964781395_269622 has out of date GS 269622 found 269942, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-22 11:22:00,184 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Error Recovery for block blk_797617860964781395_269622 failed  because recovery from primary datanode 9.1.143.59:50010 failed 4 times.  Pipeline was 9.1.143.58:50010, 9.1.143.59:50010. Will retry...
2014-07-22 11:22:00,203 INFO  [regionserver60020] regionserver.HRegionServer: stopping server sceplus-vm48.almaden.ibm.com,60020,1406052408577; all regions closed.
2014-07-22 11:22:00,203 DEBUG [RS_OPEN_META-sceplus-vm48:60020-0-WAL.AsyncNotifier] wal.FSHLog: RS_OPEN_META-sceplus-vm48:60020-0-WAL.AsyncNotifier interrupted while waiting for  notification from AsyncSyncer thread
2014-07-22 11:22:00,203 INFO  [RS_OPEN_META-sceplus-vm48:60020-0-WAL.AsyncNotifier] wal.FSHLog: RS_OPEN_META-sceplus-vm48:60020-0-WAL.AsyncNotifier exiting
2014-07-22 11:22:00,204 DEBUG [RS_OPEN_META-sceplus-vm48:60020-0-WAL.AsyncSyncer0] wal.FSHLog: RS_OPEN_META-sceplus-vm48:60020-0-WAL.AsyncSyncer0 interrupted while waiting for notification from AsyncWriter thread
2014-07-22 11:22:00,204 INFO  [RS_OPEN_META-sceplus-vm48:60020-0-WAL.AsyncSyncer0] wal.FSHLog: RS_OPEN_META-sceplus-vm48:60020-0-WAL.AsyncSyncer0 exiting
2014-07-22 11:22:00,204 DEBUG [RS_OPEN_META-sceplus-vm48:60020-0-WAL.AsyncSyncer1] wal.FSHLog: RS_OPEN_META-sceplus-vm48:60020-0-WAL.AsyncSyncer1 interrupted while waiting for notification from AsyncWriter thread
2014-07-22 11:22:00,204 INFO  [RS_OPEN_META-sceplus-vm48:60020-0-WAL.AsyncSyncer1] wal.FSHLog: RS_OPEN_META-sceplus-vm48:60020-0-WAL.AsyncSyncer1 exiting
2014-07-22 11:22:00,205 DEBUG [RS_OPEN_META-sceplus-vm48:60020-0-WAL.AsyncSyncer2] wal.FSHLog: RS_OPEN_META-sceplus-vm48:60020-0-WAL.AsyncSyncer2 interrupted while waiting for notification from AsyncWriter thread
2014-07-22 11:22:00,205 INFO  [RS_OPEN_META-sceplus-vm48:60020-0-WAL.AsyncSyncer2] wal.FSHLog: RS_OPEN_META-sceplus-vm48:60020-0-WAL.AsyncSyncer2 exiting
2014-07-22 11:22:00,206 DEBUG [RS_OPEN_META-sceplus-vm48:60020-0-WAL.AsyncSyncer3] wal.FSHLog: RS_OPEN_META-sceplus-vm48:60020-0-WAL.AsyncSyncer3 interrupted while waiting for notification from AsyncWriter thread
2014-07-22 11:22:00,206 INFO  [RS_OPEN_META-sceplus-vm48:60020-0-WAL.AsyncSyncer3] wal.FSHLog: RS_OPEN_META-sceplus-vm48:60020-0-WAL.AsyncSyncer3 exiting
2014-07-22 11:22:00,206 DEBUG [RS_OPEN_META-sceplus-vm48:60020-0-WAL.AsyncSyncer4] wal.FSHLog: RS_OPEN_META-sceplus-vm48:60020-0-WAL.AsyncSyncer4 interrupted while waiting for notification from AsyncWriter thread
2014-07-22 11:22:00,206 INFO  [RS_OPEN_META-sceplus-vm48:60020-0-WAL.AsyncSyncer4] wal.FSHLog: RS_OPEN_META-sceplus-vm48:60020-0-WAL.AsyncSyncer4 exiting
2014-07-22 11:22:00,207 DEBUG [RS_OPEN_META-sceplus-vm48:60020-0-WAL.AsyncWriter] wal.FSHLog: RS_OPEN_META-sceplus-vm48:60020-0-WAL.AsyncWriter interrupted while waiting for newer writes added to local buffer
2014-07-22 11:22:00,207 INFO  [RS_OPEN_META-sceplus-vm48:60020-0-WAL.AsyncWriter] wal.FSHLog: RS_OPEN_META-sceplus-vm48:60020-0-WAL.AsyncWriter exiting
2014-07-22 11:22:00,207 DEBUG [regionserver60020] wal.FSHLog: Closing WAL writer in hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:22:00,207 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Error Recovery for blk_797617860964781395_269622 bad datanode[0] 9.1.143.58:50010
2014-07-22 11:22:00,207 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Error Recovery for block blk_797617860964781395_269622 in pipeline 9.1.143.58:50010, 9.1.143.59:50010: bad datanode 9.1.143.58:50010
2014-07-22 11:22:00,215 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Failed recovery attempt #4 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_797617860964781395_269622 has out of date GS 269622 found 269942, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-22 11:22:00,216 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Error Recovery for block blk_797617860964781395_269622 failed  because recovery from primary datanode 9.1.143.59:50010 failed 5 times.  Pipeline was 9.1.143.58:50010, 9.1.143.59:50010. Will retry...
2014-07-22 11:22:01,216 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Error Recovery for blk_797617860964781395_269622 bad datanode[0] 9.1.143.58:50010
2014-07-22 11:22:01,216 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Error Recovery for block blk_797617860964781395_269622 in pipeline 9.1.143.58:50010, 9.1.143.59:50010: bad datanode 9.1.143.58:50010
2014-07-22 11:22:01,230 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Failed recovery attempt #5 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_797617860964781395_269622 has out of date GS 269622 found 269942, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-22 11:22:01,230 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Error Recovery for block blk_797617860964781395_269622 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.58:50010, 9.1.143.59:50010. Marking primary datanode as bad.
2014-07-22 11:22:02,264 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Failed recovery attempt #0 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_797617860964781395_269622 has out of date GS 269622 found 269942, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-22 11:22:02,264 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Error Recovery for block blk_797617860964781395_269622 failed  because recovery from primary datanode 9.1.143.58:50010 failed 1 times.  Pipeline was 9.1.143.58:50010. Will retry...
2014-07-22 11:22:03,274 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Failed recovery attempt #1 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_797617860964781395_269622 has out of date GS 269622 found 269942, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-22 11:22:03,274 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Error Recovery for block blk_797617860964781395_269622 failed  because recovery from primary datanode 9.1.143.58:50010 failed 2 times.  Pipeline was 9.1.143.58:50010. Will retry...
2014-07-22 11:22:04,282 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Failed recovery attempt #2 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_797617860964781395_269622 has out of date GS 269622 found 269942, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-22 11:22:04,283 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Error Recovery for block blk_797617860964781395_269622 failed  because recovery from primary datanode 9.1.143.58:50010 failed 3 times.  Pipeline was 9.1.143.58:50010. Will retry...
2014-07-22 11:22:05,291 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Failed recovery attempt #3 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_797617860964781395_269622 has out of date GS 269622 found 269942, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-22 11:22:05,291 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Error Recovery for block blk_797617860964781395_269622 failed  because recovery from primary datanode 9.1.143.58:50010 failed 4 times.  Pipeline was 9.1.143.58:50010. Will retry...
2014-07-22 11:22:06,299 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Failed recovery attempt #4 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_797617860964781395_269622 has out of date GS 269622 found 269942, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-22 11:22:06,299 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Error Recovery for block blk_797617860964781395_269622 failed  because recovery from primary datanode 9.1.143.58:50010 failed 5 times.  Pipeline was 9.1.143.58:50010. Will retry...
2014-07-22 11:22:07,311 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Failed recovery attempt #5 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_797617860964781395_269622 has out of date GS 269622 found 269942, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-22 11:22:07,312 WARN  [DataStreamer for file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta block blk_797617860964781395_269622] hdfs.DFSClient: Error Recovery for block blk_797617860964781395_269622 failed  because recovery from primary datanode 9.1.143.58:50010 failed 6 times.  Pipeline was 9.1.143.58:50010. Aborting...
2014-07-22 11:22:07,312 ERROR [regionserver60020] regionserver.HRegionServer: Metalog close and delete failed
java.io.IOException: Error Recovery for block blk_797617860964781395_269622 failed  because recovery from primary datanode 9.1.143.58:50010 failed 6 times.  Pipeline was 9.1.143.58:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-22 11:22:07,313 DEBUG [regionserver60020-WAL.AsyncNotifier] wal.FSHLog: regionserver60020-WAL.AsyncNotifier interrupted while waiting for  notification from AsyncSyncer thread
2014-07-22 11:22:07,313 INFO  [regionserver60020-WAL.AsyncNotifier] wal.FSHLog: regionserver60020-WAL.AsyncNotifier exiting
2014-07-22 11:22:07,314 DEBUG [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: regionserver60020-WAL.AsyncSyncer0 interrupted while waiting for notification from AsyncWriter thread
2014-07-22 11:22:07,314 INFO  [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: regionserver60020-WAL.AsyncSyncer0 exiting
2014-07-22 11:22:07,314 DEBUG [regionserver60020-WAL.AsyncSyncer1] wal.FSHLog: regionserver60020-WAL.AsyncSyncer1 interrupted while waiting for notification from AsyncWriter thread
2014-07-22 11:22:07,314 INFO  [regionserver60020-WAL.AsyncSyncer1] wal.FSHLog: regionserver60020-WAL.AsyncSyncer1 exiting
2014-07-22 11:22:07,315 DEBUG [regionserver60020-WAL.AsyncSyncer2] wal.FSHLog: regionserver60020-WAL.AsyncSyncer2 interrupted while waiting for notification from AsyncWriter thread
2014-07-22 11:22:07,315 INFO  [regionserver60020-WAL.AsyncSyncer2] wal.FSHLog: regionserver60020-WAL.AsyncSyncer2 exiting
2014-07-22 11:22:07,316 DEBUG [regionserver60020-WAL.AsyncSyncer3] wal.FSHLog: regionserver60020-WAL.AsyncSyncer3 interrupted while waiting for notification from AsyncWriter thread
2014-07-22 11:22:07,316 INFO  [regionserver60020-WAL.AsyncSyncer3] wal.FSHLog: regionserver60020-WAL.AsyncSyncer3 exiting
2014-07-22 11:22:07,316 DEBUG [regionserver60020-WAL.AsyncSyncer4] wal.FSHLog: regionserver60020-WAL.AsyncSyncer4 interrupted while waiting for notification from AsyncWriter thread
2014-07-22 11:22:07,316 INFO  [regionserver60020-WAL.AsyncSyncer4] wal.FSHLog: regionserver60020-WAL.AsyncSyncer4 exiting
2014-07-22 11:22:07,316 DEBUG [regionserver60020-WAL.AsyncWriter] wal.FSHLog: regionserver60020-WAL.AsyncWriter interrupted while waiting for newer writes added to local buffer
2014-07-22 11:22:07,316 INFO  [regionserver60020-WAL.AsyncWriter] wal.FSHLog: regionserver60020-WAL.AsyncWriter exiting
2014-07-22 11:22:07,317 DEBUG [regionserver60020] wal.FSHLog: Closing WAL writer in hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:22:07,317 ERROR [regionserver60020] regionserver.HRegionServer: Close and delete failed
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:22:07,318 INFO  [regionserver60020] regionserver.Leases: regionserver60020 closing leases
2014-07-22 11:22:07,318 INFO  [regionserver60020] regionserver.Leases: regionserver60020 closed leases
2014-07-22 11:22:07,333 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/replication/rs/sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:22:07,341 INFO  [regionserver60020] util.RetryCounter: Sleeping 1000ms before retry #0...
2014-07-22 11:22:08,341 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/replication/rs/sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:22:08,342 INFO  [regionserver60020] util.RetryCounter: Sleeping 2000ms before retry #1...
2014-07-22 11:22:10,342 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/replication/rs/sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:22:10,342 INFO  [regionserver60020] util.RetryCounter: Sleeping 4000ms before retry #2...
2014-07-22 11:22:14,342 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/replication/rs/sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:22:14,343 INFO  [regionserver60020] util.RetryCounter: Sleeping 8000ms before retry #3...
2014-07-22 11:22:22,343 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/replication/rs/sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:22:22,343 ERROR [regionserver60020] zookeeper.RecoverableZooKeeper: ZooKeeper getChildren failed after 4 attempts
2014-07-22 11:22:22,352 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/rs/sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:22:22,352 INFO  [regionserver60020] util.RetryCounter: Sleeping 1000ms before retry #0...
2014-07-22 11:22:23,353 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/rs/sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:22:23,353 INFO  [regionserver60020] util.RetryCounter: Sleeping 2000ms before retry #1...
2014-07-22 11:22:25,353 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/rs/sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:22:25,354 INFO  [regionserver60020] util.RetryCounter: Sleeping 4000ms before retry #2...
2014-07-22 11:22:29,354 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/rs/sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:22:29,354 INFO  [regionserver60020] util.RetryCounter: Sleeping 8000ms before retry #3...
2014-07-22 11:22:37,355 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/rs/sceplus-vm48.almaden.ibm.com,60020,1406052408577
2014-07-22 11:22:37,355 ERROR [regionserver60020] zookeeper.RecoverableZooKeeper: ZooKeeper delete failed after 4 attempts
2014-07-22 11:22:37,355 WARN  [regionserver60020] regionserver.HRegionServer: Failed deleting my ephemeral node
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/rs/sceplus-vm48.almaden.ibm.com,60020,1406052408577
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:127)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.delete(RecoverableZooKeeper.java:156)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.deleteNode(ZKUtil.java:1273)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.deleteNode(ZKUtil.java:1262)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.deleteMyEphemeralNode(HRegionServer.java:1292)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:1008)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:22:37,360 INFO  [regionserver60020] regionserver.HRegionServer: stopping server sceplus-vm48.almaden.ibm.com,60020,1406052408577; zookeeper connection closed.
2014-07-22 11:22:37,360 INFO  [regionserver60020] regionserver.HRegionServer: regionserver60020 exiting
2014-07-22 11:22:37,361 ERROR [main] regionserver.HRegionServerCommandLine: Region server exiting
java.lang.RuntimeException: HRegionServer Aborted
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:66)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:85)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2410)
2014-07-22 11:22:37,364 INFO  [Shutdownhook:regionserver60020] regionserver.ShutdownHook: Shutdown hook starting; hbase.shutdown.hook=true; fsShutdownHook=Thread[Thread-9,5,main]
2014-07-22 11:22:37,364 INFO  [Shutdownhook:regionserver60020] regionserver.ShutdownHook: Starting fs shutdown hook thread.
2014-07-22 11:22:37,365 ERROR [Thread-9] hdfs.DFSClient: Failed to close file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053206613 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:22:37,365 ERROR [Thread-9] hdfs.DFSClient: Failed to close file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053200662
java.io.IOException: IOException flush:org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406053200662 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sceplus-vm48.almaden.ibm.com,60020,1406052408577_1558018305_31, pendingcreates: 5]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1999)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1990)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync(FSNamesystem.java:2556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.fsync(NameNode.java:1051)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3950)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 11:22:37,365 ERROR [Thread-9] hdfs.DFSClient: Failed to close file /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1406052408577/sceplus-vm48.almaden.ibm.com%2C60020%2C1406052408577.1406052447489.meta
java.io.IOException: Error Recovery for block blk_797617860964781395_269622 failed  because recovery from primary datanode 9.1.143.58:50010 failed 6 times.  Pipeline was 9.1.143.58:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-22 11:22:37,365 INFO  [Shutdownhook:regionserver60020] regionserver.ShutdownHook: Shutdown hook finished.
