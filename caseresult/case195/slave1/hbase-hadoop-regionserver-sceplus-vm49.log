Tue Jul 22 08:33:08 PDT 2014 Starting regionserver on sceplus-vm49
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 128203
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 32768
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 128203
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
2014-07-22 08:33:09,276 INFO  [main] util.VersionInfo: HBase 0.98.3-hadoop1
2014-07-22 08:33:09,277 INFO  [main] util.VersionInfo: Subversion git://acer/usr/src/Hadoop/hbase -r d5e65a9144e315bb0a964e7730871af32f5018d5
2014-07-22 08:33:09,277 INFO  [main] util.VersionInfo: Compiled by apurtell on Sat May 31 19:34:57 PDT 2014
2014-07-22 08:33:09,512 INFO  [main] util.ServerCommandLine: env:JAVA_HOME=/usr/lib/jvm/java-1.6.0-openjdk-amd64/
2014-07-22 08:33:09,513 INFO  [main] util.ServerCommandLine: env:SHLVL=3
2014-07-22 08:33:09,513 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_DIR=/home/hadoop/hbase/bin/../logs
2014-07-22 08:33:09,513 INFO  [main] util.ServerCommandLine: env:HBASE_HOME=/home/hadoop/hbase/bin/..
2014-07-22 08:33:09,513 INFO  [main] util.ServerCommandLine: env:HBASE_OPTS=-XX:+UseConcMarkSweepGC   -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm49.log -Dhbase.home.dir=/home/hadoop/hbase/bin/.. -Dhbase.id.str=hadoop -Dhbase.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64 -Dhbase.security.logger=INFO,RFAS
2014-07-22 08:33:09,513 INFO  [main] util.ServerCommandLine: env:HBASE_ENV_INIT=true
2014-07-22 08:33:09,513 INFO  [main] util.ServerCommandLine: env:SSH_CLIENT=9.1.143.58 39500 22
2014-07-22 08:33:09,513 INFO  [main] util.ServerCommandLine: env:HBASE_HEAPSIZE=10240
2014-07-22 08:33:09,513 INFO  [main] util.ServerCommandLine: env:MAIL=/var/mail/hadoop
2014-07-22 08:33:09,514 INFO  [main] util.ServerCommandLine: env:HBASE_ZNODE_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.znode
2014-07-22 08:33:09,514 INFO  [main] util.ServerCommandLine: env:PWD=/home/hadoop/hbase
2014-07-22 08:33:09,514 INFO  [main] util.ServerCommandLine: env:LOGNAME=hadoop
2014-07-22 08:33:09,514 INFO  [main] util.ServerCommandLine: env:HBASE_MANAGES_ZK=true
2014-07-22 08:33:09,514 INFO  [main] util.ServerCommandLine: env:HBASE_NICENESS=0
2014-07-22 08:33:09,514 INFO  [main] util.ServerCommandLine: env:HBASE_REST_OPTS=
2014-07-22 08:33:09,514 INFO  [main] util.ServerCommandLine: env:LD_LIBRARY_PATH=/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64/server:/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64:/usr/lib/jvm/java-6-openjdk-amd64/jre/../lib/amd64::/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-07-22 08:33:09,514 INFO  [main] util.ServerCommandLine: env:MALLOC_ARENA_MAX=4
2014-07-22 08:33:09,514 INFO  [main] util.ServerCommandLine: env:SSH_CONNECTION=9.1.143.58 39500 9.1.143.59 22
2014-07-22 08:33:09,514 INFO  [main] util.ServerCommandLine: env:SHELL=/bin/bash
2014-07-22 08:33:09,515 INFO  [main] util.ServerCommandLine: env:HBASE_PID_DIR=/var/hadoop/pids
2014-07-22 08:33:09,515 INFO  [main] util.ServerCommandLine: env:HBASE_ROOT_LOGGER=INFO,RFA
2014-07-22 08:33:09,517 INFO  [main] util.ServerCommandLine: env:CLASSPATH=/home/hadoop/hbase/bin/../conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-07-22 08:33:09,517 INFO  [main] util.ServerCommandLine: env:HBASE_THRIFT_OPTS=
2014-07-22 08:33:09,517 INFO  [main] util.ServerCommandLine: env:PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games
2014-07-22 08:33:09,517 INFO  [main] util.ServerCommandLine: env:USER=hadoop
2014-07-22 08:33:09,517 INFO  [main] util.ServerCommandLine: env:HBASE_SECURITY_LOGGER=INFO,RFAS
2014-07-22 08:33:09,517 INFO  [main] util.ServerCommandLine: env:HOME=/home/hadoop
2014-07-22 08:33:09,518 INFO  [main] util.ServerCommandLine: env:HBASE_LIBRARY_PATH=/home/hadoop/hbase/lib/native/Linux-amd64-64
2014-07-22 08:33:09,518 INFO  [main] util.ServerCommandLine: env:HBASE_START_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.autorestart
2014-07-22 08:33:09,518 INFO  [main] util.ServerCommandLine: env:XDG_SESSION_ID=297
2014-07-22 08:33:09,518 INFO  [main] util.ServerCommandLine: env:HBASE_LOGFILE=hbase-hadoop-regionserver-sceplus-vm49.log
2014-07-22 08:33:09,518 INFO  [main] util.ServerCommandLine: env:XDG_RUNTIME_DIR=/run/user/1001
2014-07-22 08:33:09,518 INFO  [main] util.ServerCommandLine: env:LANG=en_US.UTF-8
2014-07-22 08:33:09,518 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_PREFIX=hbase-hadoop-regionserver-sceplus-vm49
2014-07-22 08:33:09,518 INFO  [main] util.ServerCommandLine: env:HBASE_IDENT_STRING=hadoop
2014-07-22 08:33:09,520 INFO  [main] util.ServerCommandLine: vmName=OpenJDK 64-Bit Server VM, vmVendor=Sun Microsystems Inc., vmVersion=23.25-b01
2014-07-22 08:33:09,521 INFO  [main] util.ServerCommandLine: vmInputArguments=[-Dproc_regionserver, -XX:OnOutOfMemoryError=kill -9 %p, -Xmx10240m, -XX:+UseConcMarkSweepGC, -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs, -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm49.log, -Dhbase.home.dir=/home/hadoop/hbase/bin/.., -Dhbase.id.str=hadoop, -Dhbase.root.logger=INFO,RFA, -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64, -Dhbase.security.logger=INFO,RFAS]
2014-07-22 08:33:09,744 DEBUG [main] regionserver.HRegionServer: regionserver/sceplus-vm49.almaden.ibm.com/9.1.143.59:60020 HConnection server-to-server retries=350
2014-07-22 08:33:10,207 INFO  [main] ipc.RpcServer: regionserver/sceplus-vm49.almaden.ibm.com/9.1.143.59:60020: started 10 reader(s).
2014-07-22 08:33:10,311 INFO  [main] impl.MetricsConfig: loaded properties from hadoop-metrics2-hbase.properties
2014-07-22 08:33:10,324 INFO  [main] impl.MetricsSinkAdapter: Sink file-all started
2014-07-22 08:33:10,389 INFO  [main] impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.
2014-07-22 08:33:10,390 INFO  [main] impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2014-07-22 08:33:10,390 INFO  [main] impl.MetricsSystemImpl: HBase metrics system started
2014-07-22 08:33:10,395 INFO  [main] impl.MetricsSourceAdapter: MBean for source jvm registered.
2014-07-22 08:33:10,400 INFO  [main] impl.MetricsSourceAdapter: MBean for source IPC,sub=IPC registered.
2014-07-22 08:33:10,483 INFO  [main] impl.MetricsSourceAdapter: MBean for source ugi registered.
2014-07-22 08:33:10,483 WARN  [main] impl.MetricsSystemImpl: Source name ugi already exists!
2014-07-22 08:33:10,487 DEBUG [main] util.DirectMemoryUtils: Failed to retrieve nio.BufferPool direct MemoryUsed attribute.
javax.management.InstanceNotFoundException: java.nio:type=BufferPool,name=direct
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1117)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:678)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:682)
	at org.apache.hadoop.hbase.util.DirectMemoryUtils.<clinit>(DirectMemoryUtils.java:72)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.instantiateBlockCache(CacheConfig.java:396)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.<init>(CacheConfig.java:179)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:621)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:534)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2393)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:61)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:85)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2410)
2014-07-22 08:33:10,490 INFO  [main] hfile.CacheConfig: Allocating LruBlockCache with maximum size 4.0g
2014-07-22 08:33:10,563 INFO  [main] mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2014-07-22 08:33:10,633 INFO  [main] http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
2014-07-22 08:33:10,644 INFO  [main] http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 60030
2014-07-22 08:33:10,646 INFO  [main] http.HttpServer: listener.getLocalPort() returned 60030 webServer.getConnectors()[0].getLocalPort() returned 60030
2014-07-22 08:33:10,646 INFO  [main] http.HttpServer: Jetty bound to port 60030
2014-07-22 08:33:10,646 INFO  [main] mortbay.log: jetty-6.1.26
2014-07-22 08:33:10,968 INFO  [main] mortbay.log: Started SelectChannelConnector@0.0.0.0:60030
2014-07-22 08:33:11,019 INFO  [main] regionserver.ShutdownHook: Installed shutdown hook thread: Shutdownhook:regionserver60020
2014-07-22 08:33:11,019 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2014-07-22 08:33:11,019 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:host.name=sceplus-vm49.almaden.ibm.com
2014-07-22 08:33:11,019 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.version=1.6.0_31
2014-07-22 08:33:11,019 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.vendor=Sun Microsystems Inc.
2014-07-22 08:33:11,019 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-6-openjdk-amd64/jre
2014-07-22 08:33:11,019 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.class.path=/home/hadoop/hbase/bin/../conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-07-22 08:33:11,019 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-07-22 08:33:11,020 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
2014-07-22 08:33:11,020 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
2014-07-22 08:33:11,020 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.name=Linux
2014-07-22 08:33:11,020 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.arch=amd64
2014-07-22 08:33:11,020 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.version=3.13.0-24-generic
2014-07-22 08:33:11,020 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.name=hadoop
2014-07-22 08:33:11,020 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.home=/home/hadoop
2014-07-22 08:33:11,020 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.dir=/home/hadoop/hbase-0.98.3-hadoop1
2014-07-22 08:33:11,021 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=regionserver:60020, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-22 08:33:11,051 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=regionserver:60020 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-22 08:33:11,053 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Opening socket connection to server master/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-22 08:33:11,058 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Socket connection established to master/9.1.143.58:2181, initiating session
2014-07-22 08:33:11,062 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x0, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-22 08:33:11,179 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/master
2014-07-22 08:33:11,179 INFO  [regionserver60020] util.RetryCounter: Sleeping 1000ms before retry #0...
2014-07-22 08:33:12,003 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-22 08:33:12,003 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-22 08:33:12,012 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, sessionid = 0x1475eb4796e0001, negotiated timeout = 90000
2014-07-22 08:33:44,624 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x5f243f31, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-22 08:33:44,626 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x5f243f31 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-22 08:33:44,626 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-22 08:33:44,627 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-22 08:33:44,632 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, sessionid = 0x1475eb4796e0003, negotiated timeout = 90000
2014-07-22 08:33:44,918 DEBUG [regionserver60020] catalog.CatalogTracker: Starting catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@6897f7ca
2014-07-22 08:33:44,922 INFO  [regionserver60020] regionserver.HRegionServer: ClusterId : 3b61b992-e8ee-43f8-b0c6-14cd23a8afbe
2014-07-22 08:33:44,929 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initializing
2014-07-22 08:33:44,937 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Node /hbase/online-snapshot/acquired already exists and this is not a retry
2014-07-22 08:33:44,974 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initialized
2014-07-22 08:33:44,981 INFO  [regionserver60020] regionserver.MemStoreFlusher: globalMemStoreLimit=4.0g, globalMemStoreLimitLowMark=3.8g, maxHeap=9.9g
2014-07-22 08:33:44,986 INFO  [regionserver60020] regionserver.HRegionServer: CompactionChecker runs every 10sec
2014-07-22 08:33:45,009 INFO  [regionserver60020] regionserver.HRegionServer: reportForDuty to master=sceplus-vm48.almaden.ibm.com,60000,1406043189999 with port=60020, startcode=1406043190412
2014-07-22 08:33:45,320 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: hbase.rootdir=hdfs://master:54310/hbase
2014-07-22 08:33:45,320 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: fs.default.name=hdfs://master:54310
2014-07-22 08:33:45,320 INFO  [regionserver60020] regionserver.HRegionServer: Master passed us a different hostname to use; was=sceplus-vm49.almaden.ibm.com, but now=slave1
2014-07-22 08:33:45,349 INFO  [regionserver60020] fs.HFileSystem: Added intercepting call to namenode#getBlockLocations so can do block reordering using class class org.apache.hadoop.hbase.fs.HFileSystem$ReorderWALBlocks
2014-07-22 08:33:45,359 DEBUG [regionserver60020] regionserver.HRegionServer: logdir=hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412
2014-07-22 08:33:45,395 DEBUG [regionserver60020] regionserver.Replication: ReplicationStatisticsThread 300
2014-07-22 08:33:45,406 INFO  [regionserver60020] wal.FSHLog: WAL/HLog configuration: blocksize=64 MB, rollsize=60.8 MB, enabled=true
2014-07-22 08:33:45,512 INFO  [regionserver60020] wal.FSHLog: New WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043225414
2014-07-22 08:33:45,528 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=WAL registered.
2014-07-22 08:33:45,533 INFO  [regionserver60020] regionserver.MetricsRegionServerWrapperImpl: Computing regionserver metrics every 5000 milliseconds
2014-07-22 08:33:45,536 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Server registered.
2014-07-22 08:33:45,541 INFO  [regionserver60020] trace.SpanReceiverHost: SpanReceiver org.cloudera.htrace.impl.LocalFileSpanReceiver was loaded successfully.
2014-07-22 08:33:45,543 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_REGION-slave1:60020, corePoolSize=3, maxPoolSize=3
2014-07-22 08:33:45,543 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_META-slave1:60020, corePoolSize=1, maxPoolSize=1
2014-07-22 08:33:45,544 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_REGION-slave1:60020, corePoolSize=3, maxPoolSize=3
2014-07-22 08:33:45,544 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_META-slave1:60020, corePoolSize=1, maxPoolSize=1
2014-07-22 08:33:45,544 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_LOG_REPLAY_OPS-slave1:60020, corePoolSize=2, maxPoolSize=2
2014-07-22 08:33:45,554 INFO  [regionserver60020] regionserver.ReplicationSourceManager: Current list of replicators: [sceplus-vm48.almaden.ibm.com,60020,1406043191888, slave1,60020,1406043190412] other RSs: [sceplus-vm48.almaden.ibm.com,60020,1406043191888, slave1,60020,1406043190412]
2014-07-22 08:33:45,578 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Replication registered.
2014-07-22 08:33:45,580 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x77b4ae76, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-22 08:33:45,581 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x77b4ae76 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-22 08:33:45,582 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Opening socket connection to server master/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-22 08:33:45,582 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Socket connection established to master/9.1.143.58:2181, initiating session
2014-07-22 08:33:45,587 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Session establishment complete on server master/9.1.143.58:2181, sessionid = 0x475eb480390004, negotiated timeout = 90000
2014-07-22 08:33:45,593 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: starting
2014-07-22 08:33:45,593 INFO  [RpcServer.listener,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: starting
2014-07-22 08:33:45,637 INFO  [regionserver60020] regionserver.HRegionServer: Serving as slave1,60020,1406043190412, RpcServer on sceplus-vm49.almaden.ibm.com/9.1.143.59:60020, sessionid=0x1475eb4796e0001
2014-07-22 08:33:45,637 INFO  [SplitLogWorker-slave1,60020,1406043190412] regionserver.SplitLogWorker: SplitLogWorker slave1,60020,1406043190412 starting
2014-07-22 08:33:45,638 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is starting
2014-07-22 08:33:45,638 DEBUG [regionserver60020] snapshot.RegionServerSnapshotManager: Start Snapshot Manager slave1,60020,1406043190412
2014-07-22 08:33:45,638 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Starting procedure member 'slave1,60020,1406043190412'
2014-07-22 08:33:45,638 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Checking for aborted procedures on node: '/hbase/online-snapshot/abort'
2014-07-22 08:33:45,639 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Looking for new procedures under znode:'/hbase/online-snapshot/acquired'
2014-07-22 08:33:45,640 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is started
2014-07-22 08:33:50,262 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user7,1406039520141.913d0bb1a1ad8f68c9342a13cea0805c.
2014-07-22 08:33:50,418 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 913d0bb1a1ad8f68c9342a13cea0805c from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:33:50,418 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user5,1406039520141.28300f665ce3e05a3eb06717408e8034.
2014-07-22 08:33:50,419 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user3,1406039520141.3d552e2b870e10dba69bc8dc15a4af32.
2014-07-22 08:33:50,420 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 28300f665ce3e05a3eb06717408e8034 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:33:50,421 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 3d552e2b870e10dba69bc8dc15a4af32 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:33:50,421 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user9,1406039520141.cc7f3458c3928f54a89d307862b26b49.
2014-07-22 08:33:50,421 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user4,1406039520141.0b70bfe57f3f3910975048288f97bf2f.
2014-07-22 08:33:50,421 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user2,1406039520140.47533a6ed53c9e75bf374ae6634deb6f.
2014-07-22 08:33:50,422 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,,1406039520140.0dc75b653fd43897cbb26a11f83e8ac9.
2014-07-22 08:33:50,422 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-22 08:33:50,433 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user8,1406039520141.274bcd746cd34a3f6b16189d66aa97de.
2014-07-22 08:33:50,433 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user6,1406039520141.0d3c482726c2c78b77beb7e33bcad23c.
2014-07-22 08:33:50,434 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user1,1406039520140.a9cb96f12245595824ef7f12ad2ccd6a.
2014-07-22 08:33:50,449 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 913d0bb1a1ad8f68c9342a13cea0805c from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:33:50,449 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 3d552e2b870e10dba69bc8dc15a4af32 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:33:50,450 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 28300f665ce3e05a3eb06717408e8034 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:33:50,470 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => 28300f665ce3e05a3eb06717408e8034, NAME => 'usertable,user5,1406039520141.28300f665ce3e05a3eb06717408e8034.', STARTKEY => 'user5', ENDKEY => 'user6'}
2014-07-22 08:33:50,470 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => 3d552e2b870e10dba69bc8dc15a4af32, NAME => 'usertable,user3,1406039520141.3d552e2b870e10dba69bc8dc15a4af32.', STARTKEY => 'user3', ENDKEY => 'user4'}
2014-07-22 08:33:50,473 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => 913d0bb1a1ad8f68c9342a13cea0805c, NAME => 'usertable,user7,1406039520141.913d0bb1a1ad8f68c9342a13cea0805c.', STARTKEY => 'user7', ENDKEY => 'user8'}
2014-07-22 08:33:50,496 INFO  [RS_OPEN_REGION-slave1:60020-2] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Regions registered.
2014-07-22 08:33:50,497 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 3d552e2b870e10dba69bc8dc15a4af32
2014-07-22 08:33:50,497 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 913d0bb1a1ad8f68c9342a13cea0805c
2014-07-22 08:33:50,497 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 28300f665ce3e05a3eb06717408e8034
2014-07-22 08:33:50,497 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user3,1406039520141.3d552e2b870e10dba69bc8dc15a4af32.
2014-07-22 08:33:50,497 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user5,1406039520141.28300f665ce3e05a3eb06717408e8034.
2014-07-22 08:33:50,497 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,user7,1406039520141.913d0bb1a1ad8f68c9342a13cea0805c.
2014-07-22 08:33:50,506 INFO  [RS_OPEN_REGION-slave1:60020-1] util.NativeCodeLoader: Loaded the native-hadoop library
2014-07-22 08:33:50,508 INFO  [RS_OPEN_REGION-slave1:60020-1] zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
2014-07-22 08:33:50,510 INFO  [RS_OPEN_REGION-slave1:60020-2] compress.CodecPool: Got brand-new compressor
2014-07-22 08:33:50,511 INFO  [RS_OPEN_REGION-slave1:60020-0] compress.CodecPool: Got brand-new compressor
2014-07-22 08:33:50,511 INFO  [RS_OPEN_REGION-slave1:60020-1] compress.CodecPool: Got brand-new compressor
2014-07-22 08:33:50,588 INFO  [StoreOpener-28300f665ce3e05a3eb06717408e8034-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 08:33:50,588 INFO  [StoreOpener-3d552e2b870e10dba69bc8dc15a4af32-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 08:33:50,595 INFO  [StoreOpener-913d0bb1a1ad8f68c9342a13cea0805c-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 08:33:50,649 INFO  [StoreFileOpenerThread-family-1] util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc32
2014-07-22 08:33:50,696 INFO  [StoreFileOpenerThread-family-1] compress.CodecPool: Got brand-new decompressor
2014-07-22 08:33:50,709 DEBUG [StoreOpener-3d552e2b870e10dba69bc8dc15a4af32-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d552e2b870e10dba69bc8dc15a4af32/family/02d6a9c9f78d409ea2186cbf2be962fe, isReference=false, isBulkLoadResult=false, seqid=18909, majorCompaction=false
2014-07-22 08:33:50,733 INFO  [StoreFileOpenerThread-family-1] compress.CodecPool: Got brand-new decompressor
2014-07-22 08:33:50,733 DEBUG [StoreOpener-28300f665ce3e05a3eb06717408e8034-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/28300f665ce3e05a3eb06717408e8034/family/16907638d0e1433cb53adc2605a4a6b9, isReference=false, isBulkLoadResult=false, seqid=6455, majorCompaction=false
2014-07-22 08:33:50,736 DEBUG [StoreOpener-913d0bb1a1ad8f68c9342a13cea0805c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/913d0bb1a1ad8f68c9342a13cea0805c/family/0715af3c28494cf99e073b6c584b23c5, isReference=false, isBulkLoadResult=false, seqid=20464, majorCompaction=false
2014-07-22 08:33:50,758 DEBUG [StoreOpener-28300f665ce3e05a3eb06717408e8034-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/28300f665ce3e05a3eb06717408e8034/family/2aeecaa9956d4991bb5e005177f5ad9a, isReference=false, isBulkLoadResult=false, seqid=8046, majorCompaction=false
2014-07-22 08:33:50,762 DEBUG [StoreOpener-3d552e2b870e10dba69bc8dc15a4af32-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d552e2b870e10dba69bc8dc15a4af32/family/410a483f6d0f4307a4ead67ae7cf2c95, isReference=false, isBulkLoadResult=false, seqid=12712, majorCompaction=false
2014-07-22 08:33:50,785 DEBUG [StoreOpener-913d0bb1a1ad8f68c9342a13cea0805c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/913d0bb1a1ad8f68c9342a13cea0805c/family/0f808da3bbd2414cb9222295fef19dd4, isReference=false, isBulkLoadResult=false, seqid=19740, majorCompaction=false
2014-07-22 08:33:50,792 DEBUG [StoreOpener-28300f665ce3e05a3eb06717408e8034-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/28300f665ce3e05a3eb06717408e8034/family/2b26d8fd571549baacdc2a6f0e843d7f, isReference=false, isBulkLoadResult=false, seqid=6731, majorCompaction=false
2014-07-22 08:33:50,797 DEBUG [StoreOpener-3d552e2b870e10dba69bc8dc15a4af32-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d552e2b870e10dba69bc8dc15a4af32/family/4842e971393840ef8ad375199ee267be, isReference=false, isBulkLoadResult=false, seqid=20267, majorCompaction=false
2014-07-22 08:33:50,828 DEBUG [StoreOpener-913d0bb1a1ad8f68c9342a13cea0805c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/913d0bb1a1ad8f68c9342a13cea0805c/family/17a929b359344bbca8465ad832751a61, isReference=false, isBulkLoadResult=false, seqid=14763, majorCompaction=false
2014-07-22 08:33:50,832 DEBUG [StoreOpener-28300f665ce3e05a3eb06717408e8034-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/28300f665ce3e05a3eb06717408e8034/family/318d13cccf5e4107b3c6796f76a20e66, isReference=false, isBulkLoadResult=false, seqid=7688, majorCompaction=false
2014-07-22 08:33:50,835 DEBUG [StoreOpener-3d552e2b870e10dba69bc8dc15a4af32-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d552e2b870e10dba69bc8dc15a4af32/family/49c8a8ad12b14a96a4286f1d5218caa9, isReference=false, isBulkLoadResult=false, seqid=19259, majorCompaction=false
2014-07-22 08:33:50,863 DEBUG [StoreOpener-3d552e2b870e10dba69bc8dc15a4af32-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d552e2b870e10dba69bc8dc15a4af32/family/4ad7a6643edb482484a68c2375190d0b, isReference=false, isBulkLoadResult=false, seqid=21358, majorCompaction=false
2014-07-22 08:33:50,868 DEBUG [StoreOpener-913d0bb1a1ad8f68c9342a13cea0805c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/913d0bb1a1ad8f68c9342a13cea0805c/family/26174f450824479c92aeb0dbd07da3f6, isReference=false, isBulkLoadResult=false, seqid=2871, majorCompaction=true
2014-07-22 08:33:50,881 DEBUG [StoreOpener-28300f665ce3e05a3eb06717408e8034-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/28300f665ce3e05a3eb06717408e8034/family/3fcd4cb0d71a457391626b2370f8fd9b, isReference=false, isBulkLoadResult=false, seqid=3370, majorCompaction=true
2014-07-22 08:33:50,894 DEBUG [StoreOpener-3d552e2b870e10dba69bc8dc15a4af32-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d552e2b870e10dba69bc8dc15a4af32/family/6598928a82024af389b2af5eac06a1e2, isReference=false, isBulkLoadResult=false, seqid=13703, majorCompaction=false
2014-07-22 08:33:50,898 DEBUG [StoreOpener-913d0bb1a1ad8f68c9342a13cea0805c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/913d0bb1a1ad8f68c9342a13cea0805c/family/29d6c1383ffe4753a72a808c69ab410f, isReference=false, isBulkLoadResult=false, seqid=14296, majorCompaction=false
2014-07-22 08:33:50,921 DEBUG [StoreOpener-28300f665ce3e05a3eb06717408e8034-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/28300f665ce3e05a3eb06717408e8034/family/47b3ff75eb0643259862d6e5ca67e8cd, isReference=false, isBulkLoadResult=false, seqid=6319, majorCompaction=false
2014-07-22 08:33:50,923 DEBUG [StoreOpener-3d552e2b870e10dba69bc8dc15a4af32-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d552e2b870e10dba69bc8dc15a4af32/family/6b75c2fcfed64b44bb02a77ba672fdf6, isReference=false, isBulkLoadResult=false, seqid=14858, majorCompaction=false
2014-07-22 08:33:50,933 DEBUG [StoreOpener-913d0bb1a1ad8f68c9342a13cea0805c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/913d0bb1a1ad8f68c9342a13cea0805c/family/2c7821a5e4fc48719fa56c2e8bd6c4ad, isReference=false, isBulkLoadResult=false, seqid=4318, majorCompaction=false
2014-07-22 08:33:50,952 DEBUG [StoreOpener-28300f665ce3e05a3eb06717408e8034-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/28300f665ce3e05a3eb06717408e8034/family/637dd2dd740040ce9bf2c2bf9249b8fa, isReference=false, isBulkLoadResult=false, seqid=7036, majorCompaction=false
2014-07-22 08:33:50,964 DEBUG [StoreOpener-913d0bb1a1ad8f68c9342a13cea0805c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/913d0bb1a1ad8f68c9342a13cea0805c/family/47c1bfea5d5740d5bf2c6a2e58dbb64b, isReference=false, isBulkLoadResult=false, seqid=9419, majorCompaction=false
2014-07-22 08:33:50,968 DEBUG [StoreOpener-3d552e2b870e10dba69bc8dc15a4af32-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d552e2b870e10dba69bc8dc15a4af32/family/713f0d583ea84cd28257286bd65a7afa, isReference=false, isBulkLoadResult=false, seqid=7020, majorCompaction=false
2014-07-22 08:33:51,036 DEBUG [StoreOpener-913d0bb1a1ad8f68c9342a13cea0805c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/913d0bb1a1ad8f68c9342a13cea0805c/family/5d6cb1412030432fa6154541a7110594, isReference=false, isBulkLoadResult=false, seqid=3881, majorCompaction=false
2014-07-22 08:33:51,040 DEBUG [StoreOpener-28300f665ce3e05a3eb06717408e8034-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/28300f665ce3e05a3eb06717408e8034/family/71efb78e075a4e2da70ca8b200369c9d, isReference=false, isBulkLoadResult=false, seqid=4772, majorCompaction=false
2014-07-22 08:33:51,050 DEBUG [StoreOpener-3d552e2b870e10dba69bc8dc15a4af32-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d552e2b870e10dba69bc8dc15a4af32/family/7562e3dced084b27b8b7751dbe3be34b, isReference=false, isBulkLoadResult=false, seqid=16181, majorCompaction=false
2014-07-22 08:33:51,055 DEBUG [StoreOpener-913d0bb1a1ad8f68c9342a13cea0805c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/913d0bb1a1ad8f68c9342a13cea0805c/family/628903e806324cb2bd1d29e6aa9570c6, isReference=false, isBulkLoadResult=false, seqid=20854, majorCompaction=false
2014-07-22 08:33:51,065 DEBUG [StoreOpener-28300f665ce3e05a3eb06717408e8034-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/28300f665ce3e05a3eb06717408e8034/family/afe971dad91f44219f0606fd9129e068, isReference=false, isBulkLoadResult=false, seqid=9192, majorCompaction=false
2014-07-22 08:33:51,077 DEBUG [StoreOpener-3d552e2b870e10dba69bc8dc15a4af32-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d552e2b870e10dba69bc8dc15a4af32/family/7ce9b9caf0cd47078397b2c72a4f7150, isReference=false, isBulkLoadResult=false, seqid=19582, majorCompaction=false
2014-07-22 08:33:51,078 DEBUG [StoreOpener-913d0bb1a1ad8f68c9342a13cea0805c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/913d0bb1a1ad8f68c9342a13cea0805c/family/834b1a13d272463aaf005dcf83ed5ff3, isReference=false, isBulkLoadResult=false, seqid=19390, majorCompaction=false
2014-07-22 08:33:51,084 DEBUG [StoreOpener-28300f665ce3e05a3eb06717408e8034-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/28300f665ce3e05a3eb06717408e8034/family/b3879f6295b244af85c4ec44a0c4af2d, isReference=false, isBulkLoadResult=false, seqid=8776, majorCompaction=false
2014-07-22 08:33:51,097 DEBUG [StoreOpener-913d0bb1a1ad8f68c9342a13cea0805c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/913d0bb1a1ad8f68c9342a13cea0805c/family/8fbdc493c8f84bef858e9f56aa757a63, isReference=false, isBulkLoadResult=false, seqid=15365, majorCompaction=false
2014-07-22 08:33:51,103 DEBUG [StoreOpener-3d552e2b870e10dba69bc8dc15a4af32-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d552e2b870e10dba69bc8dc15a4af32/family/973e6d93968d452399aee721ab0eb4a8, isReference=false, isBulkLoadResult=false, seqid=19925, majorCompaction=false
2014-07-22 08:33:51,117 DEBUG [StoreOpener-28300f665ce3e05a3eb06717408e8034-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/28300f665ce3e05a3eb06717408e8034/family/c415675b1db74aef97e0364e25485795, isReference=false, isBulkLoadResult=false, seqid=8423, majorCompaction=false
2014-07-22 08:33:51,153 DEBUG [StoreOpener-3d552e2b870e10dba69bc8dc15a4af32-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d552e2b870e10dba69bc8dc15a4af32/family/a4b21784a2164d3786c9e5bff17e9d47, isReference=false, isBulkLoadResult=false, seqid=13174, majorCompaction=false
2014-07-22 08:33:51,160 DEBUG [StoreOpener-913d0bb1a1ad8f68c9342a13cea0805c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/913d0bb1a1ad8f68c9342a13cea0805c/family/934df772e6934da480e9fde8aae01f7f, isReference=false, isBulkLoadResult=false, seqid=21329, majorCompaction=false
2014-07-22 08:33:51,182 DEBUG [StoreOpener-3d552e2b870e10dba69bc8dc15a4af32-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d552e2b870e10dba69bc8dc15a4af32/family/ab080f96f8074c8bb279ba53ff036afc, isReference=false, isBulkLoadResult=false, seqid=14104, majorCompaction=false
2014-07-22 08:33:51,183 DEBUG [StoreOpener-28300f665ce3e05a3eb06717408e8034-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/28300f665ce3e05a3eb06717408e8034/family/cfccc76b0b5d49319bbdbe75f8ee2e3a, isReference=false, isBulkLoadResult=false, seqid=4916, majorCompaction=false
2014-07-22 08:33:51,200 DEBUG [StoreOpener-913d0bb1a1ad8f68c9342a13cea0805c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/913d0bb1a1ad8f68c9342a13cea0805c/family/999a71aacf834c669901c0ca94cd3be5, isReference=false, isBulkLoadResult=false, seqid=9057, majorCompaction=false
2014-07-22 08:33:51,204 DEBUG [StoreOpener-28300f665ce3e05a3eb06717408e8034-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/28300f665ce3e05a3eb06717408e8034/family/d57fb70c6615446ba8479e1bfaaac60f, isReference=false, isBulkLoadResult=false, seqid=9233, majorCompaction=false
2014-07-22 08:33:51,213 DEBUG [StoreOpener-3d552e2b870e10dba69bc8dc15a4af32-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d552e2b870e10dba69bc8dc15a4af32/family/b3ec6be090a54c2cb299e45b4071562d, isReference=false, isBulkLoadResult=false, seqid=14514, majorCompaction=false
2014-07-22 08:33:51,227 DEBUG [StoreOpener-28300f665ce3e05a3eb06717408e8034-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/28300f665ce3e05a3eb06717408e8034/family/e40336bb3edb4cfdba097377a7be8aa7, isReference=false, isBulkLoadResult=false, seqid=6593, majorCompaction=false
2014-07-22 08:33:51,227 DEBUG [StoreOpener-913d0bb1a1ad8f68c9342a13cea0805c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/913d0bb1a1ad8f68c9342a13cea0805c/family/a34fe178238d4862949b083f1a6d081b, isReference=false, isBulkLoadResult=false, seqid=9850, majorCompaction=false
2014-07-22 08:33:51,281 DEBUG [StoreOpener-3d552e2b870e10dba69bc8dc15a4af32-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d552e2b870e10dba69bc8dc15a4af32/family/bc0456a6b75847b8957dbbe13b69e190, isReference=false, isBulkLoadResult=false, seqid=15318, majorCompaction=false
2014-07-22 08:33:51,286 DEBUG [StoreOpener-28300f665ce3e05a3eb06717408e8034-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/28300f665ce3e05a3eb06717408e8034/family/f5041333ab69429ebdb1e179cb8f3b5a, isReference=false, isBulkLoadResult=false, seqid=7341, majorCompaction=false
2014-07-22 08:33:51,293 DEBUG [StoreOpener-913d0bb1a1ad8f68c9342a13cea0805c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/913d0bb1a1ad8f68c9342a13cea0805c/family/a4da17b11dc04824bfd564d7d6ec3c30, isReference=false, isBulkLoadResult=false, seqid=18740, majorCompaction=false
2014-07-22 08:33:51,342 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/28300f665ce3e05a3eb06717408e8034
2014-07-22 08:33:51,349 DEBUG [StoreOpener-3d552e2b870e10dba69bc8dc15a4af32-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d552e2b870e10dba69bc8dc15a4af32/family/d568dc6f0ebc4068a8979b9d828563ab, isReference=false, isBulkLoadResult=false, seqid=15763, majorCompaction=false
2014-07-22 08:33:51,357 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined 28300f665ce3e05a3eb06717408e8034; next sequenceid=9234
2014-07-22 08:33:51,357 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 28300f665ce3e05a3eb06717408e8034
2014-07-22 08:33:51,361 INFO  [PostOpenDeployTasks:28300f665ce3e05a3eb06717408e8034] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user5,1406039520141.28300f665ce3e05a3eb06717408e8034.
2014-07-22 08:33:51,366 DEBUG [PostOpenDeployTasks:28300f665ce3e05a3eb06717408e8034] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:33:51,369 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 2000 blocking
2014-07-22 08:33:51,377 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 10 files of size 1272163378 starting at candidate #5 after considering 76 permutations with 66 in ratio
2014-07-22 08:33:51,380 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.HStore: 28300f665ce3e05a3eb06717408e8034 - family: Initiating minor compaction
2014-07-22 08:33:51,380 INFO  [regionserver60020-smallCompactions-1406043231365] regionserver.HRegion: Starting compaction on family in region usertable,user5,1406039520141.28300f665ce3e05a3eb06717408e8034.
2014-07-22 08:33:51,381 INFO  [regionserver60020-smallCompactions-1406043231365] regionserver.HStore: Starting compaction of 10 file(s) in family of usertable,user5,1406039520141.28300f665ce3e05a3eb06717408e8034. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/28300f665ce3e05a3eb06717408e8034/.tmp, totalSize=1.2g
2014-07-22 08:33:51,384 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/28300f665ce3e05a3eb06717408e8034/family/e40336bb3edb4cfdba097377a7be8aa7, keycount=94248, bloomtype=ROW, size=67.2m, encoding=NONE, seqNum=6593
2014-07-22 08:33:51,384 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/28300f665ce3e05a3eb06717408e8034/family/2b26d8fd571549baacdc2a6f0e843d7f, keycount=93542, bloomtype=ROW, size=66.7m, encoding=NONE, seqNum=6731
2014-07-22 08:33:51,384 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/28300f665ce3e05a3eb06717408e8034/family/637dd2dd740040ce9bf2c2bf9249b8fa, keycount=215525, bloomtype=ROW, size=153.5m, encoding=NONE, seqNum=7036
2014-07-22 08:33:51,384 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/28300f665ce3e05a3eb06717408e8034/family/f5041333ab69429ebdb1e179cb8f3b5a, keycount=204791, bloomtype=ROW, size=145.9m, encoding=NONE, seqNum=7341
2014-07-22 08:33:51,384 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/28300f665ce3e05a3eb06717408e8034/family/318d13cccf5e4107b3c6796f76a20e66, keycount=219213, bloomtype=ROW, size=156.1m, encoding=NONE, seqNum=7688
2014-07-22 08:33:51,385 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/28300f665ce3e05a3eb06717408e8034/family/2aeecaa9956d4991bb5e005177f5ad9a, keycount=211902, bloomtype=ROW, size=150.9m, encoding=NONE, seqNum=8046
2014-07-22 08:33:51,385 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/28300f665ce3e05a3eb06717408e8034/family/c415675b1db74aef97e0364e25485795, keycount=211929, bloomtype=ROW, size=151.0m, encoding=NONE, seqNum=8423
2014-07-22 08:33:51,385 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/28300f665ce3e05a3eb06717408e8034/family/b3879f6295b244af85c4ec44a0c4af2d, keycount=197598, bloomtype=ROW, size=140.8m, encoding=NONE, seqNum=8776
2014-07-22 08:33:51,385 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/28300f665ce3e05a3eb06717408e8034/family/afe971dad91f44219f0606fd9129e068, keycount=232541, bloomtype=ROW, size=165.6m, encoding=NONE, seqNum=9192
2014-07-22 08:33:51,385 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/28300f665ce3e05a3eb06717408e8034/family/d57fb70c6615446ba8479e1bfaaac60f, keycount=21895, bloomtype=ROW, size=15.6m, encoding=NONE, seqNum=9233
2014-07-22 08:33:51,389 DEBUG [StoreOpener-913d0bb1a1ad8f68c9342a13cea0805c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/913d0bb1a1ad8f68c9342a13cea0805c/family/cf194d1fdd1a440d9221fbb525b650d3, isReference=false, isBulkLoadResult=false, seqid=19069, majorCompaction=false
2014-07-22 08:33:51,395 DEBUG [StoreOpener-3d552e2b870e10dba69bc8dc15a4af32-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d552e2b870e10dba69bc8dc15a4af32/family/dfaa30aadb504d4599e377908ac5db1b, isReference=false, isBulkLoadResult=false, seqid=7856, majorCompaction=false
2014-07-22 08:33:51,408 DEBUG [StoreOpener-913d0bb1a1ad8f68c9342a13cea0805c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/913d0bb1a1ad8f68c9342a13cea0805c/family/eefc91124e2a4b0888e2d7415f316554, isReference=false, isBulkLoadResult=false, seqid=21220, majorCompaction=false
2014-07-22 08:33:51,430 DEBUG [StoreOpener-3d552e2b870e10dba69bc8dc15a4af32-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d552e2b870e10dba69bc8dc15a4af32/family/e421397c2fba4eb28b9e1ba35761bcab, isReference=false, isBulkLoadResult=false, seqid=2389, majorCompaction=true
2014-07-22 08:33:51,460 DEBUG [StoreOpener-3d552e2b870e10dba69bc8dc15a4af32-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d552e2b870e10dba69bc8dc15a4af32/family/f394983e148445b0863a5d30cfc9f530, isReference=false, isBulkLoadResult=false, seqid=20691, majorCompaction=false
2014-07-22 08:33:51,463 DEBUG [StoreOpener-913d0bb1a1ad8f68c9342a13cea0805c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/913d0bb1a1ad8f68c9342a13cea0805c/family/f579545d3a704588a688576baca8fccd, isReference=false, isBulkLoadResult=false, seqid=3413, majorCompaction=false
2014-07-22 08:33:51,475 DEBUG [StoreOpener-3d552e2b870e10dba69bc8dc15a4af32-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d552e2b870e10dba69bc8dc15a4af32/family/f75c34a68554414e898027686086d9c9, isReference=false, isBulkLoadResult=false, seqid=21083, majorCompaction=false
2014-07-22 08:33:51,480 DEBUG [StoreOpener-913d0bb1a1ad8f68c9342a13cea0805c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/913d0bb1a1ad8f68c9342a13cea0805c/family/fd3617564e2849f7a19687990a17989c, isReference=false, isBulkLoadResult=false, seqid=20065, majorCompaction=false
2014-07-22 08:33:51,485 DEBUG [regionserver60020-smallCompactions-1406043231365] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:33:51,486 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/913d0bb1a1ad8f68c9342a13cea0805c
2014-07-22 08:33:51,491 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined 913d0bb1a1ad8f68c9342a13cea0805c; next sequenceid=21330
2014-07-22 08:33:51,491 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 913d0bb1a1ad8f68c9342a13cea0805c
2014-07-22 08:33:51,493 INFO  [PostOpenDeployTasks:913d0bb1a1ad8f68c9342a13cea0805c] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user7,1406039520141.913d0bb1a1ad8f68c9342a13cea0805c.
2014-07-22 08:33:51,494 DEBUG [PostOpenDeployTasks:913d0bb1a1ad8f68c9342a13cea0805c] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-22 08:33:51,503 DEBUG [StoreOpener-3d552e2b870e10dba69bc8dc15a4af32-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d552e2b870e10dba69bc8dc15a4af32/family/f9f39bcdb35641dfa72590061a0f2e04, isReference=false, isBulkLoadResult=false, seqid=7384, majorCompaction=false
2014-07-22 08:33:51,507 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/3d552e2b870e10dba69bc8dc15a4af32
2014-07-22 08:33:51,509 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined 3d552e2b870e10dba69bc8dc15a4af32; next sequenceid=21359
2014-07-22 08:33:51,510 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 3d552e2b870e10dba69bc8dc15a4af32
2014-07-22 08:33:51,511 INFO  [PostOpenDeployTasks:3d552e2b870e10dba69bc8dc15a4af32] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user3,1406039520141.3d552e2b870e10dba69bc8dc15a4af32.
2014-07-22 08:33:51,511 DEBUG [PostOpenDeployTasks:3d552e2b870e10dba69bc8dc15a4af32] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:2), split_queue=0, merge_queue=0
2014-07-22 08:33:51,640 INFO  [PostOpenDeployTasks:3d552e2b870e10dba69bc8dc15a4af32] catalog.MetaEditor: Updated row usertable,user3,1406039520141.3d552e2b870e10dba69bc8dc15a4af32. with server=slave1,60020,1406043190412
2014-07-22 08:33:51,640 INFO  [PostOpenDeployTasks:28300f665ce3e05a3eb06717408e8034] catalog.MetaEditor: Updated row usertable,user5,1406039520141.28300f665ce3e05a3eb06717408e8034. with server=slave1,60020,1406043190412
2014-07-22 08:33:51,641 INFO  [PostOpenDeployTasks:3d552e2b870e10dba69bc8dc15a4af32] regionserver.HRegionServer: Finished post open deploy task for usertable,user3,1406039520141.3d552e2b870e10dba69bc8dc15a4af32.
2014-07-22 08:33:51,641 INFO  [PostOpenDeployTasks:28300f665ce3e05a3eb06717408e8034] regionserver.HRegionServer: Finished post open deploy task for usertable,user5,1406039520141.28300f665ce3e05a3eb06717408e8034.
2014-07-22 08:33:51,640 INFO  [PostOpenDeployTasks:913d0bb1a1ad8f68c9342a13cea0805c] catalog.MetaEditor: Updated row usertable,user7,1406039520141.913d0bb1a1ad8f68c9342a13cea0805c. with server=slave1,60020,1406043190412
2014-07-22 08:33:51,642 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 3d552e2b870e10dba69bc8dc15a4af32 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:33:51,643 INFO  [PostOpenDeployTasks:913d0bb1a1ad8f68c9342a13cea0805c] regionserver.HRegionServer: Finished post open deploy task for usertable,user7,1406039520141.913d0bb1a1ad8f68c9342a13cea0805c.
2014-07-22 08:33:51,643 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 28300f665ce3e05a3eb06717408e8034 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:33:51,643 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 913d0bb1a1ad8f68c9342a13cea0805c from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:33:51,648 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 3d552e2b870e10dba69bc8dc15a4af32 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:33:51,649 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned 3d552e2b870e10dba69bc8dc15a4af32 to OPENED in zk on slave1,60020,1406043190412
2014-07-22 08:33:51,649 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user3,1406039520141.3d552e2b870e10dba69bc8dc15a4af32. on slave1,60020,1406043190412
2014-07-22 08:33:51,649 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning cc7f3458c3928f54a89d307862b26b49 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:33:51,650 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 28300f665ce3e05a3eb06717408e8034 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:33:51,650 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned 28300f665ce3e05a3eb06717408e8034 to OPENED in zk on slave1,60020,1406043190412
2014-07-22 08:33:51,650 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user5,1406039520141.28300f665ce3e05a3eb06717408e8034. on slave1,60020,1406043190412
2014-07-22 08:33:51,650 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 913d0bb1a1ad8f68c9342a13cea0805c from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:33:51,650 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned 913d0bb1a1ad8f68c9342a13cea0805c to OPENED in zk on slave1,60020,1406043190412
2014-07-22 08:33:51,651 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,user7,1406039520141.913d0bb1a1ad8f68c9342a13cea0805c. on slave1,60020,1406043190412
2014-07-22 08:33:51,651 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 0b70bfe57f3f3910975048288f97bf2f from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:33:51,652 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 47533a6ed53c9e75bf374ae6634deb6f from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:33:51,655 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node cc7f3458c3928f54a89d307862b26b49 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:33:51,656 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => cc7f3458c3928f54a89d307862b26b49, NAME => 'usertable,user9,1406039520141.cc7f3458c3928f54a89d307862b26b49.', STARTKEY => 'user9', ENDKEY => ''}
2014-07-22 08:33:51,656 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 0b70bfe57f3f3910975048288f97bf2f from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:33:51,656 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 47533a6ed53c9e75bf374ae6634deb6f from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:33:51,656 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => 0b70bfe57f3f3910975048288f97bf2f, NAME => 'usertable,user4,1406039520141.0b70bfe57f3f3910975048288f97bf2f.', STARTKEY => 'user4', ENDKEY => 'user5'}
2014-07-22 08:33:51,656 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => 47533a6ed53c9e75bf374ae6634deb6f, NAME => 'usertable,user2,1406039520140.47533a6ed53c9e75bf374ae6634deb6f.', STARTKEY => 'user2', ENDKEY => 'user3'}
2014-07-22 08:33:51,657 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable cc7f3458c3928f54a89d307862b26b49
2014-07-22 08:33:51,657 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 0b70bfe57f3f3910975048288f97bf2f
2014-07-22 08:33:51,657 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user9,1406039520141.cc7f3458c3928f54a89d307862b26b49.
2014-07-22 08:33:51,657 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user4,1406039520141.0b70bfe57f3f3910975048288f97bf2f.
2014-07-22 08:33:51,657 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 47533a6ed53c9e75bf374ae6634deb6f
2014-07-22 08:33:51,657 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,user2,1406039520140.47533a6ed53c9e75bf374ae6634deb6f.
2014-07-22 08:33:51,668 INFO  [StoreOpener-47533a6ed53c9e75bf374ae6634deb6f-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 08:33:51,669 INFO  [StoreOpener-0b70bfe57f3f3910975048288f97bf2f-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 08:33:51,672 INFO  [StoreOpener-cc7f3458c3928f54a89d307862b26b49-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 08:33:51,695 DEBUG [StoreOpener-cc7f3458c3928f54a89d307862b26b49-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cc7f3458c3928f54a89d307862b26b49/family/0e00bc340a334e7e950e0cc99494b766, isReference=false, isBulkLoadResult=false, seqid=9184, majorCompaction=false
2014-07-22 08:33:51,697 DEBUG [StoreOpener-0b70bfe57f3f3910975048288f97bf2f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0b70bfe57f3f3910975048288f97bf2f/family/10a6ecf352584c44ac5047c8de761353, isReference=false, isBulkLoadResult=false, seqid=1820, majorCompaction=true
2014-07-22 08:33:51,701 DEBUG [StoreOpener-47533a6ed53c9e75bf374ae6634deb6f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/47533a6ed53c9e75bf374ae6634deb6f/family/0ff5036514f34493b0972104c44578d3, isReference=false, isBulkLoadResult=false, seqid=20326, majorCompaction=false
2014-07-22 08:33:51,712 DEBUG [StoreOpener-47533a6ed53c9e75bf374ae6634deb6f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/47533a6ed53c9e75bf374ae6634deb6f/family/10d538f90b1e46ddb357f306ae443120, isReference=false, isBulkLoadResult=false, seqid=21829, majorCompaction=false
2014-07-22 08:33:51,721 DEBUG [StoreOpener-cc7f3458c3928f54a89d307862b26b49-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cc7f3458c3928f54a89d307862b26b49/family/17bde5456df44c138055e5ff8311e48f, isReference=false, isBulkLoadResult=false, seqid=6952, majorCompaction=false
2014-07-22 08:33:51,725 DEBUG [StoreOpener-0b70bfe57f3f3910975048288f97bf2f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0b70bfe57f3f3910975048288f97bf2f/family/1e2a334c1b0748fdaf04a3e3a43ac6bb, isReference=false, isBulkLoadResult=false, seqid=18712, majorCompaction=false
2014-07-22 08:33:51,735 DEBUG [StoreOpener-47533a6ed53c9e75bf374ae6634deb6f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/47533a6ed53c9e75bf374ae6634deb6f/family/180d5fd743c144a89b394a00f88a6bc3, isReference=false, isBulkLoadResult=false, seqid=21119, majorCompaction=false
2014-07-22 08:33:51,745 DEBUG [StoreOpener-cc7f3458c3928f54a89d307862b26b49-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cc7f3458c3928f54a89d307862b26b49/family/67172c56e79a4dcea46354a171002787, isReference=false, isBulkLoadResult=false, seqid=1431, majorCompaction=true
2014-07-22 08:33:51,746 DEBUG [StoreOpener-0b70bfe57f3f3910975048288f97bf2f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0b70bfe57f3f3910975048288f97bf2f/family/245a1fa5a168407ca3b4ceb933d94212, isReference=false, isBulkLoadResult=false, seqid=21125, majorCompaction=false
2014-07-22 08:33:51,765 DEBUG [StoreOpener-cc7f3458c3928f54a89d307862b26b49-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cc7f3458c3928f54a89d307862b26b49/family/7fac28f3e36746198e7e8d2d6a379d93, isReference=false, isBulkLoadResult=false, seqid=8797, majorCompaction=false
2014-07-22 08:33:51,765 DEBUG [StoreOpener-47533a6ed53c9e75bf374ae6634deb6f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/47533a6ed53c9e75bf374ae6634deb6f/family/21523c3579174b3e8397c20807468b19, isReference=false, isBulkLoadResult=false, seqid=15918, majorCompaction=false
2014-07-22 08:33:51,774 DEBUG [StoreOpener-0b70bfe57f3f3910975048288f97bf2f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0b70bfe57f3f3910975048288f97bf2f/family/2a90bed5aed9442bb9a5884eb6961f66, isReference=false, isBulkLoadResult=false, seqid=7753, majorCompaction=false
2014-07-22 08:33:51,788 DEBUG [StoreOpener-47533a6ed53c9e75bf374ae6634deb6f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/47533a6ed53c9e75bf374ae6634deb6f/family/2c46aec4b7a24abea15e05e61fd4fb81, isReference=false, isBulkLoadResult=false, seqid=21533, majorCompaction=false
2014-07-22 08:33:51,792 DEBUG [StoreOpener-cc7f3458c3928f54a89d307862b26b49-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cc7f3458c3928f54a89d307862b26b49/family/a71d991dce9f41c6ac47aed59de6423b, isReference=false, isBulkLoadResult=false, seqid=7528, majorCompaction=false
2014-07-22 08:33:51,837 DEBUG [StoreOpener-0b70bfe57f3f3910975048288f97bf2f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0b70bfe57f3f3910975048288f97bf2f/family/2f3fb7390c1f4c82bae58136ad96b1d9, isReference=false, isBulkLoadResult=false, seqid=20349, majorCompaction=false
2014-07-22 08:33:51,843 DEBUG [StoreOpener-47533a6ed53c9e75bf374ae6634deb6f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/47533a6ed53c9e75bf374ae6634deb6f/family/45f1bdf02d1a4725909887ba2f5f8a2f, isReference=false, isBulkLoadResult=false, seqid=16260, majorCompaction=false
2014-07-22 08:33:51,886 DEBUG [StoreOpener-0b70bfe57f3f3910975048288f97bf2f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0b70bfe57f3f3910975048288f97bf2f/family/3db0048d79d24889a7e51822a5420714, isReference=false, isBulkLoadResult=false, seqid=21356, majorCompaction=false
2014-07-22 08:33:51,895 DEBUG [StoreOpener-cc7f3458c3928f54a89d307862b26b49-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cc7f3458c3928f54a89d307862b26b49/family/d01cac8058b64f69b5d602a7e730eeef, isReference=false, isBulkLoadResult=false, seqid=6044, majorCompaction=false
2014-07-22 08:33:51,910 DEBUG [StoreOpener-47533a6ed53c9e75bf374ae6634deb6f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/47533a6ed53c9e75bf374ae6634deb6f/family/5d734ed62cf64aec866bc139d6fe3e55, isReference=false, isBulkLoadResult=false, seqid=15199, majorCompaction=false
2014-07-22 08:33:51,918 DEBUG [StoreOpener-cc7f3458c3928f54a89d307862b26b49-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cc7f3458c3928f54a89d307862b26b49/family/d3badccad3b84e1cb5f73fadddc8fdde, isReference=false, isBulkLoadResult=false, seqid=8163, majorCompaction=false
2014-07-22 08:33:51,964 DEBUG [StoreOpener-0b70bfe57f3f3910975048288f97bf2f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0b70bfe57f3f3910975048288f97bf2f/family/4c022ebc31cb44128d9b78eac9dc3e94, isReference=false, isBulkLoadResult=false, seqid=12105, majorCompaction=false
2014-07-22 08:33:51,964 DEBUG [StoreOpener-47533a6ed53c9e75bf374ae6634deb6f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/47533a6ed53c9e75bf374ae6634deb6f/family/6ff6def662d745d9bbe4330a3a6c3e10, isReference=false, isBulkLoadResult=false, seqid=17413, majorCompaction=false
2014-07-22 08:33:51,985 DEBUG [StoreOpener-cc7f3458c3928f54a89d307862b26b49-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cc7f3458c3928f54a89d307862b26b49/family/fbcd08f9b24b44d6bab606b128670394, isReference=false, isBulkLoadResult=false, seqid=6487, majorCompaction=false
2014-07-22 08:33:51,990 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/cc7f3458c3928f54a89d307862b26b49
2014-07-22 08:33:51,990 DEBUG [StoreOpener-0b70bfe57f3f3910975048288f97bf2f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0b70bfe57f3f3910975048288f97bf2f/family/6533511828a84329a57d83dba82628b4, isReference=false, isBulkLoadResult=false, seqid=6455, majorCompaction=false
2014-07-22 08:33:51,995 DEBUG [StoreOpener-47533a6ed53c9e75bf374ae6634deb6f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/47533a6ed53c9e75bf374ae6634deb6f/family/9ea09b5982224c029741ce41ac103515, isReference=false, isBulkLoadResult=false, seqid=15575, majorCompaction=false
2014-07-22 08:33:51,997 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined cc7f3458c3928f54a89d307862b26b49; next sequenceid=9185
2014-07-22 08:33:51,997 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node cc7f3458c3928f54a89d307862b26b49
2014-07-22 08:33:51,999 INFO  [PostOpenDeployTasks:cc7f3458c3928f54a89d307862b26b49] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user9,1406039520141.cc7f3458c3928f54a89d307862b26b49.
2014-07-22 08:33:51,999 DEBUG [PostOpenDeployTasks:cc7f3458c3928f54a89d307862b26b49] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:3), split_queue=0, merge_queue=0
2014-07-22 08:33:52,010 DEBUG [StoreOpener-0b70bfe57f3f3910975048288f97bf2f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0b70bfe57f3f3910975048288f97bf2f/family/70f275138af541329db5da8e7b56ed1c, isReference=false, isBulkLoadResult=false, seqid=19991, majorCompaction=false
2014-07-22 08:33:52,012 INFO  [PostOpenDeployTasks:cc7f3458c3928f54a89d307862b26b49] catalog.MetaEditor: Updated row usertable,user9,1406039520141.cc7f3458c3928f54a89d307862b26b49. with server=slave1,60020,1406043190412
2014-07-22 08:33:52,012 INFO  [PostOpenDeployTasks:cc7f3458c3928f54a89d307862b26b49] regionserver.HRegionServer: Finished post open deploy task for usertable,user9,1406039520141.cc7f3458c3928f54a89d307862b26b49.
2014-07-22 08:33:52,013 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning cc7f3458c3928f54a89d307862b26b49 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:33:52,018 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node cc7f3458c3928f54a89d307862b26b49 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:33:52,018 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned cc7f3458c3928f54a89d307862b26b49 to OPENED in zk on slave1,60020,1406043190412
2014-07-22 08:33:52,018 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user9,1406039520141.cc7f3458c3928f54a89d307862b26b49. on slave1,60020,1406043190412
2014-07-22 08:33:52,018 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 0dc75b653fd43897cbb26a11f83e8ac9 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:33:52,022 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 0dc75b653fd43897cbb26a11f83e8ac9 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:33:52,022 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => 0dc75b653fd43897cbb26a11f83e8ac9, NAME => 'usertable,,1406039520140.0dc75b653fd43897cbb26a11f83e8ac9.', STARTKEY => '', ENDKEY => 'user1'}
2014-07-22 08:33:52,023 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 0dc75b653fd43897cbb26a11f83e8ac9
2014-07-22 08:33:52,023 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,,1406039520140.0dc75b653fd43897cbb26a11f83e8ac9.
2014-07-22 08:33:52,067 INFO  [StoreOpener-0dc75b653fd43897cbb26a11f83e8ac9-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 08:33:52,071 DEBUG [StoreOpener-0b70bfe57f3f3910975048288f97bf2f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0b70bfe57f3f3910975048288f97bf2f/family/8b97651a6213457d9f2766fa0bfdcbfa, isReference=false, isBulkLoadResult=false, seqid=7334, majorCompaction=false
2014-07-22 08:33:52,072 DEBUG [StoreOpener-47533a6ed53c9e75bf374ae6634deb6f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/47533a6ed53c9e75bf374ae6634deb6f/family/a05188e069bb46b1a67f10edd7d02cf1, isReference=false, isBulkLoadResult=false, seqid=7874, majorCompaction=true
2014-07-22 08:33:52,077 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/0dc75b653fd43897cbb26a11f83e8ac9
2014-07-22 08:33:52,129 DEBUG [StoreOpener-47533a6ed53c9e75bf374ae6634deb6f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/47533a6ed53c9e75bf374ae6634deb6f/family/a1034730f6fe42ca99b8284ed68000c6, isReference=false, isBulkLoadResult=false, seqid=19782, majorCompaction=false
2014-07-22 08:33:52,130 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined 0dc75b653fd43897cbb26a11f83e8ac9; next sequenceid=1
2014-07-22 08:33:52,130 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 0dc75b653fd43897cbb26a11f83e8ac9
2014-07-22 08:33:52,132 INFO  [PostOpenDeployTasks:0dc75b653fd43897cbb26a11f83e8ac9] regionserver.HRegionServer: Post open deploy tasks for region=usertable,,1406039520140.0dc75b653fd43897cbb26a11f83e8ac9.
2014-07-22 08:33:52,139 DEBUG [StoreOpener-0b70bfe57f3f3910975048288f97bf2f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0b70bfe57f3f3910975048288f97bf2f/family/b1f197069afe4a319159b96e8b8da169, isReference=false, isBulkLoadResult=false, seqid=6986, majorCompaction=false
2014-07-22 08:33:52,142 INFO  [PostOpenDeployTasks:0dc75b653fd43897cbb26a11f83e8ac9] catalog.MetaEditor: Updated row usertable,,1406039520140.0dc75b653fd43897cbb26a11f83e8ac9. with server=slave1,60020,1406043190412
2014-07-22 08:33:52,142 INFO  [PostOpenDeployTasks:0dc75b653fd43897cbb26a11f83e8ac9] regionserver.HRegionServer: Finished post open deploy task for usertable,,1406039520140.0dc75b653fd43897cbb26a11f83e8ac9.
2014-07-22 08:33:52,143 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 0dc75b653fd43897cbb26a11f83e8ac9 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:33:52,148 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 0dc75b653fd43897cbb26a11f83e8ac9 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:33:52,148 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned 0dc75b653fd43897cbb26a11f83e8ac9 to OPENED in zk on slave1,60020,1406043190412
2014-07-22 08:33:52,148 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,,1406039520140.0dc75b653fd43897cbb26a11f83e8ac9. on slave1,60020,1406043190412
2014-07-22 08:33:52,149 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e5ee55a21ff19d69490518939b0887e0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:33:52,154 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e5ee55a21ff19d69490518939b0887e0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:33:52,154 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => e5ee55a21ff19d69490518939b0887e0, NAME => 'hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.', STARTKEY => '', ENDKEY => ''}
2014-07-22 08:33:52,155 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table namespace e5ee55a21ff19d69490518939b0887e0
2014-07-22 08:33:52,156 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-22 08:33:52,157 DEBUG [StoreOpener-47533a6ed53c9e75bf374ae6634deb6f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/47533a6ed53c9e75bf374ae6634deb6f/family/ad248a699afe4767a62b126d060b8c24, isReference=false, isBulkLoadResult=false, seqid=20731, majorCompaction=false
2014-07-22 08:33:52,175 DEBUG [StoreOpener-0b70bfe57f3f3910975048288f97bf2f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0b70bfe57f3f3910975048288f97bf2f/family/bed95f9f04f04a65b46ece63c8218eec, isReference=false, isBulkLoadResult=false, seqid=20730, majorCompaction=false
2014-07-22 08:33:52,177 INFO  [StoreOpener-e5ee55a21ff19d69490518939b0887e0-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 604800000, major jitter 0.500000
2014-07-22 08:33:52,179 DEBUG [StoreOpener-47533a6ed53c9e75bf374ae6634deb6f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/47533a6ed53c9e75bf374ae6634deb6f/family/b74889fa0fc34e598d31ae388e8d4de7, isReference=false, isBulkLoadResult=false, seqid=17857, majorCompaction=false
2014-07-22 08:33:52,224 DEBUG [StoreOpener-0b70bfe57f3f3910975048288f97bf2f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0b70bfe57f3f3910975048288f97bf2f/family/cd8124cd946b461c84b4a70b76c8ba07, isReference=false, isBulkLoadResult=false, seqid=18963, majorCompaction=false
2014-07-22 08:33:52,274 DEBUG [StoreOpener-47533a6ed53c9e75bf374ae6634deb6f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/47533a6ed53c9e75bf374ae6634deb6f/family/cebf4ca29d3345dfbd913d3ede8bd72e, isReference=false, isBulkLoadResult=false, seqid=19090, majorCompaction=false
2014-07-22 08:33:52,278 DEBUG [StoreOpener-0b70bfe57f3f3910975048288f97bf2f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0b70bfe57f3f3910975048288f97bf2f/family/d2a68f84eac54379b96e96763fc5a12f, isReference=false, isBulkLoadResult=false, seqid=19619, majorCompaction=false
2014-07-22 08:33:52,279 DEBUG [StoreOpener-e5ee55a21ff19d69490518939b0887e0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/hbase/namespace/e5ee55a21ff19d69490518939b0887e0/info/5b0102065d284f308d4c0a8d64d9fab5, isReference=false, isBulkLoadResult=false, seqid=4, majorCompaction=false
2014-07-22 08:33:52,299 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/hbase/namespace/e5ee55a21ff19d69490518939b0887e0
2014-07-22 08:33:52,303 DEBUG [StoreOpener-47533a6ed53c9e75bf374ae6634deb6f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/47533a6ed53c9e75bf374ae6634deb6f/family/e09eca1ee793426da8d26d455cd57f79, isReference=false, isBulkLoadResult=false, seqid=18316, majorCompaction=false
2014-07-22 08:33:52,304 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined e5ee55a21ff19d69490518939b0887e0; next sequenceid=5
2014-07-22 08:33:52,304 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node e5ee55a21ff19d69490518939b0887e0
2014-07-22 08:33:52,305 DEBUG [StoreOpener-0b70bfe57f3f3910975048288f97bf2f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0b70bfe57f3f3910975048288f97bf2f/family/dd206ec4fe964e2294522a3a581f9ad5, isReference=false, isBulkLoadResult=false, seqid=19294, majorCompaction=false
2014-07-22 08:33:52,306 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] regionserver.HRegionServer: Post open deploy tasks for region=hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-22 08:33:52,316 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] catalog.MetaEditor: Updated row hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0. with server=slave1,60020,1406043190412
2014-07-22 08:33:52,316 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] regionserver.HRegionServer: Finished post open deploy task for hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-22 08:33:52,318 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e5ee55a21ff19d69490518939b0887e0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:33:52,321 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e5ee55a21ff19d69490518939b0887e0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:33:52,322 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned e5ee55a21ff19d69490518939b0887e0 to OPENED in zk on slave1,60020,1406043190412
2014-07-22 08:33:52,322 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0. on slave1,60020,1406043190412
2014-07-22 08:33:52,322 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 274bcd746cd34a3f6b16189d66aa97de from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:33:52,327 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 274bcd746cd34a3f6b16189d66aa97de from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:33:52,327 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => 274bcd746cd34a3f6b16189d66aa97de, NAME => 'usertable,user8,1406039520141.274bcd746cd34a3f6b16189d66aa97de.', STARTKEY => 'user8', ENDKEY => 'user9'}
2014-07-22 08:33:52,328 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 274bcd746cd34a3f6b16189d66aa97de
2014-07-22 08:33:52,329 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user8,1406039520141.274bcd746cd34a3f6b16189d66aa97de.
2014-07-22 08:33:52,336 DEBUG [StoreOpener-47533a6ed53c9e75bf374ae6634deb6f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/47533a6ed53c9e75bf374ae6634deb6f/family/eedd3bcb9a4044e8b336c3da65117656, isReference=false, isBulkLoadResult=false, seqid=16585, majorCompaction=false
2014-07-22 08:33:52,376 INFO  [StoreOpener-274bcd746cd34a3f6b16189d66aa97de-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 08:33:52,379 DEBUG [StoreOpener-0b70bfe57f3f3910975048288f97bf2f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0b70bfe57f3f3910975048288f97bf2f/family/f2ff0bb80109490896978e2ae250bc6b, isReference=false, isBulkLoadResult=false, seqid=5956, majorCompaction=false
2014-07-22 08:33:52,392 DEBUG [StoreOpener-274bcd746cd34a3f6b16189d66aa97de-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/274bcd746cd34a3f6b16189d66aa97de/family/06d626c0d9024b65b515d511c6af2347, isReference=false, isBulkLoadResult=false, seqid=9228, majorCompaction=false
2014-07-22 08:33:52,394 DEBUG [StoreOpener-47533a6ed53c9e75bf374ae6634deb6f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/47533a6ed53c9e75bf374ae6634deb6f/family/f2729ba9334740828fa892c656ff8052, isReference=false, isBulkLoadResult=false, seqid=17008, majorCompaction=false
2014-07-22 08:33:52,397 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/47533a6ed53c9e75bf374ae6634deb6f
2014-07-22 08:33:52,399 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined 47533a6ed53c9e75bf374ae6634deb6f; next sequenceid=21830
2014-07-22 08:33:52,399 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 47533a6ed53c9e75bf374ae6634deb6f
2014-07-22 08:33:52,401 INFO  [PostOpenDeployTasks:47533a6ed53c9e75bf374ae6634deb6f] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user2,1406039520140.47533a6ed53c9e75bf374ae6634deb6f.
2014-07-22 08:33:52,401 DEBUG [PostOpenDeployTasks:47533a6ed53c9e75bf374ae6634deb6f] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:4), split_queue=0, merge_queue=0
2014-07-22 08:33:52,410 INFO  [PostOpenDeployTasks:47533a6ed53c9e75bf374ae6634deb6f] catalog.MetaEditor: Updated row usertable,user2,1406039520140.47533a6ed53c9e75bf374ae6634deb6f. with server=slave1,60020,1406043190412
2014-07-22 08:33:52,410 INFO  [PostOpenDeployTasks:47533a6ed53c9e75bf374ae6634deb6f] regionserver.HRegionServer: Finished post open deploy task for usertable,user2,1406039520140.47533a6ed53c9e75bf374ae6634deb6f.
2014-07-22 08:33:52,411 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 47533a6ed53c9e75bf374ae6634deb6f from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:33:52,415 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 47533a6ed53c9e75bf374ae6634deb6f from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:33:52,415 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned 47533a6ed53c9e75bf374ae6634deb6f to OPENED in zk on slave1,60020,1406043190412
2014-07-22 08:33:52,415 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,user2,1406039520140.47533a6ed53c9e75bf374ae6634deb6f. on slave1,60020,1406043190412
2014-07-22 08:33:52,415 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 0d3c482726c2c78b77beb7e33bcad23c from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:33:52,417 DEBUG [StoreOpener-274bcd746cd34a3f6b16189d66aa97de-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/274bcd746cd34a3f6b16189d66aa97de/family/0a8cc7ca23a64ba2b3026417e1dd31d9, isReference=false, isBulkLoadResult=false, seqid=7701, majorCompaction=false
2014-07-22 08:33:52,417 DEBUG [StoreOpener-0b70bfe57f3f3910975048288f97bf2f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0b70bfe57f3f3910975048288f97bf2f/family/f8b3fb7972394ca3b0e6ca85de09047a, isReference=false, isBulkLoadResult=false, seqid=16291, majorCompaction=false
2014-07-22 08:33:52,420 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 0d3c482726c2c78b77beb7e33bcad23c from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:33:52,421 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => 0d3c482726c2c78b77beb7e33bcad23c, NAME => 'usertable,user6,1406039520141.0d3c482726c2c78b77beb7e33bcad23c.', STARTKEY => 'user6', ENDKEY => 'user7'}
2014-07-22 08:33:52,422 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 0d3c482726c2c78b77beb7e33bcad23c
2014-07-22 08:33:52,422 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,user6,1406039520141.0d3c482726c2c78b77beb7e33bcad23c.
2014-07-22 08:33:52,422 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/0b70bfe57f3f3910975048288f97bf2f
2014-07-22 08:33:52,425 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined 0b70bfe57f3f3910975048288f97bf2f; next sequenceid=21357
2014-07-22 08:33:52,425 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 0b70bfe57f3f3910975048288f97bf2f
2014-07-22 08:33:52,427 INFO  [PostOpenDeployTasks:0b70bfe57f3f3910975048288f97bf2f] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user4,1406039520141.0b70bfe57f3f3910975048288f97bf2f.
2014-07-22 08:33:52,427 DEBUG [PostOpenDeployTasks:0b70bfe57f3f3910975048288f97bf2f] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:5), split_queue=0, merge_queue=0
2014-07-22 08:33:52,428 INFO  [StoreOpener-0d3c482726c2c78b77beb7e33bcad23c-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 08:33:52,432 DEBUG [StoreOpener-274bcd746cd34a3f6b16189d66aa97de-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/274bcd746cd34a3f6b16189d66aa97de/family/2cbae709c5be45cb8b7f9838e907db7f, isReference=false, isBulkLoadResult=false, seqid=8837, majorCompaction=false
2014-07-22 08:33:52,436 INFO  [PostOpenDeployTasks:0b70bfe57f3f3910975048288f97bf2f] catalog.MetaEditor: Updated row usertable,user4,1406039520141.0b70bfe57f3f3910975048288f97bf2f. with server=slave1,60020,1406043190412
2014-07-22 08:33:52,436 INFO  [PostOpenDeployTasks:0b70bfe57f3f3910975048288f97bf2f] regionserver.HRegionServer: Finished post open deploy task for usertable,user4,1406039520141.0b70bfe57f3f3910975048288f97bf2f.
2014-07-22 08:33:52,437 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 0b70bfe57f3f3910975048288f97bf2f from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:33:52,461 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 0b70bfe57f3f3910975048288f97bf2f from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:33:52,462 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned 0b70bfe57f3f3910975048288f97bf2f to OPENED in zk on slave1,60020,1406043190412
2014-07-22 08:33:52,462 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user4,1406039520141.0b70bfe57f3f3910975048288f97bf2f. on slave1,60020,1406043190412
2014-07-22 08:33:52,463 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning a9cb96f12245595824ef7f12ad2ccd6a from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:33:52,508 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node a9cb96f12245595824ef7f12ad2ccd6a from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:33:52,508 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => a9cb96f12245595824ef7f12ad2ccd6a, NAME => 'usertable,user1,1406039520140.a9cb96f12245595824ef7f12ad2ccd6a.', STARTKEY => 'user1', ENDKEY => 'user2'}
2014-07-22 08:33:52,509 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable a9cb96f12245595824ef7f12ad2ccd6a
2014-07-22 08:33:52,509 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user1,1406039520140.a9cb96f12245595824ef7f12ad2ccd6a.
2014-07-22 08:33:52,513 DEBUG [StoreOpener-274bcd746cd34a3f6b16189d66aa97de-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/274bcd746cd34a3f6b16189d66aa97de/family/39a33c7fe79e49969134c80e50696e7a, isReference=false, isBulkLoadResult=false, seqid=6587, majorCompaction=false
2014-07-22 08:33:52,516 DEBUG [StoreOpener-0d3c482726c2c78b77beb7e33bcad23c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0d3c482726c2c78b77beb7e33bcad23c/family/13f0d42c9ebb4f95bd1a82ddea51f82e, isReference=false, isBulkLoadResult=false, seqid=6766, majorCompaction=false
2014-07-22 08:33:52,519 INFO  [StoreOpener-a9cb96f12245595824ef7f12ad2ccd6a-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 08:33:52,542 DEBUG [StoreOpener-274bcd746cd34a3f6b16189d66aa97de-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/274bcd746cd34a3f6b16189d66aa97de/family/7a21d22314694ec8922b1fef2a2e8cb1, isReference=false, isBulkLoadResult=false, seqid=7042, majorCompaction=false
2014-07-22 08:33:52,551 DEBUG [StoreOpener-0d3c482726c2c78b77beb7e33bcad23c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0d3c482726c2c78b77beb7e33bcad23c/family/15a57906100d47198eee249502e6761f, isReference=false, isBulkLoadResult=false, seqid=18759, majorCompaction=false
2014-07-22 08:33:52,591 DEBUG [StoreOpener-a9cb96f12245595824ef7f12ad2ccd6a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/a9cb96f12245595824ef7f12ad2ccd6a/family/176ac880703b4c9ea70752c93d12e36e, isReference=false, isBulkLoadResult=false, seqid=14083, majorCompaction=false
2014-07-22 08:33:52,592 DEBUG [StoreOpener-274bcd746cd34a3f6b16189d66aa97de-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/274bcd746cd34a3f6b16189d66aa97de/family/7e53451766c3491da8f8dc98521cf7ef, isReference=false, isBulkLoadResult=false, seqid=6725, majorCompaction=false
2014-07-22 08:33:52,610 DEBUG [StoreOpener-a9cb96f12245595824ef7f12ad2ccd6a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/a9cb96f12245595824ef7f12ad2ccd6a/family/24d1fa58b1bd4fc98a92a801cfba1cea, isReference=false, isBulkLoadResult=false, seqid=27860, majorCompaction=false
2014-07-22 08:33:52,613 DEBUG [StoreOpener-0d3c482726c2c78b77beb7e33bcad23c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0d3c482726c2c78b77beb7e33bcad23c/family/17b47994b4e243f9b701fb6a6f134096, isReference=false, isBulkLoadResult=false, seqid=11730, majorCompaction=false
2014-07-22 08:33:52,617 DEBUG [StoreOpener-274bcd746cd34a3f6b16189d66aa97de-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/274bcd746cd34a3f6b16189d66aa97de/family/8bf5c74c702d4332a5e6330dc7c38052, isReference=false, isBulkLoadResult=false, seqid=7365, majorCompaction=false
2014-07-22 08:33:52,625 DEBUG [StoreOpener-a9cb96f12245595824ef7f12ad2ccd6a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/a9cb96f12245595824ef7f12ad2ccd6a/family/2be7e0291ce44a4094f579653c643ce4, isReference=false, isBulkLoadResult=false, seqid=18218, majorCompaction=false
2014-07-22 08:33:52,645 DEBUG [StoreOpener-274bcd746cd34a3f6b16189d66aa97de-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/274bcd746cd34a3f6b16189d66aa97de/family/8e8436b7deb34ddbae3482f5cd77eaca, isReference=false, isBulkLoadResult=false, seqid=8462, majorCompaction=false
2014-07-22 08:33:52,649 DEBUG [StoreOpener-0d3c482726c2c78b77beb7e33bcad23c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0d3c482726c2c78b77beb7e33bcad23c/family/1f13e84dee734931b5b4ee0780205046, isReference=false, isBulkLoadResult=false, seqid=14635, majorCompaction=false
2014-07-22 08:33:52,655 DEBUG [StoreOpener-a9cb96f12245595824ef7f12ad2ccd6a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/a9cb96f12245595824ef7f12ad2ccd6a/family/3cca83ef4d3a487195aba452e1d78bc2, isReference=false, isBulkLoadResult=false, seqid=27056, majorCompaction=false
2014-07-22 08:33:52,663 DEBUG [StoreOpener-274bcd746cd34a3f6b16189d66aa97de-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/274bcd746cd34a3f6b16189d66aa97de/family/93981cd119324b93a101dd276c86899e, isReference=false, isBulkLoadResult=false, seqid=8038, majorCompaction=false
2014-07-22 08:33:52,675 DEBUG [StoreOpener-0d3c482726c2c78b77beb7e33bcad23c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0d3c482726c2c78b77beb7e33bcad23c/family/30855cf2bef544a0a42c3853683249ea, isReference=false, isBulkLoadResult=false, seqid=19990, majorCompaction=false
2014-07-22 08:33:52,688 DEBUG [StoreOpener-a9cb96f12245595824ef7f12ad2ccd6a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/a9cb96f12245595824ef7f12ad2ccd6a/family/4e4149a024974c7581ad06d43c8b8242, isReference=false, isBulkLoadResult=false, seqid=15487, majorCompaction=false
2014-07-22 08:33:52,695 DEBUG [StoreOpener-0d3c482726c2c78b77beb7e33bcad23c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0d3c482726c2c78b77beb7e33bcad23c/family/425157492e64450cab8b76d079acd8ec, isReference=false, isBulkLoadResult=false, seqid=21158, majorCompaction=false
2014-07-22 08:33:52,707 DEBUG [StoreOpener-274bcd746cd34a3f6b16189d66aa97de-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/274bcd746cd34a3f6b16189d66aa97de/family/b57ec2975d824a6790d5b1fb0508396b, isReference=false, isBulkLoadResult=false, seqid=5405, majorCompaction=false
2014-07-22 08:33:52,726 DEBUG [StoreOpener-0d3c482726c2c78b77beb7e33bcad23c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0d3c482726c2c78b77beb7e33bcad23c/family/45076bf2b7a9422e8345551d6da05f6b, isReference=false, isBulkLoadResult=false, seqid=6361, majorCompaction=false
2014-07-22 08:33:52,731 DEBUG [StoreOpener-a9cb96f12245595824ef7f12ad2ccd6a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/a9cb96f12245595824ef7f12ad2ccd6a/family/5ae68a7a6785419a86ac8dfc96e6bed1, isReference=false, isBulkLoadResult=false, seqid=9869, majorCompaction=false
2014-07-22 08:33:52,740 DEBUG [StoreOpener-274bcd746cd34a3f6b16189d66aa97de-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/274bcd746cd34a3f6b16189d66aa97de/family/dca1f5a708994e2b8eb24cd87b0f9ef7, isReference=false, isBulkLoadResult=false, seqid=3948, majorCompaction=true
2014-07-22 08:33:52,743 DEBUG [StoreOpener-0d3c482726c2c78b77beb7e33bcad23c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0d3c482726c2c78b77beb7e33bcad23c/family/5536a1a8085f467391bcabe34db35ec3, isReference=false, isBulkLoadResult=false, seqid=20417, majorCompaction=false
2014-07-22 08:33:52,752 DEBUG [StoreOpener-274bcd746cd34a3f6b16189d66aa97de-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/274bcd746cd34a3f6b16189d66aa97de/family/ec32ff7142b042c5a2fd5ae713cdc8ee, isReference=false, isBulkLoadResult=false, seqid=9190, majorCompaction=false
2014-07-22 08:33:52,754 DEBUG [StoreOpener-0d3c482726c2c78b77beb7e33bcad23c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0d3c482726c2c78b77beb7e33bcad23c/family/5ea72b46b51d46469c9941d5237cc6ee, isReference=false, isBulkLoadResult=false, seqid=21348, majorCompaction=false
2014-07-22 08:33:52,755 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/274bcd746cd34a3f6b16189d66aa97de
2014-07-22 08:33:52,756 DEBUG [StoreOpener-a9cb96f12245595824ef7f12ad2ccd6a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/a9cb96f12245595824ef7f12ad2ccd6a/family/762ea8e887884aec9200f034a840a838, isReference=false, isBulkLoadResult=false, seqid=26338, majorCompaction=false
2014-07-22 08:33:52,758 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined 274bcd746cd34a3f6b16189d66aa97de; next sequenceid=9229
2014-07-22 08:33:52,759 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 274bcd746cd34a3f6b16189d66aa97de
2014-07-22 08:33:52,761 INFO  [PostOpenDeployTasks:274bcd746cd34a3f6b16189d66aa97de] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user8,1406039520141.274bcd746cd34a3f6b16189d66aa97de.
2014-07-22 08:33:52,761 DEBUG [PostOpenDeployTasks:274bcd746cd34a3f6b16189d66aa97de] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:6), split_queue=0, merge_queue=0
2014-07-22 08:33:52,768 INFO  [PostOpenDeployTasks:274bcd746cd34a3f6b16189d66aa97de] catalog.MetaEditor: Updated row usertable,user8,1406039520141.274bcd746cd34a3f6b16189d66aa97de. with server=slave1,60020,1406043190412
2014-07-22 08:33:52,768 INFO  [PostOpenDeployTasks:274bcd746cd34a3f6b16189d66aa97de] regionserver.HRegionServer: Finished post open deploy task for usertable,user8,1406039520141.274bcd746cd34a3f6b16189d66aa97de.
2014-07-22 08:33:52,769 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 274bcd746cd34a3f6b16189d66aa97de from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:33:52,773 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 274bcd746cd34a3f6b16189d66aa97de from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:33:52,773 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned 274bcd746cd34a3f6b16189d66aa97de to OPENED in zk on slave1,60020,1406043190412
2014-07-22 08:33:52,773 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user8,1406039520141.274bcd746cd34a3f6b16189d66aa97de. on slave1,60020,1406043190412
2014-07-22 08:33:52,775 DEBUG [StoreOpener-a9cb96f12245595824ef7f12ad2ccd6a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/a9cb96f12245595824ef7f12ad2ccd6a/family/8a3bc212b27a40a8ad3f6a1eaf5c0654, isReference=false, isBulkLoadResult=false, seqid=8169, majorCompaction=false
2014-07-22 08:33:52,777 DEBUG [StoreOpener-0d3c482726c2c78b77beb7e33bcad23c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0d3c482726c2c78b77beb7e33bcad23c/family/606f6032e4414284a5a3f8d35e34ad05, isReference=false, isBulkLoadResult=false, seqid=7585, majorCompaction=false
2014-07-22 08:33:52,794 DEBUG [StoreOpener-0d3c482726c2c78b77beb7e33bcad23c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0d3c482726c2c78b77beb7e33bcad23c/family/69583ca318024302b9bf4f1ff96d8174, isReference=false, isBulkLoadResult=false, seqid=20775, majorCompaction=false
2014-07-22 08:33:52,816 DEBUG [StoreOpener-a9cb96f12245595824ef7f12ad2ccd6a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/a9cb96f12245595824ef7f12ad2ccd6a/family/8acf80d7180741c5a2602fa97c8cfbf8, isReference=false, isBulkLoadResult=false, seqid=17503, majorCompaction=false
2014-07-22 08:33:52,843 DEBUG [StoreOpener-0d3c482726c2c78b77beb7e33bcad23c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0d3c482726c2c78b77beb7e33bcad23c/family/73448d363b634e1d8048c2528ac4f5c2, isReference=false, isBulkLoadResult=false, seqid=15232, majorCompaction=false
2014-07-22 08:33:52,846 DEBUG [StoreOpener-a9cb96f12245595824ef7f12ad2ccd6a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/a9cb96f12245595824ef7f12ad2ccd6a/family/8d64654fb60c490ab51ea7a9bcc9200d, isReference=false, isBulkLoadResult=false, seqid=12398, majorCompaction=false
2014-07-22 08:33:52,851 DEBUG [StoreOpener-0d3c482726c2c78b77beb7e33bcad23c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0d3c482726c2c78b77beb7e33bcad23c/family/7a9e1bc6d4e648bbbd8a89ca332a20a9, isReference=false, isBulkLoadResult=false, seqid=4860, majorCompaction=false
2014-07-22 08:33:52,860 DEBUG [StoreOpener-a9cb96f12245595824ef7f12ad2ccd6a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/a9cb96f12245595824ef7f12ad2ccd6a/family/92f3626bf7ab4c17bf0491c991de82fc, isReference=false, isBulkLoadResult=false, seqid=28209, majorCompaction=false
2014-07-22 08:33:52,869 DEBUG [StoreOpener-a9cb96f12245595824ef7f12ad2ccd6a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/a9cb96f12245595824ef7f12ad2ccd6a/family/a07e84b09d554ec1a0a54bd30ca5c81a, isReference=false, isBulkLoadResult=false, seqid=28759, majorCompaction=false
2014-07-22 08:33:52,880 DEBUG [StoreOpener-0d3c482726c2c78b77beb7e33bcad23c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0d3c482726c2c78b77beb7e33bcad23c/family/8141904644894fdfa7d07f40c8b70374, isReference=false, isBulkLoadResult=false, seqid=12659, majorCompaction=false
2014-07-22 08:33:52,894 DEBUG [StoreOpener-a9cb96f12245595824ef7f12ad2ccd6a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/a9cb96f12245595824ef7f12ad2ccd6a/family/aea3b9c65f9b449bb88bb5d562dda4d8, isReference=false, isBulkLoadResult=false, seqid=7413, majorCompaction=true
2014-07-22 08:33:52,936 DEBUG [StoreOpener-0d3c482726c2c78b77beb7e33bcad23c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0d3c482726c2c78b77beb7e33bcad23c/family/86be9359341d42e6b89b31083eb5f288, isReference=false, isBulkLoadResult=false, seqid=3970, majorCompaction=true
2014-07-22 08:33:52,951 DEBUG [StoreOpener-0d3c482726c2c78b77beb7e33bcad23c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0d3c482726c2c78b77beb7e33bcad23c/family/8866025d676e4af7bd1d73c8a26ebabe, isReference=false, isBulkLoadResult=false, seqid=4423, majorCompaction=false
2014-07-22 08:33:52,988 DEBUG [StoreOpener-a9cb96f12245595824ef7f12ad2ccd6a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/a9cb96f12245595824ef7f12ad2ccd6a/family/bb1d0c57dfbb41ab9bc00143e47467a5, isReference=false, isBulkLoadResult=false, seqid=24920, majorCompaction=false
2014-07-22 08:33:53,010 DEBUG [StoreOpener-0d3c482726c2c78b77beb7e33bcad23c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0d3c482726c2c78b77beb7e33bcad23c/family/8c87be65ab454d3ca8290da0de88cfaa, isReference=false, isBulkLoadResult=false, seqid=19651, majorCompaction=false
2014-07-22 08:33:53,029 DEBUG [StoreOpener-a9cb96f12245595824ef7f12ad2ccd6a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/a9cb96f12245595824ef7f12ad2ccd6a/family/bcd0c3a156d547078ef5c69f819cba30, isReference=false, isBulkLoadResult=false, seqid=16277, majorCompaction=false
2014-07-22 08:33:53,033 DEBUG [StoreOpener-0d3c482726c2c78b77beb7e33bcad23c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0d3c482726c2c78b77beb7e33bcad23c/family/99fb1873714f45d2be1d71543aba131e, isReference=false, isBulkLoadResult=false, seqid=5361, majorCompaction=false
2014-07-22 08:33:53,048 DEBUG [StoreOpener-a9cb96f12245595824ef7f12ad2ccd6a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/a9cb96f12245595824ef7f12ad2ccd6a/family/bd5b61d6a98b493d80e9c4a7fa71ff7d, isReference=false, isBulkLoadResult=false, seqid=11158, majorCompaction=false
2014-07-22 08:33:53,068 DEBUG [StoreOpener-0d3c482726c2c78b77beb7e33bcad23c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0d3c482726c2c78b77beb7e33bcad23c/family/a3871c4c7a824829b9265d8feb40dda7, isReference=false, isBulkLoadResult=false, seqid=14075, majorCompaction=false
2014-07-22 08:33:53,070 DEBUG [StoreOpener-a9cb96f12245595824ef7f12ad2ccd6a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/a9cb96f12245595824ef7f12ad2ccd6a/family/c6ac275d96d041df82d2f72a657b557b, isReference=false, isBulkLoadResult=false, seqid=16800, majorCompaction=false
2014-07-22 08:33:53,087 DEBUG [StoreOpener-0d3c482726c2c78b77beb7e33bcad23c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0d3c482726c2c78b77beb7e33bcad23c/family/c6bfcd983c5c4566a3425d8cb9540a19, isReference=false, isBulkLoadResult=false, seqid=19326, majorCompaction=false
2014-07-22 08:33:53,092 DEBUG [StoreOpener-a9cb96f12245595824ef7f12ad2ccd6a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/a9cb96f12245595824ef7f12ad2ccd6a/family/d42abb35ad7a42dfb7d99da1ce2ae986, isReference=false, isBulkLoadResult=false, seqid=11793, majorCompaction=false
2014-07-22 08:33:53,099 DEBUG [StoreOpener-0d3c482726c2c78b77beb7e33bcad23c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0d3c482726c2c78b77beb7e33bcad23c/family/cbb71635b2904e6ab8733829ea13816f, isReference=false, isBulkLoadResult=false, seqid=5861, majorCompaction=false
2014-07-22 08:33:53,120 DEBUG [StoreOpener-a9cb96f12245595824ef7f12ad2ccd6a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/a9cb96f12245595824ef7f12ad2ccd6a/family/d757018ec9764954aad559590f31591d, isReference=false, isBulkLoadResult=false, seqid=14764, majorCompaction=false
2014-07-22 08:33:53,132 DEBUG [StoreOpener-0d3c482726c2c78b77beb7e33bcad23c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0d3c482726c2c78b77beb7e33bcad23c/family/d26eaa273fa94dfdb58d235a69a15a71, isReference=false, isBulkLoadResult=false, seqid=18973, majorCompaction=false
2014-07-22 08:33:53,139 DEBUG [StoreOpener-a9cb96f12245595824ef7f12ad2ccd6a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/a9cb96f12245595824ef7f12ad2ccd6a/family/e0191da1277948a5a2691b77cb575a55, isReference=false, isBulkLoadResult=false, seqid=13233, majorCompaction=false
2014-07-22 08:33:53,153 DEBUG [StoreOpener-0d3c482726c2c78b77beb7e33bcad23c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0d3c482726c2c78b77beb7e33bcad23c/family/db228f5a187a47b0867c4889e5b28274, isReference=false, isBulkLoadResult=false, seqid=13054, majorCompaction=false
2014-07-22 08:33:53,159 DEBUG [StoreOpener-a9cb96f12245595824ef7f12ad2ccd6a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/a9cb96f12245595824ef7f12ad2ccd6a/family/e20aa2715eb048de92137ac364d89b69, isReference=false, isBulkLoadResult=false, seqid=25709, majorCompaction=false
2014-07-22 08:33:53,169 DEBUG [StoreOpener-a9cb96f12245595824ef7f12ad2ccd6a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/a9cb96f12245595824ef7f12ad2ccd6a/family/e6c11115e05648dd8090fda9c3099484, isReference=false, isBulkLoadResult=false, seqid=28627, majorCompaction=false
2014-07-22 08:33:53,173 DEBUG [StoreOpener-0d3c482726c2c78b77beb7e33bcad23c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0d3c482726c2c78b77beb7e33bcad23c/family/de2efe6e881c400f9593a83cdb2356a6, isReference=false, isBulkLoadResult=false, seqid=7128, majorCompaction=false
2014-07-22 08:33:53,192 DEBUG [StoreOpener-a9cb96f12245595824ef7f12ad2ccd6a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/a9cb96f12245595824ef7f12ad2ccd6a/family/e94ce7995ea14459a4300ae6018ea167, isReference=false, isBulkLoadResult=false, seqid=10585, majorCompaction=false
2014-07-22 08:33:53,201 DEBUG [StoreOpener-0d3c482726c2c78b77beb7e33bcad23c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0d3c482726c2c78b77beb7e33bcad23c/family/e07be19afa3a43dd8241e15fe978d650, isReference=false, isBulkLoadResult=false, seqid=12156, majorCompaction=false
2014-07-22 08:33:53,212 DEBUG [StoreOpener-a9cb96f12245595824ef7f12ad2ccd6a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/a9cb96f12245595824ef7f12ad2ccd6a/family/ea3ca2ff9424438d9e94d44e20511404, isReference=false, isBulkLoadResult=false, seqid=8930, majorCompaction=false
2014-07-22 08:33:53,215 DEBUG [StoreOpener-0d3c482726c2c78b77beb7e33bcad23c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0d3c482726c2c78b77beb7e33bcad23c/family/e60f07a11c2347ccbbe96684d38994c9, isReference=false, isBulkLoadResult=false, seqid=13539, majorCompaction=false
2014-07-22 08:33:53,223 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/0d3c482726c2c78b77beb7e33bcad23c
2014-07-22 08:33:53,225 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined 0d3c482726c2c78b77beb7e33bcad23c; next sequenceid=21349
2014-07-22 08:33:53,225 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 0d3c482726c2c78b77beb7e33bcad23c
2014-07-22 08:33:53,227 INFO  [PostOpenDeployTasks:0d3c482726c2c78b77beb7e33bcad23c] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user6,1406039520141.0d3c482726c2c78b77beb7e33bcad23c.
2014-07-22 08:33:53,227 DEBUG [PostOpenDeployTasks:0d3c482726c2c78b77beb7e33bcad23c] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:7), split_queue=0, merge_queue=0
2014-07-22 08:33:53,236 INFO  [PostOpenDeployTasks:0d3c482726c2c78b77beb7e33bcad23c] catalog.MetaEditor: Updated row usertable,user6,1406039520141.0d3c482726c2c78b77beb7e33bcad23c. with server=slave1,60020,1406043190412
2014-07-22 08:33:53,237 INFO  [PostOpenDeployTasks:0d3c482726c2c78b77beb7e33bcad23c] regionserver.HRegionServer: Finished post open deploy task for usertable,user6,1406039520141.0d3c482726c2c78b77beb7e33bcad23c.
2014-07-22 08:33:53,237 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 0d3c482726c2c78b77beb7e33bcad23c from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:33:53,238 DEBUG [StoreOpener-a9cb96f12245595824ef7f12ad2ccd6a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/a9cb96f12245595824ef7f12ad2ccd6a/family/ebbabc2a42844790b30a640577a3ce16, isReference=false, isBulkLoadResult=false, seqid=27479, majorCompaction=false
2014-07-22 08:33:53,243 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 0d3c482726c2c78b77beb7e33bcad23c from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:33:53,243 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned 0d3c482726c2c78b77beb7e33bcad23c to OPENED in zk on slave1,60020,1406043190412
2014-07-22 08:33:53,243 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,user6,1406039520141.0d3c482726c2c78b77beb7e33bcad23c. on slave1,60020,1406043190412
2014-07-22 08:33:53,244 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/a9cb96f12245595824ef7f12ad2ccd6a
2014-07-22 08:33:53,247 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined a9cb96f12245595824ef7f12ad2ccd6a; next sequenceid=28760
2014-07-22 08:33:53,247 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node a9cb96f12245595824ef7f12ad2ccd6a
2014-07-22 08:33:53,268 INFO  [PostOpenDeployTasks:a9cb96f12245595824ef7f12ad2ccd6a] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user1,1406039520140.a9cb96f12245595824ef7f12ad2ccd6a.
2014-07-22 08:33:53,268 DEBUG [PostOpenDeployTasks:a9cb96f12245595824ef7f12ad2ccd6a] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:8), split_queue=0, merge_queue=0
2014-07-22 08:33:53,275 INFO  [PostOpenDeployTasks:a9cb96f12245595824ef7f12ad2ccd6a] catalog.MetaEditor: Updated row usertable,user1,1406039520140.a9cb96f12245595824ef7f12ad2ccd6a. with server=slave1,60020,1406043190412
2014-07-22 08:33:53,275 INFO  [PostOpenDeployTasks:a9cb96f12245595824ef7f12ad2ccd6a] regionserver.HRegionServer: Finished post open deploy task for usertable,user1,1406039520140.a9cb96f12245595824ef7f12ad2ccd6a.
2014-07-22 08:33:53,276 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning a9cb96f12245595824ef7f12ad2ccd6a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:33:53,280 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node a9cb96f12245595824ef7f12ad2ccd6a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:33:53,280 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned a9cb96f12245595824ef7f12ad2ccd6a to OPENED in zk on slave1,60020,1406043190412
2014-07-22 08:33:53,280 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user1,1406039520140.a9cb96f12245595824ef7f12ad2ccd6a. on slave1,60020,1406043190412
2014-07-22 08:33:55,547 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:9), split_queue=0, merge_queue=0
2014-07-22 08:33:55,547 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:10), split_queue=0, merge_queue=0
2014-07-22 08:33:55,547 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:11), split_queue=0, merge_queue=0
2014-07-22 08:33:55,548 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:12), split_queue=0, merge_queue=0
2014-07-22 08:33:55,548 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:13), split_queue=0, merge_queue=0
2014-07-22 08:33:55,548 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:14), split_queue=0, merge_queue=0
2014-07-22 08:33:55,548 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:15), split_queue=0, merge_queue=0
2014-07-22 08:33:55,548 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:16), split_queue=0, merge_queue=0
2014-07-22 08:33:55,548 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:17), split_queue=0, merge_queue=0
2014-07-22 08:34:17,313 INFO  [Priority.RpcServer.handler=2,port=60020] regionserver.HRegionServer: Close cc7f3458c3928f54a89d307862b26b49, via zk=yes, znode version=0, on null
2014-07-22 08:34:17,313 INFO  [Priority.RpcServer.handler=3,port=60020] regionserver.HRegionServer: Close 0dc75b653fd43897cbb26a11f83e8ac9, via zk=yes, znode version=0, on null
2014-07-22 08:34:17,313 INFO  [Priority.RpcServer.handler=4,port=60020] regionserver.HRegionServer: Close 913d0bb1a1ad8f68c9342a13cea0805c, via zk=yes, znode version=0, on null
2014-07-22 08:34:17,314 INFO  [Priority.RpcServer.handler=5,port=60020] regionserver.HRegionServer: Close 0d3c482726c2c78b77beb7e33bcad23c, via zk=yes, znode version=0, on null
2014-07-22 08:34:17,314 INFO  [Priority.RpcServer.handler=6,port=60020] regionserver.HRegionServer: Close 28300f665ce3e05a3eb06717408e8034, via zk=yes, znode version=0, on null
2014-07-22 08:34:17,314 INFO  [Priority.RpcServer.handler=7,port=60020] regionserver.HRegionServer: Close 0b70bfe57f3f3910975048288f97bf2f, via zk=yes, znode version=0, on null
2014-07-22 08:34:17,314 INFO  [Priority.RpcServer.handler=8,port=60020] regionserver.HRegionServer: Close a9cb96f12245595824ef7f12ad2ccd6a, via zk=yes, znode version=0, on null
2014-07-22 08:34:17,315 INFO  [Priority.RpcServer.handler=9,port=60020] regionserver.HRegionServer: Close 3d552e2b870e10dba69bc8dc15a4af32, via zk=yes, znode version=0, on null
2014-07-22 08:34:17,317 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Processing close of usertable,user5,1406039520141.28300f665ce3e05a3eb06717408e8034.
2014-07-22 08:34:17,318 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Processing close of usertable,user9,1406039520141.cc7f3458c3928f54a89d307862b26b49.
2014-07-22 08:34:17,318 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Processing close of usertable,,1406039520140.0dc75b653fd43897cbb26a11f83e8ac9.
2014-07-22 08:34:17,320 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Close 47533a6ed53c9e75bf374ae6634deb6f, via zk=yes, znode version=0, on null
2014-07-22 08:34:17,320 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Close 274bcd746cd34a3f6b16189d66aa97de, via zk=yes, znode version=0, on null
2014-07-22 08:34:17,323 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closing usertable,user9,1406039520141.cc7f3458c3928f54a89d307862b26b49.: disabling compactions & flushes
2014-07-22 08:34:17,323 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closing usertable,user5,1406039520141.28300f665ce3e05a3eb06717408e8034.: disabling compactions & flushes
2014-07-22 08:34:17,323 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: waiting for 1 compactions to complete for region usertable,user5,1406039520141.28300f665ce3e05a3eb06717408e8034.
2014-07-22 08:34:17,323 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Updates disabled for region usertable,user9,1406039520141.cc7f3458c3928f54a89d307862b26b49.
2014-07-22 08:34:17,324 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closing usertable,,1406039520140.0dc75b653fd43897cbb26a11f83e8ac9.: disabling compactions & flushes
2014-07-22 08:34:17,324 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Updates disabled for region usertable,,1406039520140.0dc75b653fd43897cbb26a11f83e8ac9.
2014-07-22 08:34:17,325 INFO  [StoreCloserThread-usertable,,1406039520140.0dc75b653fd43897cbb26a11f83e8ac9.-1] regionserver.HStore: Closed family
2014-07-22 08:34:17,330 INFO  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closed usertable,,1406039520140.0dc75b653fd43897cbb26a11f83e8ac9.
2014-07-22 08:34:17,330 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 0dc75b653fd43897cbb26a11f83e8ac9 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 08:34:17,334 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 0dc75b653fd43897cbb26a11f83e8ac9 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 08:34:17,334 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Set closed state in zk for usertable,,1406039520140.0dc75b653fd43897cbb26a11f83e8ac9. on slave1,60020,1406043190412
2014-07-22 08:34:17,335 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Closed usertable,,1406039520140.0dc75b653fd43897cbb26a11f83e8ac9.
2014-07-22 08:34:17,335 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Processing close of usertable,user4,1406039520141.0b70bfe57f3f3910975048288f97bf2f.
2014-07-22 08:34:17,336 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closing usertable,user4,1406039520141.0b70bfe57f3f3910975048288f97bf2f.: disabling compactions & flushes
2014-07-22 08:34:17,337 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Updates disabled for region usertable,user4,1406039520141.0b70bfe57f3f3910975048288f97bf2f.
2014-07-22 08:34:17,366 INFO  [StoreCloserThread-usertable,user9,1406039520141.cc7f3458c3928f54a89d307862b26b49.-1] regionserver.HStore: Closed family
2014-07-22 08:34:17,366 INFO  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closed usertable,user9,1406039520141.cc7f3458c3928f54a89d307862b26b49.
2014-07-22 08:34:17,367 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning cc7f3458c3928f54a89d307862b26b49 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 08:34:17,368 INFO  [StoreCloserThread-usertable,user4,1406039520141.0b70bfe57f3f3910975048288f97bf2f.-1] regionserver.HStore: Closed family
2014-07-22 08:34:17,369 INFO  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closed usertable,user4,1406039520141.0b70bfe57f3f3910975048288f97bf2f.
2014-07-22 08:34:17,369 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 0b70bfe57f3f3910975048288f97bf2f from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 08:34:17,372 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node cc7f3458c3928f54a89d307862b26b49 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 08:34:17,372 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Set closed state in zk for usertable,user9,1406039520141.cc7f3458c3928f54a89d307862b26b49. on slave1,60020,1406043190412
2014-07-22 08:34:17,372 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Closed usertable,user9,1406039520141.cc7f3458c3928f54a89d307862b26b49.
2014-07-22 08:34:17,372 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Processing close of usertable,user1,1406039520140.a9cb96f12245595824ef7f12ad2ccd6a.
2014-07-22 08:34:17,374 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closing usertable,user1,1406039520140.a9cb96f12245595824ef7f12ad2ccd6a.: disabling compactions & flushes
2014-07-22 08:34:17,375 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Updates disabled for region usertable,user1,1406039520140.a9cb96f12245595824ef7f12ad2ccd6a.
2014-07-22 08:34:17,376 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 0b70bfe57f3f3910975048288f97bf2f from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 08:34:17,376 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Set closed state in zk for usertable,user4,1406039520141.0b70bfe57f3f3910975048288f97bf2f. on slave1,60020,1406043190412
2014-07-22 08:34:17,376 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Closed usertable,user4,1406039520141.0b70bfe57f3f3910975048288f97bf2f.
2014-07-22 08:34:17,376 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Processing close of usertable,user7,1406039520141.913d0bb1a1ad8f68c9342a13cea0805c.
2014-07-22 08:34:17,379 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closing usertable,user7,1406039520141.913d0bb1a1ad8f68c9342a13cea0805c.: disabling compactions & flushes
2014-07-22 08:34:17,379 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Updates disabled for region usertable,user7,1406039520141.913d0bb1a1ad8f68c9342a13cea0805c.
2014-07-22 08:34:17,384 INFO  [StoreCloserThread-usertable,user1,1406039520140.a9cb96f12245595824ef7f12ad2ccd6a.-1] regionserver.HStore: Closed family
2014-07-22 08:34:17,385 INFO  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closed usertable,user1,1406039520140.a9cb96f12245595824ef7f12ad2ccd6a.
2014-07-22 08:34:17,386 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning a9cb96f12245595824ef7f12ad2ccd6a from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 08:34:17,388 INFO  [StoreCloserThread-usertable,user7,1406039520141.913d0bb1a1ad8f68c9342a13cea0805c.-1] regionserver.HStore: Closed family
2014-07-22 08:34:17,389 INFO  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closed usertable,user7,1406039520141.913d0bb1a1ad8f68c9342a13cea0805c.
2014-07-22 08:34:17,389 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 913d0bb1a1ad8f68c9342a13cea0805c from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 08:34:17,393 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node a9cb96f12245595824ef7f12ad2ccd6a from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 08:34:17,394 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Set closed state in zk for usertable,user1,1406039520140.a9cb96f12245595824ef7f12ad2ccd6a. on slave1,60020,1406043190412
2014-07-22 08:34:17,394 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Closed usertable,user1,1406039520140.a9cb96f12245595824ef7f12ad2ccd6a.
2014-07-22 08:34:17,394 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Processing close of usertable,user3,1406039520141.3d552e2b870e10dba69bc8dc15a4af32.
2014-07-22 08:34:17,396 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closing usertable,user3,1406039520141.3d552e2b870e10dba69bc8dc15a4af32.: disabling compactions & flushes
2014-07-22 08:34:17,396 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Updates disabled for region usertable,user3,1406039520141.3d552e2b870e10dba69bc8dc15a4af32.
2014-07-22 08:34:17,399 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 913d0bb1a1ad8f68c9342a13cea0805c from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 08:34:17,399 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Set closed state in zk for usertable,user7,1406039520141.913d0bb1a1ad8f68c9342a13cea0805c. on slave1,60020,1406043190412
2014-07-22 08:34:17,399 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Closed usertable,user7,1406039520141.913d0bb1a1ad8f68c9342a13cea0805c.
2014-07-22 08:34:17,400 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Processing close of usertable,user6,1406039520141.0d3c482726c2c78b77beb7e33bcad23c.
2014-07-22 08:34:17,403 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closing usertable,user6,1406039520141.0d3c482726c2c78b77beb7e33bcad23c.: disabling compactions & flushes
2014-07-22 08:34:17,403 INFO  [StoreCloserThread-usertable,user3,1406039520141.3d552e2b870e10dba69bc8dc15a4af32.-1] regionserver.HStore: Closed family
2014-07-22 08:34:17,404 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Updates disabled for region usertable,user6,1406039520141.0d3c482726c2c78b77beb7e33bcad23c.
2014-07-22 08:34:17,404 INFO  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closed usertable,user3,1406039520141.3d552e2b870e10dba69bc8dc15a4af32.
2014-07-22 08:34:17,404 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 3d552e2b870e10dba69bc8dc15a4af32 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 08:34:17,409 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 3d552e2b870e10dba69bc8dc15a4af32 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 08:34:17,409 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Set closed state in zk for usertable,user3,1406039520141.3d552e2b870e10dba69bc8dc15a4af32. on slave1,60020,1406043190412
2014-07-22 08:34:17,409 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Closed usertable,user3,1406039520141.3d552e2b870e10dba69bc8dc15a4af32.
2014-07-22 08:34:17,409 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Processing close of usertable,user2,1406039520140.47533a6ed53c9e75bf374ae6634deb6f.
2014-07-22 08:34:17,411 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closing usertable,user2,1406039520140.47533a6ed53c9e75bf374ae6634deb6f.: disabling compactions & flushes
2014-07-22 08:34:17,411 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Updates disabled for region usertable,user2,1406039520140.47533a6ed53c9e75bf374ae6634deb6f.
2014-07-22 08:34:17,415 INFO  [StoreCloserThread-usertable,user6,1406039520141.0d3c482726c2c78b77beb7e33bcad23c.-1] regionserver.HStore: Closed family
2014-07-22 08:34:17,416 INFO  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closed usertable,user6,1406039520141.0d3c482726c2c78b77beb7e33bcad23c.
2014-07-22 08:34:17,416 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 0d3c482726c2c78b77beb7e33bcad23c from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 08:34:17,418 INFO  [StoreCloserThread-usertable,user2,1406039520140.47533a6ed53c9e75bf374ae6634deb6f.-1] regionserver.HStore: Closed family
2014-07-22 08:34:17,418 INFO  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closed usertable,user2,1406039520140.47533a6ed53c9e75bf374ae6634deb6f.
2014-07-22 08:34:17,418 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 47533a6ed53c9e75bf374ae6634deb6f from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 08:34:17,423 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 0d3c482726c2c78b77beb7e33bcad23c from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 08:34:17,423 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Set closed state in zk for usertable,user6,1406039520141.0d3c482726c2c78b77beb7e33bcad23c. on slave1,60020,1406043190412
2014-07-22 08:34:17,423 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Closed usertable,user6,1406039520141.0d3c482726c2c78b77beb7e33bcad23c.
2014-07-22 08:34:17,423 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Processing close of usertable,user8,1406039520141.274bcd746cd34a3f6b16189d66aa97de.
2014-07-22 08:34:17,423 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 47533a6ed53c9e75bf374ae6634deb6f from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 08:34:17,423 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Set closed state in zk for usertable,user2,1406039520140.47533a6ed53c9e75bf374ae6634deb6f. on slave1,60020,1406043190412
2014-07-22 08:34:17,423 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Closed usertable,user2,1406039520140.47533a6ed53c9e75bf374ae6634deb6f.
2014-07-22 08:34:17,424 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closing usertable,user8,1406039520141.274bcd746cd34a3f6b16189d66aa97de.: disabling compactions & flushes
2014-07-22 08:34:17,424 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Updates disabled for region usertable,user8,1406039520141.274bcd746cd34a3f6b16189d66aa97de.
2014-07-22 08:34:17,427 INFO  [StoreCloserThread-usertable,user8,1406039520141.274bcd746cd34a3f6b16189d66aa97de.-1] regionserver.HStore: Closed family
2014-07-22 08:34:17,428 INFO  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closed usertable,user8,1406039520141.274bcd746cd34a3f6b16189d66aa97de.
2014-07-22 08:34:17,429 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 274bcd746cd34a3f6b16189d66aa97de from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 08:34:17,444 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 274bcd746cd34a3f6b16189d66aa97de from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 08:34:17,444 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Set closed state in zk for usertable,user8,1406039520141.274bcd746cd34a3f6b16189d66aa97de. on slave1,60020,1406043190412
2014-07-22 08:34:17,444 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Closed usertable,user8,1406039520141.274bcd746cd34a3f6b16189d66aa97de.
2014-07-22 08:34:17,624 INFO  [regionserver60020-smallCompactions-1406043231365] regionserver.HRegion: compaction interrupted
java.io.InterruptedIOException: Aborting compaction of store family in region usertable,user5,1406039520141.28300f665ce3e05a3eb06717408e8034. because it was interrupted.
	at org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.compact(DefaultCompactor.java:81)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext.compact(DefaultStoreEngine.java:109)
	at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:1086)
	at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1481)
	at org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.run(CompactSplitThread.java:475)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 08:34:17,625 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Updates disabled for region usertable,user5,1406039520141.28300f665ce3e05a3eb06717408e8034.
2014-07-22 08:34:17,631 INFO  [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Aborted compaction: Request = regionName=usertable,user5,1406039520141.28300f665ce3e05a3eb06717408e8034., storeName=family, fileCount=10, fileSize=1.2g (140.8m, 165.6m, 15.6m), priority=1985, time=140211769867179; duration=26sec
2014-07-22 08:34:17,631 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:17), split_queue=0, merge_queue=0
2014-07-22 08:34:17,631 INFO  [StoreCloserThread-usertable,user5,1406039520141.28300f665ce3e05a3eb06717408e8034.-1] regionserver.HStore: Closed family
2014-07-22 08:34:17,632 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closed usertable,user5,1406039520141.28300f665ce3e05a3eb06717408e8034.
2014-07-22 08:34:17,633 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 28300f665ce3e05a3eb06717408e8034 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 08:34:17,633 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user9,1406039520141.cc7f3458c3928f54a89d307862b26b49. because compaction request was cancelled
2014-07-22 08:34:17,633 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user9,1406039520141.cc7f3458c3928f54a89d307862b26b49. because compaction request was cancelled
2014-07-22 08:34:17,633 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user6,1406039520141.0d3c482726c2c78b77beb7e33bcad23c. because compaction request was cancelled
2014-07-22 08:34:17,633 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406039520140.a9cb96f12245595824ef7f12ad2ccd6a. because compaction request was cancelled
2014-07-22 08:34:17,633 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user2,1406039520140.47533a6ed53c9e75bf374ae6634deb6f. because compaction request was cancelled
2014-07-22 08:34:17,633 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406039520141.0b70bfe57f3f3910975048288f97bf2f. because compaction request was cancelled
2014-07-22 08:34:17,633 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user2,1406039520140.47533a6ed53c9e75bf374ae6634deb6f. because compaction request was cancelled
2014-07-22 08:34:17,633 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user6,1406039520141.0d3c482726c2c78b77beb7e33bcad23c. because compaction request was cancelled
2014-07-22 08:34:17,633 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406039520141.3d552e2b870e10dba69bc8dc15a4af32. because compaction request was cancelled
2014-07-22 08:34:17,634 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406039520141.3d552e2b870e10dba69bc8dc15a4af32. because compaction request was cancelled
2014-07-22 08:34:17,634 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406039520140.a9cb96f12245595824ef7f12ad2ccd6a. because compaction request was cancelled
2014-07-22 08:34:17,634 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406039520141.0b70bfe57f3f3910975048288f97bf2f. because compaction request was cancelled
2014-07-22 08:34:17,634 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406039520141.28300f665ce3e05a3eb06717408e8034. because compaction request was cancelled
2014-07-22 08:34:17,634 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user7,1406039520141.913d0bb1a1ad8f68c9342a13cea0805c. because compaction request was cancelled
2014-07-22 08:34:17,634 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user7,1406039520141.913d0bb1a1ad8f68c9342a13cea0805c. because compaction request was cancelled
2014-07-22 08:34:17,634 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user8,1406039520141.274bcd746cd34a3f6b16189d66aa97de. because compaction request was cancelled
2014-07-22 08:34:17,634 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user8,1406039520141.274bcd746cd34a3f6b16189d66aa97de. because compaction request was cancelled
2014-07-22 08:34:17,637 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 28300f665ce3e05a3eb06717408e8034 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 08:34:17,637 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Set closed state in zk for usertable,user5,1406039520141.28300f665ce3e05a3eb06717408e8034. on slave1,60020,1406043190412
2014-07-22 08:34:17,637 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Closed usertable,user5,1406039520141.28300f665ce3e05a3eb06717408e8034.
2014-07-22 08:38:10,500 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=6603, hits=3, hitRatio=0.04%, , cachingAccesses=5, cachingHits=3, cachingHitsRatio=60.00%, evictions=0, evicted=0, evictedPerRun=NaN
2014-07-22 08:39:28,670 INFO  [Priority.RpcServer.handler=7,port=60020] regionserver.HRegionServer: Open usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:39:28,682 INFO  [Priority.RpcServer.handler=7,port=60020] regionserver.HRegionServer: Open usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b.
2014-07-22 08:39:28,682 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 97ad17fd41c306f4acbbcbfe821c8a7e from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:39:28,683 INFO  [Priority.RpcServer.handler=7,port=60020] regionserver.HRegionServer: Open usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:39:28,683 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 285afaafd9634c6707beeeeb8207bf2b from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:39:28,684 INFO  [Priority.RpcServer.handler=7,port=60020] regionserver.HRegionServer: Open usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:39:28,684 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 7c5c8675017da2a4119014a8dd35278d from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:39:28,685 INFO  [Priority.RpcServer.handler=7,port=60020] regionserver.HRegionServer: Open usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:39:28,689 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 97ad17fd41c306f4acbbcbfe821c8a7e from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:39:28,690 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => 97ad17fd41c306f4acbbcbfe821c8a7e, NAME => 'usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.', STARTKEY => 'user5', ENDKEY => 'user6'}
2014-07-22 08:39:28,691 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 97ad17fd41c306f4acbbcbfe821c8a7e
2014-07-22 08:39:28,691 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:39:28,694 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 285afaafd9634c6707beeeeb8207bf2b from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:39:28,695 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 7c5c8675017da2a4119014a8dd35278d from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:39:28,695 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => 7c5c8675017da2a4119014a8dd35278d, NAME => 'usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.', STARTKEY => 'user4', ENDKEY => 'user5'}
2014-07-22 08:39:28,695 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => 285afaafd9634c6707beeeeb8207bf2b, NAME => 'usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b.', STARTKEY => 'user9', ENDKEY => ''}
2014-07-22 08:39:28,695 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 7c5c8675017da2a4119014a8dd35278d
2014-07-22 08:39:28,696 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:39:28,696 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:39:28,696 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b.
2014-07-22 08:39:28,703 INFO  [StoreOpener-97ad17fd41c306f4acbbcbfe821c8a7e-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 08:39:28,703 INFO  [StoreOpener-7c5c8675017da2a4119014a8dd35278d-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 08:39:28,705 INFO  [StoreOpener-285afaafd9634c6707beeeeb8207bf2b-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 08:39:28,707 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e
2014-07-22 08:39:28,709 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d
2014-07-22 08:39:28,710 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:39:28,711 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined 97ad17fd41c306f4acbbcbfe821c8a7e; next sequenceid=1
2014-07-22 08:39:28,711 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 97ad17fd41c306f4acbbcbfe821c8a7e
2014-07-22 08:39:28,712 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined 7c5c8675017da2a4119014a8dd35278d; next sequenceid=1
2014-07-22 08:39:28,712 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 7c5c8675017da2a4119014a8dd35278d
2014-07-22 08:39:28,713 INFO  [PostOpenDeployTasks:97ad17fd41c306f4acbbcbfe821c8a7e] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:39:28,713 INFO  [PostOpenDeployTasks:7c5c8675017da2a4119014a8dd35278d] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:39:28,727 INFO  [PostOpenDeployTasks:97ad17fd41c306f4acbbcbfe821c8a7e] catalog.MetaEditor: Updated row usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. with server=slave1,60020,1406043190412
2014-07-22 08:39:28,727 INFO  [PostOpenDeployTasks:97ad17fd41c306f4acbbcbfe821c8a7e] regionserver.HRegionServer: Finished post open deploy task for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:39:28,728 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 97ad17fd41c306f4acbbcbfe821c8a7e from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:39:28,729 INFO  [PostOpenDeployTasks:7c5c8675017da2a4119014a8dd35278d] catalog.MetaEditor: Updated row usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. with server=slave1,60020,1406043190412
2014-07-22 08:39:28,730 INFO  [PostOpenDeployTasks:7c5c8675017da2a4119014a8dd35278d] regionserver.HRegionServer: Finished post open deploy task for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:39:28,731 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 7c5c8675017da2a4119014a8dd35278d from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:39:28,734 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 97ad17fd41c306f4acbbcbfe821c8a7e from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:39:28,734 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned 97ad17fd41c306f4acbbcbfe821c8a7e to OPENED in zk on slave1,60020,1406043190412
2014-07-22 08:39:28,734 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. on slave1,60020,1406043190412
2014-07-22 08:39:28,734 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 55c4b5c170380616b272a52c96d0e4ef from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:39:28,737 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 7c5c8675017da2a4119014a8dd35278d from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:39:28,737 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned 7c5c8675017da2a4119014a8dd35278d to OPENED in zk on slave1,60020,1406043190412
2014-07-22 08:39:28,737 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. on slave1,60020,1406043190412
2014-07-22 08:39:28,737 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 92dc45bba255d14c6f2d7cb1b56ba708 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:39:28,738 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 55c4b5c170380616b272a52c96d0e4ef from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:39:28,738 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => 55c4b5c170380616b272a52c96d0e4ef, NAME => 'usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.', STARTKEY => 'user1', ENDKEY => 'user2'}
2014-07-22 08:39:28,739 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 55c4b5c170380616b272a52c96d0e4ef
2014-07-22 08:39:28,739 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:39:28,740 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined 285afaafd9634c6707beeeeb8207bf2b; next sequenceid=1
2014-07-22 08:39:28,740 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:39:28,741 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 92dc45bba255d14c6f2d7cb1b56ba708 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 08:39:28,742 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => 92dc45bba255d14c6f2d7cb1b56ba708, NAME => 'usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.', STARTKEY => 'user3', ENDKEY => 'user4'}
2014-07-22 08:39:28,742 INFO  [PostOpenDeployTasks:285afaafd9634c6707beeeeb8207bf2b] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b.
2014-07-22 08:39:28,742 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 92dc45bba255d14c6f2d7cb1b56ba708
2014-07-22 08:39:28,743 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:39:28,751 INFO  [PostOpenDeployTasks:285afaafd9634c6707beeeeb8207bf2b] catalog.MetaEditor: Updated row usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. with server=slave1,60020,1406043190412
2014-07-22 08:39:28,751 INFO  [PostOpenDeployTasks:285afaafd9634c6707beeeeb8207bf2b] regionserver.HRegionServer: Finished post open deploy task for usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b.
2014-07-22 08:39:28,752 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 285afaafd9634c6707beeeeb8207bf2b from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:39:28,752 INFO  [StoreOpener-55c4b5c170380616b272a52c96d0e4ef-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 08:39:28,756 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 285afaafd9634c6707beeeeb8207bf2b from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:39:28,756 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned 285afaafd9634c6707beeeeb8207bf2b to OPENED in zk on slave1,60020,1406043190412
2014-07-22 08:39:28,756 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. on slave1,60020,1406043190412
2014-07-22 08:39:28,756 INFO  [StoreOpener-92dc45bba255d14c6f2d7cb1b56ba708-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 08:39:28,760 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef
2014-07-22 08:39:28,761 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708
2014-07-22 08:39:28,763 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined 55c4b5c170380616b272a52c96d0e4ef; next sequenceid=1
2014-07-22 08:39:28,763 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 55c4b5c170380616b272a52c96d0e4ef
2014-07-22 08:39:28,767 INFO  [PostOpenDeployTasks:55c4b5c170380616b272a52c96d0e4ef] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:39:28,775 INFO  [PostOpenDeployTasks:55c4b5c170380616b272a52c96d0e4ef] catalog.MetaEditor: Updated row usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. with server=slave1,60020,1406043190412
2014-07-22 08:39:28,776 INFO  [PostOpenDeployTasks:55c4b5c170380616b272a52c96d0e4ef] regionserver.HRegionServer: Finished post open deploy task for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:39:28,776 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 55c4b5c170380616b272a52c96d0e4ef from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:39:28,780 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 55c4b5c170380616b272a52c96d0e4ef from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:39:28,781 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned 55c4b5c170380616b272a52c96d0e4ef to OPENED in zk on slave1,60020,1406043190412
2014-07-22 08:39:28,781 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. on slave1,60020,1406043190412
2014-07-22 08:39:28,803 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined 92dc45bba255d14c6f2d7cb1b56ba708; next sequenceid=1
2014-07-22 08:39:28,803 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 92dc45bba255d14c6f2d7cb1b56ba708
2014-07-22 08:39:28,805 INFO  [PostOpenDeployTasks:92dc45bba255d14c6f2d7cb1b56ba708] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:39:28,815 INFO  [PostOpenDeployTasks:92dc45bba255d14c6f2d7cb1b56ba708] catalog.MetaEditor: Updated row usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. with server=slave1,60020,1406043190412
2014-07-22 08:39:28,815 INFO  [PostOpenDeployTasks:92dc45bba255d14c6f2d7cb1b56ba708] regionserver.HRegionServer: Finished post open deploy task for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:39:28,815 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 92dc45bba255d14c6f2d7cb1b56ba708 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:39:28,819 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475eb4796e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 92dc45bba255d14c6f2d7cb1b56ba708 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 08:39:28,819 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned 92dc45bba255d14c6f2d7cb1b56ba708 to OPENED in zk on slave1,60020,1406043190412
2014-07-22 08:39:28,819 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. on slave1,60020,1406043190412
2014-07-22 08:39:47,878 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:39:48,000 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 104 synced till here 89
2014-07-22 08:39:48,228 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043225414 with entries=104, filesize=84.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043587879
2014-07-22 08:39:49,846 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:39:49,931 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 249 synced till here 211
2014-07-22 08:39:50,369 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043587879 with entries=145, filesize=81.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043589847
2014-07-22 08:39:52,325 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:39:52,340 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 362 synced till here 337
2014-07-22 08:39:53,117 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043589847 with entries=113, filesize=84.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043592325
2014-07-22 08:39:54,984 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:39:55,143 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 491 synced till here 462
2014-07-22 08:39:55,774 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043592325 with entries=129, filesize=90.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043594985
2014-07-22 08:39:58,172 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:39:58,377 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 619 synced till here 591
2014-07-22 08:39:58,945 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043594985 with entries=128, filesize=92.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043598173
2014-07-22 08:40:00,815 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:40:00,991 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 752 synced till here 724
2014-07-22 08:40:01,400 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043598173 with entries=133, filesize=95.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043600816
2014-07-22 08:40:02,258 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:40:02,299 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 269.4m
2014-07-22 08:40:03,289 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:40:03,423 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 878 synced till here 847
2014-07-22 08:40:03,771 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:40:03,774 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 261.5m
2014-07-22 08:40:03,920 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043600816 with entries=126, filesize=89.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043603290
2014-07-22 08:40:04,129 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:40:04,684 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:40:04,688 INFO  [MemStoreFlusher.0] compress.CodecPool: Got brand-new compressor
2014-07-22 08:40:04,997 DEBUG [RpcServer.handler=46,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:40:05,381 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:40:05,515 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 975 synced till here 955
2014-07-22 08:40:05,794 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043603290 with entries=97, filesize=77.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043605382
2014-07-22 08:40:06,138 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:40:07,129 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:40:07,223 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1100 synced till here 1077
2014-07-22 08:40:07,383 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043605382 with entries=125, filesize=83.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043607130
2014-07-22 08:40:09,014 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:40:09,134 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1192 synced till here 1191
2014-07-22 08:40:09,155 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043607130 with entries=92, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043609015
2014-07-22 08:40:09,706 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=182, memsize=49.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/553dd5dd7bd344898f5b2860df66e20f
2014-07-22 08:40:09,706 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=181, memsize=48.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/e9e0a087cde84fd2b00c08b038950a04
2014-07-22 08:40:09,723 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/553dd5dd7bd344898f5b2860df66e20f as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/553dd5dd7bd344898f5b2860df66e20f
2014-07-22 08:40:09,723 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/e9e0a087cde84fd2b00c08b038950a04 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/e9e0a087cde84fd2b00c08b038950a04
2014-07-22 08:40:09,736 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/e9e0a087cde84fd2b00c08b038950a04, entries=177250, sequenceid=181, filesize=12.6m
2014-07-22 08:40:09,736 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/553dd5dd7bd344898f5b2860df66e20f, entries=178520, sequenceid=182, filesize=12.7m
2014-07-22 08:40:09,737 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~275.1m/288435920, currentsize=94.5m/99054880 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 5963ms, sequenceid=182, compaction requested=false
2014-07-22 08:40:09,737 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~273.5m/286770320, currentsize=97.4m/102108400 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 7439ms, sequenceid=181, compaction requested=false
2014-07-22 08:40:09,740 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 363.4m
2014-07-22 08:40:09,740 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 359.6m
2014-07-22 08:40:10,837 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:40:10,857 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1284 synced till here 1281
2014-07-22 08:40:10,864 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:40:10,897 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:40:10,903 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043609015 with entries=92, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043610838
2014-07-22 08:40:12,824 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:40:12,847 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1378 synced till here 1371
2014-07-22 08:40:12,884 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043610838 with entries=94, filesize=64.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043612824
2014-07-22 08:40:14,571 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:40:14,593 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1470 synced till here 1468
2014-07-22 08:40:14,627 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043612824 with entries=92, filesize=65.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043614572
2014-07-22 08:40:16,357 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:40:16,375 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1555 synced till here 1554
2014-07-22 08:40:16,412 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043614572 with entries=85, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043616358
2014-07-22 08:40:17,882 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=243, memsize=48.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/6d8218b04ce34760a587e5949fd0339f
2014-07-22 08:40:18,015 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/6d8218b04ce34760a587e5949fd0339f as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/6d8218b04ce34760a587e5949fd0339f
2014-07-22 08:40:18,057 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/6d8218b04ce34760a587e5949fd0339f, entries=177610, sequenceid=243, filesize=12.7m
2014-07-22 08:40:18,058 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~368.1m/385984560, currentsize=118.6m/124402400 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 8318ms, sequenceid=243, compaction requested=false
2014-07-22 08:40:18,066 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=240, memsize=48.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/02b8b5eeec4b4f4a9c6c0719545a2a76
2014-07-22 08:40:18,082 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/02b8b5eeec4b4f4a9c6c0719545a2a76 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/02b8b5eeec4b4f4a9c6c0719545a2a76
2014-07-22 08:40:18,205 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/02b8b5eeec4b4f4a9c6c0719545a2a76, entries=177320, sequenceid=240, filesize=12.6m
2014-07-22 08:40:18,205 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~362.7m/380320080, currentsize=120.0m/125794800 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 8465ms, sequenceid=240, compaction requested=false
2014-07-22 08:40:18,412 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:40:18,444 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1672 synced till here 1665
2014-07-22 08:40:18,614 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043616358 with entries=117, filesize=68.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043618412
2014-07-22 08:40:20,174 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:40:20,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1770 synced till here 1764
2014-07-22 08:40:20,214 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:40:20,215 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 256.9m
2014-07-22 08:40:20,419 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043618412 with entries=98, filesize=67.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043620174
2014-07-22 08:40:20,664 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:40:20,665 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 257.1m
2014-07-22 08:40:20,886 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:40:20,964 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:40:22,300 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:40:24,291 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1975 synced till here 1955
2014-07-22 08:40:24,714 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043620174 with entries=205, filesize=141.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043622318
2014-07-22 08:40:26,633 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:40:26,719 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2076 synced till here 2063
2014-07-22 08:40:27,015 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043622318 with entries=101, filesize=75.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043626634
2014-07-22 08:40:27,934 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:40:27,934 DEBUG [RpcServer.handler=37,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:40:28,800 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:40:28,967 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2151 synced till here 2143
2014-07-22 08:40:29,024 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043626634 with entries=75, filesize=68.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043628800
2014-07-22 08:40:31,068 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:40:31,083 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2231 synced till here 2225
2014-07-22 08:40:31,321 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043628800 with entries=80, filesize=68.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043631068
2014-07-22 08:40:33,160 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:40:33,327 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2326 synced till here 2321
2014-07-22 08:40:33,368 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043631068 with entries=95, filesize=67.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043633161
2014-07-22 08:40:35,285 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:40:35,314 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2409 synced till here 2404
2014-07-22 08:40:35,519 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043633161 with entries=83, filesize=68.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043635286
2014-07-22 08:40:37,058 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=342, memsize=156.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/42eb232cf9b34fa0b5778a3b4c653b41
2014-07-22 08:40:37,187 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/42eb232cf9b34fa0b5778a3b4c653b41 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/42eb232cf9b34fa0b5778a3b4c653b41
2014-07-22 08:40:37,199 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=341, memsize=155.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/13a72130877c429db0ef58f1c7897c75
2014-07-22 08:40:37,205 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/42eb232cf9b34fa0b5778a3b4c653b41, entries=568460, sequenceid=342, filesize=40.6m
2014-07-22 08:40:37,206 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~260.2m/272836160, currentsize=229.6m/240723120 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 16541ms, sequenceid=342, compaction requested=false
2014-07-22 08:40:37,208 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 390.4m
2014-07-22 08:40:37,219 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/13a72130877c429db0ef58f1c7897c75 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/13a72130877c429db0ef58f1c7897c75
2014-07-22 08:40:37,246 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/13a72130877c429db0ef58f1c7897c75, entries=567090, sequenceid=341, filesize=40.5m
2014-07-22 08:40:37,340 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~260.1m/272709680, currentsize=233.5m/244878320 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 17031ms, sequenceid=341, compaction requested=false
2014-07-22 08:40:37,340 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 384.4m
2014-07-22 08:40:37,521 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:40:37,538 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2486 synced till here 2481
2014-07-22 08:40:37,884 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043635286 with entries=77, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043637522
2014-07-22 08:40:39,798 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:40:39,826 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:40:39,997 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:40:40,167 DEBUG [RpcServer.handler=39,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:40:40,700 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:40:40,854 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2573 synced till here 2564
2014-07-22 08:40:41,047 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043637522 with entries=87, filesize=70.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043640701
2014-07-22 08:40:42,925 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:40:42,944 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2658 synced till here 2648
2014-07-22 08:40:43,113 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043640701 with entries=85, filesize=73.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043642926
2014-07-22 08:40:45,038 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:40:45,055 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2738 synced till here 2731
2014-07-22 08:40:45,222 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043642926 with entries=80, filesize=67.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043645038
2014-07-22 08:40:47,030 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:40:48,734 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2898 synced till here 2889
2014-07-22 08:40:48,929 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043645038 with entries=160, filesize=116.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043647031
2014-07-22 08:40:59,895 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 10297ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=10707ms
2014-07-22 08:40:59,961 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1861 service: ClientService methodName: Multi size: 33.8k connection: 9.1.143.53:54547: output error
2014-07-22 08:40:59,963 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:40:59,963 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1860 service: ClientService methodName: Multi size: 5.1k connection: 9.1.143.53:54547: output error
2014-07-22 08:40:59,963 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:40:59,963 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1856 service: ClientService methodName: Multi size: 33.8k connection: 9.1.143.53:54547: output error
2014-07-22 08:40:59,963 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:40:59,963 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1857 service: ClientService methodName: Multi size: 35.0k connection: 9.1.143.53:54547: output error
2014-07-22 08:40:59,963 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:40:59,990 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1854 service: ClientService methodName: Multi size: 201.1k connection: 9.1.143.53:54547: output error
2014-07-22 08:40:59,990 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:40:59,998 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1849 service: ClientService methodName: Multi size: 216.1k connection: 9.1.143.53:54547: output error
2014-07-22 08:40:59,998 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:40:59,998 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1847 service: ClientService methodName: Multi size: 5.1k connection: 9.1.143.53:54547: output error
2014-07-22 08:40:59,998 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:40:59,998 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1852 service: ClientService methodName: Multi size: 7.6k connection: 9.1.143.53:54547: output error
2014-07-22 08:40:59,998 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:40:59,999 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1843 service: ClientService methodName: Multi size: 30.0k connection: 9.1.143.53:54547: output error
2014-07-22 08:40:59,999 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:41:00,010 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10811,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54547","starttimems":1406043649148,"queuetimems":1,"class":"HRegionServer","responsesize":153,"method":"Multi"}
2014-07-22 08:41:00,010 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11635,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54547","starttimems":1406043648324,"queuetimems":0,"class":"HRegionServer","responsesize":18574,"method":"Multi"}
2014-07-22 08:41:00,010 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11731,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54547","starttimems":1406043648165,"queuetimems":1,"class":"HRegionServer","responsesize":18574,"method":"Multi"}
2014-07-22 08:41:00,010 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10959,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54547","starttimems":1406043648937,"queuetimems":0,"class":"HRegionServer","responsesize":1283,"method":"Multi"}
2014-07-22 08:41:00,010 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1864 service: ClientService methodName: Multi size: 31.3k connection: 9.1.143.53:54547: output error
2014-07-22 08:41:00,010 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:41:00,011 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1806 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54547: output error
2014-07-22 08:41:00,011 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1802 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54547: output error
2014-07-22 08:41:00,011 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:41:00,011 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1814 service: ClientService methodName: Multi size: 233.5k connection: 9.1.143.53:54547: output error
2014-07-22 08:41:00,011 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:41:00,011 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:41:00,069 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11559,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54547","starttimems":1406043648509,"queuetimems":0,"class":"HRegionServer","responsesize":18784,"method":"Multi"}
2014-07-22 08:41:00,069 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1798 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54547: output error
2014-07-22 08:41:00,069 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:41:00,085 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11725,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54547","starttimems":1406043648360,"queuetimems":0,"class":"HRegionServer","responsesize":18683,"method":"Multi"}
2014-07-22 08:41:00,086 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1801 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54547: output error
2014-07-22 08:41:00,086 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:41:00,111 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11556,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54547","starttimems":1406043648555,"queuetimems":0,"class":"HRegionServer","responsesize":18435,"method":"Multi"}
2014-07-22 08:41:00,112 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1833 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54547: output error
2014-07-22 08:41:00,112 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:41:00,180 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11471,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54547","starttimems":1406043648708,"queuetimems":0,"class":"HRegionServer","responsesize":19572,"method":"Multi"}
2014-07-22 08:41:00,180 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1821 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:54547: output error
2014-07-22 08:41:00,180 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:41:00,582 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11801,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54547","starttimems":1406043648781,"queuetimems":0,"class":"HRegionServer","responsesize":18853,"method":"Multi"}
2014-07-22 08:41:00,583 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1817 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54547: output error
2014-07-22 08:41:00,583 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:41:00,669 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11559,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54547","starttimems":1406043649108,"queuetimems":1,"class":"HRegionServer","responsesize":18591,"method":"Multi"}
2014-07-22 08:41:00,671 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1811 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54547: output error
2014-07-22 08:41:00,671 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:41:00,805 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11731,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54547","starttimems":1406043649073,"queuetimems":0,"class":"HRegionServer","responsesize":18414,"method":"Multi"}
2014-07-22 08:41:00,805 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1812 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54547: output error
2014-07-22 08:41:00,805 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:41:00,899 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:00,901 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11965,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54547","starttimems":1406043648935,"queuetimems":0,"class":"HRegionServer","responsesize":18297,"method":"Multi"}
2014-07-22 08:41:00,902 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1816 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54547: output error
2014-07-22 08:41:00,902 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:41:00,906 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1846 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54547: output error
2014-07-22 08:41:00,906 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:41:00,915 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3003 synced till here 2993
2014-07-22 08:41:00,992 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11845,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54547","starttimems":1406043649147,"queuetimems":0,"class":"HRegionServer","responsesize":18756,"method":"Multi"}
2014-07-22 08:41:00,993 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1810 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54547: output error
2014-07-22 08:41:00,993 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:41:01,014 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043647031 with entries=105, filesize=76.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043660900
2014-07-22 08:41:03,327 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:03,352 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043660900 with entries=85, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043663327
2014-07-22 08:41:05,053 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:05,097 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043663327 with entries=94, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043665054
2014-07-22 08:41:05,356 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=469, memsize=218.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/4d4056b487ad49668cfd2619877c23eb
2014-07-22 08:41:05,381 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/4d4056b487ad49668cfd2619877c23eb as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/4d4056b487ad49668cfd2619877c23eb
2014-07-22 08:41:05,394 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/4d4056b487ad49668cfd2619877c23eb, entries=793840, sequenceid=469, filesize=56.6m
2014-07-22 08:41:05,394 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~393.9m/413060480, currentsize=243.8m/255629680 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 28054ms, sequenceid=469, compaction requested=false
2014-07-22 08:41:05,394 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 474.5m
2014-07-22 08:41:06,135 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=471, memsize=219.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/2ed5d7516df54dab94d8b352e92ed2ed
2014-07-22 08:41:06,194 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/2ed5d7516df54dab94d8b352e92ed2ed as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/2ed5d7516df54dab94d8b352e92ed2ed
2014-07-22 08:41:06,226 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/2ed5d7516df54dab94d8b352e92ed2ed, entries=798380, sequenceid=471, filesize=56.9m
2014-07-22 08:41:06,227 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~393.8m/412881440, currentsize=247.8m/259818400 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 29018ms, sequenceid=471, compaction requested=false
2014-07-22 08:41:06,227 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 489.5m
2014-07-22 08:41:06,409 DEBUG [RpcServer.handler=7,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:41:06,439 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:41:06,473 DEBUG [RpcServer.handler=13,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:41:06,708 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:06,737 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3272 synced till here 3269
2014-07-22 08:41:06,797 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043665054 with entries=90, filesize=66.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043666709
2014-07-22 08:41:06,919 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:41:09,109 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:09,124 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3360 synced till here 3358
2014-07-22 08:41:09,176 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043666709 with entries=88, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043669110
2014-07-22 08:41:10,436 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:10,455 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3446 synced till here 3441
2014-07-22 08:41:10,499 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043669110 with entries=86, filesize=66.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043670436
2014-07-22 08:41:10,499 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:41:12,036 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:12,061 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3545 synced till here 3536
2014-07-22 08:41:12,092 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043670436 with entries=99, filesize=64.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043672037
2014-07-22 08:41:12,093 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:41:12,975 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:13,016 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3628 synced till here 3626
2014-07-22 08:41:13,056 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043672037 with entries=83, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043672976
2014-07-22 08:41:13,057 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:41:14,715 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b.
2014-07-22 08:41:14,953 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:14,972 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3726 synced till here 3723
2014-07-22 08:41:14,999 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043672976 with entries=98, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043674953
2014-07-22 08:41:15,000 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:41:16,493 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:18,123 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3889 synced till here 3886
2014-07-22 08:41:18,163 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043674953 with entries=163, filesize=104.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043676494
2014-07-22 08:41:18,164 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:41:18,828 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:18,854 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3977 synced till here 3972
2014-07-22 08:41:18,888 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043676494 with entries=88, filesize=67.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043678828
2014-07-22 08:41:18,892 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:41:19,533 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=612, memsize=208.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/68622a14c07a485095b9e73dda5d57f1
2014-07-22 08:41:19,550 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/68622a14c07a485095b9e73dda5d57f1 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/68622a14c07a485095b9e73dda5d57f1
2014-07-22 08:41:19,563 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/68622a14c07a485095b9e73dda5d57f1, entries=759140, sequenceid=612, filesize=54.1m
2014-07-22 08:41:19,564 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~478.2m/501390960, currentsize=235.2m/246576640 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 14170ms, sequenceid=612, compaction requested=true
2014-07-22 08:41:19,564 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:41:19,565 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 2000 blocking
2014-07-22 08:41:19,565 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 478.5m
2014-07-22 08:41:19,565 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-22 08:41:19,565 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:41:19,565 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:41:19,565 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 08:41:19,784 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=617, memsize=209.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/64eb2b8070a249dfaac84ce40b78f438
2014-07-22 08:41:19,802 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/64eb2b8070a249dfaac84ce40b78f438 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/64eb2b8070a249dfaac84ce40b78f438
2014-07-22 08:41:19,817 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/64eb2b8070a249dfaac84ce40b78f438, entries=761520, sequenceid=617, filesize=54.3m
2014-07-22 08:41:19,817 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~489.5m/513242240, currentsize=227.0m/238054960 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 13590ms, sequenceid=617, compaction requested=true
2014-07-22 08:41:19,818 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:41:19,818 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 2000 blocking
2014-07-22 08:41:19,818 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-22 08:41:19,818 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:41:19,818 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:41:19,819 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 08:41:19,819 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 485.2m
2014-07-22 08:41:19,993 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:41:20,193 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:41:20,325 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:41:20,361 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:20,379 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4073 synced till here 4062
2014-07-22 08:41:20,457 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043678828 with entries=96, filesize=70.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043680362
2014-07-22 08:41:20,458 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:41:21,254 DEBUG [RpcServer.handler=5,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:41:21,868 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:21,901 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4164 synced till here 4157
2014-07-22 08:41:21,950 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043680362 with entries=91, filesize=70.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043681868
2014-07-22 08:41:21,951 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:41:23,402 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:23,475 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4289 synced till here 4265
2014-07-22 08:41:23,608 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043681868 with entries=125, filesize=81.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043683402
2014-07-22 08:41:23,609 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:41:25,288 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:25,342 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4391 synced till here 4381
2014-07-22 08:41:25,707 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043683402 with entries=102, filesize=71.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043685289
2014-07-22 08:41:25,707 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:41:27,554 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:27,582 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4481 synced till here 4472
2014-07-22 08:41:27,668 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043685289 with entries=90, filesize=71.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043687555
2014-07-22 08:41:27,670 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:41:29,332 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:29,347 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4559 synced till here 4556
2014-07-22 08:41:29,378 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043687555 with entries=78, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043689332
2014-07-22 08:41:29,379 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=44, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:41:30,993 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:33,754 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2000ms
GC pool 'ParNew' had collection(s): count=1 time=2061ms
2014-07-22 08:41:33,767 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4719 synced till here 4708
2014-07-22 08:41:33,807 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043689332 with entries=160, filesize=120.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043690993
2014-07-22 08:41:33,808 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=45, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:41:34,535 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:34,553 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4808 synced till here 4804
2014-07-22 08:41:34,607 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043690993 with entries=89, filesize=66.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043694536
2014-07-22 08:41:34,608 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=46, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:41:35,732 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:35,776 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043694536 with entries=90, filesize=61.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043695732
2014-07-22 08:41:35,777 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=47, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:41:37,323 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:37,339 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4993 synced till here 4992
2014-07-22 08:41:37,361 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043695732 with entries=95, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043697323
2014-07-22 08:41:37,362 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=48, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:41:38,374 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=745, memsize=249.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/b2e1a4eadd1e4e5bb14be12f10c249bb
2014-07-22 08:41:38,392 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/b2e1a4eadd1e4e5bb14be12f10c249bb as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/b2e1a4eadd1e4e5bb14be12f10c249bb
2014-07-22 08:41:38,410 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/b2e1a4eadd1e4e5bb14be12f10c249bb, entries=907190, sequenceid=745, filesize=64.6m
2014-07-22 08:41:38,410 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~480.4m/503687840, currentsize=333.6m/349833200 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 18845ms, sequenceid=745, compaction requested=true
2014-07-22 08:41:38,411 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 2000 blocking
2014-07-22 08:41:38,411 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-22 08:41:38,411 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:41:38,411 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:41:38,411 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 08:41:38,411 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:41:38,412 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b., current region memstore size 328.0m
2014-07-22 08:41:38,448 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:41:38,570 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:38,593 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5092 synced till here 5085
2014-07-22 08:41:38,683 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043697323 with entries=99, filesize=66.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043698570
2014-07-22 08:41:38,806 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:41:39,031 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=745, memsize=249.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/83b87534a4aa4408b85164b6d8cc57bd
2014-07-22 08:41:39,053 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/83b87534a4aa4408b85164b6d8cc57bd as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/83b87534a4aa4408b85164b6d8cc57bd
2014-07-22 08:41:39,069 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/83b87534a4aa4408b85164b6d8cc57bd, entries=909790, sequenceid=745, filesize=64.8m
2014-07-22 08:41:39,070 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~487.0m/510676240, currentsize=347.1m/363985840 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 19251ms, sequenceid=745, compaction requested=true
2014-07-22 08:41:39,070 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:41:39,070 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 2000 blocking
2014-07-22 08:41:39,071 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-22 08:41:39,071 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 589.8m
2014-07-22 08:41:39,071 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:41:39,071 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:41:39,071 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 08:41:39,082 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:41:39,603 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:41:40,712 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:40,734 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5173 synced till here 5171
2014-07-22 08:41:40,776 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043698570 with entries=81, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043700713
2014-07-22 08:41:41,511 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:41,535 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5250 synced till here 5249
2014-07-22 08:41:42,102 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043700713 with entries=77, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043701511
2014-07-22 08:41:43,381 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:44,352 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043701511 with entries=92, filesize=73.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043703382
2014-07-22 08:41:45,130 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:47,254 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5528 synced till here 5493
2014-07-22 08:41:47,460 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043703382 with entries=186, filesize=130.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043705131
2014-07-22 08:41:49,486 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:49,512 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5629 synced till here 5618
2014-07-22 08:41:49,600 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1304, memsize=187.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/4130980fd84b441985c65a89afaa8a5e
2014-07-22 08:41:49,614 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/4130980fd84b441985c65a89afaa8a5e as hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/4130980fd84b441985c65a89afaa8a5e
2014-07-22 08:41:49,627 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043705131 with entries=101, filesize=73.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043709487
2014-07-22 08:41:49,633 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/4130980fd84b441985c65a89afaa8a5e, entries=681870, sequenceid=1304, filesize=48.6m
2014-07-22 08:41:49,633 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~331.3m/347423280, currentsize=50.8m/53251600 for region usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. in 11221ms, sequenceid=1304, compaction requested=false
2014-07-22 08:41:49,633 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 744.3m
2014-07-22 08:41:51,062 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1189ms
GC pool 'ParNew' had collection(s): count=1 time=1201ms
2014-07-22 08:41:51,608 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:51,646 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5731 synced till here 5715
2014-07-22 08:41:51,680 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:41:51,783 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043709487 with entries=102, filesize=83.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043711608
2014-07-22 08:41:51,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043225414
2014-07-22 08:41:51,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043587879
2014-07-22 08:41:51,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043589847
2014-07-22 08:41:51,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043592325
2014-07-22 08:41:51,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043594985
2014-07-22 08:41:51,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043598173
2014-07-22 08:41:51,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043600816
2014-07-22 08:41:51,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043603290
2014-07-22 08:41:51,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043605382
2014-07-22 08:41:51,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043607130
2014-07-22 08:41:51,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043609015
2014-07-22 08:41:51,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043610838
2014-07-22 08:41:51,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043612824
2014-07-22 08:41:51,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043614572
2014-07-22 08:41:51,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043616358
2014-07-22 08:41:51,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043618412
2014-07-22 08:41:51,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043620174
2014-07-22 08:41:51,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043622318
2014-07-22 08:41:51,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043626634
2014-07-22 08:41:51,785 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043628800
2014-07-22 08:41:51,785 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043631068
2014-07-22 08:41:51,785 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043633161
2014-07-22 08:41:51,785 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043635286
2014-07-22 08:41:51,785 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043637522
2014-07-22 08:41:51,785 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043640701
2014-07-22 08:41:51,785 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043642926
2014-07-22 08:41:51,785 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043645038
2014-07-22 08:41:51,785 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043647031
2014-07-22 08:41:51,785 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043660900
2014-07-22 08:41:51,785 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043663327
2014-07-22 08:41:53,070 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1006ms
GC pool 'ParNew' had collection(s): count=1 time=1180ms
2014-07-22 08:41:53,617 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:53,646 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5821 synced till here 5808
2014-07-22 08:41:53,766 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043711608 with entries=90, filesize=73.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043713618
2014-07-22 08:41:55,123 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:55,143 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5909 synced till here 5902
2014-07-22 08:41:55,218 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043713618 with entries=88, filesize=67.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043715123
2014-07-22 08:41:56,929 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:56,953 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5986 synced till here 5981
2014-07-22 08:41:57,008 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043715123 with entries=77, filesize=66.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043716930
2014-07-22 08:41:58,341 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:41:58,722 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6125 synced till here 6119
2014-07-22 08:41:59,115 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043716930 with entries=139, filesize=100.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043718341
2014-07-22 08:42:00,362 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:00,524 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=960, memsize=292.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/2a4673b952a348a195a8cceb3387f3e2
2014-07-22 08:42:00,540 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6228 synced till here 6222
2014-07-22 08:42:00,587 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043718341 with entries=103, filesize=78.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043720363
2014-07-22 08:42:00,622 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/2a4673b952a348a195a8cceb3387f3e2 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/2a4673b952a348a195a8cceb3387f3e2
2014-07-22 08:42:00,637 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/2a4673b952a348a195a8cceb3387f3e2, entries=1064490, sequenceid=960, filesize=75.8m
2014-07-22 08:42:00,637 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~591.7m/620406080, currentsize=372.5m/390615600 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 21566ms, sequenceid=960, compaction requested=true
2014-07-22 08:42:00,638 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:42:00,638 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 719.6m
2014-07-22 08:42:00,638 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 2000 blocking
2014-07-22 08:42:00,638 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-22 08:42:00,639 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:42:00,639 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:42:00,639 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 08:42:00,717 DEBUG [RpcServer.handler=41,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:42:01,888 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:01,949 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:42:02,346 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6361 synced till here 6358
2014-07-22 08:42:02,379 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043720363 with entries=133, filesize=104.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043721889
2014-07-22 08:42:03,766 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:04,762 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043721889 with entries=90, filesize=74.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043723766
2014-07-22 08:42:05,948 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:05,966 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6534 synced till here 6528
2014-07-22 08:42:06,036 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043723766 with entries=83, filesize=66.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043725949
2014-07-22 08:42:07,369 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:07,393 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6622 synced till here 6621
2014-07-22 08:42:07,419 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043725949 with entries=88, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043727369
2014-07-22 08:42:08,668 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:08,688 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6713 synced till here 6710
2014-07-22 08:42:08,720 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043727369 with entries=91, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043728669
2014-07-22 08:42:09,925 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:10,294 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6852 synced till here 6846
2014-07-22 08:42:10,402 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043728669 with entries=139, filesize=95.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043729926
2014-07-22 08:42:11,724 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:12,067 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6970 synced till here 6969
2014-07-22 08:42:12,084 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043729926 with entries=118, filesize=87.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043731724
2014-07-22 08:42:13,343 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:13,384 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043731724 with entries=75, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043733343
2014-07-22 08:42:14,978 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1060, memsize=394.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/cd8826fd327b4471aa18a932b1b8f101
2014-07-22 08:42:14,994 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/cd8826fd327b4471aa18a932b1b8f101 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/cd8826fd327b4471aa18a932b1b8f101
2014-07-22 08:42:15,009 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/cd8826fd327b4471aa18a932b1b8f101, entries=1434780, sequenceid=1060, filesize=102.2m
2014-07-22 08:42:15,010 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~751.0m/787486880, currentsize=478.9m/502195920 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 25377ms, sequenceid=1060, compaction requested=true
2014-07-22 08:42:15,010 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:42:15,011 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 2000 blocking
2014-07-22 08:42:15,011 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 993.6m
2014-07-22 08:42:15,011 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-22 08:42:15,011 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:42:15,011 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:42:15,011 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 08:42:15,056 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:42:15,252 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:16,401 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:42:16,678 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7224 synced till here 7219
2014-07-22 08:42:16,765 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043733343 with entries=179, filesize=131.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043735252
2014-07-22 08:42:16,765 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043665054
2014-07-22 08:42:16,765 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043666709
2014-07-22 08:42:16,765 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043669110
2014-07-22 08:42:16,765 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043670436
2014-07-22 08:42:16,766 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043672037
2014-07-22 08:42:16,766 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043672976
2014-07-22 08:42:16,766 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043674953
2014-07-22 08:42:16,767 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043676494
2014-07-22 08:42:18,054 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:18,076 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7330 synced till here 7316
2014-07-22 08:42:18,123 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043735252 with entries=106, filesize=74.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043738055
2014-07-22 08:42:19,162 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:19,187 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7414 synced till here 7413
2014-07-22 08:42:19,201 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043738055 with entries=84, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043739162
2014-07-22 08:42:19,937 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1174, memsize=344.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/e830c9a78d2044bc93a65f94074638cb
2014-07-22 08:42:19,962 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/e830c9a78d2044bc93a65f94074638cb as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/e830c9a78d2044bc93a65f94074638cb
2014-07-22 08:42:19,979 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/e830c9a78d2044bc93a65f94074638cb, entries=1255770, sequenceid=1174, filesize=89.4m
2014-07-22 08:42:19,980 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~725.0m/760266480, currentsize=393.8m/412910640 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 19342ms, sequenceid=1174, compaction requested=true
2014-07-22 08:42:19,980 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:42:19,980 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 2000 blocking
2014-07-22 08:42:19,981 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 772.4m
2014-07-22 08:42:19,981 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-22 08:42:19,981 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:42:19,981 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:42:19,981 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 08:42:19,987 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:42:20,056 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:20,080 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7498 synced till here 7493
2014-07-22 08:42:20,796 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043739162 with entries=84, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043740056
2014-07-22 08:42:21,353 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:42:21,593 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:21,614 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7586 synced till here 7582
2014-07-22 08:42:21,649 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043740056 with entries=88, filesize=66.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043741593
2014-07-22 08:42:22,775 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:23,014 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7692 synced till here 7690
2014-07-22 08:42:23,039 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043741593 with entries=106, filesize=77.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043742775
2014-07-22 08:42:24,275 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:24,511 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7788 synced till here 7786
2014-07-22 08:42:24,523 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043742775 with entries=96, filesize=73.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043744276
2014-07-22 08:42:25,945 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:25,971 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7886 synced till here 7869
2014-07-22 08:42:26,095 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043744276 with entries=98, filesize=70.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043745945
2014-07-22 08:42:27,695 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:28,430 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8012 synced till here 7999
2014-07-22 08:42:29,330 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043745945 with entries=126, filesize=94.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043747695
2014-07-22 08:42:30,125 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:30,142 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8125 synced till here 8115
2014-07-22 08:42:30,256 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043747695 with entries=113, filesize=76.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043750126
2014-07-22 08:42:31,966 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:32,023 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8225 synced till here 8214
2014-07-22 08:42:32,150 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043750126 with entries=100, filesize=75.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043751966
2014-07-22 08:42:33,268 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,270 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,272 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,272 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,273 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,275 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,275 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,276 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,277 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,290 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,311 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,321 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,321 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,322 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,322 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,323 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,323 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,327 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,329 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,347 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,356 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,394 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,435 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,477 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,519 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,564 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,564 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,564 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,564 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,565 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,565 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,567 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,567 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,569 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,569 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,572 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,609 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,660 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,705 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,747 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,792 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,830 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,865 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,922 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:33,950 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:34,070 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:35,390 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:35,399 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:35,400 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:35,406 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:42:37,106 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1038ms
GC pool 'ParNew' had collection(s): count=1 time=1053ms
2014-07-22 08:42:38,269 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:42:38,272 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 08:42:38,272 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:42:38,273 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:42:38,273 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:42:38,275 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:42:38,276 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:42:38,276 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:42:38,278 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:42:38,290 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:42:38,311 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:42:38,321 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:42:38,322 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:42:38,322 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:42:38,323 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 08:42:38,323 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:42:38,324 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:42:38,327 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:42:38,329 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:42:38,348 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:42:38,356 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:42:38,394 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:42:38,436 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:42:38,478 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:42:38,519 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:42:38,564 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:42:38,564 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:42:38,564 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:42:38,565 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:42:38,565 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:42:38,565 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:42:38,567 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:42:38,568 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:42:38,569 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:42:38,569 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:42:38,572 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:42:38,609 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:42:38,661 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:42:38,706 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:42:38,748 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:42:38,793 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:42:38,830 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:42:38,865 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:42:38,923 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:42:38,950 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:42:39,071 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:42:40,390 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:42:40,400 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:42:40,400 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:42:40,406 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:42:42,186 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1411, memsize=394.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/cb710a1b2e1f416bbe29c6d48b1f1b0a
2014-07-22 08:42:42,651 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1333, memsize=484.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/e2f6719aeaf0450a8b107e7f5d830458
2014-07-22 08:42:42,863 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/cb710a1b2e1f416bbe29c6d48b1f1b0a as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/cb710a1b2e1f416bbe29c6d48b1f1b0a
2014-07-22 08:42:42,907 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/e2f6719aeaf0450a8b107e7f5d830458 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/e2f6719aeaf0450a8b107e7f5d830458
2014-07-22 08:42:43,269 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 08:42:43,273 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 08:42:43,273 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 08:42:43,274 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 08:42:43,274 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 08:42:43,275 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 08:42:43,276 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 08:42:43,276 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 08:42:43,278 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 08:42:43,291 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 08:42:43,311 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 08:42:43,322 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 08:42:43,323 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 08:42:43,323 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 08:42:43,323 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 08:42:43,323 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 08:42:43,324 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 08:42:43,328 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 08:42:43,329 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 08:42:43,348 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 08:42:43,356 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 08:42:43,394 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 08:42:43,425 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/cb710a1b2e1f416bbe29c6d48b1f1b0a, entries=1436920, sequenceid=1411, filesize=102.3m
2014-07-22 08:42:43,427 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~777.2m/814925760, currentsize=252.7m/264998960 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 23445ms, sequenceid=1411, compaction requested=true
2014-07-22 08:42:43,427 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:42:43,427 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 2000 blocking
2014-07-22 08:42:43,428 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-22 08:42:43,428 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:42:43,428 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:42:43,428 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10035ms
2014-07-22 08:42:43,428 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 08:42:43,428 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 845.8m
2014-07-22 08:42:43,428 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,428 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10073ms
2014-07-22 08:42:43,428 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,428 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10081ms
2014-07-22 08:42:43,429 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,429 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10100ms
2014-07-22 08:42:43,429 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,429 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10102ms
2014-07-22 08:42:43,429 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,429 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10106ms
2014-07-22 08:42:43,429 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,433 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10110ms
2014-07-22 08:42:43,433 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,433 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10112ms
2014-07-22 08:42:43,433 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,433 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10112ms
2014-07-22 08:42:43,434 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,434 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10113ms
2014-07-22 08:42:43,434 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,439 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-22 08:42:43,439 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,439 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10118ms
2014-07-22 08:42:43,439 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,439 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10128ms
2014-07-22 08:42:43,439 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,440 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10150ms
2014-07-22 08:42:43,440 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,441 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10164ms
2014-07-22 08:42:43,441 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,441 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10165ms
2014-07-22 08:42:43,441 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,449 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10174ms
2014-07-22 08:42:43,449 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,449 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10174ms
2014-07-22 08:42:43,449 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,450 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10178ms
2014-07-22 08:42:43,450 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,450 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10178ms
2014-07-22 08:42:43,450 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,450 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10178ms
2014-07-22 08:42:43,450 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,455 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10185ms
2014-07-22 08:42:43,455 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,455 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10187ms
2014-07-22 08:42:43,455 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,455 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8049ms
2014-07-22 08:42:43,455 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,455 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8055ms
2014-07-22 08:42:43,456 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,456 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8057ms
2014-07-22 08:42:43,456 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,465 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8075ms
2014-07-22 08:42:43,465 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,465 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9395ms
2014-07-22 08:42:43,466 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,466 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9516ms
2014-07-22 08:42:43,466 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,469 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9546ms
2014-07-22 08:42:43,469 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,478 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9612ms
2014-07-22 08:42:43,478 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,479 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9648ms
2014-07-22 08:42:43,479 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,479 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9687ms
2014-07-22 08:42:43,479 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,479 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9732ms
2014-07-22 08:42:43,480 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,480 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9775ms
2014-07-22 08:42:43,481 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,481 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9821ms
2014-07-22 08:42:43,481 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,487 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9878ms
2014-07-22 08:42:43,487 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,494 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9922ms
2014-07-22 08:42:43,494 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,501 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9932ms
2014-07-22 08:42:43,501 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,503 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9933ms
2014-07-22 08:42:43,503 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,503 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9936ms
2014-07-22 08:42:43,503 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,504 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9936ms
2014-07-22 08:42:43,504 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,505 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9939ms
2014-07-22 08:42:43,505 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,507 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9942ms
2014-07-22 08:42:43,507 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,507 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9943ms
2014-07-22 08:42:43,507 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,507 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9944ms
2014-07-22 08:42:43,507 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,508 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9944ms
2014-07-22 08:42:43,508 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:43,510 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9945ms
2014-07-22 08:42:43,510 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:44,443 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10922ms
2014-07-22 08:42:44,443 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:44,443 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10966ms
2014-07-22 08:42:44,444 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:42:44,449 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/e2f6719aeaf0450a8b107e7f5d830458, entries=1762590, sequenceid=1333, filesize=125.5m
2014-07-22 08:42:44,454 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~999.0m/1047499440, currentsize=353.3m/370464960 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 29443ms, sequenceid=1333, compaction requested=true
2014-07-22 08:42:44,455 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 2000 blocking
2014-07-22 08:42:44,455 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-22 08:42:44,455 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:42:44,455 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:42:44,455 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 08:42:44,455 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:42:44,455 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 636.1m
2014-07-22 08:42:44,555 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:42:44,654 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11091,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753563,"queuetimems":0,"class":"HRegionServer","responsesize":26,"method":"Multi"}
2014-07-22 08:42:44,655 DEBUG [RpcServer.handler=41,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:42:44,655 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11090,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753564,"queuetimems":0,"class":"HRegionServer","responsesize":297,"method":"Multi"}
2014-07-22 08:42:44,656 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11088,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753567,"queuetimems":1,"class":"HRegionServer","responsesize":1337,"method":"Multi"}
2014-07-22 08:42:44,656 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13039,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043751617,"queuetimems":0,"class":"HRegionServer","responsesize":18479,"method":"Multi"}
2014-07-22 08:42:44,654 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11085,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753569,"queuetimems":0,"class":"HRegionServer","responsesize":2093,"method":"Multi"}
2014-07-22 08:42:44,678 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11091,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753563,"queuetimems":0,"class":"HRegionServer","responsesize":32,"method":"Multi"}
2014-07-22 08:42:44,678 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11091,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753564,"queuetimems":0,"class":"HRegionServer","responsesize":68,"method":"Multi"}
2014-07-22 08:42:44,678 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11085,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753569,"queuetimems":0,"class":"HRegionServer","responsesize":38,"method":"Multi"}
2014-07-22 08:42:44,678 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11090,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753564,"queuetimems":1,"class":"HRegionServer","responsesize":26,"method":"Multi"}
2014-07-22 08:42:44,678 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11089,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753565,"queuetimems":1,"class":"HRegionServer","responsesize":14,"method":"Multi"}
2014-07-22 08:42:44,678 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11085,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753569,"queuetimems":0,"class":"HRegionServer","responsesize":50,"method":"Multi"}
2014-07-22 08:42:44,879 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:44,881 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13337,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043751543,"queuetimems":0,"class":"HRegionServer","responsesize":18897,"method":"Multi"}
2014-07-22 08:42:44,944 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8334 synced till here 8312
2014-07-22 08:42:45,117 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13391,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043751726,"queuetimems":0,"class":"HRegionServer","responsesize":18482,"method":"Multi"}
2014-07-22 08:42:45,117 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13303,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043751814,"queuetimems":1,"class":"HRegionServer","responsesize":18526,"method":"Multi"}
2014-07-22 08:42:45,142 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13561,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043751581,"queuetimems":0,"class":"HRegionServer","responsesize":17098,"method":"Multi"}
2014-07-22 08:42:45,166 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043751966 with entries=109, filesize=87.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043764879
2014-07-22 08:42:45,166 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043678828
2014-07-22 08:42:45,166 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043680362
2014-07-22 08:42:45,166 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043681868
2014-07-22 08:42:45,166 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043683402
2014-07-22 08:42:45,166 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043685289
2014-07-22 08:42:45,167 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043687555
2014-07-22 08:42:45,167 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043689332
2014-07-22 08:42:45,167 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043690993
2014-07-22 08:42:45,167 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043694536
2014-07-22 08:42:45,167 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043695732
2014-07-22 08:42:45,351 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:42:46,431 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:42:46,471 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14698,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043751773,"queuetimems":0,"class":"HRegionServer","responsesize":18428,"method":"Multi"}
2014-07-22 08:42:46,471 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14816,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043751655,"queuetimems":1,"class":"HRegionServer","responsesize":18720,"method":"Multi"}
2014-07-22 08:42:46,482 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14588,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043751893,"queuetimems":0,"class":"HRegionServer","responsesize":18695,"method":"Multi"}
2014-07-22 08:42:46,484 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14634,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043751849,"queuetimems":0,"class":"HRegionServer","responsesize":18795,"method":"Multi"}
2014-07-22 08:42:46,493 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13241,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753252,"queuetimems":0,"class":"HRegionServer","responsesize":18526,"method":"Multi"}
2014-07-22 08:42:46,494 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13313,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753180,"queuetimems":0,"class":"HRegionServer","responsesize":18920,"method":"Multi"}
2014-07-22 08:42:46,498 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13398,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753099,"queuetimems":0,"class":"HRegionServer","responsesize":15434,"method":"Multi"}
2014-07-22 08:42:46,498 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14405,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043752092,"queuetimems":1,"class":"HRegionServer","responsesize":18543,"method":"Multi"}
2014-07-22 08:42:46,499 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14355,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043752143,"queuetimems":1,"class":"HRegionServer","responsesize":18678,"method":"Multi"}
2014-07-22 08:42:46,501 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14534,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043751959,"queuetimems":0,"class":"HRegionServer","responsesize":18632,"method":"Multi"}
2014-07-22 08:42:46,638 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13499,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753138,"queuetimems":0,"class":"HRegionServer","responsesize":18428,"method":"Multi"}
2014-07-22 08:42:46,861 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:46,862 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13777,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753085,"queuetimems":0,"class":"HRegionServer","responsesize":18121,"method":"Multi"}
2014-07-22 08:42:46,862 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15175,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043751687,"queuetimems":1,"class":"HRegionServer","responsesize":17096,"method":"Multi"}
2014-07-22 08:42:46,862 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14834,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043752028,"queuetimems":1,"class":"HRegionServer","responsesize":16885,"method":"Multi"}
2014-07-22 08:42:46,870 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13438,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753432,"queuetimems":1,"class":"HRegionServer","responsesize":18224,"method":"Multi"}
2014-07-22 08:42:46,870 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13480,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753390,"queuetimems":0,"class":"HRegionServer","responsesize":17989,"method":"Multi"}
2014-07-22 08:42:46,874 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13521,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753353,"queuetimems":1,"class":"HRegionServer","responsesize":18583,"method":"Multi"}
2014-07-22 08:42:46,875 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13567,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753308,"queuetimems":0,"class":"HRegionServer","responsesize":18540,"method":"Multi"}
2014-07-22 08:42:46,908 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8444 synced till here 8421
2014-07-22 08:42:47,057 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043764879 with entries=110, filesize=97.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043766861
2014-07-22 08:42:47,057 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:42:48,011 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13944,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043754067,"queuetimems":1,"class":"HRegionServer","responsesize":18116,"method":"Multi"}
2014-07-22 08:42:48,018 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14104,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753914,"queuetimems":0,"class":"HRegionServer","responsesize":18897,"method":"Multi"}
2014-07-22 08:42:48,020 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14275,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753744,"queuetimems":0,"class":"HRegionServer","responsesize":18810,"method":"Multi"}
2014-07-22 08:42:48,011 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14448,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753563,"queuetimems":0,"class":"HRegionServer","responsesize":18610,"method":"Multi"}
2014-07-22 08:42:48,022 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14365,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753657,"queuetimems":0,"class":"HRegionServer","responsesize":18720,"method":"Multi"}
2014-07-22 08:42:48,019 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14156,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753862,"queuetimems":0,"class":"HRegionServer","responsesize":15786,"method":"Multi"}
2014-07-22 08:42:48,034 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14331,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753702,"queuetimems":0,"class":"HRegionServer","responsesize":19052,"method":"Multi"}
2014-07-22 08:42:48,042 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14526,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753515,"queuetimems":1,"class":"HRegionServer","responsesize":18835,"method":"Multi"}
2014-07-22 08:42:48,050 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14223,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753827,"queuetimems":0,"class":"HRegionServer","responsesize":18479,"method":"Multi"}
2014-07-22 08:42:48,062 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14587,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753474,"queuetimems":0,"class":"HRegionServer","responsesize":18575,"method":"Multi"}
2014-07-22 08:42:48,419 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:48,421 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14473,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753948,"queuetimems":0,"class":"HRegionServer","responsesize":17098,"method":"Multi"}
2014-07-22 08:42:48,423 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14633,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753788,"queuetimems":0,"class":"HRegionServer","responsesize":17096,"method":"Multi"}
2014-07-22 08:42:48,426 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14819,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043753606,"queuetimems":0,"class":"HRegionServer","responsesize":18482,"method":"Multi"}
2014-07-22 08:42:48,469 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8548 synced till here 8515
2014-07-22 08:42:48,762 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043766861 with entries=104, filesize=87.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043768419
2014-07-22 08:42:48,763 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:42:50,245 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:50,323 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8654 synced till here 8616
2014-07-22 08:42:50,595 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043768419 with entries=106, filesize=100.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043770245
2014-07-22 08:42:50,596 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:42:51,828 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:51,861 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8770 synced till here 8741
2014-07-22 08:42:52,070 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043770245 with entries=116, filesize=97.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043771828
2014-07-22 08:42:52,071 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:42:53,631 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:53,665 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8874 synced till here 8845
2014-07-22 08:42:54,049 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043771828 with entries=104, filesize=93.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043773632
2014-07-22 08:42:54,049 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:42:55,245 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:55,266 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8948 synced till here 8945
2014-07-22 08:42:55,337 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043773632 with entries=74, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043775246
2014-07-22 08:42:55,337 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:42:56,717 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:56,739 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9025 synced till here 9022
2014-07-22 08:42:56,761 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043775246 with entries=77, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043776718
2014-07-22 08:42:56,761 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:42:57,049 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b.
2014-07-22 08:42:58,123 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:58,145 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9119 synced till here 9118
2014-07-22 08:42:58,197 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043776718 with entries=94, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043778123
2014-07-22 08:42:58,198 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:42:59,111 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:42:59,126 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9194 synced till here 9190
2014-07-22 08:42:59,151 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043778123 with entries=75, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043779112
2014-07-22 08:42:59,151 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:43:00,339 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:43:00,972 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9321 synced till here 9317
2014-07-22 08:43:01,159 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043779112 with entries=127, filesize=86.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043780340
2014-07-22 08:43:01,161 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:43:02,128 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:43:02,167 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043780340 with entries=87, filesize=61.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043782129
2014-07-22 08:43:02,168 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:43:02,830 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:02,848 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:02,850 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:02,853 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:02,863 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:02,877 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:02,883 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:02,905 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:02,907 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:02,911 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:02,946 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:02,946 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:02,947 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:02,948 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:02,984 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:02,986 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,061 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,098 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,100 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,100 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,101 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,103 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,104 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,106 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,108 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,173 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,213 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,214 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,216 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,255 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,304 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,343 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,344 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,345 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,345 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,345 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,347 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,348 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,349 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,349 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,350 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,351 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,421 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,525 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,525 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,526 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:03,527 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:04,261 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:04,264 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:04,305 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:07,321 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1544, memsize=410.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/23e68053849244efa454a89ce248a9b0
2014-07-22 08:43:07,344 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/23e68053849244efa454a89ce248a9b0 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/23e68053849244efa454a89ce248a9b0
2014-07-22 08:43:07,361 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/23e68053849244efa454a89ce248a9b0, entries=1496040, sequenceid=1544, filesize=106.6m
2014-07-22 08:43:07,362 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~646.9m/678362560, currentsize=418.1m/438386560 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 22906ms, sequenceid=1544, compaction requested=true
2014-07-22 08:43:07,362 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:43:07,362 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 2000 blocking
2014-07-22 08:43:07,363 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-22 08:43:07,363 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:43:07,363 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3059ms
2014-07-22 08:43:07,363 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:43:07,363 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,363 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 08:43:07,363 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3099ms
2014-07-22 08:43:07,363 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,363 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 662.8m
2014-07-22 08:43:07,363 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3102ms
2014-07-22 08:43:07,364 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,364 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3837ms
2014-07-22 08:43:07,364 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,364 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3839ms
2014-07-22 08:43:07,364 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,365 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3840ms
2014-07-22 08:43:07,365 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,365 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3840ms
2014-07-22 08:43:07,365 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,365 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3944ms
2014-07-22 08:43:07,366 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,370 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4019ms
2014-07-22 08:43:07,370 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,370 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4021ms
2014-07-22 08:43:07,370 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,375 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4025ms
2014-07-22 08:43:07,375 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,375 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4026ms
2014-07-22 08:43:07,375 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,375 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4027ms
2014-07-22 08:43:07,375 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,375 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4028ms
2014-07-22 08:43:07,375 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,376 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4031ms
2014-07-22 08:43:07,376 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,376 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4031ms
2014-07-22 08:43:07,376 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,377 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4033ms
2014-07-22 08:43:07,377 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,378 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4033ms
2014-07-22 08:43:07,378 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,378 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4035ms
2014-07-22 08:43:07,378 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,379 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4074ms
2014-07-22 08:43:07,379 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,379 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4124ms
2014-07-22 08:43:07,379 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,379 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4163ms
2014-07-22 08:43:07,379 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,380 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4166ms
2014-07-22 08:43:07,380 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,380 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4167ms
2014-07-22 08:43:07,380 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,383 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4211ms
2014-07-22 08:43:07,384 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,384 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4276ms
2014-07-22 08:43:07,385 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,386 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4279ms
2014-07-22 08:43:07,386 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,386 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4282ms
2014-07-22 08:43:07,386 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,389 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4286ms
2014-07-22 08:43:07,389 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,389 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4288ms
2014-07-22 08:43:07,389 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,397 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4297ms
2014-07-22 08:43:07,398 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,398 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4298ms
2014-07-22 08:43:07,398 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,400 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4302ms
2014-07-22 08:43:07,400 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,400 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4339ms
2014-07-22 08:43:07,400 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,405 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4419ms
2014-07-22 08:43:07,405 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,409 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4425ms
2014-07-22 08:43:07,409 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,410 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4461ms
2014-07-22 08:43:07,410 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,410 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4464ms
2014-07-22 08:43:07,410 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,411 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4464ms
2014-07-22 08:43:07,411 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,412 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4465ms
2014-07-22 08:43:07,412 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,413 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4503ms
2014-07-22 08:43:07,413 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,413 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4506ms
2014-07-22 08:43:07,413 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,413 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4508ms
2014-07-22 08:43:07,413 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,414 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4530ms
2014-07-22 08:43:07,414 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,414 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4537ms
2014-07-22 08:43:07,414 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,416 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4553ms
2014-07-22 08:43:07,416 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,417 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4564ms
2014-07-22 08:43:07,417 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,417 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4567ms
2014-07-22 08:43:07,417 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,417 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4569ms
2014-07-22 08:43:07,417 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,418 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4587ms
2014-07-22 08:43:07,418 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:07,783 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:43:08,028 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:43:08,081 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:43:08,121 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9570 synced till here 9530
2014-07-22 08:43:09,364 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043782129 with entries=162, filesize=95.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043788081
2014-07-22 08:43:09,364 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=44, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:43:10,368 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:43:11,075 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=6603, hits=3, hitRatio=0.04%, , cachingAccesses=5, cachingHits=3, cachingHitsRatio=60.00%, evictions=0, evicted=0, evictedPerRun=NaN
2014-07-22 08:43:11,130 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9685 synced till here 9668
2014-07-22 08:43:11,461 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043788081 with entries=115, filesize=95.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043790368
2014-07-22 08:43:11,461 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=45, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:43:12,068 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:43:12,103 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9811 synced till here 9782
2014-07-22 08:43:12,955 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1545, memsize=482.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/42e23e2f19914537ab56aff9008dbfa1
2014-07-22 08:43:12,969 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/42e23e2f19914537ab56aff9008dbfa1 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/42e23e2f19914537ab56aff9008dbfa1
2014-07-22 08:43:12,986 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/42e23e2f19914537ab56aff9008dbfa1, entries=1755530, sequenceid=1545, filesize=125.1m
2014-07-22 08:43:12,987 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~845.8m/886867200, currentsize=525.7m/551209760 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 29559ms, sequenceid=1545, compaction requested=true
2014-07-22 08:43:12,987 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:43:12,987 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 2000 blocking
2014-07-22 08:43:12,987 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-22 08:43:12,987 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:43:12,987 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 860.4m
2014-07-22 08:43:12,987 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:43:12,988 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 08:43:13,038 DEBUG [RpcServer.handler=13,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:43:13,101 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043790368 with entries=126, filesize=81.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043792068
2014-07-22 08:43:13,102 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=46, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:43:13,832 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:43:13,999 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9951 synced till here 9903
2014-07-22 08:43:14,940 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:43:15,354 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043792068 with entries=140, filesize=96.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043793832
2014-07-22 08:43:15,356 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=47, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:43:16,062 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:43:16,711 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10056 synced till here 10036
2014-07-22 08:43:16,803 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043793832 with entries=105, filesize=81.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043796062
2014-07-22 08:43:16,803 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=48, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:43:17,473 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:43:17,492 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10158 synced till here 10148
2014-07-22 08:43:17,588 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043796062 with entries=102, filesize=74.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043797474
2014-07-22 08:43:17,589 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=49, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:43:19,040 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:43:19,085 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043797474 with entries=73, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043799040
2014-07-22 08:43:19,088 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=50, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:43:20,349 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:43:20,373 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10307 synced till here 10306
2014-07-22 08:43:20,407 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043799040 with entries=76, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043800349
2014-07-22 08:43:20,408 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=51, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:43:21,601 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:43:21,632 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043800349 with entries=90, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043801602
2014-07-22 08:43:21,633 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=52, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:43:23,005 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:43:23,032 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043801602 with entries=72, filesize=61.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043803005
2014-07-22 08:43:23,032 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=53, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:43:24,482 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1784, memsize=292.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/9e1e9dc799cf43c5bc5c2c7f8190456b
2014-07-22 08:43:24,498 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/9e1e9dc799cf43c5bc5c2c7f8190456b as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/9e1e9dc799cf43c5bc5c2c7f8190456b
2014-07-22 08:43:24,512 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/9e1e9dc799cf43c5bc5c2c7f8190456b, entries=1066300, sequenceid=1784, filesize=76.0m
2014-07-22 08:43:24,512 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~662.8m/695043040, currentsize=348.3m/365194400 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 17149ms, sequenceid=1784, compaction requested=true
2014-07-22 08:43:24,513 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 2000 blocking
2014-07-22 08:43:24,513 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:43:24,513 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-22 08:43:24,513 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:43:24,513 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:43:24,513 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 08:43:24,513 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b., current region memstore size 358.6m
2014-07-22 08:43:24,567 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:43:25,029 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:43:25,056 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10555 synced till here 10547
2014-07-22 08:43:25,132 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043803005 with entries=86, filesize=68.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043805029
2014-07-22 08:43:25,270 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:43:25,858 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:43:26,512 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10642 synced till here 10640
2014-07-22 08:43:26,767 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043805029 with entries=87, filesize=76.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043805859
2014-07-22 08:43:27,561 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:43:28,541 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10752 synced till here 10741
2014-07-22 08:43:28,620 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043805859 with entries=110, filesize=76.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043807562
2014-07-22 08:43:30,665 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1141ms
GC pool 'ParNew' had collection(s): count=1 time=1195ms
2014-07-22 08:43:30,666 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:43:30,726 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10871 synced till here 10854
2014-07-22 08:43:30,902 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043807562 with entries=119, filesize=78.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043810667
2014-07-22 08:43:33,255 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1087ms
GC pool 'ParNew' had collection(s): count=1 time=1151ms
2014-07-22 08:43:33,320 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:43:33,351 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10962 synced till here 10944
2014-07-22 08:43:33,543 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043810667 with entries=91, filesize=73.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043813320
2014-07-22 08:43:35,281 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1025ms
GC pool 'ParNew' had collection(s): count=1 time=1044ms
2014-07-22 08:43:35,470 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:43:35,476 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,481 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,482 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,483 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,483 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,484 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,484 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,484 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,484 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,485 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,486 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,489 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,489 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,490 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,491 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,492 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,495 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,495 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,496 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,496 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,496 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,496 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,496 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,497 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,498 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,498 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,498 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,499 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11076 synced till here 11058
2014-07-22 08:43:35,519 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,556 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,573 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,574 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,578 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,578 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,578 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,579 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,579 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,580 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,580 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,581 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043813320 with entries=114, filesize=86.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043815471
2014-07-22 08:43:35,583 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,600 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,601 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,632 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,649 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,650 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,688 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,690 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,720 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,740 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:35,778 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:40,123 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2663, memsize=245.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/09b2a4a147934f3194acab7992f14acb
2014-07-22 08:43:40,140 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/09b2a4a147934f3194acab7992f14acb as hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/09b2a4a147934f3194acab7992f14acb
2014-07-22 08:43:40,152 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/09b2a4a147934f3194acab7992f14acb, entries=894760, sequenceid=2663, filesize=63.7m
2014-07-22 08:43:40,152 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~358.6m/376039600, currentsize=27.4m/28737600 for region usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. in 15639ms, sequenceid=2663, compaction requested=false
2014-07-22 08:43:40,153 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4375ms
2014-07-22 08:43:40,153 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,153 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 939.1m
2014-07-22 08:43:40,154 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4413ms
2014-07-22 08:43:40,154 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,155 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4434ms
2014-07-22 08:43:40,155 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,161 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4471ms
2014-07-22 08:43:40,161 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,162 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4473ms
2014-07-22 08:43:40,162 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,162 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4512ms
2014-07-22 08:43:40,162 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,162 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4513ms
2014-07-22 08:43:40,162 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,162 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4530ms
2014-07-22 08:43:40,163 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,165 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4565ms
2014-07-22 08:43:40,166 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,166 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4566ms
2014-07-22 08:43:40,166 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,170 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4586ms
2014-07-22 08:43:40,170 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,170 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4590ms
2014-07-22 08:43:40,170 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,171 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4592ms
2014-07-22 08:43:40,171 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,171 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4592ms
2014-07-22 08:43:40,171 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,171 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4592ms
2014-07-22 08:43:40,171 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,175 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4597ms
2014-07-22 08:43:40,175 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,176 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4598ms
2014-07-22 08:43:40,176 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,177 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4600ms
2014-07-22 08:43:40,177 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,178 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4603ms
2014-07-22 08:43:40,178 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,178 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4605ms
2014-07-22 08:43:40,178 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,178 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4622ms
2014-07-22 08:43:40,178 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,217 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4698ms
2014-07-22 08:43:40,217 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,219 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4719ms
2014-07-22 08:43:40,219 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,229 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4731ms
2014-07-22 08:43:40,230 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,230 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4732ms
2014-07-22 08:43:40,230 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,230 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4732ms
2014-07-22 08:43:40,230 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,230 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4734ms
2014-07-22 08:43:40,230 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,230 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4734ms
2014-07-22 08:43:40,231 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,231 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4735ms
2014-07-22 08:43:40,231 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,241 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4745ms
2014-07-22 08:43:40,241 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,249 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4753ms
2014-07-22 08:43:40,249 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,249 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4753ms
2014-07-22 08:43:40,250 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,250 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4755ms
2014-07-22 08:43:40,250 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,250 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4755ms
2014-07-22 08:43:40,250 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,255 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4763ms
2014-07-22 08:43:40,255 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,257 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4766ms
2014-07-22 08:43:40,257 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,266 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4775ms
2014-07-22 08:43:40,266 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,266 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4777ms
2014-07-22 08:43:40,266 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,268 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4778ms
2014-07-22 08:43:40,268 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,268 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4782ms
2014-07-22 08:43:40,269 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,269 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4784ms
2014-07-22 08:43:40,269 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,269 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4785ms
2014-07-22 08:43:40,269 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,273 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4789ms
2014-07-22 08:43:40,273 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,273 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4789ms
2014-07-22 08:43:40,273 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,274 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4791ms
2014-07-22 08:43:40,274 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,274 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4791ms
2014-07-22 08:43:40,274 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,276 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4793ms
2014-07-22 08:43:40,276 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,276 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4795ms
2014-07-22 08:43:40,276 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,281 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4800ms
2014-07-22 08:43:40,281 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,289 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4813ms
2014-07-22 08:43:40,289 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:43:40,517 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1833, memsize=411.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/777d66ee52354f3385d8079757128d49
2014-07-22 08:43:40,541 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/777d66ee52354f3385d8079757128d49 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/777d66ee52354f3385d8079757128d49
2014-07-22 08:43:40,559 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/777d66ee52354f3385d8079757128d49, entries=1498610, sequenceid=1833, filesize=106.8m
2014-07-22 08:43:40,562 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~885.6m/928615760, currentsize=410.9m/430853120 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 27574ms, sequenceid=1833, compaction requested=true
2014-07-22 08:43:40,562 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:43:40,562 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 2000 blocking
2014-07-22 08:43:40,562 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 953.3m
2014-07-22 08:43:40,562 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-22 08:43:40,562 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:43:40,562 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:43:40,562 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 08:43:40,655 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:43:42,114 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:43:42,131 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11199 synced till here 11160
2014-07-22 08:43:42,216 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:43:42,496 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043815471 with entries=123, filesize=93.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043822115
2014-07-22 08:43:42,497 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043697323
2014-07-22 08:43:42,497 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043698570
2014-07-22 08:43:42,497 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043700713
2014-07-22 08:43:42,497 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043701511
2014-07-22 08:43:42,497 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043703382
2014-07-22 08:43:42,497 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043705131
2014-07-22 08:43:42,497 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043709487
2014-07-22 08:43:42,497 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043711608
2014-07-22 08:43:42,497 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043713618
2014-07-22 08:43:42,497 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043715123
2014-07-22 08:43:42,497 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043716930
2014-07-22 08:43:42,497 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043718341
2014-07-22 08:43:42,497 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043720363
2014-07-22 08:43:42,498 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043721889
2014-07-22 08:43:42,498 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043723766
2014-07-22 08:43:42,498 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043725949
2014-07-22 08:43:42,498 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043727369
2014-07-22 08:43:42,498 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043728669
2014-07-22 08:43:42,498 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043729926
2014-07-22 08:43:42,498 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043731724
2014-07-22 08:43:42,498 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043733343
2014-07-22 08:43:42,498 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043735252
2014-07-22 08:43:42,498 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043738055
2014-07-22 08:43:42,498 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043739162
2014-07-22 08:43:42,498 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043740056
2014-07-22 08:43:42,498 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043741593
2014-07-22 08:43:42,498 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043742775
2014-07-22 08:43:42,498 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043744276
2014-07-22 08:43:42,498 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043745945
2014-07-22 08:43:42,498 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043747695
2014-07-22 08:43:42,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043750126
2014-07-22 08:43:42,853 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:43:43,260 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:43:44,141 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11309 synced till here 11275
2014-07-22 08:43:44,335 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043822115 with entries=110, filesize=85.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043823261
2014-07-22 08:43:45,064 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:43:45,822 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11414 synced till here 11393
2014-07-22 08:43:46,036 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043823261 with entries=105, filesize=80.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043825065
2014-07-22 08:43:47,072 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:43:47,962 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11529 synced till here 11495
2014-07-22 08:43:48,216 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043825065 with entries=115, filesize=98.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043827073
2014-07-22 08:43:49,089 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:43:50,710 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11730 synced till here 11697
2014-07-22 08:43:50,902 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043827073 with entries=201, filesize=142.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043829090
2014-07-22 08:43:52,493 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:43:52,526 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043829090 with entries=84, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043832495
2014-07-22 08:43:53,784 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:43:53,806 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11900 synced till here 11899
2014-07-22 08:43:53,819 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043832495 with entries=86, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043833784
2014-07-22 08:43:54,275 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,284 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,309 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,314 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,315 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,324 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,354 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,401 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,420 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,448 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,495 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,497 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,516 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,541 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,581 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,582 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,582 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,583 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,585 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,627 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,629 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,722 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,723 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,857 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,857 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,860 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,860 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,865 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,942 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,997 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:54,998 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:55,008 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:55,033 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:55,158 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:55,431 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:55,642 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:57,200 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:57,213 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:57,250 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:57,285 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:57,322 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:58,282 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:58,410 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:58,410 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:58,421 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:58,423 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:58,425 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:58,425 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:58,426 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:58,427 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:43:59,276 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:43:59,284 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:43:59,309 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:43:59,314 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:43:59,315 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:43:59,324 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:43:59,354 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:43:59,401 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:43:59,421 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:43:59,448 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:43:59,495 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:43:59,497 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:43:59,517 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:43:59,542 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:43:59,581 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:43:59,583 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:43:59,583 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 08:43:59,584 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 08:43:59,585 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:43:59,628 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:43:59,630 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:43:59,722 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:43:59,724 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:43:59,857 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:43:59,858 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:43:59,861 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:43:59,861 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:43:59,865 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:43:59,942 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:43:59,998 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:43:59,998 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:44:00,008 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:44:00,033 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:44:00,159 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:44:00,432 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:44:00,643 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:44:01,253 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2081, memsize=349.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/826bb8efa76d4222b56b203440db1d89
2014-07-22 08:44:01,270 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/826bb8efa76d4222b56b203440db1d89 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/826bb8efa76d4222b56b203440db1d89
2014-07-22 08:44:01,281 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/826bb8efa76d4222b56b203440db1d89, entries=1273000, sequenceid=2081, filesize=90.7m
2014-07-22 08:44:01,281 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~939.1m/984700320, currentsize=297.1m/311512640 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 21128ms, sequenceid=2081, compaction requested=true
2014-07-22 08:44:01,282 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:44:01,282 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 2000 blocking
2014-07-22 08:44:01,282 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-22 08:44:01,282 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:44:01,282 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:44:01,282 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5640ms
2014-07-22 08:44:01,282 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,282 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 08:44:01,283 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 815.2m
2014-07-22 08:44:01,283 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5852ms
2014-07-22 08:44:01,283 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,283 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6125ms
2014-07-22 08:44:01,283 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,283 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6250ms
2014-07-22 08:44:01,283 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,284 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6276ms
2014-07-22 08:44:01,284 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,284 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6287ms
2014-07-22 08:44:01,284 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,284 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6287ms
2014-07-22 08:44:01,284 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,284 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6342ms
2014-07-22 08:44:01,285 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,285 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6420ms
2014-07-22 08:44:01,285 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,285 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6425ms
2014-07-22 08:44:01,285 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,285 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6425ms
2014-07-22 08:44:01,285 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,285 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6428ms
2014-07-22 08:44:01,285 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,286 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6429ms
2014-07-22 08:44:01,286 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,286 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6563ms
2014-07-22 08:44:01,286 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,286 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6565ms
2014-07-22 08:44:01,286 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,287 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6658ms
2014-07-22 08:44:01,287 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,287 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6660ms
2014-07-22 08:44:01,287 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,289 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6704ms
2014-07-22 08:44:01,289 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,293 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6712ms
2014-07-22 08:44:01,293 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,293 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6712ms
2014-07-22 08:44:01,293 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,294 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6713ms
2014-07-22 08:44:01,294 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,295 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6714ms
2014-07-22 08:44:01,295 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,295 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6754ms
2014-07-22 08:44:01,295 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,295 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6779ms
2014-07-22 08:44:01,295 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,296 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6800ms
2014-07-22 08:44:01,296 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,296 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6801ms
2014-07-22 08:44:01,296 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,296 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6848ms
2014-07-22 08:44:01,296 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,299 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6879ms
2014-07-22 08:44:01,299 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,299 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6898ms
2014-07-22 08:44:01,299 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,301 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6947ms
2014-07-22 08:44:01,301 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,301 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6977ms
2014-07-22 08:44:01,302 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,302 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6987ms
2014-07-22 08:44:01,302 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,304 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6990ms
2014-07-22 08:44:01,304 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,304 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6995ms
2014-07-22 08:44:01,304 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,304 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7021ms
2014-07-22 08:44:01,304 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,305 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7030ms
2014-07-22 08:44:01,305 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,306 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2879ms
2014-07-22 08:44:01,306 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,306 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2880ms
2014-07-22 08:44:01,306 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,307 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2882ms
2014-07-22 08:44:01,307 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,308 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2883ms
2014-07-22 08:44:01,309 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,309 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2886ms
2014-07-22 08:44:01,309 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,309 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2888ms
2014-07-22 08:44:01,309 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,309 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2899ms
2014-07-22 08:44:01,310 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,310 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2901ms
2014-07-22 08:44:01,310 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,311 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3029ms
2014-07-22 08:44:01,311 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,312 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3990ms
2014-07-22 08:44:01,312 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,313 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4029ms
2014-07-22 08:44:01,313 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,317 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4067ms
2014-07-22 08:44:01,317 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,318 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4105ms
2014-07-22 08:44:01,318 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,325 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4126ms
2014-07-22 08:44:01,325 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:44:01,501 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:44:02,591 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:02,682 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12049 synced till here 12026
2014-07-22 08:44:02,785 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:44:02,803 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043833784 with entries=149, filesize=94.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043842591
2014-07-22 08:44:03,185 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2101, memsize=349.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/3e42cc6d87c747f68fd7419d33e1b8c6
2014-07-22 08:44:03,205 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/3e42cc6d87c747f68fd7419d33e1b8c6 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/3e42cc6d87c747f68fd7419d33e1b8c6
2014-07-22 08:44:03,235 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/3e42cc6d87c747f68fd7419d33e1b8c6, entries=1272310, sequenceid=2101, filesize=90.6m
2014-07-22 08:44:03,236 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~965.7m/1012612160, currentsize=343.4m/360078240 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 22674ms, sequenceid=2101, compaction requested=true
2014-07-22 08:44:03,236 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:44:03,236 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 2000 blocking
2014-07-22 08:44:03,236 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 729.6m
2014-07-22 08:44:03,236 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-22 08:44:03,236 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:44:03,236 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:44:03,236 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 08:44:03,262 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:44:04,464 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:04,471 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10024,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043834446,"queuetimems":0,"class":"HRegionServer","responsesize":13262,"method":"Multi"}
2014-07-22 08:44:04,490 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10136,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043834353,"queuetimems":1,"class":"HRegionServer","responsesize":11274,"method":"Multi"}
2014-07-22 08:44:04,523 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12165 synced till here 12154
2014-07-22 08:44:04,888 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043842591 with entries=116, filesize=89.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043844464
2014-07-22 08:44:04,888 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043751966
2014-07-22 08:44:04,888 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043764879
2014-07-22 08:44:04,888 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043766861
2014-07-22 08:44:04,888 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043768419
2014-07-22 08:44:04,888 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043770245
2014-07-22 08:44:04,888 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043771828
2014-07-22 08:44:04,888 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043773632
2014-07-22 08:44:04,888 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043775246
2014-07-22 08:44:04,888 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043776718
2014-07-22 08:44:04,888 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043778123
2014-07-22 08:44:04,888 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043779112
2014-07-22 08:44:04,888 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043780340
2014-07-22 08:44:05,127 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:44:05,410 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11011,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043834398,"queuetimems":0,"class":"HRegionServer","responsesize":18531,"method":"Multi"}
2014-07-22 08:44:05,410 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11088,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043834321,"queuetimems":0,"class":"HRegionServer","responsesize":18641,"method":"Multi"}
2014-07-22 08:44:05,519 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:05,548 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12241 synced till here 12237
2014-07-22 08:44:05,583 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043844464 with entries=76, filesize=67.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043845520
2014-07-22 08:44:06,851 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:09,693 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12483 synced till here 12479
2014-07-22 08:44:09,741 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043845520 with entries=242, filesize=164.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043846852
2014-07-22 08:44:10,715 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:10,730 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12573 synced till here 12570
2014-07-22 08:44:10,771 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043846852 with entries=90, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043850716
2014-07-22 08:44:11,967 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:11,986 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12662 synced till here 12660
2014-07-22 08:44:12,029 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043850716 with entries=89, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043851968
2014-07-22 08:44:13,600 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:13,625 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12741 synced till here 12739
2014-07-22 08:44:13,659 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043851968 with entries=79, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043853601
2014-07-22 08:44:15,422 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:15,439 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12817 synced till here 12812
2014-07-22 08:44:15,495 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043853601 with entries=76, filesize=65.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043855422
2014-07-22 08:44:16,519 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:16,546 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12897 synced till here 12896
2014-07-22 08:44:16,563 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043855422 with entries=80, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043856520
2014-07-22 08:44:17,189 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2277, memsize=231.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/4e68e6c4dc9f4594b963a57d55ba84fe
2014-07-22 08:44:17,215 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/4e68e6c4dc9f4594b963a57d55ba84fe as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/4e68e6c4dc9f4594b963a57d55ba84fe
2014-07-22 08:44:17,231 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/4e68e6c4dc9f4594b963a57d55ba84fe, entries=841280, sequenceid=2277, filesize=60.0m
2014-07-22 08:44:17,231 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~815.2m/854746960, currentsize=317.6m/332986320 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 15948ms, sequenceid=2277, compaction requested=true
2014-07-22 08:44:17,232 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:44:17,232 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 2000 blocking
2014-07-22 08:44:17,232 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 616.1m
2014-07-22 08:44:17,232 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-22 08:44:17,232 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:44:17,232 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:44:17,233 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 08:44:17,248 DEBUG [RpcServer.handler=41,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:44:17,758 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:17,765 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:44:17,781 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12983 synced till here 12981
2014-07-22 08:44:18,215 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043856520 with entries=86, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043857758
2014-07-22 08:44:18,215 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043782129
2014-07-22 08:44:18,215 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043788081
2014-07-22 08:44:18,216 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043790368
2014-07-22 08:44:19,359 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:19,483 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043857758 with entries=81, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043859359
2014-07-22 08:44:19,525 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2286, memsize=239.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/de58c3bfa6b7448a9010ed527719ecfd
2014-07-22 08:44:19,545 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/de58c3bfa6b7448a9010ed527719ecfd as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/de58c3bfa6b7448a9010ed527719ecfd
2014-07-22 08:44:19,562 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/de58c3bfa6b7448a9010ed527719ecfd, entries=873550, sequenceid=2286, filesize=62.3m
2014-07-22 08:44:19,562 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~741.9m/777975200, currentsize=337.8m/354170240 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 16326ms, sequenceid=2286, compaction requested=true
2014-07-22 08:44:19,563 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:44:19,563 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 2000 blocking
2014-07-22 08:44:19,563 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 628.6m
2014-07-22 08:44:19,563 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-22 08:44:19,563 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:44:19,564 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:44:19,564 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 08:44:19,629 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:44:19,978 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:44:21,105 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:22,242 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13212 synced till here 13210
2014-07-22 08:44:22,274 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043859359 with entries=148, filesize=116.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043861105
2014-07-22 08:44:22,274 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043792068
2014-07-22 08:44:22,274 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043793832
2014-07-22 08:44:22,274 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043796062
2014-07-22 08:44:22,274 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043797474
2014-07-22 08:44:22,274 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043799040
2014-07-22 08:44:22,274 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043800349
2014-07-22 08:44:22,274 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043801602
2014-07-22 08:44:23,650 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:23,667 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13289 synced till here 13288
2014-07-22 08:44:23,683 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043861105 with entries=77, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043863651
2014-07-22 08:44:24,427 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:24,453 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13370 synced till here 13369
2014-07-22 08:44:25,066 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043863651 with entries=81, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043864428
2014-07-22 08:44:25,948 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:26,123 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13465 synced till here 13462
2014-07-22 08:44:26,157 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043864428 with entries=95, filesize=71.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043865949
2014-07-22 08:44:28,018 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:29,159 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13608 synced till here 13607
2014-07-22 08:44:29,353 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043865949 with entries=143, filesize=119.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043868019
2014-07-22 08:44:31,684 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:31,706 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13697 synced till here 13695
2014-07-22 08:44:31,739 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043868019 with entries=89, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043871685
2014-07-22 08:44:31,949 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2461, memsize=280.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/2d05266119f74815a9a03920b88c10c5
2014-07-22 08:44:31,962 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/2d05266119f74815a9a03920b88c10c5 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/2d05266119f74815a9a03920b88c10c5
2014-07-22 08:44:31,974 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/2d05266119f74815a9a03920b88c10c5, entries=1022740, sequenceid=2461, filesize=72.8m
2014-07-22 08:44:31,974 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~616.1m/646061120, currentsize=252.5m/264741360 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 14742ms, sequenceid=2461, compaction requested=true
2014-07-22 08:44:31,975 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:44:31,975 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 2000 blocking
2014-07-22 08:44:31,975 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-22 08:44:31,975 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:44:31,975 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 573.8m
2014-07-22 08:44:31,975 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:44:31,975 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 08:44:31,996 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:44:32,645 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:44:32,837 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:32,854 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13776 synced till here 13774
2014-07-22 08:44:32,873 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043871685 with entries=79, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043872838
2014-07-22 08:44:34,061 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2484, memsize=321.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/ad57bf5164c346be90f219288a709625
2014-07-22 08:44:34,092 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/ad57bf5164c346be90f219288a709625 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/ad57bf5164c346be90f219288a709625
2014-07-22 08:44:34,116 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/ad57bf5164c346be90f219288a709625, entries=1171210, sequenceid=2484, filesize=83.4m
2014-07-22 08:44:34,116 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~628.6m/659104720, currentsize=244.0m/255871920 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 14553ms, sequenceid=2484, compaction requested=true
2014-07-22 08:44:34,117 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:44:34,117 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 2000 blocking
2014-07-22 08:44:34,117 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-22 08:44:34,117 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:44:34,117 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:44:34,117 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 585.0m
2014-07-22 08:44:34,117 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 08:44:34,492 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:44:35,865 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:44:36,027 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:36,055 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13863 synced till here 13862
2014-07-22 08:44:36,075 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043872838 with entries=87, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043876027
2014-07-22 08:44:38,150 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:38,174 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13940 synced till here 13937
2014-07-22 08:44:38,211 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043876027 with entries=77, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043878151
2014-07-22 08:44:38,212 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:44:39,969 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:40,845 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14116 synced till here 14112
2014-07-22 08:44:40,909 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043878151 with entries=176, filesize=131.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043879969
2014-07-22 08:44:40,917 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:44:41,998 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:42,020 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14204 synced till here 14198
2014-07-22 08:44:42,065 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043879969 with entries=88, filesize=66.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043881998
2014-07-22 08:44:42,065 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:44:44,004 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:44,026 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14280 synced till here 14279
2014-07-22 08:44:44,049 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043881998 with entries=76, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043884005
2014-07-22 08:44:44,050 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:44:45,756 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:45,778 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14377 synced till here 14369
2014-07-22 08:44:45,832 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043884005 with entries=97, filesize=68.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043885756
2014-07-22 08:44:45,832 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:44:47,071 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b.
2014-07-22 08:44:47,281 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:47,296 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14489 synced till here 14480
2014-07-22 08:44:47,403 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043885756 with entries=112, filesize=77.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043887281
2014-07-22 08:44:47,404 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:44:48,974 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1092ms
GC pool 'ParNew' had collection(s): count=1 time=1155ms
2014-07-22 08:44:49,667 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:49,711 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14593 synced till here 14578
2014-07-22 08:44:49,899 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043887281 with entries=104, filesize=82.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043889668
2014-07-22 08:44:49,900 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:44:52,299 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:54,496 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1746ms
GC pool 'ParNew' had collection(s): count=1 time=2061ms
2014-07-22 08:44:54,556 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14718 synced till here 14714
2014-07-22 08:44:54,610 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043889668 with entries=125, filesize=99.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043892299
2014-07-22 08:44:54,611 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:44:55,211 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:55,269 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14802 synced till here 14789
2014-07-22 08:44:56,414 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043892299 with entries=84, filesize=74.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043895212
2014-07-22 08:44:56,415 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:44:57,205 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:44:57,268 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14890 synced till here 14876
2014-07-22 08:44:57,420 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043895212 with entries=88, filesize=74.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043897205
2014-07-22 08:44:57,421 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:44:59,079 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2615, memsize=472.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/4b0c05f351d342ff8746a7429fae86b5
2014-07-22 08:44:59,094 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/4b0c05f351d342ff8746a7429fae86b5 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/4b0c05f351d342ff8746a7429fae86b5
2014-07-22 08:44:59,108 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/4b0c05f351d342ff8746a7429fae86b5, entries=1718580, sequenceid=2615, filesize=122.4m
2014-07-22 08:44:59,109 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~575.6m/603528880, currentsize=423.5m/444093200 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 27134ms, sequenceid=2615, compaction requested=true
2014-07-22 08:44:59,110 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 2000 blocking
2014-07-22 08:44:59,110 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-22 08:44:59,110 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:44:59,110 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:44:59,110 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 08:44:59,110 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:44:59,111 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 663.2m
2014-07-22 08:44:59,138 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:45:00,492 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1039ms
GC pool 'ParNew' had collection(s): count=1 time=1137ms
2014-07-22 08:45:00,493 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:45:00,526 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15004 synced till here 14978
2014-07-22 08:45:00,707 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043897205 with entries=114, filesize=92.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043900494
2014-07-22 08:45:00,721 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:45:01,129 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:45:01,513 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:45:02,554 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15101 synced till here 15078
2014-07-22 08:45:02,921 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043900494 with entries=97, filesize=81.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043901514
2014-07-22 08:45:02,921 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=44, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:45:04,691 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:45:05,004 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15252 synced till here 15212
2014-07-22 08:45:05,230 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043901514 with entries=151, filesize=118.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043904692
2014-07-22 08:45:05,231 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=45, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:45:05,432 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2627, memsize=496.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/133779af9b0d413e9919438b493d9c7b
2014-07-22 08:45:05,467 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/133779af9b0d413e9919438b493d9c7b as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/133779af9b0d413e9919438b493d9c7b
2014-07-22 08:45:05,479 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/133779af9b0d413e9919438b493d9c7b, entries=1808480, sequenceid=2627, filesize=128.8m
2014-07-22 08:45:05,479 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~585.0m/613442640, currentsize=460.6m/482988080 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 31362ms, sequenceid=2627, compaction requested=true
2014-07-22 08:45:05,480 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:45:05,480 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 2000 blocking
2014-07-22 08:45:05,480 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-22 08:45:05,480 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 752.2m
2014-07-22 08:45:05,480 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:45:05,480 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:45:05,480 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 08:45:05,579 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:45:06,660 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:45:06,697 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15336 synced till here 15325
2014-07-22 08:45:06,815 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043904692 with entries=84, filesize=74.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043906660
2014-07-22 08:45:06,815 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=46, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:45:07,230 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:45:08,187 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:45:08,209 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15411 synced till here 15408
2014-07-22 08:45:08,257 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043906660 with entries=75, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043908187
2014-07-22 08:45:08,258 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=47, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:45:08,982 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:45:09,011 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15492 synced till here 15486
2014-07-22 08:45:09,711 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043908187 with entries=81, filesize=66.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043908982
2014-07-22 08:45:09,712 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=48, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:45:10,542 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:45:10,575 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15590 synced till here 15587
2014-07-22 08:45:10,632 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043908982 with entries=98, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043910543
2014-07-22 08:45:10,633 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=49, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:45:12,391 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:45:12,709 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15700 synced till here 15696
2014-07-22 08:45:13,299 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043910543 with entries=110, filesize=86.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043912392
2014-07-22 08:45:13,300 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=50, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:45:13,990 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:45:14,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15789 synced till here 15786
2014-07-22 08:45:14,046 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043912392 with entries=89, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043913990
2014-07-22 08:45:14,047 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=51, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:45:15,623 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:45:15,655 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15890 synced till here 15874
2014-07-22 08:45:15,724 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043913990 with entries=101, filesize=67.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043915623
2014-07-22 08:45:15,726 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=52, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:45:17,830 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1579ms
GC pool 'ParNew' had collection(s): count=1 time=1673ms
2014-07-22 08:45:18,426 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:45:18,467 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15979 synced till here 15969
2014-07-22 08:45:18,561 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043915623 with entries=89, filesize=71.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043918426
2014-07-22 08:45:18,562 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=53, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:45:19,907 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1075ms
GC pool 'ParNew' had collection(s): count=1 time=1122ms
2014-07-22 08:45:20,501 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:45:20,515 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16073 synced till here 16061
2014-07-22 08:45:20,613 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043918426 with entries=94, filesize=73.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043920502
2014-07-22 08:45:20,613 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=54, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:45:22,064 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,069 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,089 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,089 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,090 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,111 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,121 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:45:22,126 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,126 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,129 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,140 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16152 synced till here 16149
2014-07-22 08:45:22,155 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,159 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,160 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,161 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,173 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,177 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043920502 with entries=79, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043922121
2014-07-22 08:45:22,184 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=55, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:45:22,207 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,222 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,222 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,223 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,227 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,251 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,288 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,344 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,375 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,413 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,447 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,481 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,483 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,483 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,483 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,483 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,483 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,484 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,484 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,484 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,485 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,485 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,485 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,486 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,486 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,489 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,527 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,576 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,577 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,619 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,621 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,655 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,657 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,691 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:22,692 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:23,457 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:25,889 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1174ms
GC pool 'ParNew' had collection(s): count=1 time=1233ms
2014-07-22 08:45:27,065 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:45:27,069 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:45:27,089 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:45:27,090 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:45:27,090 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:45:27,112 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:45:27,126 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:45:27,127 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:45:27,130 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:45:27,156 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:45:27,160 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:45:27,160 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:45:27,161 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:45:27,174 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:45:27,207 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:45:27,222 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:45:27,222 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:45:27,224 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:45:27,228 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:45:27,251 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:45:27,288 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:45:27,345 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:45:27,375 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:45:27,414 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:45:27,447 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:45:27,482 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:45:27,483 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:45:27,484 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:45:27,484 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:45:27,484 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:45:27,484 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:45:27,485 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 08:45:27,486 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 08:45:27,486 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:45:27,486 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:45:27,487 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5003ms
2014-07-22 08:45:27,487 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5003ms
2014-07-22 08:45:27,488 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 08:45:27,488 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 08:45:27,489 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:45:27,527 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:45:27,577 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:45:27,577 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:45:27,620 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:45:27,622 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:45:27,655 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:45:27,659 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 08:45:27,691 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:45:27,693 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:45:28,458 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:45:29,486 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2854, memsize=440.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/529cd0bb9539426d955f47805fe6b986
2014-07-22 08:45:29,575 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/529cd0bb9539426d955f47805fe6b986 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/529cd0bb9539426d955f47805fe6b986
2014-07-22 08:45:29,587 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/529cd0bb9539426d955f47805fe6b986, entries=1602600, sequenceid=2854, filesize=114.1m
2014-07-22 08:45:29,587 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~673.7m/706462960, currentsize=404.3m/423940480 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 30476ms, sequenceid=2854, compaction requested=true
2014-07-22 08:45:29,590 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:45:29,590 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 2000 blocking
2014-07-22 08:45:29,590 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6133ms
2014-07-22 08:45:29,590 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-22 08:45:29,591 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,591 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:45:29,591 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6899ms
2014-07-22 08:45:29,591 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:45:29,591 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,591 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6900ms
2014-07-22 08:45:29,591 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,591 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6934ms
2014-07-22 08:45:29,592 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,592 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6937ms
2014-07-22 08:45:29,592 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,592 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6971ms
2014-07-22 08:45:29,592 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,592 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6973ms
2014-07-22 08:45:29,592 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,592 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7015ms
2014-07-22 08:45:29,592 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,593 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7016ms
2014-07-22 08:45:29,593 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,591 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b., current region memstore size 342.6m
2014-07-22 08:45:29,591 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 08:45:29,594 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7067ms
2014-07-22 08:45:29,594 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,594 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7106ms
2014-07-22 08:45:29,594 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,594 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7108ms
2014-07-22 08:45:29,595 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,595 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7109ms
2014-07-22 08:45:29,595 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,595 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7111ms
2014-07-22 08:45:29,595 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,595 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7111ms
2014-07-22 08:45:29,595 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,596 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7111ms
2014-07-22 08:45:29,596 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,596 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7111ms
2014-07-22 08:45:29,596 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,596 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7112ms
2014-07-22 08:45:29,596 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,597 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7113ms
2014-07-22 08:45:29,597 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,597 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7114ms
2014-07-22 08:45:29,597 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,597 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7114ms
2014-07-22 08:45:29,597 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,598 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7115ms
2014-07-22 08:45:29,598 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,599 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7115ms
2014-07-22 08:45:29,602 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,602 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7119ms
2014-07-22 08:45:29,603 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,603 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7122ms
2014-07-22 08:45:29,603 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,603 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7156ms
2014-07-22 08:45:29,603 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,603 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7190ms
2014-07-22 08:45:29,603 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,604 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7229ms
2014-07-22 08:45:29,604 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,605 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7261ms
2014-07-22 08:45:29,605 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,605 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7318ms
2014-07-22 08:45:29,605 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,606 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7355ms
2014-07-22 08:45:29,606 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,609 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7382ms
2014-07-22 08:45:29,609 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,609 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7386ms
2014-07-22 08:45:29,609 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,610 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7388ms
2014-07-22 08:45:29,610 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,613 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7392ms
2014-07-22 08:45:29,613 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,613 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7406ms
2014-07-22 08:45:29,613 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,614 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7441ms
2014-07-22 08:45:29,614 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,614 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7454ms
2014-07-22 08:45:29,614 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,616 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7457ms
2014-07-22 08:45:29,616 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,617 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7458ms
2014-07-22 08:45:29,618 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,618 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7463ms
2014-07-22 08:45:29,618 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,618 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7489ms
2014-07-22 08:45:29,618 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,618 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7493ms
2014-07-22 08:45:29,619 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,619 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7493ms
2014-07-22 08:45:29,619 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,619 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7508ms
2014-07-22 08:45:29,619 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,619 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7530ms
2014-07-22 08:45:29,619 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,622 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7533ms
2014-07-22 08:45:29,622 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,623 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7533ms
2014-07-22 08:45:29,623 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,623 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7554ms
2014-07-22 08:45:29,623 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:29,625 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7562ms
2014-07-22 08:45:29,625 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:30,001 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:45:31,826 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1429ms
GC pool 'ParNew' had collection(s): count=1 time=1535ms
2014-07-22 08:45:32,069 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:45:32,194 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11364,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043920830,"queuetimems":0,"class":"HRegionServer","responsesize":18509,"method":"Multi"}
2014-07-22 08:45:32,194 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11289,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043920905,"queuetimems":0,"class":"HRegionServer","responsesize":18921,"method":"Multi"}
2014-07-22 08:45:32,194 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10381,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043921813,"queuetimems":0,"class":"HRegionServer","responsesize":15605,"method":"Multi"}
2014-07-22 08:45:32,194 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11250,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043920944,"queuetimems":1,"class":"HRegionServer","responsesize":18567,"method":"Multi"}
2014-07-22 08:45:32,200 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10456,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043921743,"queuetimems":0,"class":"HRegionServer","responsesize":18519,"method":"Multi"}
2014-07-22 08:45:32,398 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:45:32,405 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10682,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043921722,"queuetimems":0,"class":"HRegionServer","responsesize":18490,"method":"Multi"}
2014-07-22 08:45:32,414 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10706,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043921708,"queuetimems":0,"class":"HRegionServer","responsesize":15605,"method":"Multi"}
2014-07-22 08:45:32,477 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16261 synced till here 16250
2014-07-22 08:45:32,562 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10782,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043921780,"queuetimems":1,"class":"HRegionServer","responsesize":18490,"method":"Multi"}
2014-07-22 08:45:32,566 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11569,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043920996,"queuetimems":0,"class":"HRegionServer","responsesize":18602,"method":"Multi"}
2014-07-22 08:45:32,585 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043922121 with entries=109, filesize=74.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043932398
2014-07-22 08:45:32,860 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10939,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043921920,"queuetimems":1,"class":"HRegionServer","responsesize":18567,"method":"Multi"}
2014-07-22 08:45:32,860 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10876,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043921984,"queuetimems":0,"class":"HRegionServer","responsesize":18737,"method":"Multi"}
2014-07-22 08:45:33,104 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11083,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043922020,"queuetimems":0,"class":"HRegionServer","responsesize":18680,"method":"Multi"}
2014-07-22 08:45:34,386 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1058ms
GC pool 'ParNew' had collection(s): count=1 time=1128ms
2014-07-22 08:45:34,472 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12598,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043921874,"queuetimems":0,"class":"HRegionServer","responsesize":18602,"method":"Multi"}
2014-07-22 08:45:34,477 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11786,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043922690,"queuetimems":0,"class":"HRegionServer","responsesize":18920,"method":"Multi"}
2014-07-22 08:45:34,477 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11822,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043922655,"queuetimems":1,"class":"HRegionServer","responsesize":18519,"method":"Multi"}
2014-07-22 08:45:34,642 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12116,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043922525,"queuetimems":1,"class":"HRegionServer","responsesize":18665,"method":"Multi"}
2014-07-22 08:45:34,642 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12023,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043922619,"queuetimems":1,"class":"HRegionServer","responsesize":18490,"method":"Multi"}
2014-07-22 08:45:34,863 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:45:35,006 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16395 synced till here 16358
2014-07-22 08:45:35,138 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12981,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043922157,"queuetimems":0,"class":"HRegionServer","responsesize":18496,"method":"Multi"}
2014-07-22 08:45:35,445 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12871,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043922574,"queuetimems":0,"class":"HRegionServer","responsesize":18629,"method":"Multi"}
2014-07-22 08:45:35,463 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12010,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043923452,"queuetimems":1,"class":"HRegionServer","responsesize":18657,"method":"Multi"}
2014-07-22 08:45:35,550 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043932398 with entries=134, filesize=107.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043934864
2014-07-22 08:45:35,738 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13532,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043922205,"queuetimems":0,"class":"HRegionServer","responsesize":16803,"method":"Multi"}
2014-07-22 08:45:36,905 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14532,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043922372,"queuetimems":1,"class":"HRegionServer","responsesize":15709,"method":"Multi"}
2014-07-22 08:45:36,934 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14489,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043922445,"queuetimems":1,"class":"HRegionServer","responsesize":15978,"method":"Multi"}
2014-07-22 08:45:36,951 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14886,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043922065,"queuetimems":0,"class":"HRegionServer","responsesize":18833,"method":"Multi"}
2014-07-22 08:45:36,954 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14704,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043922249,"queuetimems":1,"class":"HRegionServer","responsesize":19136,"method":"Multi"}
2014-07-22 08:45:36,955 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14613,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043922342,"queuetimems":1,"class":"HRegionServer","responsesize":18282,"method":"Multi"}
2014-07-22 08:45:36,999 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2928, memsize=406.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/0f3bea008d744b1c87e98bc5d1168d7b
2014-07-22 08:45:37,031 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/0f3bea008d744b1c87e98bc5d1168d7b as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/0f3bea008d744b1c87e98bc5d1168d7b
2014-07-22 08:45:37,057 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/0f3bea008d744b1c87e98bc5d1168d7b, entries=1478980, sequenceid=2928, filesize=105.4m
2014-07-22 08:45:37,057 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~768.6m/805893440, currentsize=375.1m/393368240 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 31577ms, sequenceid=2928, compaction requested=true
2014-07-22 08:45:37,058 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:45:37,058 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 2000 blocking
2014-07-22 08:45:37,058 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-22 08:45:37,058 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 933.2m
2014-07-22 08:45:37,058 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:45:37,058 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:45:37,058 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 08:45:37,168 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15059,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043922108,"queuetimems":0,"class":"HRegionServer","responsesize":18493,"method":"Multi"}
2014-07-22 08:45:37,172 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:45:37,176 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14876,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043922285,"queuetimems":0,"class":"HRegionServer","responsesize":18420,"method":"Multi"}
2014-07-22 08:45:37,374 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:45:37,434 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16515 synced till here 16486
2014-07-22 08:45:37,659 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15178,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043922481,"queuetimems":1,"class":"HRegionServer","responsesize":18351,"method":"Multi"}
2014-07-22 08:45:37,661 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15250,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043922410,"queuetimems":0,"class":"HRegionServer","responsesize":18413,"method":"Multi"}
2014-07-22 08:45:37,729 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043934864 with entries=120, filesize=84.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043937375
2014-07-22 08:45:39,732 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:45:39,764 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16642 synced till here 16610
2014-07-22 08:45:39,929 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:45:40,200 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043937375 with entries=127, filesize=97.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043939732
2014-07-22 08:45:41,539 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:45:41,662 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16739 synced till here 16720
2014-07-22 08:45:41,965 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043939732 with entries=97, filesize=84.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043941539
2014-07-22 08:45:43,652 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:45:43,675 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16819 synced till here 16817
2014-07-22 08:45:43,720 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043941539 with entries=80, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043943653
2014-07-22 08:45:44,761 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:45:44,790 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16909 synced till here 16907
2014-07-22 08:45:44,810 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043943653 with entries=90, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043944762
2014-07-22 08:45:46,327 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:45:46,362 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17002 synced till here 17001
2014-07-22 08:45:46,411 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043944762 with entries=93, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043946328
2014-07-22 08:45:48,217 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:45:48,258 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17118 synced till here 17111
2014-07-22 08:45:48,343 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043946328 with entries=116, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043948218
2014-07-22 08:45:49,677 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:45:49,952 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:49,953 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:49,953 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:49,976 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:49,997 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17231 synced till here 17230
2014-07-22 08:45:50,013 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:50,023 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043948218 with entries=113, filesize=81.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043949678
2014-07-22 08:45:50,045 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:50,055 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:50,078 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:50,079 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:50,093 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:50,094 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:50,095 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:50,112 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:50,112 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:50,139 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:50,211 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:50,212 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:50,256 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:50,257 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:50,257 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:50,258 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:50,258 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:50,261 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:50,308 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:50,310 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:50,386 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:50,427 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:50,475 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:50,550 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:50,615 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:50,616 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:50,623 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:50,624 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:52,087 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:52,088 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:52,088 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:52,088 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:52,089 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:52,089 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:52,091 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:52,105 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:52,123 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:52,129 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:52,226 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:52,237 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:52,239 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:52,242 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:52,245 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:52,246 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:52,254 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:53,131 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3860, memsize=259.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/ebcc84d6917047958bc494be7f42284f
2014-07-22 08:45:53,196 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/ebcc84d6917047958bc494be7f42284f as hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/ebcc84d6917047958bc494be7f42284f
2014-07-22 08:45:53,259 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/ebcc84d6917047958bc494be7f42284f, entries=944310, sequenceid=3860, filesize=67.3m
2014-07-22 08:45:53,259 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~342.6m/359238480, currentsize=53.5m/56076320 for region usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. in 23668ms, sequenceid=3860, compaction requested=true
2014-07-22 08:45:53,260 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:45:53,260 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 2000 blocking
2014-07-22 08:45:53,260 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-22 08:45:53,260 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1006ms
2014-07-22 08:45:53,260 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:45:53,260 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,260 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 1.1g
2014-07-22 08:45:53,260 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:45:53,260 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1014ms
2014-07-22 08:45:53,260 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,260 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. because compaction request was cancelled
2014-07-22 08:45:53,262 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1017ms
2014-07-22 08:45:53,262 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,262 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1020ms
2014-07-22 08:45:53,262 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,263 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1024ms
2014-07-22 08:45:53,263 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,264 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1027ms
2014-07-22 08:45:53,264 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,264 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1039ms
2014-07-22 08:45:53,264 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,264 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1135ms
2014-07-22 08:45:53,264 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,264 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1141ms
2014-07-22 08:45:53,264 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,265 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1161ms
2014-07-22 08:45:53,265 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,266 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1176ms
2014-07-22 08:45:53,266 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,267 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1178ms
2014-07-22 08:45:53,267 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,267 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1179ms
2014-07-22 08:45:53,267 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,275 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1187ms
2014-07-22 08:45:53,275 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,275 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1188ms
2014-07-22 08:45:53,275 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,275 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1188ms
2014-07-22 08:45:53,276 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,276 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1189ms
2014-07-22 08:45:53,276 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,276 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2652ms
2014-07-22 08:45:53,276 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,276 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2653ms
2014-07-22 08:45:53,276 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,276 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2660ms
2014-07-22 08:45:53,277 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,281 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2666ms
2014-07-22 08:45:53,281 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,291 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2742ms
2014-07-22 08:45:53,291 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,291 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2816ms
2014-07-22 08:45:53,291 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,293 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2866ms
2014-07-22 08:45:53,293 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,297 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2911ms
2014-07-22 08:45:53,297 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,297 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2987ms
2014-07-22 08:45:53,298 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,298 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2990ms
2014-07-22 08:45:53,298 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,298 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3037ms
2014-07-22 08:45:53,298 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,298 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3041ms
2014-07-22 08:45:53,298 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,305 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3048ms
2014-07-22 08:45:53,305 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,306 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3049ms
2014-07-22 08:45:53,306 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,306 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3049ms
2014-07-22 08:45:53,306 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,308 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3053ms
2014-07-22 08:45:53,308 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,308 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3096ms
2014-07-22 08:45:53,308 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,311 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3101ms
2014-07-22 08:45:53,311 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,311 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3172ms
2014-07-22 08:45:53,311 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,312 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3199ms
2014-07-22 08:45:53,312 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,312 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3200ms
2014-07-22 08:45:53,312 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,312 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3217ms
2014-07-22 08:45:53,312 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,312 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3218ms
2014-07-22 08:45:53,312 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,313 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3219ms
2014-07-22 08:45:53,313 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,313 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3235ms
2014-07-22 08:45:53,313 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,319 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3241ms
2014-07-22 08:45:53,319 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,329 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3274ms
2014-07-22 08:45:53,329 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,329 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3284ms
2014-07-22 08:45:53,330 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,337 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3324ms
2014-07-22 08:45:53,337 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,337 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3361ms
2014-07-22 08:45:53,337 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,337 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3385ms
2014-07-22 08:45:53,338 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,338 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3386ms
2014-07-22 08:45:53,338 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:53,395 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3443ms
2014-07-22 08:45:53,395 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:45:54,440 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:45:55,607 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1016ms
GC pool 'ParNew' had collection(s): count=1 time=1158ms
2014-07-22 08:45:55,675 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17366 synced till here 17343
2014-07-22 08:45:55,751 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:45:55,956 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043949678 with entries=135, filesize=87.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043954440
2014-07-22 08:45:55,957 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043803005
2014-07-22 08:45:55,957 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043805029
2014-07-22 08:45:55,957 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043805859
2014-07-22 08:45:55,957 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043807562
2014-07-22 08:45:55,957 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043810667
2014-07-22 08:45:55,957 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043813320
2014-07-22 08:45:55,958 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043815471
2014-07-22 08:45:55,958 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043822115
2014-07-22 08:45:55,958 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043823261
2014-07-22 08:45:55,958 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043825065
2014-07-22 08:45:55,958 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043827073
2014-07-22 08:45:55,958 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043829090
2014-07-22 08:45:55,958 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043832495
2014-07-22 08:45:55,958 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043833784
2014-07-22 08:45:55,958 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043842591
2014-07-22 08:45:55,958 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043844464
2014-07-22 08:45:55,958 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043845520
2014-07-22 08:45:55,959 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043846852
2014-07-22 08:45:55,959 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043850716
2014-07-22 08:45:55,959 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043851968
2014-07-22 08:45:55,959 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043853601
2014-07-22 08:45:55,959 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043855422
2014-07-22 08:45:55,959 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043856520
2014-07-22 08:45:55,959 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043857758
2014-07-22 08:45:55,959 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043859359
2014-07-22 08:45:55,959 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043861105
2014-07-22 08:45:55,960 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043863651
2014-07-22 08:45:55,960 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043864428
2014-07-22 08:45:55,960 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043865949
2014-07-22 08:45:55,960 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043868019
2014-07-22 08:45:58,175 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1066ms
GC pool 'ParNew' had collection(s): count=1 time=1347ms
2014-07-22 08:45:58,245 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:45:58,342 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17469 synced till here 17442
2014-07-22 08:45:58,528 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043954440 with entries=103, filesize=94.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043958246
2014-07-22 08:45:58,629 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,632 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,633 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,633 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,633 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,633 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,633 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,646 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,684 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,720 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,722 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,762 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,765 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,766 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,767 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,768 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,768 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,769 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,769 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,772 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,772 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,773 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,773 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,773 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,773 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,778 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,791 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,791 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,794 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,832 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,868 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,868 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:45:58,870 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:00,886 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:00,887 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:01,360 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:01,399 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:01,448 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:01,515 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:01,565 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:01,623 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:01,677 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:01,730 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:01,787 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:02,766 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:02,776 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:02,812 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:02,846 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:02,848 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:02,883 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:03,630 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:46:03,633 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:46:03,633 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:46:03,633 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:46:03,634 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:46:03,634 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 08:46:03,634 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 08:46:03,647 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:46:03,684 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:46:03,720 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:46:03,722 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:46:03,762 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:46:03,765 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:46:03,766 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:46:03,768 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:46:03,768 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:46:03,769 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:46:03,769 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:46:03,769 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:46:03,772 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:46:03,772 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:46:03,773 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:46:03,773 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:46:03,773 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:46:03,774 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:46:03,778 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:46:03,791 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:46:03,791 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:46:03,795 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:46:03,832 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:46:03,868 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:46:03,868 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:46:03,870 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:46:03,899 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3162, memsize=329.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/c54a34edc4d5432590422fe6c7d471fe
2014-07-22 08:46:03,935 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/c54a34edc4d5432590422fe6c7d471fe as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/c54a34edc4d5432590422fe6c7d471fe
2014-07-22 08:46:03,950 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/c54a34edc4d5432590422fe6c7d471fe, entries=1199460, sequenceid=3162, filesize=85.5m
2014-07-22 08:46:03,951 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~949.8m/995936000, currentsize=321.2m/336789280 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 26893ms, sequenceid=3162, compaction requested=true
2014-07-22 08:46:03,951 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:46:03,951 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 2000 blocking
2014-07-22 08:46:03,952 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-22 08:46:03,952 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5082ms
2014-07-22 08:46:03,952 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:46:03,952 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:03,952 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 836.8m
2014-07-22 08:46:03,952 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:46:03,952 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5084ms
2014-07-22 08:46:03,952 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 08:46:03,952 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:03,952 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5084ms
2014-07-22 08:46:03,952 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:03,953 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5121ms
2014-07-22 08:46:03,953 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:03,953 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5159ms
2014-07-22 08:46:03,953 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:03,953 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5162ms
2014-07-22 08:46:03,954 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:03,954 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5163ms
2014-07-22 08:46:03,954 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:03,954 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5176ms
2014-07-22 08:46:03,954 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:03,954 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5181ms
2014-07-22 08:46:03,954 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:03,958 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5185ms
2014-07-22 08:46:03,958 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:03,960 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5187ms
2014-07-22 08:46:03,960 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:03,960 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5187ms
2014-07-22 08:46:03,960 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:03,961 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5189ms
2014-07-22 08:46:03,961 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:03,961 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5189ms
2014-07-22 08:46:03,962 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:03,962 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5193ms
2014-07-22 08:46:03,962 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:03,962 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5193ms
2014-07-22 08:46:03,962 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:03,965 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5197ms
2014-07-22 08:46:03,965 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:03,969 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5201ms
2014-07-22 08:46:03,969 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:03,969 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5202ms
2014-07-22 08:46:03,969 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:03,970 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5204ms
2014-07-22 08:46:03,970 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:03,970 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5205ms
2014-07-22 08:46:03,970 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:03,970 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5209ms
2014-07-22 08:46:03,971 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:03,975 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5253ms
2014-07-22 08:46:03,976 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:03,976 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5257ms
2014-07-22 08:46:03,976 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:03,976 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5292ms
2014-07-22 08:46:03,976 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:03,998 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5352ms
2014-07-22 08:46:03,998 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:03,998 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5366ms
2014-07-22 08:46:03,998 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:03,998 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5366ms
2014-07-22 08:46:03,998 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:03,998 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5365ms
2014-07-22 08:46:03,998 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:04,005 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5372ms
2014-07-22 08:46:04,005 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:04,005 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5373ms
2014-07-22 08:46:04,006 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:04,007 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5375ms
2014-07-22 08:46:04,007 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:04,007 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5378ms
2014-07-22 08:46:04,007 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:04,008 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1124ms
2014-07-22 08:46:04,008 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:04,009 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1161ms
2014-07-22 08:46:04,009 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:04,017 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1171ms
2014-07-22 08:46:04,017 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:04,018 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1206ms
2014-07-22 08:46:04,018 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:04,018 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1242ms
2014-07-22 08:46:04,018 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:04,019 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1252ms
2014-07-22 08:46:04,019 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:04,020 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2233ms
2014-07-22 08:46:04,020 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:04,025 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2295ms
2014-07-22 08:46:04,025 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:04,026 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2348ms
2014-07-22 08:46:04,026 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:04,027 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2403ms
2014-07-22 08:46:04,027 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:04,034 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2468ms
2014-07-22 08:46:04,034 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:04,034 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2519ms
2014-07-22 08:46:04,034 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:04,036 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2588ms
2014-07-22 08:46:04,036 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:04,037 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2638ms
2014-07-22 08:46:04,037 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:04,038 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2677ms
2014-07-22 08:46:04,038 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:04,041 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3154ms
2014-07-22 08:46:04,041 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:04,041 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3155ms
2014-07-22 08:46:04,041 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:04,042 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10625,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54549","starttimems":1406043953417,"queuetimems":845,"class":"HRegionServer","responsesize":18709,"method":"Multi"}
2014-07-22 08:46:04,483 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:46:04,844 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:46:04,860 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:46:04,959 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17587 synced till here 17571
2014-07-22 08:46:05,134 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043958246 with entries=118, filesize=97.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043964860
2014-07-22 08:46:05,134 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043871685
2014-07-22 08:46:06,953 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:46:07,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17684 synced till here 17663
2014-07-22 08:46:08,202 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043964860 with entries=97, filesize=92.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043966953
2014-07-22 08:46:09,158 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:46:09,189 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17754 synced till here 17752
2014-07-22 08:46:09,226 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043966953 with entries=70, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043969159
2014-07-22 08:46:11,781 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:46:11,831 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17832 synced till here 17830
2014-07-22 08:46:11,883 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043969159 with entries=78, filesize=65.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043971782
2014-07-22 08:46:13,390 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:46:13,406 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17936 synced till here 17933
2014-07-22 08:46:13,462 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043971782 with entries=104, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043973391
2014-07-22 08:46:15,983 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:46:17,082 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18097 synced till here 18095
2014-07-22 08:46:17,248 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043973391 with entries=161, filesize=123.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043975983
2014-07-22 08:46:18,875 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:18,930 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:18,931 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:18,932 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:18,967 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:18,968 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:18,970 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,003 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,050 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,116 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,119 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,191 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,194 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,245 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,290 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,293 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,355 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,359 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,416 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,467 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,469 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,469 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,470 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,522 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,565 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,569 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,613 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,614 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,615 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,662 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,663 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,663 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,715 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,716 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,721 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,728 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,728 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,728 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,744 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,799 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,802 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,861 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,953 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,995 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:19,996 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:20,067 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:20,362 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:20,402 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:20,614 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:20,703 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:46:20,929 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3272, memsize=396.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/2d91518f4e6f484791827284d0880b3d
2014-07-22 08:46:20,962 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/2d91518f4e6f484791827284d0880b3d as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/2d91518f4e6f484791827284d0880b3d
2014-07-22 08:46:20,979 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/2d91518f4e6f484791827284d0880b3d, entries=1443090, sequenceid=3272, filesize=102.8m
2014-07-22 08:46:20,979 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.1g/1184830960, currentsize=317.9m/333386160 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 27719ms, sequenceid=3272, compaction requested=true
2014-07-22 08:46:20,980 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:46:20,980 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 2000 blocking
2014-07-22 08:46:20,980 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-22 08:46:20,980 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 277ms
2014-07-22 08:46:20,980 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:20,980 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:46:20,980 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 366ms
2014-07-22 08:46:20,980 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:20,980 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 929.5m
2014-07-22 08:46:20,980 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:46:20,980 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 08:46:20,985 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 583ms
2014-07-22 08:46:20,985 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:20,985 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 623ms
2014-07-22 08:46:20,986 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:20,986 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 919ms
2014-07-22 08:46:20,986 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:20,986 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 990ms
2014-07-22 08:46:20,986 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:20,986 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 991ms
2014-07-22 08:46:20,986 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:20,989 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1036ms
2014-07-22 08:46:20,989 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:20,989 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1128ms
2014-07-22 08:46:20,989 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:20,996 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1194ms
2014-07-22 08:46:20,996 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:20,996 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1197ms
2014-07-22 08:46:20,996 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:20,996 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1252ms
2014-07-22 08:46:20,997 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:20,997 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1269ms
2014-07-22 08:46:20,997 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:20,997 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1269ms
2014-07-22 08:46:20,997 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:20,998 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1269ms
2014-07-22 08:46:20,998 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:20,998 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1277ms
2014-07-22 08:46:20,998 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:20,998 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1282ms
2014-07-22 08:46:20,998 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:20,999 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1284ms
2014-07-22 08:46:20,999 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:20,999 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1336ms
2014-07-22 08:46:20,999 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:20,999 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1337ms
2014-07-22 08:46:20,999 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:20,999 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1337ms
2014-07-22 08:46:21,000 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,000 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1385ms
2014-07-22 08:46:21,000 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,002 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1389ms
2014-07-22 08:46:21,002 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,002 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1389ms
2014-07-22 08:46:21,002 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,012 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1443ms
2014-07-22 08:46:21,012 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,012 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1447ms
2014-07-22 08:46:21,013 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,013 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1491ms
2014-07-22 08:46:21,013 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,016 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1546ms
2014-07-22 08:46:21,016 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,017 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1548ms
2014-07-22 08:46:21,017 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,017 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1548ms
2014-07-22 08:46:21,017 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,017 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1550ms
2014-07-22 08:46:21,017 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,017 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1601ms
2014-07-22 08:46:21,017 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,017 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1658ms
2014-07-22 08:46:21,017 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,023 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1668ms
2014-07-22 08:46:21,023 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,023 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1730ms
2014-07-22 08:46:21,023 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,027 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1737ms
2014-07-22 08:46:21,027 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,027 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1782ms
2014-07-22 08:46:21,027 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,027 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1833ms
2014-07-22 08:46:21,027 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,027 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1836ms
2014-07-22 08:46:21,027 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,027 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1908ms
2014-07-22 08:46:21,028 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,028 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1912ms
2014-07-22 08:46:21,028 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,028 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1978ms
2014-07-22 08:46:21,028 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,028 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2025ms
2014-07-22 08:46:21,028 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,029 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2059ms
2014-07-22 08:46:21,029 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,029 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2061ms
2014-07-22 08:46:21,029 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,037 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2070ms
2014-07-22 08:46:21,037 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,045 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2113ms
2014-07-22 08:46:21,045 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,045 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2115ms
2014-07-22 08:46:21,045 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,046 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2115ms
2014-07-22 08:46:21,046 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:21,046 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2171ms
2014-07-22 08:46:21,046 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:46:22,227 DEBUG [RpcServer.handler=19,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:46:22,392 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:46:22,457 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18211 synced till here 18191
2014-07-22 08:46:22,654 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043975983 with entries=114, filesize=75.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043982393
2014-07-22 08:46:22,654 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043872838
2014-07-22 08:46:22,654 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043876027
2014-07-22 08:46:22,654 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043878151
2014-07-22 08:46:22,654 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043879969
2014-07-22 08:46:22,654 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043881998
2014-07-22 08:46:22,654 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043884005
2014-07-22 08:46:22,654 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043885756
2014-07-22 08:46:22,654 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043887281
2014-07-22 08:46:22,655 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043889668
2014-07-22 08:46:22,655 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043892299
2014-07-22 08:46:22,655 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043895212
2014-07-22 08:46:22,862 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:46:24,707 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1098ms
GC pool 'ParNew' had collection(s): count=1 time=1324ms
2014-07-22 08:46:25,090 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:46:25,160 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18299 synced till here 18281
2014-07-22 08:46:25,402 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043982393 with entries=88, filesize=80.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043985091
2014-07-22 08:46:25,849 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3329, memsize=310.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/712e82b2ed554594ae488242daa0d62e
2014-07-22 08:46:25,862 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/712e82b2ed554594ae488242daa0d62e as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/712e82b2ed554594ae488242daa0d62e
2014-07-22 08:46:25,872 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/712e82b2ed554594ae488242daa0d62e, entries=1130930, sequenceid=3329, filesize=80.6m
2014-07-22 08:46:25,873 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~836.8m/877403200, currentsize=290.0m/304119680 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 21921ms, sequenceid=3329, compaction requested=true
2014-07-22 08:46:25,873 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:46:25,873 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 2000 blocking
2014-07-22 08:46:25,874 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-22 08:46:25,874 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 611.7m
2014-07-22 08:46:25,874 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:46:25,874 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:46:25,874 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 08:46:25,989 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:46:27,382 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:46:27,425 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18399 synced till here 18382
2014-07-22 08:46:27,589 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043985091 with entries=100, filesize=86.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043987382
2014-07-22 08:46:27,589 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043897205
2014-07-22 08:46:27,589 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043900494
2014-07-22 08:46:27,589 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043901514
2014-07-22 08:46:27,757 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:46:29,648 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:46:29,678 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18502 synced till here 18481
2014-07-22 08:46:29,871 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043987382 with entries=103, filesize=93.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043989648
2014-07-22 08:46:31,393 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:46:31,409 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18584 synced till here 18572
2014-07-22 08:46:31,542 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043989648 with entries=82, filesize=72.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043991393
2014-07-22 08:46:33,193 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:46:33,474 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18686 synced till here 18684
2014-07-22 08:46:33,535 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043991393 with entries=102, filesize=82.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043993193
2014-07-22 08:46:35,552 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:46:35,918 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18783 synced till here 18779
2014-07-22 08:46:35,955 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043993193 with entries=97, filesize=85.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043995553
2014-07-22 08:46:37,289 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:46:37,556 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18895 synced till here 18892
2014-07-22 08:46:37,593 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043995553 with entries=112, filesize=84.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043997290
2014-07-22 08:46:39,111 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:46:40,263 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043997290 with entries=110, filesize=82.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043999112
2014-07-22 08:46:41,885 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1265ms
GC pool 'ParNew' had collection(s): count=1 time=1267ms
2014-07-22 08:46:42,632 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:46:42,686 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19111 synced till here 19106
2014-07-22 08:46:42,792 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043999112 with entries=106, filesize=67.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044002633
2014-07-22 08:46:44,164 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:46:44,178 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19209 synced till here 19207
2014-07-22 08:46:44,224 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044002633 with entries=98, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044004165
2014-07-22 08:46:44,789 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3517, memsize=267.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/524da43326f94e9297bf7f2ee0681762
2014-07-22 08:46:44,802 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/524da43326f94e9297bf7f2ee0681762 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/524da43326f94e9297bf7f2ee0681762
2014-07-22 08:46:44,813 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/524da43326f94e9297bf7f2ee0681762, entries=974410, sequenceid=3517, filesize=69.4m
2014-07-22 08:46:44,814 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~627.8m/658269040, currentsize=291.9m/306123600 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 18940ms, sequenceid=3517, compaction requested=true
2014-07-22 08:46:44,814 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:46:44,814 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 2000 blocking
2014-07-22 08:46:44,815 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 699.5m
2014-07-22 08:46:44,815 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-22 08:46:44,815 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:46:44,815 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:46:44,815 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 08:46:44,895 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:46:45,767 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:46:45,805 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19307 synced till here 19305
2014-07-22 08:46:45,837 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044004165 with entries=98, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044005768
2014-07-22 08:46:46,127 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:46:47,399 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:46:47,460 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044005768 with entries=82, filesize=61.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044007399
2014-07-22 08:46:48,464 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:46:48,494 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19469 synced till here 19466
2014-07-22 08:46:48,551 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044007399 with entries=80, filesize=66.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044008465
2014-07-22 08:46:50,006 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3456, memsize=384.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/fbbf690882c542409b9e99988fb4c083
2014-07-22 08:46:50,021 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/fbbf690882c542409b9e99988fb4c083 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/fbbf690882c542409b9e99988fb4c083
2014-07-22 08:46:50,113 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:46:50,215 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/fbbf690882c542409b9e99988fb4c083, entries=1398680, sequenceid=3456, filesize=99.7m
2014-07-22 08:46:50,215 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~929.5m/974636720, currentsize=478.4m/501653840 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 29235ms, sequenceid=3456, compaction requested=true
2014-07-22 08:46:50,216 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:46:50,216 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 2000 blocking
2014-07-22 08:46:50,216 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-22 08:46:50,216 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 709.7m
2014-07-22 08:46:50,216 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:46:50,216 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:46:50,216 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 08:46:50,250 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044008465 with entries=73, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044010113
2014-07-22 08:46:50,250 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043904692
2014-07-22 08:46:50,250 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043906660
2014-07-22 08:46:50,250 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043908187
2014-07-22 08:46:50,250 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043908982
2014-07-22 08:46:50,250 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043910543
2014-07-22 08:46:50,251 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043912392
2014-07-22 08:46:50,251 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043913990
2014-07-22 08:46:50,251 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043915623
2014-07-22 08:46:50,251 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043918426
2014-07-22 08:46:50,251 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043920502
2014-07-22 08:46:50,360 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:46:50,660 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:46:50,807 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:46:52,361 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:46:53,941 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19668 synced till here 19664
2014-07-22 08:46:54,066 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044010113 with entries=126, filesize=95.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044012362
2014-07-22 08:46:54,067 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:46:55,771 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:46:55,805 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19747 synced till here 19745
2014-07-22 08:46:55,843 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044012362 with entries=79, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044015773
2014-07-22 08:46:55,843 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:46:57,356 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:46:57,433 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19832 synced till here 19823
2014-07-22 08:46:57,488 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044015773 with entries=85, filesize=69.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044017356
2014-07-22 08:46:57,488 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:46:59,145 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:46:59,186 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044017356 with entries=80, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044019146
2014-07-22 08:46:59,186 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:47:12,715 WARN  [regionserver60020] util.Sleeper: We slept 14005ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-22 08:47:12,723 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 13013ms
No GCs detected
2014-07-22 08:47:20,229 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1955ms
GC pool 'ParNew' had collection(s): count=1 time=1973ms
2014-07-22 08:47:21,726 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:47:21,809 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044019146 with entries=76, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044041727
2014-07-22 08:47:21,810 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:47:28,010 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1243ms
GC pool 'ParNew' had collection(s): count=1 time=1692ms
2014-07-22 08:47:29,294 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:47:29,790 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20090 synced till here 20086
2014-07-22 08:47:30,221 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044041727 with entries=102, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044049297
2014-07-22 08:47:30,226 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:47:34,899 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2340ms
GC pool 'ParNew' had collection(s): count=1 time=2489ms
2014-07-22 08:47:35,294 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:47:35,374 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20177 synced till here 20172
2014-07-22 08:47:35,495 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044049297 with entries=87, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044055295
2014-07-22 08:47:35,496 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:47:37,235 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3666, memsize=288.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/01fed9770f12431aae0198cc8c8bbbbd
2014-07-22 08:47:37,333 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/01fed9770f12431aae0198cc8c8bbbbd as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/01fed9770f12431aae0198cc8c8bbbbd
2014-07-22 08:47:37,357 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/01fed9770f12431aae0198cc8c8bbbbd, entries=1049100, sequenceid=3666, filesize=74.7m
2014-07-22 08:47:37,358 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~703.2m/737347200, currentsize=321.0m/336606400 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 52543ms, sequenceid=3666, compaction requested=true
2014-07-22 08:47:37,361 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:47:37,362 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 2000 blocking
2014-07-22 08:47:37,362 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-22 08:47:37,362 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:47:37,362 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 613.7m
2014-07-22 08:47:37,362 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:47:37,362 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 08:47:37,395 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:47:37,853 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:47:37,915 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20280 synced till here 20274
2014-07-22 08:47:38,098 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044055295 with entries=103, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044057854
2014-07-22 08:47:38,101 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:47:40,171 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1760ms
GC pool 'ParNew' had collection(s): count=1 time=1801ms
2014-07-22 08:47:40,803 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:47:43,053 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:47:43,074 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20384 synced till here 20382
2014-07-22 08:47:43,103 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044057854 with entries=104, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044063053
2014-07-22 08:47:43,105 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:47:44,854 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1172ms
GC pool 'ParNew' had collection(s): count=1 time=1388ms
2014-07-22 08:47:46,750 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:47:46,810 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044063053 with entries=97, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044066750
2014-07-22 08:47:46,811 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:47:50,315 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1951ms
GC pool 'ParNew' had collection(s): count=1 time=2009ms
2014-07-22 08:47:50,537 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b.
2014-07-22 08:47:51,448 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:47:51,480 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20593 synced till here 20572
2014-07-22 08:47:51,736 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044066750 with entries=112, filesize=70.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044071448
2014-07-22 08:47:51,738 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=44, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:47:54,216 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1394ms
GC pool 'ParNew' had collection(s): count=1 time=1432ms
2014-07-22 08:47:54,877 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:47:54,902 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20687 synced till here 20680
2014-07-22 08:47:55,077 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044071448 with entries=94, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044074877
2014-07-22 08:47:55,078 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=45, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:47:57,929 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:47:57,989 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20783 synced till here 20776
2014-07-22 08:47:58,196 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044074877 with entries=96, filesize=66.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044077929
2014-07-22 08:47:58,197 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=46, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:48:03,240 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1240ms
GC pool 'ParNew' had collection(s): count=1 time=1515ms
2014-07-22 08:48:03,261 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:48:03,323 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20894 synced till here 20882
2014-07-22 08:48:03,489 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044077929 with entries=111, filesize=67.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044083262
2014-07-22 08:48:03,497 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=47, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:48:04,242 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3728, memsize=372.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/74f935b999f443319c5c352cf2a8eeff
2014-07-22 08:48:04,261 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/74f935b999f443319c5c352cf2a8eeff as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/74f935b999f443319c5c352cf2a8eeff
2014-07-22 08:48:04,272 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/74f935b999f443319c5c352cf2a8eeff, entries=1354970, sequenceid=3728, filesize=96.5m
2014-07-22 08:48:04,273 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~709.7m/744154480, currentsize=407.1m/426869360 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 74057ms, sequenceid=3728, compaction requested=true
2014-07-22 08:48:04,273 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:48:04,274 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 2000 blocking
2014-07-22 08:48:04,274 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-22 08:48:04,274 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:48:04,274 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b., current region memstore size 278.7m
2014-07-22 08:48:04,274 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:48:04,274 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 08:48:04,368 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:48:05,167 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:48:05,445 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:48:05,493 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20998 synced till here 20988
2014-07-22 08:48:05,685 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044083262 with entries=104, filesize=65.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044085445
2014-07-22 08:48:07,191 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1441ms
GC pool 'ParNew' had collection(s): count=1 time=1455ms
2014-07-22 08:48:09,678 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:48:09,787 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21082 synced till here 21080
2014-07-22 08:48:09,866 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044085445 with entries=84, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044089682
2014-07-22 08:48:10,498 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=6603, hits=3, hitRatio=0.04%, , cachingAccesses=5, cachingHits=3, cachingHitsRatio=60.00%, evictions=0, evicted=0, evictedPerRun=NaN
2014-07-22 08:48:12,453 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1176ms
GC pool 'ParNew' had collection(s): count=1 time=1632ms
2014-07-22 08:48:14,717 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:48:14,796 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21186 synced till here 21181
2014-07-22 08:48:14,899 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044089682 with entries=104, filesize=66.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044094719
2014-07-22 08:48:17,495 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1526ms
GC pool 'ParNew' had collection(s): count=1 time=1605ms
2014-07-22 08:48:18,166 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:48:18,187 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21272 synced till here 21268
2014-07-22 08:48:18,245 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044094719 with entries=86, filesize=65.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044098166
2014-07-22 08:48:20,236 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1235ms
GC pool 'ParNew' had collection(s): count=1 time=1222ms
2014-07-22 08:48:20,534 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:48:20,570 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21387 synced till here 21386
2014-07-22 08:48:20,588 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044098166 with entries=115, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044100534
2014-07-22 08:48:23,475 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1237ms
GC pool 'ParNew' had collection(s): count=1 time=1382ms
2014-07-22 08:48:23,635 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:48:23,667 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21490 synced till here 21481
2014-07-22 08:48:23,761 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044100534 with entries=103, filesize=66.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044103635
2014-07-22 08:48:25,093 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:48:25,126 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21577 synced till here 21570
2014-07-22 08:48:26,515 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1028ms
GC pool 'ParNew' had collection(s): count=1 time=1381ms
2014-07-22 08:48:26,576 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044103635 with entries=87, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044105094
2014-07-22 08:48:27,282 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5083, memsize=184.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/78a4c6352fb0486db78d82bdf93fde37
2014-07-22 08:48:27,301 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/78a4c6352fb0486db78d82bdf93fde37 as hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/78a4c6352fb0486db78d82bdf93fde37
2014-07-22 08:48:27,328 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/78a4c6352fb0486db78d82bdf93fde37, entries=673070, sequenceid=5083, filesize=48.0m
2014-07-22 08:48:27,328 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~282.9m/296644320, currentsize=42.6m/44716720 for region usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. in 23054ms, sequenceid=5083, compaction requested=true
2014-07-22 08:48:27,329 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:48:27,329 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 2000 blocking
2014-07-22 08:48:27,329 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-22 08:48:27,329 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:48:27,329 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:48:27,329 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. because compaction request was cancelled
2014-07-22 08:48:27,330 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 1.1g
2014-07-22 08:48:28,721 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:48:28,775 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:48:28,819 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044105094 with entries=89, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044108775
2014-07-22 08:48:28,820 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043922121
2014-07-22 08:48:28,820 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043932398
2014-07-22 08:48:28,820 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043934864
2014-07-22 08:48:28,820 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043937375
2014-07-22 08:48:28,820 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043939732
2014-07-22 08:48:28,820 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043941539
2014-07-22 08:48:28,820 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043943653
2014-07-22 08:48:28,820 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043944762
2014-07-22 08:48:28,820 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043946328
2014-07-22 08:48:28,820 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043948218
2014-07-22 08:48:28,820 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043949678
2014-07-22 08:48:28,820 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043954440
2014-07-22 08:48:28,820 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043958246
2014-07-22 08:48:28,820 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043964860
2014-07-22 08:48:28,821 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043966953
2014-07-22 08:48:28,821 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043969159
2014-07-22 08:48:28,821 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043971782
2014-07-22 08:48:28,821 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043973391
2014-07-22 08:48:32,060 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:48:32,077 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21769 synced till here 21759
2014-07-22 08:48:32,091 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3868, memsize=427.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/fadc39a0832d4934875e08b1d02c012f
2014-07-22 08:48:32,104 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/fadc39a0832d4934875e08b1d02c012f as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/fadc39a0832d4934875e08b1d02c012f
2014-07-22 08:48:32,130 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/fadc39a0832d4934875e08b1d02c012f, entries=1557130, sequenceid=3868, filesize=111.0m
2014-07-22 08:48:32,131 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~615.1m/645019520, currentsize=421.5m/442018160 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 54769ms, sequenceid=3868, compaction requested=true
2014-07-22 08:48:32,131 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:48:32,131 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 2000 blocking
2014-07-22 08:48:32,131 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 747.7m
2014-07-22 08:48:32,134 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-22 08:48:32,134 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:48:32,134 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:48:32,135 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 08:48:32,163 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044108775 with entries=103, filesize=68.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044112060
2014-07-22 08:48:32,200 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:48:34,302 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:48:34,772 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:48:34,798 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21854 synced till here 21853
2014-07-22 08:48:34,834 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044112060 with entries=85, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044114773
2014-07-22 08:48:37,327 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1383ms
GC pool 'ParNew' had collection(s): count=1 time=1136ms
2014-07-22 08:48:37,921 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:48:37,994 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21941 synced till here 21936
2014-07-22 08:48:38,074 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044114773 with entries=87, filesize=67.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044117922
2014-07-22 08:48:40,879 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2039ms
GC pool 'ParNew' had collection(s): count=1 time=1976ms
2014-07-22 08:48:41,201 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:48:41,280 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22040 synced till here 22032
2014-07-22 08:48:41,434 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044117922 with entries=99, filesize=69.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044121201
2014-07-22 08:48:44,031 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1150ms
GC pool 'ParNew' had collection(s): count=1 time=1540ms
2014-07-22 08:48:44,296 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:48:44,324 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22134 synced till here 22124
2014-07-22 08:48:44,457 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044121201 with entries=94, filesize=70.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044124297
2014-07-22 08:48:47,446 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1411ms
GC pool 'ParNew' had collection(s): count=1 time=1716ms
2014-07-22 08:48:47,637 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:48:47,860 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22248 synced till here 22232
2014-07-22 08:48:47,979 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044124297 with entries=114, filesize=87.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044127638
2014-07-22 08:48:50,080 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1133ms
GC pool 'ParNew' had collection(s): count=1 time=1441ms
2014-07-22 08:48:50,399 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:48:50,453 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22327 synced till here 22317
2014-07-22 08:48:50,568 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044127638 with entries=79, filesize=73.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044130399
2014-07-22 08:48:53,333 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1752ms
GC pool 'ParNew' had collection(s): count=1 time=1746ms
2014-07-22 08:48:53,513 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:48:53,569 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22406 synced till here 22398
2014-07-22 08:48:53,685 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044130399 with entries=79, filesize=71.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044133514
2014-07-22 08:48:54,817 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:48:54,837 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22476 synced till here 22475
2014-07-22 08:48:54,863 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044133514 with entries=70, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044134818
2014-07-22 08:48:56,543 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1190ms
GC pool 'ParNew' had collection(s): count=1 time=1628ms
2014-07-22 08:48:56,760 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:56,795 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:56,807 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:56,822 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:56,822 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:56,864 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:56,883 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:56,888 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:56,913 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:56,978 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:56,978 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:57,090 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:57,295 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:57,446 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:57,536 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:57,601 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:57,656 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:57,656 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:57,658 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:57,658 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:57,659 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:57,722 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:57,724 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:58,953 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:59,219 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:59,424 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:59,495 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:59,577 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:59,675 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:59,738 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:59,889 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:59,966 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:59,967 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:59,967 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:59,968 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:59,968 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:48:59,968 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:49:00,066 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:49:00,405 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:49:00,473 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:49:00,532 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:49:00,581 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:49:00,677 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:49:00,731 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:49:01,772 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5012ms
2014-07-22 08:49:01,811 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5012ms
2014-07-22 08:49:01,811 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5004ms
2014-07-22 08:49:01,822 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:49:01,822 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:49:01,864 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:49:01,883 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:49:01,888 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5004ms
2014-07-22 08:49:01,913 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:49:01,978 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:49:01,978 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:49:02,100 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5010ms
2014-07-22 08:49:02,324 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:49:02,450 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5004ms
2014-07-22 08:49:02,537 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:49:02,601 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:49:02,658 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:49:02,659 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:49:02,659 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5003ms
2014-07-22 08:49:02,659 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 08:49:02,660 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:49:02,723 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:49:02,724 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:49:03,222 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:49:03,251 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:49:03,310 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:49:03,410 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:49:03,497 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:49:03,576 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:49:05,177 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5111ms
2014-07-22 08:49:05,178 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5440ms
2014-07-22 08:49:05,178 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1106ms
GC pool 'ParNew' had collection(s): count=1 time=1584ms
2014-07-22 08:49:05,179 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6226ms
2014-07-22 08:49:05,179 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5960ms
2014-07-22 08:49:05,179 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5755ms
2014-07-22 08:49:05,179 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5684ms
2014-07-22 08:49:05,180 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5603ms
2014-07-22 08:49:05,180 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5505ms
2014-07-22 08:49:05,180 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5292ms
2014-07-22 08:49:05,180 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5212ms
2014-07-22 08:49:05,181 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5212ms
2014-07-22 08:49:05,181 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5214ms
2014-07-22 08:49:05,181 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5215ms
2014-07-22 08:49:05,181 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5214ms
2014-07-22 08:49:05,181 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5213ms
2014-07-22 08:49:05,406 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 08:49:05,474 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:49:05,533 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:49:05,581 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:49:05,688 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5011ms
2014-07-22 08:49:05,732 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:49:06,142 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4112, memsize=301.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/152c8ed27f1b4581aac3a72a3bf5919e
2014-07-22 08:49:06,179 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/152c8ed27f1b4581aac3a72a3bf5919e as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/152c8ed27f1b4581aac3a72a3bf5919e
2014-07-22 08:49:06,199 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/152c8ed27f1b4581aac3a72a3bf5919e, entries=1098540, sequenceid=4112, filesize=78.3m
2014-07-22 08:49:06,200 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~749.7m/786117360, currentsize=257.2m/269693600 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 34069ms, sequenceid=4112, compaction requested=true
2014-07-22 08:49:06,200 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:49:06,200 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 2000 blocking
2014-07-22 08:49:06,200 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-22 08:49:06,200 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5469ms
2014-07-22 08:49:06,200 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,200 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 905.8m
2014-07-22 08:49:06,201 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5524ms
2014-07-22 08:49:06,201 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,200 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:49:06,201 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:49:06,201 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 08:49:06,205 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5624ms
2014-07-22 08:49:06,205 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,209 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5677ms
2014-07-22 08:49:06,209 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,209 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5736ms
2014-07-22 08:49:06,209 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,210 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5806ms
2014-07-22 08:49:06,210 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,214 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6246ms
2014-07-22 08:49:06,214 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,214 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6247ms
2014-07-22 08:49:06,214 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,214 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6248ms
2014-07-22 08:49:06,214 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,215 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6248ms
2014-07-22 08:49:06,215 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,215 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6247ms
2014-07-22 08:49:06,215 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,218 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6250ms
2014-07-22 08:49:06,218 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,218 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6330ms
2014-07-22 08:49:06,218 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,219 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6544ms
2014-07-22 08:49:06,219 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,219 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6642ms
2014-07-22 08:49:06,219 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,219 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6724ms
2014-07-22 08:49:06,219 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,219 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6795ms
2014-07-22 08:49:06,219 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,219 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7000ms
2014-07-22 08:49:06,219 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,240 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7287ms
2014-07-22 08:49:06,240 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,242 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6504ms
2014-07-22 08:49:06,242 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,242 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6176ms
2014-07-22 08:49:06,242 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,242 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2666ms
2014-07-22 08:49:06,242 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,243 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2747ms
2014-07-22 08:49:06,243 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,248 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2838ms
2014-07-22 08:49:06,248 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,254 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2944ms
2014-07-22 08:49:06,255 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,255 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3004ms
2014-07-22 08:49:06,255 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,256 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3033ms
2014-07-22 08:49:06,256 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,271 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8547ms
2014-07-22 08:49:06,271 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,271 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8549ms
2014-07-22 08:49:06,271 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,274 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8615ms
2014-07-22 08:49:06,274 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,274 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8617ms
2014-07-22 08:49:06,274 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,275 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8618ms
2014-07-22 08:49:06,275 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,276 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8617ms
2014-07-22 08:49:06,276 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,277 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8620ms
2014-07-22 08:49:06,277 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,277 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8676ms
2014-07-22 08:49:06,277 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,277 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8741ms
2014-07-22 08:49:06,278 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,278 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8832ms
2014-07-22 08:49:06,279 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,285 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8990ms
2014-07-22 08:49:06,285 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,286 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9196ms
2014-07-22 08:49:06,286 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,287 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9308ms
2014-07-22 08:49:06,287 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,287 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9310ms
2014-07-22 08:49:06,287 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,289 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9376ms
2014-07-22 08:49:06,289 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,290 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9406ms
2014-07-22 08:49:06,290 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,291 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9407ms
2014-07-22 08:49:06,291 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,291 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9427ms
2014-07-22 08:49:06,291 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,292 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9469ms
2014-07-22 08:49:06,292 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,292 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9470ms
2014-07-22 08:49:06,292 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,293 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9486ms
2014-07-22 08:49:06,293 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,294 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9499ms
2014-07-22 08:49:06,294 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,334 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9574ms
2014-07-22 08:49:06,334 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:49:06,881 DEBUG [RpcServer.handler=41,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:49:06,882 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12217,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044134664,"queuetimems":14,"class":"HRegionServer","responsesize":18165,"method":"Multi"}
2014-07-22 08:49:06,882 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12276,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044134605,"queuetimems":0,"class":"HRegionServer","responsesize":18675,"method":"Multi"}
2014-07-22 08:49:07,209 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:49:07,217 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12429,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044134787,"queuetimems":0,"class":"HRegionServer","responsesize":18647,"method":"Multi"}
2014-07-22 08:49:07,217 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12378,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044134838,"queuetimems":0,"class":"HRegionServer","responsesize":18698,"method":"Multi"}
2014-07-22 08:49:07,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22565 synced till here 22554
2014-07-22 08:49:07,530 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12810,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044134720,"queuetimems":0,"class":"HRegionServer","responsesize":18618,"method":"Multi"}
2014-07-22 08:49:07,569 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044134818 with entries=89, filesize=70.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044147209
2014-07-22 08:49:07,895 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:49:09,274 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12593,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044136681,"queuetimems":1,"class":"HRegionServer","responsesize":19271,"method":"Multi"}
2014-07-22 08:49:09,290 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14395,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044134895,"queuetimems":1,"class":"HRegionServer","responsesize":18584,"method":"Multi"}
2014-07-22 08:49:09,295 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12727,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044136567,"queuetimems":0,"class":"HRegionServer","responsesize":18232,"method":"Multi"}
2014-07-22 08:49:09,383 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12649,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044136734,"queuetimems":0,"class":"HRegionServer","responsesize":17291,"method":"Multi"}
2014-07-22 08:49:09,776 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:49:09,847 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22675 synced till here 22643
2014-07-22 08:49:10,492 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044147209 with entries=110, filesize=93.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044149777
2014-07-22 08:49:10,741 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11323,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044139417,"queuetimems":3,"class":"HRegionServer","responsesize":18618,"method":"Multi"}
2014-07-22 08:49:11,318 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11748,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044139569,"queuetimems":1,"class":"HRegionServer","responsesize":18675,"method":"Multi"}
2014-07-22 08:49:11,319 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11828,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044139491,"queuetimems":1,"class":"HRegionServer","responsesize":18698,"method":"Multi"}
2014-07-22 08:49:11,321 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10792,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044140529,"queuetimems":1,"class":"HRegionServer","responsesize":15740,"method":"Multi"}
2014-07-22 08:49:11,322 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12372,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044138949,"queuetimems":0,"class":"HRegionServer","responsesize":18584,"method":"Multi"}
2014-07-22 08:49:11,323 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14051,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044137272,"queuetimems":0,"class":"HRegionServer","responsesize":18766,"method":"Multi"}
2014-07-22 08:49:11,324 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11261,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044140062,"queuetimems":0,"class":"HRegionServer","responsesize":19271,"method":"Multi"}
2014-07-22 08:49:11,338 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11666,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044139671,"queuetimems":1,"class":"HRegionServer","responsesize":18232,"method":"Multi"}
2014-07-22 08:49:11,338 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10870,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044140467,"queuetimems":1,"class":"HRegionServer","responsesize":17782,"method":"Multi"}
2014-07-22 08:49:11,343 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10956,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044140387,"queuetimems":0,"class":"HRegionServer","responsesize":18312,"method":"Multi"}
2014-07-22 08:49:11,344 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11465,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044139879,"queuetimems":3,"class":"HRegionServer","responsesize":18647,"method":"Multi"}
2014-07-22 08:49:11,346 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10767,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044140578,"queuetimems":1,"class":"HRegionServer","responsesize":18070,"method":"Multi"}
2014-07-22 08:49:11,348 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11614,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044139734,"queuetimems":0,"class":"HRegionServer","responsesize":18165,"method":"Multi"}
2014-07-22 08:49:11,460 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14376,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044137084,"queuetimems":1,"class":"HRegionServer","responsesize":18950,"method":"Multi"}
2014-07-22 08:49:11,460 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14019,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044137441,"queuetimems":8,"class":"HRegionServer","responsesize":18994,"method":"Multi"}
2014-07-22 08:49:11,461 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14661,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044136800,"queuetimems":1,"class":"HRegionServer","responsesize":18932,"method":"Multi"}
2014-07-22 08:49:11,462 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13866,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044137596,"queuetimems":1,"class":"HRegionServer","responsesize":19089,"method":"Multi"}
2014-07-22 08:49:11,484 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13955,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044137529,"queuetimems":1,"class":"HRegionServer","responsesize":18332,"method":"Multi"}
2014-07-22 08:49:11,491 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11529,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044139961,"queuetimems":1,"class":"HRegionServer","responsesize":17291,"method":"Multi"}
2014-07-22 08:49:11,493 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13836,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044137656,"queuetimems":1,"class":"HRegionServer","responsesize":16941,"method":"Multi"}
2014-07-22 08:49:11,534 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13815,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044137719,"queuetimems":0,"class":"HRegionServer","responsesize":17291,"method":"Multi"}
2014-07-22 08:49:11,591 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14686,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044136904,"queuetimems":11,"class":"HRegionServer","responsesize":18996,"method":"Multi"}
2014-07-22 08:49:12,472 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13267,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044139204,"queuetimems":3,"class":"HRegionServer","responsesize":18998,"method":"Multi"}
2014-07-22 08:49:15,177 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:49:15,228 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044149777 with entries=79, filesize=60.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044155178
2014-07-22 08:49:15,552 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4095, memsize=451.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/f2fcacee0d9549b58aa2ead553ab7ed6
2014-07-22 08:49:15,569 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/f2fcacee0d9549b58aa2ead553ab7ed6 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/f2fcacee0d9549b58aa2ead553ab7ed6
2014-07-22 08:49:15,610 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/f2fcacee0d9549b58aa2ead553ab7ed6, entries=1645220, sequenceid=4095, filesize=117.3m
2014-07-22 08:49:15,611 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.1g/1141364720, currentsize=397.4m/416702240 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 48281ms, sequenceid=4095, compaction requested=true
2014-07-22 08:49:15,611 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:49:15,612 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 2000 blocking
2014-07-22 08:49:15,612 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-22 08:49:15,612 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 771.3m
2014-07-22 08:49:15,612 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:49:15,612 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:49:15,612 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 08:49:15,615 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:49:16,367 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:49:17,411 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:49:17,430 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22834 synced till here 22833
2014-07-22 08:49:17,460 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044155178 with entries=80, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044157411
2014-07-22 08:49:17,460 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043975983
2014-07-22 08:49:17,460 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043982393
2014-07-22 08:49:17,460 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043985091
2014-07-22 08:49:17,460 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043987382
2014-07-22 08:49:17,460 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043989648
2014-07-22 08:49:17,460 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043991393
2014-07-22 08:49:17,460 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043993193
2014-07-22 08:49:17,460 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043995553
2014-07-22 08:49:17,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043997290
2014-07-22 08:49:17,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406043999112
2014-07-22 08:49:17,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044002633
2014-07-22 08:49:17,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044004165
2014-07-22 08:49:17,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044005768
2014-07-22 08:49:17,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044007399
2014-07-22 08:49:17,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044008465
2014-07-22 08:49:18,628 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:49:18,758 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22920 synced till here 22916
2014-07-22 08:49:18,816 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044157411 with entries=86, filesize=67.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044158629
2014-07-22 08:49:20,350 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:49:20,374 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23008 synced till here 22999
2014-07-22 08:49:20,472 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044158629 with entries=88, filesize=67.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044160350
2014-07-22 08:49:22,444 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:49:22,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23111 synced till here 23101
2014-07-22 08:49:22,628 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044160350 with entries=103, filesize=71.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044162445
2014-07-22 08:49:24,472 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:49:25,043 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23235 synced till here 23218
2014-07-22 08:49:26,378 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044162445 with entries=124, filesize=96.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044164472
2014-07-22 08:49:27,086 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:49:27,106 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23332 synced till here 23321
2014-07-22 08:49:28,260 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044164472 with entries=97, filesize=72.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044167086
2014-07-22 08:49:29,074 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4264, memsize=260.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/2ab36d5ae7374146b05990319dae10f8
2014-07-22 08:49:29,090 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/2ab36d5ae7374146b05990319dae10f8 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/2ab36d5ae7374146b05990319dae10f8
2014-07-22 08:49:29,124 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/2ab36d5ae7374146b05990319dae10f8, entries=947710, sequenceid=4264, filesize=67.6m
2014-07-22 08:49:29,125 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~905.8m/949810160, currentsize=292.4m/306622720 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 22925ms, sequenceid=4264, compaction requested=true
2014-07-22 08:49:29,125 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:49:29,125 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 2000 blocking
2014-07-22 08:49:29,126 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 549.3m
2014-07-22 08:49:29,126 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-22 08:49:29,126 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:49:29,126 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:49:29,126 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 08:49:29,128 DEBUG [RpcServer.handler=7,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:49:29,426 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:49:29,449 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23422 synced till here 23409
2014-07-22 08:49:30,494 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044167086 with entries=90, filesize=73.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044169426
2014-07-22 08:49:30,494 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044010113
2014-07-22 08:49:30,494 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044012362
2014-07-22 08:49:30,494 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044015773
2014-07-22 08:49:30,494 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044017356
2014-07-22 08:49:30,494 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044019146
2014-07-22 08:49:30,494 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044041727
2014-07-22 08:49:30,494 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044049297
2014-07-22 08:49:30,535 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:49:30,741 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4329, memsize=155.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/c3f534630a0744b296e35c57fe06ccc6
2014-07-22 08:49:30,755 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/c3f534630a0744b296e35c57fe06ccc6 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/c3f534630a0744b296e35c57fe06ccc6
2014-07-22 08:49:30,768 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/c3f534630a0744b296e35c57fe06ccc6, entries=567550, sequenceid=4329, filesize=40.5m
2014-07-22 08:49:30,768 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~773.1m/810687200, currentsize=236.9m/248377440 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 15156ms, sequenceid=4329, compaction requested=true
2014-07-22 08:49:30,769 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:49:30,769 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 2000 blocking
2014-07-22 08:49:30,769 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-22 08:49:30,769 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:49:30,769 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:49:30,769 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 630.6m
2014-07-22 08:49:30,769 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 08:49:31,121 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:49:31,223 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:49:31,236 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23509 synced till here 23494
2014-07-22 08:49:31,356 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044169426 with entries=87, filesize=77.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044171223
2014-07-22 08:49:31,356 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044055295
2014-07-22 08:49:31,356 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044057854
2014-07-22 08:49:31,356 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044063053
2014-07-22 08:49:31,357 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044066750
2014-07-22 08:49:31,357 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044071448
2014-07-22 08:49:31,357 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044074877
2014-07-22 08:49:31,357 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044077929
2014-07-22 08:49:32,551 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:49:33,091 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:49:33,158 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23590 synced till here 23575
2014-07-22 08:49:33,300 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044171223 with entries=81, filesize=72.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044173091
2014-07-22 08:49:34,843 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:49:35,165 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23693 synced till here 23691
2014-07-22 08:49:35,225 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044173091 with entries=103, filesize=86.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044174844
2014-07-22 08:49:37,296 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:49:38,349 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23811 synced till here 23807
2014-07-22 08:49:38,397 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044174844 with entries=118, filesize=82.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044177297
2014-07-22 08:49:39,313 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:49:39,354 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23902 synced till here 23889
2014-07-22 08:49:40,378 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044177297 with entries=91, filesize=72.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044179313
2014-07-22 08:49:41,620 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:49:41,684 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24000 synced till here 23994
2014-07-22 08:49:42,954 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044179313 with entries=98, filesize=73.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044181621
2014-07-22 08:49:42,955 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:49:43,995 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:49:44,097 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24090 synced till here 24076
2014-07-22 08:49:44,152 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4426, memsize=190.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/a730176b126f4afc9f69e13e13c2cf49
2014-07-22 08:49:44,180 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/a730176b126f4afc9f69e13e13c2cf49 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/a730176b126f4afc9f69e13e13c2cf49
2014-07-22 08:49:44,194 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/a730176b126f4afc9f69e13e13c2cf49, entries=694450, sequenceid=4426, filesize=49.5m
2014-07-22 08:49:44,194 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~554.4m/581340080, currentsize=233.5m/244825840 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 15068ms, sequenceid=4426, compaction requested=true
2014-07-22 08:49:44,195 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:49:44,195 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 2000 blocking
2014-07-22 08:49:44,195 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-22 08:49:44,195 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 541.0m
2014-07-22 08:49:44,195 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:49:44,195 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:49:44,195 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 08:49:44,231 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044181621 with entries=90, filesize=75.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044183996
2014-07-22 08:49:44,232 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:49:46,204 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:49:46,226 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:49:46,409 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:49:46,424 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24169 synced till here 24160
2014-07-22 08:49:46,540 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044183996 with entries=79, filesize=72.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044186410
2014-07-22 08:49:46,540 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:49:48,694 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:49:49,880 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24286 synced till here 24260
2014-07-22 08:49:50,162 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044186410 with entries=117, filesize=91.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044188695
2014-07-22 08:49:50,163 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:49:52,227 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4456, memsize=238.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/6292eadd710040a183a999e6605ef2a9
2014-07-22 08:49:52,249 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/6292eadd710040a183a999e6605ef2a9 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/6292eadd710040a183a999e6605ef2a9
2014-07-22 08:49:52,275 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/6292eadd710040a183a999e6605ef2a9, entries=868970, sequenceid=4456, filesize=61.9m
2014-07-22 08:49:52,276 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~639.8m/670862080, currentsize=327.3m/343183520 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 21507ms, sequenceid=4456, compaction requested=true
2014-07-22 08:49:52,277 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:49:52,277 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 2000 blocking
2014-07-22 08:49:52,277 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-22 08:49:52,277 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 580.4m
2014-07-22 08:49:52,277 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:49:52,277 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:49:52,278 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 08:49:52,288 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:49:52,291 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:49:52,321 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24388 synced till here 24359
2014-07-22 08:49:52,742 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044188695 with entries=102, filesize=95.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044192288
2014-07-22 08:49:52,743 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:49:55,099 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1489ms
GC pool 'ParNew' had collection(s): count=1 time=1785ms
2014-07-22 08:49:55,464 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:49:55,509 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24491 synced till here 24460
2014-07-22 08:49:55,581 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:49:55,815 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044192288 with entries=103, filesize=94.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044195465
2014-07-22 08:49:55,815 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:49:57,669 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1069ms
GC pool 'ParNew' had collection(s): count=1 time=1519ms
2014-07-22 08:49:58,303 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:49:58,349 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24596 synced till here 24578
2014-07-22 08:49:58,560 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044195465 with entries=105, filesize=93.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044198303
2014-07-22 08:49:58,561 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:50:00,354 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:50:00,384 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24672 synced till here 24663
2014-07-22 08:50:00,495 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044198303 with entries=76, filesize=68.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044200354
2014-07-22 08:50:00,496 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:50:01,599 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:50:01,627 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24759 synced till here 24757
2014-07-22 08:50:01,679 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044200354 with entries=87, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044201600
2014-07-22 08:50:01,680 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:50:03,388 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:50:03,409 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24852 synced till here 24843
2014-07-22 08:50:03,519 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044201600 with entries=93, filesize=66.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044203389
2014-07-22 08:50:03,520 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:50:04,445 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:50:04,490 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24930 synced till here 24929
2014-07-22 08:50:04,574 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044203389 with entries=78, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044204446
2014-07-22 08:50:04,575 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:50:05,960 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1134ms
GC pool 'ParNew' had collection(s): count=1 time=1356ms
2014-07-22 08:50:07,531 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:50:07,653 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4582, memsize=252.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/9521bf1e86dc47619e7a0a5f0d351f7a
2014-07-22 08:50:07,670 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/9521bf1e86dc47619e7a0a5f0d351f7a as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/9521bf1e86dc47619e7a0a5f0d351f7a
2014-07-22 08:50:07,691 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25026 synced till here 25009
2014-07-22 08:50:07,817 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/9521bf1e86dc47619e7a0a5f0d351f7a, entries=920550, sequenceid=4582, filesize=65.6m
2014-07-22 08:50:07,818 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~566.8m/594345520, currentsize=334.4m/350651360 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 23623ms, sequenceid=4582, compaction requested=true
2014-07-22 08:50:07,818 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:50:07,818 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 2000 blocking
2014-07-22 08:50:07,818 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-22 08:50:07,819 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:50:07,819 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:50:07,819 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 08:50:07,819 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b., current region memstore size 179.5m
2014-07-22 08:50:08,932 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044204446 with entries=96, filesize=74.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044207531
2014-07-22 08:50:09,071 DEBUG [RpcServer.handler=41,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:50:09,072 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:50:09,683 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:50:09,701 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25121 synced till here 25107
2014-07-22 08:50:11,007 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044207531 with entries=95, filesize=75.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044209684
2014-07-22 08:50:13,423 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1031ms
GC pool 'ParNew' had collection(s): count=1 time=1206ms
2014-07-22 08:50:13,524 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:50:13,554 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25239 synced till here 25208
2014-07-22 08:50:13,830 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044209684 with entries=118, filesize=93.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044213525
2014-07-22 08:50:16,610 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1680ms
GC pool 'ParNew' had collection(s): count=1 time=1973ms
2014-07-22 08:50:16,967 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:50:17,139 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25348 synced till here 25340
2014-07-22 08:50:17,240 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044213525 with entries=109, filesize=89.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044216967
2014-07-22 08:50:17,265 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4664, memsize=246.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/4f455e2e974a4384a3d6f48f5958fbf3
2014-07-22 08:50:17,285 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/4f455e2e974a4384a3d6f48f5958fbf3 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/4f455e2e974a4384a3d6f48f5958fbf3
2014-07-22 08:50:17,337 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/4f455e2e974a4384a3d6f48f5958fbf3, entries=897970, sequenceid=4664, filesize=63.9m
2014-07-22 08:50:17,342 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~598.7m/627833280, currentsize=346.1m/362905360 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 25064ms, sequenceid=4664, compaction requested=true
2014-07-22 08:50:17,345 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:50:17,345 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 2000 blocking
2014-07-22 08:50:17,346 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-22 08:50:17,346 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:50:17,346 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:50:17,346 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 08:50:17,347 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 696.4m
2014-07-22 08:50:17,640 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:50:19,433 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1322ms
GC pool 'ParNew' had collection(s): count=1 time=1524ms
2014-07-22 08:50:19,983 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:50:20,268 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:50:20,328 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25465 synced till here 25439
2014-07-22 08:50:20,871 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044216967 with entries=117, filesize=96.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044220269
2014-07-22 08:50:24,276 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3338ms
GC pool 'ParNew' had collection(s): count=1 time=3366ms
2014-07-22 08:50:24,276 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5995, memsize=123.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/853d2298cf5442998f4156f332a36878
2014-07-22 08:50:24,294 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/853d2298cf5442998f4156f332a36878 as hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/853d2298cf5442998f4156f332a36878
2014-07-22 08:50:24,307 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/853d2298cf5442998f4156f332a36878, entries=451100, sequenceid=5995, filesize=32.1m
2014-07-22 08:50:24,307 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~182.5m/191391280, currentsize=22.2m/23322960 for region usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. in 16488ms, sequenceid=5995, compaction requested=true
2014-07-22 08:50:24,307 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:50:24,307 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 2000 blocking
2014-07-22 08:50:24,308 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-22 08:50:24,308 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 726.0m
2014-07-22 08:50:24,308 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:50:24,308 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:50:24,308 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. because compaction request was cancelled
2014-07-22 08:50:24,691 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10278,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044214412,"queuetimems":0,"class":"HRegionServer","responsesize":18790,"method":"Multi"}
2014-07-22 08:50:24,692 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17746 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:24,696 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:24,714 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10199,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044214514,"queuetimems":1,"class":"HRegionServer","responsesize":18632,"method":"Multi"}
2014-07-22 08:50:24,714 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17744 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:24,714 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:24,722 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17734 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:24,722 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:24,723 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17733 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:24,723 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:24,723 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17735 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:24,723 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:24,724 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17742 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:24,724 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:24,725 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10128,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54550","starttimems":1406044214596,"queuetimems":16,"class":"HRegionServer","responsesize":18337,"method":"Multi"}
2014-07-22 08:50:24,725 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17743 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:24,725 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:24,726 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17741 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:24,726 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:24,727 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17724 service: ClientService methodName: Multi size: 1.9m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:24,727 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:24,746 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17725 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:24,746 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:24,750 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17727 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:24,750 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:24,756 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17730 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:24,756 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:24,756 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17723 service: ClientService methodName: Multi size: 1.7m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:24,756 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:24,794 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17753 service: ClientService methodName: Multi size: 32.5k connection: 9.1.143.53:54550: output error
2014-07-22 08:50:24,794 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:24,795 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17754 service: ClientService methodName: Multi size: 1.3k connection: 9.1.143.53:54550: output error
2014-07-22 08:50:24,795 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:24,795 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17732 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:24,795 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:24,805 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17755 service: ClientService methodName: Multi size: 191.1k connection: 9.1.143.53:54550: output error
2014-07-22 08:50:24,806 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:24,947 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17806 service: ClientService methodName: Multi size: 1.7m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:24,947 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:24,947 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17731 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:24,947 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:24,947 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17799 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:24,948 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:25,103 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:50:25,105 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17802 service: ClientService methodName: Multi size: 1.9m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:25,105 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:25,106 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17722 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:25,106 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:25,106 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17721 service: ClientService methodName: Multi size: 1.4m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:25,107 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:25,107 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17720 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:25,107 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:25,110 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17803 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:25,110 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:25,140 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25576 synced till here 25554
2014-07-22 08:50:25,274 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17719 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:25,274 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:25,301 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044220269 with entries=111, filesize=84.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044225104
2014-07-22 08:50:25,302 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044083262
2014-07-22 08:50:25,302 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044085445
2014-07-22 08:50:25,302 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044089682
2014-07-22 08:50:25,302 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044094719
2014-07-22 08:50:25,302 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044098166
2014-07-22 08:50:25,302 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044100534
2014-07-22 08:50:25,302 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044103635
2014-07-22 08:50:25,302 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044105094
2014-07-22 08:50:25,302 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044108775
2014-07-22 08:50:25,303 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044112060
2014-07-22 08:50:25,303 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044114773
2014-07-22 08:50:25,303 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044117922
2014-07-22 08:50:25,303 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044121201
2014-07-22 08:50:25,303 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044124297
2014-07-22 08:50:25,303 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044127638
2014-07-22 08:50:25,303 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044130399
2014-07-22 08:50:25,303 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044133514
2014-07-22 08:50:25,303 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044134818
2014-07-22 08:50:25,303 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044147209
2014-07-22 08:50:25,304 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044149777
2014-07-22 08:50:25,304 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044155178
2014-07-22 08:50:25,304 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044157411
2014-07-22 08:50:25,304 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044158629
2014-07-22 08:50:25,304 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044160350
2014-07-22 08:50:25,304 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044162445
2014-07-22 08:50:25,304 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044164472
2014-07-22 08:50:26,807 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:50:26,955 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17784 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:26,955 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:26,955 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17796 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:26,955 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:26,963 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17786 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:26,963 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:26,964 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17775 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:26,964 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:26,966 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17794 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:26,966 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:26,966 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17770 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:26,966 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:26,966 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17785 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:26,966 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:26,966 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17778 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:26,966 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:26,966 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17774 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:26,966 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:26,967 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17798 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:26,967 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:27,044 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17790 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:27,044 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:27,090 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17789 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:27,090 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:27,091 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17769 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:27,091 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:27,092 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17771 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:27,092 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:27,470 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:50:27,472 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17768 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:27,472 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:27,490 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17764 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:27,490 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:27,515 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25680 synced till here 25662
2014-07-22 08:50:27,723 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17767 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:27,723 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:27,724 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17762 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:27,725 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:27,726 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17760 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:27,726 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:27,728 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17763 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:27,728 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:27,734 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17765 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:27,734 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:27,735 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17761 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:27,735 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:27,760 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044225104 with entries=104, filesize=83.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044227471
2014-07-22 08:50:27,933 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17766 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:27,933 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:27,933 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17752 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54550: output error
2014-07-22 08:50:27,933 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 08:50:29,814 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:50:29,827 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25783 synced till here 25763
2014-07-22 08:50:29,940 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044227471 with entries=103, filesize=79.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044229815
2014-07-22 08:50:31,346 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:50:31,382 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25865 synced till here 25863
2014-07-22 08:50:31,431 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044229815 with entries=82, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044231346
2014-07-22 08:50:32,708 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:50:32,744 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25941 synced till here 25938
2014-07-22 08:50:32,788 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044231346 with entries=76, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044232708
2014-07-22 08:50:34,120 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:50:34,149 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26017 synced till here 26015
2014-07-22 08:50:34,183 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044232708 with entries=76, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044234121
2014-07-22 08:50:36,464 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:50:36,481 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26095 synced till here 26092
2014-07-22 08:50:36,580 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044234121 with entries=78, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044236465
2014-07-22 08:50:39,966 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4826, memsize=233.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/c1fb54c7b88a477abec7ff8609066ad6
2014-07-22 08:50:39,991 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/c1fb54c7b88a477abec7ff8609066ad6 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/c1fb54c7b88a477abec7ff8609066ad6
2014-07-22 08:50:40,023 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/c1fb54c7b88a477abec7ff8609066ad6, entries=850350, sequenceid=4826, filesize=60.6m
2014-07-22 08:50:40,023 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~709.9m/744385760, currentsize=283.0m/296719360 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 22676ms, sequenceid=4826, compaction requested=true
2014-07-22 08:50:40,025 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:50:40,025 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 2000 blocking
2014-07-22 08:50:40,025 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-22 08:50:40,025 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:50:40,025 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:50:40,025 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 728.2m
2014-07-22 08:50:40,025 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 08:50:40,597 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:50:40,782 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:50:41,056 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:50:41,072 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26192 synced till here 26191
2014-07-22 08:50:41,098 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044236465 with entries=97, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044241056
2014-07-22 08:50:41,099 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044167086
2014-07-22 08:50:42,434 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4880, memsize=236.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/65cbf7897b224fd489aca139691bc4f1
2014-07-22 08:50:42,452 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/65cbf7897b224fd489aca139691bc4f1 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/65cbf7897b224fd489aca139691bc4f1
2014-07-22 08:50:42,475 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/65cbf7897b224fd489aca139691bc4f1, entries=859200, sequenceid=4880, filesize=61.2m
2014-07-22 08:50:42,475 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~750.7m/787182000, currentsize=239.9m/251518000 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 18167ms, sequenceid=4880, compaction requested=true
2014-07-22 08:50:42,476 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:50:42,476 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 2000 blocking
2014-07-22 08:50:42,476 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 630.3m
2014-07-22 08:50:42,476 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-22 08:50:42,476 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:50:42,476 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:50:42,476 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 08:50:43,042 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:50:43,218 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:50:43,264 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044241056 with entries=79, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044243218
2014-07-22 08:50:43,264 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044169426
2014-07-22 08:50:43,264 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044171223
2014-07-22 08:50:43,264 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044173091
2014-07-22 08:50:43,264 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044174844
2014-07-22 08:50:43,264 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044177297
2014-07-22 08:50:43,265 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044179313
2014-07-22 08:50:43,265 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044181621
2014-07-22 08:50:44,454 DEBUG [RpcServer.handler=6,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:50:45,207 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:50:45,254 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26342 synced till here 26338
2014-07-22 08:50:45,302 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044243218 with entries=71, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044245208
2014-07-22 08:50:49,048 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:50:49,083 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044245208 with entries=75, filesize=61.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044249048
2014-07-22 08:50:50,238 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4997, memsize=200.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/bf118a71aa0649719f14b5a3e0ae8ec6
2014-07-22 08:50:50,255 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/bf118a71aa0649719f14b5a3e0ae8ec6 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/bf118a71aa0649719f14b5a3e0ae8ec6
2014-07-22 08:50:50,273 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/bf118a71aa0649719f14b5a3e0ae8ec6, entries=731570, sequenceid=4997, filesize=52.2m
2014-07-22 08:50:50,273 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~728.2m/763598560, currentsize=102.4m/107390320 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 10248ms, sequenceid=4997, compaction requested=true
2014-07-22 08:50:50,274 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:50:50,274 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 2000 blocking
2014-07-22 08:50:50,274 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-22 08:50:50,274 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:50:50,274 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:50:50,274 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 384.7m
2014-07-22 08:50:50,274 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 08:50:50,574 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:50:53,523 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5030, memsize=243.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/0c9b99a3e6274a9385ceb17fbed5368f
2014-07-22 08:50:53,541 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/0c9b99a3e6274a9385ceb17fbed5368f as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/0c9b99a3e6274a9385ceb17fbed5368f
2014-07-22 08:50:53,561 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/0c9b99a3e6274a9385ceb17fbed5368f, entries=886630, sequenceid=5030, filesize=63.2m
2014-07-22 08:50:53,561 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~633.1m/663891760, currentsize=79.8m/83705440 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 11085ms, sequenceid=5030, compaction requested=true
2014-07-22 08:50:53,562 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:50:53,562 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 2000 blocking
2014-07-22 08:50:53,562 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-22 08:50:53,562 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 321.5m
2014-07-22 08:50:53,562 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:50:53,562 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:50:53,562 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 08:50:53,799 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:50:54,654 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:50:54,694 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044249048 with entries=102, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044254655
2014-07-22 08:50:54,694 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044183996
2014-07-22 08:50:54,694 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044186410
2014-07-22 08:50:54,694 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044188695
2014-07-22 08:50:54,694 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044192288
2014-07-22 08:50:54,694 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044195465
2014-07-22 08:50:54,695 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044198303
2014-07-22 08:50:54,695 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044200354
2014-07-22 08:50:54,695 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044201600
2014-07-22 08:50:54,695 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044203389
2014-07-22 08:50:54,695 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044204446
2014-07-22 08:50:56,980 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:50:57,011 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26615 synced till here 26612
2014-07-22 08:50:57,072 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044254655 with entries=96, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044256980
2014-07-22 08:50:58,412 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:50:58,454 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044256980 with entries=71, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044258412
2014-07-22 08:50:59,663 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:50:59,686 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26763 synced till here 26761
2014-07-22 08:50:59,718 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044258412 with entries=77, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044259664
2014-07-22 08:50:59,934 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5055, memsize=206.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/d004291d51e8491f81f958a944be9367
2014-07-22 08:50:59,962 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/d004291d51e8491f81f958a944be9367 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/d004291d51e8491f81f958a944be9367
2014-07-22 08:50:59,998 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/d004291d51e8491f81f958a944be9367, entries=752240, sequenceid=5055, filesize=53.6m
2014-07-22 08:50:59,998 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~384.7m/403398400, currentsize=107.1m/112351200 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 9724ms, sequenceid=5055, compaction requested=true
2014-07-22 08:50:59,999 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:50:59,999 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 2000 blocking
2014-07-22 08:50:59,999 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-22 08:50:59,999 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:50:59,999 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:50:59,999 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 08:51:02,164 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:51:02,209 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044259664 with entries=85, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044262165
2014-07-22 08:51:02,881 DEBUG [RpcServer.handler=41,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:51:02,882 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 256.1m
2014-07-22 08:51:03,065 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:51:03,066 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:51:03,708 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26934 synced till here 26933
2014-07-22 08:51:03,727 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044262165 with entries=86, filesize=69.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044263065
2014-07-22 08:51:04,187 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5072, memsize=226.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/13e1c813f08548dbb7401dc396cf0cfa
2014-07-22 08:51:04,206 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/13e1c813f08548dbb7401dc396cf0cfa as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/13e1c813f08548dbb7401dc396cf0cfa
2014-07-22 08:51:04,219 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/13e1c813f08548dbb7401dc396cf0cfa, entries=824380, sequenceid=5072, filesize=58.8m
2014-07-22 08:51:04,220 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~321.5m/337160720, currentsize=141.3m/148179040 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 10658ms, sequenceid=5072, compaction requested=true
2014-07-22 08:51:04,220 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:51:04,220 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 2000 blocking
2014-07-22 08:51:04,220 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-22 08:51:04,220 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:51:04,220 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:51:04,221 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 08:51:06,673 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:51:06,691 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27035 synced till here 27034
2014-07-22 08:51:06,737 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044263065 with entries=101, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044266673
2014-07-22 08:51:07,300 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:51:07,301 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 256.0m
2014-07-22 08:51:07,524 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:51:08,273 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:51:08,295 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27110 synced till here 27109
2014-07-22 08:51:08,342 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044266673 with entries=75, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044268274
2014-07-22 08:51:10,363 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1635ms
GC pool 'ParNew' had collection(s): count=1 time=1852ms
2014-07-22 08:51:11,383 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:51:11,411 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044268274 with entries=83, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044271383
2014-07-22 08:51:12,541 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:51:12,634 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:51:12,673 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27275 synced till here 27274
2014-07-22 08:51:12,687 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044271383 with entries=82, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044272634
2014-07-22 08:51:13,674 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:51:13,980 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5150, memsize=252.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/f1c18fcd22b14a8d8aaa2b59c52d1db3
2014-07-22 08:51:13,997 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/f1c18fcd22b14a8d8aaa2b59c52d1db3 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/f1c18fcd22b14a8d8aaa2b59c52d1db3
2014-07-22 08:51:14,265 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/f1c18fcd22b14a8d8aaa2b59c52d1db3, entries=919420, sequenceid=5150, filesize=65.5m
2014-07-22 08:51:14,265 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~256.1m/268513600, currentsize=128.0m/134268640 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 11383ms, sequenceid=5150, compaction requested=true
2014-07-22 08:51:14,265 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:51:14,266 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 2000 blocking
2014-07-22 08:51:14,266 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 279.5m
2014-07-22 08:51:14,266 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-22 08:51:14,266 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:51:14,266 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:51:14,266 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 08:51:14,483 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:51:15,101 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:51:15,137 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27372 synced till here 27370
2014-07-22 08:51:15,179 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044272634 with entries=97, filesize=65.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044275101
2014-07-22 08:51:16,564 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:51:16,584 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27456 synced till here 27449
2014-07-22 08:51:16,655 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044275101 with entries=84, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044276564
2014-07-22 08:51:17,952 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:51:17,982 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27534 synced till here 27526
2014-07-22 08:51:18,059 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044276564 with entries=78, filesize=68.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044277953
2014-07-22 08:51:19,790 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5184, memsize=252.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/2c98ca33926e4d21837a339ea28d12ee
2014-07-22 08:51:19,810 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/2c98ca33926e4d21837a339ea28d12ee as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/2c98ca33926e4d21837a339ea28d12ee
2014-07-22 08:51:19,837 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/2c98ca33926e4d21837a339ea28d12ee, entries=919510, sequenceid=5184, filesize=65.5m
2014-07-22 08:51:19,838 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~256.0m/268442160, currentsize=185.4m/194448800 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 12537ms, sequenceid=5184, compaction requested=true
2014-07-22 08:51:19,838 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:51:19,838 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 2000 blocking
2014-07-22 08:51:19,838 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:51:19,838 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-22 08:51:19,839 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:51:19,839 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:51:19,839 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 354.9m
2014-07-22 08:51:19,839 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 08:51:19,864 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27623 synced till here 27619
2014-07-22 08:51:19,930 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044277953 with entries=89, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044279838
2014-07-22 08:51:19,933 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044241056
2014-07-22 08:51:20,267 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:51:23,959 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:51:23,994 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044279838 with entries=84, filesize=60.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044283960
2014-07-22 08:51:24,539 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:51:24,693 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5227, memsize=258.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/39da3cc0c8c343c7bfb69e1eb54f6735
2014-07-22 08:51:24,708 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/39da3cc0c8c343c7bfb69e1eb54f6735 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/39da3cc0c8c343c7bfb69e1eb54f6735
2014-07-22 08:51:24,725 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/39da3cc0c8c343c7bfb69e1eb54f6735, entries=940870, sequenceid=5227, filesize=67.0m
2014-07-22 08:51:24,726 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~282.8m/296588720, currentsize=131.4m/137809920 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 10460ms, sequenceid=5227, compaction requested=true
2014-07-22 08:51:24,726 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:51:24,727 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 2000 blocking
2014-07-22 08:51:24,727 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-22 08:51:24,727 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:51:24,727 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:51:24,727 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 260.8m
2014-07-22 08:51:24,727 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 08:51:24,966 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:51:25,152 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:51:25,180 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27784 synced till here 27783
2014-07-22 08:51:25,206 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044283960 with entries=77, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044285152
2014-07-22 08:51:29,012 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:51:29,332 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:51:29,374 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044285152 with entries=95, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044289332
2014-07-22 08:51:31,133 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5294, memsize=338.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/53202b3578fd4b1484ceccfdb98abfdb
2014-07-22 08:51:31,150 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/53202b3578fd4b1484ceccfdb98abfdb as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/53202b3578fd4b1484ceccfdb98abfdb
2014-07-22 08:51:31,176 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/53202b3578fd4b1484ceccfdb98abfdb, entries=1232380, sequenceid=5294, filesize=87.8m
2014-07-22 08:51:31,176 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~362.9m/380523920, currentsize=93.8m/98399440 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 11337ms, sequenceid=5294, compaction requested=true
2014-07-22 08:51:31,177 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:51:31,177 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 2000 blocking
2014-07-22 08:51:31,178 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 278.8m
2014-07-22 08:51:31,178 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-22 08:51:31,178 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:51:31,178 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:51:31,178 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 08:51:31,384 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:51:31,735 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:51:31,769 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044289332 with entries=106, filesize=61.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044291735
2014-07-22 08:51:32,919 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5318, memsize=244.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/e0f2093a161e4818b74557744382339f
2014-07-22 08:51:32,939 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/e0f2093a161e4818b74557744382339f as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/e0f2093a161e4818b74557744382339f
2014-07-22 08:51:32,952 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/e0f2093a161e4818b74557744382339f, entries=889190, sequenceid=5318, filesize=63.4m
2014-07-22 08:51:32,952 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~264.1m/276894240, currentsize=81.1m/85089760 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 8225ms, sequenceid=5318, compaction requested=true
2014-07-22 08:51:32,952 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:51:32,952 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 2000 blocking
2014-07-22 08:51:32,953 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-22 08:51:32,953 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:51:32,953 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:51:32,953 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 08:51:33,919 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:51:33,944 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28074 synced till here 28073
2014-07-22 08:51:33,975 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044291735 with entries=89, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044293919
2014-07-22 08:51:33,976 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:51:33,977 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b., current region memstore size 224.8m
2014-07-22 08:51:34,424 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:51:35,652 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:51:35,683 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044293919 with entries=83, filesize=62.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044295653
2014-07-22 08:51:36,958 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:51:38,344 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:51:38,610 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28264 synced till here 28263
2014-07-22 08:51:38,636 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044295653 with entries=107, filesize=71.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044298344
2014-07-22 08:51:40,432 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5368, memsize=258.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/c6875b2ebe0b4171b877d77ffc1410ef
2014-07-22 08:51:40,459 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/c6875b2ebe0b4171b877d77ffc1410ef as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/c6875b2ebe0b4171b877d77ffc1410ef
2014-07-22 08:51:40,493 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/c6875b2ebe0b4171b877d77ffc1410ef, entries=940190, sequenceid=5368, filesize=67.0m
2014-07-22 08:51:40,493 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~278.8m/292326560, currentsize=113.7m/119225680 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 9315ms, sequenceid=5368, compaction requested=true
2014-07-22 08:51:40,494 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:51:40,494 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 2000 blocking
2014-07-22 08:51:40,494 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-22 08:51:40,494 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:51:40,494 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 305.4m
2014-07-22 08:51:40,494 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:51:40,494 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 08:51:40,707 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:51:41,398 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:51:41,468 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044298344 with entries=108, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044301398
2014-07-22 08:51:41,622 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6614, memsize=194.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/40f573a50258400ebd1d2171b2e7e9d5
2014-07-22 08:51:41,642 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/40f573a50258400ebd1d2171b2e7e9d5 as hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/40f573a50258400ebd1d2171b2e7e9d5
2014-07-22 08:51:41,661 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/40f573a50258400ebd1d2171b2e7e9d5, entries=707330, sequenceid=6614, filesize=50.4m
2014-07-22 08:51:41,661 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~224.8m/235734640, currentsize=24.3m/25440080 for region usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. in 7684ms, sequenceid=6614, compaction requested=true
2014-07-22 08:51:41,662 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:51:41,662 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 2000 blocking
2014-07-22 08:51:41,662 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-22 08:51:41,662 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:51:41,662 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:51:41,662 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. because compaction request was cancelled
2014-07-22 08:51:42,417 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:51:42,436 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28454 synced till here 28453
2014-07-22 08:51:42,451 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044301398 with entries=82, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044302417
2014-07-22 08:51:42,451 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044207531
2014-07-22 08:51:42,451 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044209684
2014-07-22 08:51:42,451 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044213525
2014-07-22 08:51:42,451 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044216967
2014-07-22 08:51:42,451 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044220269
2014-07-22 08:51:42,451 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044225104
2014-07-22 08:51:42,451 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044227471
2014-07-22 08:51:42,451 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044229815
2014-07-22 08:51:42,451 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044231346
2014-07-22 08:51:42,451 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044232708
2014-07-22 08:51:42,451 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044234121
2014-07-22 08:51:42,451 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044236465
2014-07-22 08:51:42,451 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044243218
2014-07-22 08:51:42,451 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044245208
2014-07-22 08:51:42,452 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044249048
2014-07-22 08:51:42,452 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044254655
2014-07-22 08:51:42,452 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044256980
2014-07-22 08:51:42,452 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044258412
2014-07-22 08:51:42,452 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044259664
2014-07-22 08:51:42,452 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044262165
2014-07-22 08:51:42,452 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044263065
2014-07-22 08:51:42,452 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044266673
2014-07-22 08:51:42,452 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044268274
2014-07-22 08:51:42,452 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044271383
2014-07-22 08:51:43,915 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:51:43,917 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 256.1m
2014-07-22 08:51:44,092 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:51:44,094 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:51:44,131 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28538 synced till here 28537
2014-07-22 08:51:44,183 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044302417 with entries=84, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044304094
2014-07-22 08:51:51,104 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5433, memsize=307.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/71cf29ea1fe44867873c67d4ac8af7d7
2014-07-22 08:51:51,136 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/71cf29ea1fe44867873c67d4ac8af7d7 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/71cf29ea1fe44867873c67d4ac8af7d7
2014-07-22 08:51:51,150 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/71cf29ea1fe44867873c67d4ac8af7d7, entries=1118410, sequenceid=5433, filesize=79.6m
2014-07-22 08:51:51,150 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~307.2m/322093120, currentsize=74.5m/78102880 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 10656ms, sequenceid=5433, compaction requested=true
2014-07-22 08:51:51,151 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:51:51,151 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 2000 blocking
2014-07-22 08:51:51,151 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-22 08:51:51,151 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:51:51,151 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:51:51,151 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 08:51:51,573 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5467, memsize=257.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/43339bd0f50f40adb3384f4cd91ef70d
2014-07-22 08:51:51,596 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/43339bd0f50f40adb3384f4cd91ef70d as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/43339bd0f50f40adb3384f4cd91ef70d
2014-07-22 08:51:51,618 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/43339bd0f50f40adb3384f4cd91ef70d, entries=937010, sequenceid=5467, filesize=66.7m
2014-07-22 08:51:51,618 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~257.4m/269851600, currentsize=24.4m/25559200 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 7701ms, sequenceid=5467, compaction requested=true
2014-07-22 08:51:51,619 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:51:51,619 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 2000 blocking
2014-07-22 08:51:51,619 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-22 08:51:51,619 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:51:51,619 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:51:51,619 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 08:52:12,496 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:52:12,535 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044304094 with entries=74, filesize=61.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044332497
2014-07-22 08:52:12,536 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044272634
2014-07-22 08:52:12,536 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044275101
2014-07-22 08:52:12,536 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044276564
2014-07-22 08:52:12,536 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044277953
2014-07-22 08:52:12,536 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044279838
2014-07-22 08:52:13,151 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:52:13,151 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 256.1m
2014-07-22 08:52:13,320 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:52:16,277 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:52:16,320 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28702 synced till here 28700
2014-07-22 08:52:16,396 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044332497 with entries=90, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044336277
2014-07-22 08:52:19,841 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:52:19,882 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28789 synced till here 28785
2014-07-22 08:52:19,936 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044336277 with entries=87, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044339842
2014-07-22 08:52:20,088 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:52:20,089 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 256.9m
2014-07-22 08:52:20,711 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:52:21,077 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:52:21,406 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28872 synced till here 28866
2014-07-22 08:52:21,443 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044339842 with entries=83, filesize=70.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044341078
2014-07-22 08:52:21,892 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5490, memsize=256.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/275d5782248e4d69a300ac030a8f309b
2014-07-22 08:52:21,910 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/275d5782248e4d69a300ac030a8f309b as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/275d5782248e4d69a300ac030a8f309b
2014-07-22 08:52:21,925 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/275d5782248e4d69a300ac030a8f309b, entries=932290, sequenceid=5490, filesize=66.4m
2014-07-22 08:52:21,925 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~256.1m/268491680, currentsize=93.8m/98352080 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 8774ms, sequenceid=5490, compaction requested=true
2014-07-22 08:52:21,926 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:52:21,926 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 2000 blocking
2014-07-22 08:52:21,926 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-22 08:52:21,926 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:52:21,926 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:52:21,927 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 08:52:23,225 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:52:23,244 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28951 synced till here 28946
2014-07-22 08:52:23,298 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044341078 with entries=79, filesize=64.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044343225
2014-07-22 08:52:23,298 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044283960
2014-07-22 08:52:23,298 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044285152
2014-07-22 08:52:25,104 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:52:25,133 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044343225 with entries=77, filesize=61.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044345105
2014-07-22 08:52:26,480 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:52:26,836 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044345105 with entries=82, filesize=61.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044346480
2014-07-22 08:52:27,507 DEBUG [RpcServer.handler=38,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:52:27,508 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 258.6m
2014-07-22 08:52:27,698 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:52:27,840 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:52:27,859 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29194 synced till here 29192
2014-07-22 08:52:27,890 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044346480 with entries=84, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044347840
2014-07-22 08:52:29,113 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:52:29,160 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29286 synced till here 29277
2014-07-22 08:52:29,217 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044347840 with entries=92, filesize=69.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044349113
2014-07-22 08:52:29,337 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:52:30,371 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:52:30,680 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5533, memsize=259.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/0fd1d403b3234fdaa0c0a1f6f4738c7d
2014-07-22 08:52:30,830 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29388 synced till here 29387
2014-07-22 08:52:30,832 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/0fd1d403b3234fdaa0c0a1f6f4738c7d as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/0fd1d403b3234fdaa0c0a1f6f4738c7d
2014-07-22 08:52:30,839 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044349113 with entries=102, filesize=84.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044350372
2014-07-22 08:52:30,849 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/0fd1d403b3234fdaa0c0a1f6f4738c7d, entries=946330, sequenceid=5533, filesize=67.4m
2014-07-22 08:52:30,849 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~259.9m/272535680, currentsize=194.8m/204272480 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 10760ms, sequenceid=5533, compaction requested=true
2014-07-22 08:52:30,850 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:52:30,850 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 17 store files, 0 compacting, 17 eligible, 2000 blocking
2014-07-22 08:52:30,850 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 17 files from compaction candidates
2014-07-22 08:52:30,850 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 288.2m
2014-07-22 08:52:30,850 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:52:30,851 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:52:30,851 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 08:52:31,051 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:52:31,115 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:52:32,125 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:52:32,156 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29474 synced till here 29470
2014-07-22 08:52:32,192 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044350372 with entries=86, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044352125
2014-07-22 08:52:32,192 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044289332
2014-07-22 08:52:32,192 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044291735
2014-07-22 08:52:33,508 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:52:33,585 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29560 synced till here 29553
2014-07-22 08:52:33,642 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044352125 with entries=86, filesize=69.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044353508
2014-07-22 08:52:33,747 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:52:34,853 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:52:34,995 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29644 synced till here 29637
2014-07-22 08:52:35,046 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044353508 with entries=84, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044354854
2014-07-22 08:52:37,514 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5590, memsize=239.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/777b26d3ee1f4adcb9540e9b0e4fe7ca
2014-07-22 08:52:37,538 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/777b26d3ee1f4adcb9540e9b0e4fe7ca as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/777b26d3ee1f4adcb9540e9b0e4fe7ca
2014-07-22 08:52:37,555 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/777b26d3ee1f4adcb9540e9b0e4fe7ca, entries=873370, sequenceid=5590, filesize=62.3m
2014-07-22 08:52:37,556 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~258.6m/271166560, currentsize=173.7m/182133920 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 10048ms, sequenceid=5590, compaction requested=true
2014-07-22 08:52:37,556 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:52:37,556 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 2000 blocking
2014-07-22 08:52:37,557 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-22 08:52:37,557 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 348.2m
2014-07-22 08:52:37,557 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:52:37,557 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:52:37,557 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 08:52:37,782 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:52:40,198 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5641, memsize=249.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/940c3cb031094011a899c946e382cac0
2014-07-22 08:52:40,222 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/940c3cb031094011a899c946e382cac0 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/940c3cb031094011a899c946e382cac0
2014-07-22 08:52:40,253 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/940c3cb031094011a899c946e382cac0, entries=906940, sequenceid=5641, filesize=64.6m
2014-07-22 08:52:40,253 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~290.1m/304173920, currentsize=91.5m/95967280 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 9403ms, sequenceid=5641, compaction requested=true
2014-07-22 08:52:40,254 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:52:40,254 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 2000 blocking
2014-07-22 08:52:40,255 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-22 08:52:40,255 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 287.0m
2014-07-22 08:52:40,255 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:52:40,255 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:52:40,255 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 08:52:40,417 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:52:47,321 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:52:47,363 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044354854 with entries=123, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044367322
2014-07-22 08:52:49,027 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5698, memsize=302.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/20926e947a24410d94ce732755501ed2
2014-07-22 08:52:49,047 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/20926e947a24410d94ce732755501ed2 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/20926e947a24410d94ce732755501ed2
2014-07-22 08:52:49,060 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/20926e947a24410d94ce732755501ed2, entries=1100190, sequenceid=5698, filesize=78.3m
2014-07-22 08:52:49,061 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~348.2m/365083520, currentsize=35.2m/36903680 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 11505ms, sequenceid=5698, compaction requested=true
2014-07-22 08:52:49,062 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:52:49,062 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 17 store files, 0 compacting, 17 eligible, 2000 blocking
2014-07-22 08:52:49,062 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 17 files from compaction candidates
2014-07-22 08:52:49,062 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:52:49,062 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:52:49,062 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 08:52:49,949 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:52:49,966 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29846 synced till here 29844
2014-07-22 08:52:50,001 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044367322 with entries=79, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044369949
2014-07-22 08:52:50,190 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5704, memsize=240.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/357ec2a966b9436a8630a3c0febf0aa9
2014-07-22 08:52:50,207 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/357ec2a966b9436a8630a3c0febf0aa9 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/357ec2a966b9436a8630a3c0febf0aa9
2014-07-22 08:52:50,227 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/357ec2a966b9436a8630a3c0febf0aa9, entries=876190, sequenceid=5704, filesize=62.3m
2014-07-22 08:52:50,227 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~287.0m/300972160, currentsize=53.3m/55859440 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 9973ms, sequenceid=5704, compaction requested=true
2014-07-22 08:52:50,228 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:52:50,228 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 18 store files, 0 compacting, 18 eligible, 2000 blocking
2014-07-22 08:52:50,228 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 18 files from compaction candidates
2014-07-22 08:52:50,228 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:52:50,228 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:52:50,228 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 08:52:50,793 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:52:50,930 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29939 synced till here 29937
2014-07-22 08:52:50,942 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044369949 with entries=93, filesize=70.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044370793
2014-07-22 08:52:52,095 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:52:52,095 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 257.3m
2014-07-22 08:52:52,300 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:52:52,360 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:52:52,627 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30030 synced till here 30028
2014-07-22 08:52:52,667 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044370793 with entries=91, filesize=72.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044372361
2014-07-22 08:52:54,634 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:52:54,682 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30126 synced till here 30119
2014-07-22 08:52:54,702 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044372361 with entries=96, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044374635
2014-07-22 08:52:55,949 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:52:55,981 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30211 synced till here 30209
2014-07-22 08:52:56,007 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044374635 with entries=85, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044375963
2014-07-22 08:52:56,138 DEBUG [RpcServer.handler=46,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:52:56,138 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 257.7m
2014-07-22 08:52:56,382 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:52:58,105 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:52:58,150 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30303 synced till here 30296
2014-07-22 08:52:58,214 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044375963 with entries=92, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044378106
2014-07-22 08:53:00,454 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:00,526 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30414 synced till here 30385
2014-07-22 08:53:00,820 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044378106 with entries=111, filesize=81.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044380455
2014-07-22 08:53:02,473 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:53:03,559 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:03,680 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30535 synced till here 30519
2014-07-22 08:53:05,039 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044380455 with entries=121, filesize=91.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044383562
2014-07-22 08:53:05,390 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:53:05,910 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:05,931 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30646 synced till here 30615
2014-07-22 08:53:07,542 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044383562 with entries=111, filesize=91.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044385910
2014-07-22 08:53:10,397 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:10,435 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30763 synced till here 30730
2014-07-22 08:53:10,473 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5745, memsize=231.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/2189e0072c2745de8acd60a2e6b85164
2014-07-22 08:53:10,491 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/2189e0072c2745de8acd60a2e6b85164 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/2189e0072c2745de8acd60a2e6b85164
2014-07-22 08:53:10,498 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=6603, hits=3, hitRatio=0.04%, , cachingAccesses=5, cachingHits=3, cachingHitsRatio=60.00%, evictions=0, evicted=0, evictedPerRun=NaN
2014-07-22 08:53:10,504 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/2189e0072c2745de8acd60a2e6b85164, entries=841490, sequenceid=5745, filesize=59.9m
2014-07-22 08:53:10,505 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~259.1m/271699760, currentsize=217.9m/228503040 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 18410ms, sequenceid=5745, compaction requested=true
2014-07-22 08:53:10,505 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:53:10,505 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 17 store files, 0 compacting, 17 eligible, 2000 blocking
2014-07-22 08:53:10,505 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 17 files from compaction candidates
2014-07-22 08:53:10,505 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 354.7m
2014-07-22 08:53:10,506 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:53:10,506 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:53:10,506 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 08:53:10,795 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044385910 with entries=117, filesize=95.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044390398
2014-07-22 08:53:11,232 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:53:12,418 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:53:12,732 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:12,770 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30895 synced till here 30873
2014-07-22 08:53:13,127 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044390398 with entries=132, filesize=91.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044392732
2014-07-22 08:53:14,898 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5799, memsize=200.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/5526c24522474a95a0b2a89572752c88
2014-07-22 08:53:14,910 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/5526c24522474a95a0b2a89572752c88 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/5526c24522474a95a0b2a89572752c88
2014-07-22 08:53:14,932 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/5526c24522474a95a0b2a89572752c88, entries=731560, sequenceid=5799, filesize=52.1m
2014-07-22 08:53:14,932 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~261.2m/273877040, currentsize=219.2m/229809760 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 18794ms, sequenceid=5799, compaction requested=true
2014-07-22 08:53:14,937 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:53:14,937 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 17 store files, 0 compacting, 17 eligible, 2000 blocking
2014-07-22 08:53:14,937 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 17 files from compaction candidates
2014-07-22 08:53:14,937 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:53:14,937 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 381.7m
2014-07-22 08:53:14,937 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:53:14,937 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 08:53:15,593 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:53:15,817 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:15,950 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31018 synced till here 30986
2014-07-22 08:53:16,042 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:53:16,166 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044392732 with entries=123, filesize=99.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044395818
2014-07-22 08:53:18,092 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:18,150 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31122 synced till here 31092
2014-07-22 08:53:19,615 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044395818 with entries=104, filesize=85.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044398092
2014-07-22 08:53:20,646 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:20,682 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31254 synced till here 31230
2014-07-22 08:53:21,751 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044398092 with entries=132, filesize=89.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044400647
2014-07-22 08:53:21,752 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:53:22,778 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:22,781 DEBUG [RpcServer.handler=15,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b.
2014-07-22 08:53:22,881 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31367 synced till here 31339
2014-07-22 08:53:23,841 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044400647 with entries=113, filesize=85.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044402780
2014-07-22 08:53:23,843 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:53:24,008 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5932, memsize=131.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/107915210af84b87aa7aa1fd68ac2f4e
2014-07-22 08:53:24,027 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/107915210af84b87aa7aa1fd68ac2f4e as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/107915210af84b87aa7aa1fd68ac2f4e
2014-07-22 08:53:24,039 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/107915210af84b87aa7aa1fd68ac2f4e, entries=478360, sequenceid=5932, filesize=34.1m
2014-07-22 08:53:24,039 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~375.3m/393536800, currentsize=175.6m/184108000 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 13534ms, sequenceid=5932, compaction requested=true
2014-07-22 08:53:24,040 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:53:24,040 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 19 store files, 0 compacting, 19 eligible, 2000 blocking
2014-07-22 08:53:24,040 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 428.1m
2014-07-22 08:53:24,040 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 19 files from compaction candidates
2014-07-22 08:53:24,040 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:53:24,040 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:53:24,040 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 08:53:24,916 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:24,945 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:53:24,995 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31496 synced till here 31468
2014-07-22 08:53:26,199 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044402780 with entries=129, filesize=92.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044404916
2014-07-22 08:53:26,199 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:53:27,200 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5931, memsize=131.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/71d10a7cb1514372aa3525bf0c7c1949
2014-07-22 08:53:27,219 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/71d10a7cb1514372aa3525bf0c7c1949 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/71d10a7cb1514372aa3525bf0c7c1949
2014-07-22 08:53:27,234 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/71d10a7cb1514372aa3525bf0c7c1949, entries=480130, sequenceid=5931, filesize=34.2m
2014-07-22 08:53:27,234 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~384.1m/402760640, currentsize=217.8m/228391040 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 12297ms, sequenceid=5931, compaction requested=true
2014-07-22 08:53:27,235 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:53:27,235 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 18 store files, 0 compacting, 18 eligible, 2000 blocking
2014-07-22 08:53:27,235 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 18 files from compaction candidates
2014-07-22 08:53:27,235 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:53:27,235 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 439.8m
2014-07-22 08:53:27,235 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:53:27,235 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 08:53:27,546 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:53:28,074 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:28,096 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31589 synced till here 31587
2014-07-22 08:53:28,115 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044404916 with entries=93, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044408074
2014-07-22 08:53:28,116 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:53:29,863 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:53:29,944 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:30,066 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31708 synced till here 31704
2014-07-22 08:53:30,082 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044408074 with entries=119, filesize=70.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044409945
2014-07-22 08:53:30,082 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:53:30,206 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6024, memsize=84.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/fca0b601cdf34d27a69451efca188f20
2014-07-22 08:53:30,218 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/fca0b601cdf34d27a69451efca188f20 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/fca0b601cdf34d27a69451efca188f20
2014-07-22 08:53:30,232 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/fca0b601cdf34d27a69451efca188f20, entries=306110, sequenceid=6024, filesize=21.8m
2014-07-22 08:53:30,232 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~460.2m/482605440, currentsize=92.0m/96509680 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 6192ms, sequenceid=6024, compaction requested=true
2014-07-22 08:53:30,233 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:53:30,233 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 18 store files, 0 compacting, 18 eligible, 2000 blocking
2014-07-22 08:53:30,233 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 18 files from compaction candidates
2014-07-22 08:53:30,233 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b., current region memstore size 284.9m
2014-07-22 08:53:30,233 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:53:30,233 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:53:30,233 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 08:53:30,242 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:53:30,978 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:53:31,321 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:31,450 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31808 synced till here 31806
2014-07-22 08:53:31,472 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044409945 with entries=100, filesize=69.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044411321
2014-07-22 08:53:31,723 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6064, memsize=66.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/07ee4a96f25c440b8406784eb538e55e
2014-07-22 08:53:31,737 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/07ee4a96f25c440b8406784eb538e55e as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/07ee4a96f25c440b8406784eb538e55e
2014-07-22 08:53:31,754 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/07ee4a96f25c440b8406784eb538e55e, entries=243110, sequenceid=6064, filesize=17.3m
2014-07-22 08:53:31,755 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~439.8m/461191920, currentsize=70.0m/73440720 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 4519ms, sequenceid=6064, compaction requested=true
2014-07-22 08:53:31,755 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:53:31,755 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 18 store files, 0 compacting, 18 eligible, 2000 blocking
2014-07-22 08:53:31,755 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 299.3m
2014-07-22 08:53:31,755 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 18 files from compaction candidates
2014-07-22 08:53:31,755 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:53:31,755 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:53:31,756 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 08:53:32,009 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:53:33,011 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:33,045 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31892 synced till here 31890
2014-07-22 08:53:33,092 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044411321 with entries=84, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044413011
2014-07-22 08:53:34,208 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:34,635 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32030 synced till here 32007
2014-07-22 08:53:34,802 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044413011 with entries=138, filesize=93.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044414209
2014-07-22 08:53:36,206 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:36,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32128 synced till here 32127
2014-07-22 08:53:36,505 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044414209 with entries=98, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044416207
2014-07-22 08:53:38,225 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:38,381 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32217 synced till here 32200
2014-07-22 08:53:38,560 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044416207 with entries=89, filesize=76.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044418226
2014-07-22 08:53:38,705 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6114, memsize=116.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/0bd65a8979d540339ec8d3b914a17ce1
2014-07-22 08:53:38,718 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/0bd65a8979d540339ec8d3b914a17ce1 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/0bd65a8979d540339ec8d3b914a17ce1
2014-07-22 08:53:38,769 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/0bd65a8979d540339ec8d3b914a17ce1, entries=425140, sequenceid=6114, filesize=30.3m
2014-07-22 08:53:38,770 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~300.6m/315245040, currentsize=138.1m/144812000 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 7015ms, sequenceid=6114, compaction requested=true
2014-07-22 08:53:38,770 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 20 store files, 0 compacting, 20 eligible, 2000 blocking
2014-07-22 08:53:38,771 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 20 files from compaction candidates
2014-07-22 08:53:38,771 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:53:38,771 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:53:38,771 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:53:38,771 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 08:53:38,771 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 405.4m
2014-07-22 08:53:40,121 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:53:40,183 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:40,193 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:53:40,665 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044418226 with entries=145, filesize=95.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044420184
2014-07-22 08:53:41,169 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7457, memsize=192.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/714402eb0d1f478db8de08c580b6eace
2014-07-22 08:53:41,687 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/714402eb0d1f478db8de08c580b6eace as hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/714402eb0d1f478db8de08c580b6eace
2014-07-22 08:53:41,712 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/714402eb0d1f478db8de08c580b6eace, entries=699190, sequenceid=7457, filesize=49.8m
2014-07-22 08:53:41,713 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~284.9m/298731120, currentsize=42.6m/44627520 for region usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. in 11480ms, sequenceid=7457, compaction requested=true
2014-07-22 08:53:41,713 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 2000 blocking
2014-07-22 08:53:41,713 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:53:41,713 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-22 08:53:41,713 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:53:41,714 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:53:41,714 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 294.9m
2014-07-22 08:53:41,714 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. because compaction request was cancelled
2014-07-22 08:53:41,928 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:53:42,018 DEBUG [RpcServer.handler=41,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:53:42,127 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:42,141 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32452 synced till here 32444
2014-07-22 08:53:42,208 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044420184 with entries=90, filesize=67.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044422127
2014-07-22 08:53:42,209 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044293919
2014-07-22 08:53:42,212 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044295653
2014-07-22 08:53:42,212 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044298344
2014-07-22 08:53:42,213 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044301398
2014-07-22 08:53:42,213 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044302417
2014-07-22 08:53:42,213 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044304094
2014-07-22 08:53:42,213 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044332497
2014-07-22 08:53:42,213 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044336277
2014-07-22 08:53:42,213 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044339842
2014-07-22 08:53:42,213 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044341078
2014-07-22 08:53:42,213 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044343225
2014-07-22 08:53:42,213 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044345105
2014-07-22 08:53:42,213 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044346480
2014-07-22 08:53:42,213 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044347840
2014-07-22 08:53:42,213 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044349113
2014-07-22 08:53:42,213 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044350372
2014-07-22 08:53:42,213 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044352125
2014-07-22 08:53:42,214 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044353508
2014-07-22 08:53:42,214 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044354854
2014-07-22 08:53:42,214 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044367322
2014-07-22 08:53:42,214 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044369949
2014-07-22 08:53:42,214 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044370793
2014-07-22 08:53:42,214 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044372361
2014-07-22 08:53:42,214 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044374635
2014-07-22 08:53:42,214 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044375963
2014-07-22 08:53:42,214 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044378106
2014-07-22 08:53:42,214 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044380455
2014-07-22 08:53:42,214 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044383562
2014-07-22 08:53:42,214 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044385910
2014-07-22 08:53:42,214 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044390398
2014-07-22 08:53:43,412 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:43,767 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32550 synced till here 32547
2014-07-22 08:53:44,181 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044422127 with entries=98, filesize=72.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044423412
2014-07-22 08:53:45,189 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:45,244 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32637 synced till here 32635
2014-07-22 08:53:45,674 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044423412 with entries=87, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044425190
2014-07-22 08:53:45,790 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:53:47,226 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:47,249 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044425190 with entries=75, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044427227
2014-07-22 08:53:48,366 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:48,567 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32795 synced till here 32794
2014-07-22 08:53:48,599 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044427227 with entries=83, filesize=65.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044428367
2014-07-22 08:53:50,389 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:50,418 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32871 synced till here 32870
2014-07-22 08:53:50,632 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044428367 with entries=76, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044430390
2014-07-22 08:53:52,049 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6182, memsize=192.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/f21e0f0b0a8e466a918a7ae9fe7fb6bd
2014-07-22 08:53:52,074 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/f21e0f0b0a8e466a918a7ae9fe7fb6bd as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/f21e0f0b0a8e466a918a7ae9fe7fb6bd
2014-07-22 08:53:52,101 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/f21e0f0b0a8e466a918a7ae9fe7fb6bd, entries=700680, sequenceid=6182, filesize=49.9m
2014-07-22 08:53:52,102 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~418.1m/438391200, currentsize=211.8m/222093200 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 13330ms, sequenceid=6182, compaction requested=true
2014-07-22 08:53:52,102 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:53:52,102 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 19 store files, 0 compacting, 19 eligible, 2000 blocking
2014-07-22 08:53:52,102 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 19 files from compaction candidates
2014-07-22 08:53:52,103 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:53:52,103 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:53:52,103 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 415.1m
2014-07-22 08:53:52,103 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 08:53:52,545 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:53:52,586 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:52,601 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32951 synced till here 32948
2014-07-22 08:53:52,626 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044430390 with entries=80, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044432587
2014-07-22 08:53:52,627 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044392732
2014-07-22 08:53:52,627 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044395818
2014-07-22 08:53:52,627 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044398092
2014-07-22 08:53:52,627 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044400647
2014-07-22 08:53:53,798 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6198, memsize=205.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/788c16c3163342b78650e2829d2c6090
2014-07-22 08:53:53,836 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/788c16c3163342b78650e2829d2c6090 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/788c16c3163342b78650e2829d2c6090
2014-07-22 08:53:53,855 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/788c16c3163342b78650e2829d2c6090, entries=748990, sequenceid=6198, filesize=53.3m
2014-07-22 08:53:53,855 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~294.9m/309203840, currentsize=202.2m/212057360 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 12141ms, sequenceid=6198, compaction requested=true
2014-07-22 08:53:53,856 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:53:53,856 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 19 store files, 0 compacting, 19 eligible, 2000 blocking
2014-07-22 08:53:53,856 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 19 files from compaction candidates
2014-07-22 08:53:53,856 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 381.5m
2014-07-22 08:53:53,856 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:53:53,857 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:53:53,857 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 08:53:54,071 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:54,102 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33032 synced till here 33023
2014-07-22 08:53:54,171 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044432587 with entries=81, filesize=67.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044434072
2014-07-22 08:53:54,171 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044402780
2014-07-22 08:53:54,257 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:53:54,294 DEBUG [RpcServer.handler=15,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:53:55,830 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:56,362 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33176 synced till here 33170
2014-07-22 08:53:56,368 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:53:56,405 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044434072 with entries=144, filesize=108.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044435863
2014-07-22 08:53:57,610 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:57,773 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33300 synced till here 33283
2014-07-22 08:53:58,299 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044435863 with entries=124, filesize=82.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044437610
2014-07-22 08:53:58,945 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:53:59,092 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33396 synced till here 33391
2014-07-22 08:53:59,148 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044437610 with entries=96, filesize=75.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044438945
2014-07-22 08:54:00,453 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:00,853 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33510 synced till here 33506
2014-07-22 08:54:00,976 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044438945 with entries=114, filesize=77.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044440454
2014-07-22 08:54:02,519 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:03,402 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33627 synced till here 33602
2014-07-22 08:54:03,520 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044440454 with entries=117, filesize=84.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044442520
2014-07-22 08:54:04,392 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:05,408 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33721 synced till here 33699
2014-07-22 08:54:05,546 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044442520 with entries=94, filesize=80.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044444393
2014-07-22 08:54:07,043 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:07,072 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33838 synced till here 33816
2014-07-22 08:54:07,231 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044444393 with entries=117, filesize=77.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044447044
2014-07-22 08:54:08,085 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:09,745 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33934 synced till here 33933
2014-07-22 08:54:09,767 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044447044 with entries=96, filesize=81.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044448085
2014-07-22 08:54:11,432 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:11,454 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34033 synced till here 34023
2014-07-22 08:54:11,733 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044448085 with entries=99, filesize=75.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044451432
2014-07-22 08:54:12,556 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:12,572 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34121 synced till here 34113
2014-07-22 08:54:13,602 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044451432 with entries=88, filesize=70.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044452557
2014-07-22 08:54:14,247 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6335, memsize=304.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/6cd19f0f19b24ba3856e9237113418ec
2014-07-22 08:54:14,267 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/6cd19f0f19b24ba3856e9237113418ec as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/6cd19f0f19b24ba3856e9237113418ec
2014-07-22 08:54:14,281 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/6cd19f0f19b24ba3856e9237113418ec, entries=1106980, sequenceid=6335, filesize=78.8m
2014-07-22 08:54:14,282 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~385.0m/403653520, currentsize=377.5m/395830000 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 20426ms, sequenceid=6335, compaction requested=true
2014-07-22 08:54:14,283 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 2000 blocking
2014-07-22 08:54:14,283 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-22 08:54:14,283 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:54:14,283 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:54:14,283 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 08:54:14,283 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:54:14,283 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 623.6m
2014-07-22 08:54:14,316 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:54:14,516 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:14,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34205 synced till here 34193
2014-07-22 08:54:14,669 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044452557 with entries=84, filesize=68.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044454516
2014-07-22 08:54:16,021 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:54:16,504 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:16,520 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34311 synced till here 34299
2014-07-22 08:54:16,715 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044454516 with entries=106, filesize=77.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044456504
2014-07-22 08:54:17,758 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6306, memsize=358.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/6d23d259f57147a7b4f3578b5acc3adb
2014-07-22 08:54:17,771 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/6d23d259f57147a7b4f3578b5acc3adb as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/6d23d259f57147a7b4f3578b5acc3adb
2014-07-22 08:54:17,781 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/6d23d259f57147a7b4f3578b5acc3adb, entries=1306250, sequenceid=6306, filesize=93.0m
2014-07-22 08:54:17,782 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~420.3m/440706560, currentsize=463.3m/485810720 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 25679ms, sequenceid=6306, compaction requested=true
2014-07-22 08:54:17,782 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:54:17,782 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 19 store files, 0 compacting, 19 eligible, 2000 blocking
2014-07-22 08:54:17,782 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 19 files from compaction candidates
2014-07-22 08:54:17,782 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 626.3m
2014-07-22 08:54:17,783 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:54:17,783 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:54:17,783 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 08:54:18,002 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:54:18,346 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:54:18,354 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:19,713 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34420 synced till here 34411
2014-07-22 08:54:19,774 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044456504 with entries=109, filesize=98.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044458354
2014-07-22 08:54:19,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044404916
2014-07-22 08:54:19,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044408074
2014-07-22 08:54:20,598 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:20,643 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044458354 with entries=79, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044460598
2014-07-22 08:54:22,051 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:22,101 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34597 synced till here 34595
2014-07-22 08:54:22,131 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044460598 with entries=98, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044462052
2014-07-22 08:54:23,465 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:23,487 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34689 synced till here 34686
2014-07-22 08:54:23,534 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044462052 with entries=92, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044463466
2014-07-22 08:54:25,019 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:25,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34781 synced till here 34771
2014-07-22 08:54:25,149 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044463466 with entries=92, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044465019
2014-07-22 08:54:26,361 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:26,392 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044465019 with entries=78, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044466361
2014-07-22 08:54:28,856 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:28,878 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34939 synced till here 34935
2014-07-22 08:54:28,942 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044466361 with entries=80, filesize=65.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044468856
2014-07-22 08:54:28,944 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:54:29,924 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:29,943 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35023 synced till here 35017
2014-07-22 08:54:30,475 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044468856 with entries=84, filesize=68.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044469925
2014-07-22 08:54:30,476 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:54:31,461 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:31,481 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35121 synced till here 35101
2014-07-22 08:54:33,139 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1168ms
GC pool 'ParNew' had collection(s): count=1 time=1550ms
2014-07-22 08:54:33,180 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044469925 with entries=98, filesize=71.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044471463
2014-07-22 08:54:33,180 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:54:33,998 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:34,018 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35230 synced till here 35217
2014-07-22 08:54:34,125 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044471463 with entries=109, filesize=76.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044473999
2014-07-22 08:54:34,126 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:54:35,895 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:35,921 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35321 synced till here 35307
2014-07-22 08:54:37,444 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1304ms
GC pool 'ParNew' had collection(s): count=1 time=1273ms
2014-07-22 08:54:37,449 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044473999 with entries=91, filesize=75.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044475896
2014-07-22 08:54:37,450 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:54:38,305 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:38,362 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35422 synced till here 35407
2014-07-22 08:54:39,438 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044475896 with entries=101, filesize=74.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044478306
2014-07-22 08:54:39,439 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:54:40,203 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6547, memsize=322.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/eebbbb720a174711864d79fa5ffd25c6
2014-07-22 08:54:40,235 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/eebbbb720a174711864d79fa5ffd25c6 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/eebbbb720a174711864d79fa5ffd25c6
2014-07-22 08:54:40,348 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/eebbbb720a174711864d79fa5ffd25c6, entries=1174430, sequenceid=6547, filesize=83.6m
2014-07-22 08:54:40,348 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~629.1m/659650560, currentsize=425.9m/446571680 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 26065ms, sequenceid=6547, compaction requested=true
2014-07-22 08:54:40,349 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:54:40,349 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 20 store files, 0 compacting, 20 eligible, 2000 blocking
2014-07-22 08:54:40,350 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 825.3m
2014-07-22 08:54:40,350 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 20 files from compaction candidates
2014-07-22 08:54:40,350 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:54:40,350 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:54:40,350 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 08:54:40,417 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:40,426 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:54:40,480 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35524 synced till here 35500
2014-07-22 08:54:41,643 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044478306 with entries=102, filesize=84.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044480418
2014-07-22 08:54:41,644 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:54:41,826 DEBUG [RpcServer.handler=5,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b.
2014-07-22 08:54:42,339 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:54:42,439 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:42,494 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35625 synced till here 35605
2014-07-22 08:54:43,701 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044480418 with entries=101, filesize=77.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044482440
2014-07-22 08:54:43,705 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:54:43,788 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6566, memsize=324.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/7dd83e25325d48ae9c7f16925e709ab7
2014-07-22 08:54:43,809 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/7dd83e25325d48ae9c7f16925e709ab7 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/7dd83e25325d48ae9c7f16925e709ab7
2014-07-22 08:54:43,820 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/7dd83e25325d48ae9c7f16925e709ab7, entries=1180800, sequenceid=6566, filesize=84.1m
2014-07-22 08:54:43,820 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~635.3m/666132320, currentsize=418.0m/438342640 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 26038ms, sequenceid=6566, compaction requested=true
2014-07-22 08:54:43,821 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:54:43,821 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 20 store files, 0 compacting, 20 eligible, 2000 blocking
2014-07-22 08:54:43,821 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 901.7m
2014-07-22 08:54:43,821 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 20 files from compaction candidates
2014-07-22 08:54:43,821 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:54:43,821 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:54:43,821 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 08:54:43,933 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:54:44,461 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:44,522 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35732 synced till here 35706
2014-07-22 08:54:45,710 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1035ms
GC pool 'ParNew' had collection(s): count=1 time=1129ms
2014-07-22 08:54:45,912 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044482440 with entries=107, filesize=87.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044484461
2014-07-22 08:54:45,913 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:54:46,271 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:54:47,976 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:48,052 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35842 synced till here 35811
2014-07-22 08:54:48,330 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044484461 with entries=110, filesize=96.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044487976
2014-07-22 08:54:48,332 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:54:50,186 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1242ms
GC pool 'ParNew' had collection(s): count=1 time=1311ms
2014-07-22 08:54:50,488 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:50,541 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35948 synced till here 35926
2014-07-22 08:54:50,798 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044487976 with entries=106, filesize=93.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044490488
2014-07-22 08:54:50,798 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:54:51,876 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:52,447 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36027 synced till here 36025
2014-07-22 08:54:52,468 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044490488 with entries=79, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044491876
2014-07-22 08:54:52,468 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=44, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:54:53,251 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:54,279 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36117 synced till here 36115
2014-07-22 08:54:54,303 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044491876 with entries=90, filesize=76.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044493252
2014-07-22 08:54:54,303 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=45, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:54:55,173 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:55,210 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36206 synced till here 36190
2014-07-22 08:54:55,281 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044493252 with entries=89, filesize=67.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044495174
2014-07-22 08:54:55,282 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=46, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:54:56,680 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:56,698 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36288 synced till here 36286
2014-07-22 08:54:56,787 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044495174 with entries=82, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044496681
2014-07-22 08:54:56,787 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=47, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:54:58,305 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1203ms
GC pool 'ParNew' had collection(s): count=1 time=1298ms
2014-07-22 08:54:59,003 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:54:59,199 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:54:59,206 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:54:59,221 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:54:59,222 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:54:59,270 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:54:59,290 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:54:59,310 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:54:59,312 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:54:59,331 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:54:59,341 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:54:59,341 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:54:59,347 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:54:59,349 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:54:59,387 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:54:59,428 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:54:59,460 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:54:59,462 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:54:59,462 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:54:59,463 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:54:59,463 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:54:59,523 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:54:59,527 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:54:59,576 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:54:59,619 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:54:59,624 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:54:59,660 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:54:59,693 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:54:59,694 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:54:59,700 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044496681 with entries=103, filesize=83.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044499004
2014-07-22 08:54:59,700 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=48, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:54:59,741 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:00,669 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:00,684 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:00,708 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:00,742 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:00,778 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:00,809 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:00,810 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:00,845 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:00,880 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:00,882 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:00,916 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:00,916 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:00,918 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:00,919 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:00,919 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:00,953 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:00,954 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:01,423 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:01,423 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:01,423 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:01,426 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:04,199 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:55:04,206 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:55:04,222 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:55:04,223 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:55:04,270 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:55:04,291 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:55:04,310 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:55:04,312 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:55:04,332 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:55:04,341 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:55:04,341 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:55:04,347 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:55:04,350 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:55:04,387 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:55:04,428 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:55:04,460 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:55:04,462 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:55:04,463 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:55:04,463 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:55:04,463 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:55:04,523 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:55:04,527 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:55:04,577 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:55:04,619 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:55:04,624 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:55:04,661 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:55:04,694 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:55:04,694 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:55:04,742 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:55:05,670 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:55:05,685 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:55:05,708 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:55:05,743 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:55:05,779 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:55:05,810 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:55:05,811 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:55:05,845 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:55:05,881 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:55:05,882 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:55:05,916 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:55:05,917 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:55:05,918 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:55:05,919 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:55:05,920 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:55:05,953 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:55:05,954 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:55:07,547 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6121ms
2014-07-22 08:55:07,548 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6125ms
2014-07-22 08:55:07,548 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6126ms
2014-07-22 08:55:07,549 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6125ms
2014-07-22 08:55:08,159 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6821, memsize=372.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/09e08a36426f49bdb5e678b50a4602ec
2014-07-22 08:55:08,171 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/09e08a36426f49bdb5e678b50a4602ec as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/09e08a36426f49bdb5e678b50a4602ec
2014-07-22 08:55:08,179 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/09e08a36426f49bdb5e678b50a4602ec, entries=1355840, sequenceid=6821, filesize=96.6m
2014-07-22 08:55:08,180 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~838.9m/879610240, currentsize=297.6m/312083360 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 27831ms, sequenceid=6821, compaction requested=true
2014-07-22 08:55:08,180 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:55:08,181 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6758ms
2014-07-22 08:55:08,181 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 2000 blocking
2014-07-22 08:55:08,181 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,181 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-22 08:55:08,181 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6759ms
2014-07-22 08:55:08,181 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:55:08,181 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b., current region memstore size 298.1m
2014-07-22 08:55:08,181 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:55:08,181 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,182 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 08:55:08,182 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6759ms
2014-07-22 08:55:08,182 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,182 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6756ms
2014-07-22 08:55:08,182 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,182 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7228ms
2014-07-22 08:55:08,182 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,182 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7229ms
2014-07-22 08:55:08,182 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,182 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7263ms
2014-07-22 08:55:08,182 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,182 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7263ms
2014-07-22 08:55:08,183 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,183 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7265ms
2014-07-22 08:55:08,183 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,183 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7267ms
2014-07-22 08:55:08,183 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,183 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7267ms
2014-07-22 08:55:08,185 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,185 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7303ms
2014-07-22 08:55:08,185 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,188 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7308ms
2014-07-22 08:55:08,188 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,189 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7344ms
2014-07-22 08:55:08,189 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,189 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7379ms
2014-07-22 08:55:08,189 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,189 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7380ms
2014-07-22 08:55:08,189 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,189 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7411ms
2014-07-22 08:55:08,189 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,192 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7450ms
2014-07-22 08:55:08,192 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,192 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7484ms
2014-07-22 08:55:08,192 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,193 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7509ms
2014-07-22 08:55:08,193 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,193 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7524ms
2014-07-22 08:55:08,193 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,198 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8457ms
2014-07-22 08:55:08,198 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,201 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8504ms
2014-07-22 08:55:08,201 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,205 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8512ms
2014-07-22 08:55:08,206 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,206 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8546ms
2014-07-22 08:55:08,207 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,214 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8589ms
2014-07-22 08:55:08,214 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,215 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8595ms
2014-07-22 08:55:08,215 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,215 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8639ms
2014-07-22 08:55:08,215 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,216 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8688ms
2014-07-22 08:55:08,216 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,217 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8693ms
2014-07-22 08:55:08,217 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,218 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8754ms
2014-07-22 08:55:08,218 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,218 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8756ms
2014-07-22 08:55:08,219 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,219 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8757ms
2014-07-22 08:55:08,220 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,220 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8758ms
2014-07-22 08:55:08,221 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,221 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8761ms
2014-07-22 08:55:08,221 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,225 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8797ms
2014-07-22 08:55:08,225 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,225 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8838ms
2014-07-22 08:55:08,225 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,226 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8876ms
2014-07-22 08:55:08,226 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,226 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8879ms
2014-07-22 08:55:08,226 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,226 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8886ms
2014-07-22 08:55:08,226 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,227 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8886ms
2014-07-22 08:55:08,227 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,233 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8902ms
2014-07-22 08:55:08,233 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,234 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8923ms
2014-07-22 08:55:08,234 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,245 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8936ms
2014-07-22 08:55:08,245 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,246 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8955ms
2014-07-22 08:55:08,246 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,246 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8977ms
2014-07-22 08:55:08,246 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,253 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9031ms
2014-07-22 08:55:08,253 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,258 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9037ms
2014-07-22 08:55:08,258 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,259 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9052ms
2014-07-22 08:55:08,259 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,259 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9060ms
2014-07-22 08:55:08,259 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:08,499 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:55:08,621 DEBUG [RpcServer.handler=6,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:55:08,886 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10052,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044498833,"queuetimems":1,"class":"HRegionServer","responsesize":18803,"method":"Multi"}
2014-07-22 08:55:08,886 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10250,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044498635,"queuetimems":1,"class":"HRegionServer","responsesize":18487,"method":"Multi"}
2014-07-22 08:55:09,137 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:55:09,146 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10462,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044498683,"queuetimems":0,"class":"HRegionServer","responsesize":16938,"method":"Multi"}
2014-07-22 08:55:09,238 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36518 synced till here 36488
2014-07-22 08:55:09,332 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10393,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044498938,"queuetimems":0,"class":"HRegionServer","responsesize":18668,"method":"Multi"}
2014-07-22 08:55:10,350 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11466,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044498883,"queuetimems":1,"class":"HRegionServer","responsesize":18315,"method":"Multi"}
2014-07-22 08:55:10,350 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11338,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044499011,"queuetimems":0,"class":"HRegionServer","responsesize":18879,"method":"Multi"}
2014-07-22 08:55:10,378 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044499004 with entries=127, filesize=85.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044509137
2014-07-22 08:55:10,653 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11488,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044499165,"queuetimems":0,"class":"HRegionServer","responsesize":18502,"method":"Multi"}
2014-07-22 08:55:11,135 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10431,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044500704,"queuetimems":0,"class":"HRegionServer","responsesize":18630,"method":"Multi"}
2014-07-22 08:55:11,135 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12045,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044499089,"queuetimems":1,"class":"HRegionServer","responsesize":16983,"method":"Multi"}
2014-07-22 08:55:11,139 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10258,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044500880,"queuetimems":1,"class":"HRegionServer","responsesize":18758,"method":"Multi"}
2014-07-22 08:55:11,146 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10372,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044500774,"queuetimems":0,"class":"HRegionServer","responsesize":18435,"method":"Multi"}
2014-07-22 08:55:11,147 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10338,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044500808,"queuetimems":0,"class":"HRegionServer","responsesize":18313,"method":"Multi"}
2014-07-22 08:55:11,146 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10464,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044500682,"queuetimems":1,"class":"HRegionServer","responsesize":17974,"method":"Multi"}
2014-07-22 08:55:11,148 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10481,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044500666,"queuetimems":0,"class":"HRegionServer","responsesize":18508,"method":"Multi"}
2014-07-22 08:55:11,147 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10407,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044500739,"queuetimems":0,"class":"HRegionServer","responsesize":18843,"method":"Multi"}
2014-07-22 08:55:11,239 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:55:11,320 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36626 synced till here 36588
2014-07-22 08:55:11,406 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10562,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044500843,"queuetimems":1,"class":"HRegionServer","responsesize":17017,"method":"Multi"}
2014-07-22 08:55:11,406 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10455,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044500951,"queuetimems":0,"class":"HRegionServer","responsesize":16730,"method":"Multi"}
2014-07-22 08:55:12,364 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11449,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044500915,"queuetimems":0,"class":"HRegionServer","responsesize":18440,"method":"Multi"}
2014-07-22 08:55:12,378 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12641,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044499736,"queuetimems":1,"class":"HRegionServer","responsesize":18886,"method":"Multi"}
2014-07-22 08:55:12,378 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12995,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044499383,"queuetimems":0,"class":"HRegionServer","responsesize":18834,"method":"Multi"}
2014-07-22 08:55:12,391 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12868,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044499523,"queuetimems":1,"class":"HRegionServer","responsesize":17996,"method":"Multi"}
2014-07-22 08:55:12,394 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044509137 with entries=108, filesize=94.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044511239
2014-07-22 08:55:12,415 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12725,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044499690,"queuetimems":0,"class":"HRegionServer","responsesize":15226,"method":"Multi"}
2014-07-22 08:55:12,430 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13142,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044499287,"queuetimems":1,"class":"HRegionServer","responsesize":15606,"method":"Multi"}
2014-07-22 08:55:12,430 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13005,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044499425,"queuetimems":0,"class":"HRegionServer","responsesize":18561,"method":"Multi"}
2014-07-22 08:55:12,516 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12858,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044499657,"queuetimems":1,"class":"HRegionServer","responsesize":16945,"method":"Multi"}
2014-07-22 08:55:12,566 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12993,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044499572,"queuetimems":0,"class":"HRegionServer","responsesize":18576,"method":"Multi"}
2014-07-22 08:55:12,578 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13359,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044499218,"queuetimems":1,"class":"HRegionServer","responsesize":18469,"method":"Multi"}
2014-07-22 08:55:12,770 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13427,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044499342,"queuetimems":1,"class":"HRegionServer","responsesize":17304,"method":"Multi"}
2014-07-22 08:55:13,008 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13549,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044499458,"queuetimems":0,"class":"HRegionServer","responsesize":15918,"method":"Multi"}
2014-07-22 08:55:13,009 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13390,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044499618,"queuetimems":0,"class":"HRegionServer","responsesize":18733,"method":"Multi"}
2014-07-22 08:55:13,289 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:55:14,204 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36753 synced till here 36719
2014-07-22 08:55:14,593 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6834, memsize=388.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/a94b9acd5a0540b2a42eb19776c9a123
2014-07-22 08:55:14,607 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/a94b9acd5a0540b2a42eb19776c9a123 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/a94b9acd5a0540b2a42eb19776c9a123
2014-07-22 08:55:14,758 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044511239 with entries=127, filesize=100.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044513289
2014-07-22 08:55:14,808 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/a94b9acd5a0540b2a42eb19776c9a123, entries=1415170, sequenceid=6834, filesize=100.8m
2014-07-22 08:55:14,808 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~917.9m/962471360, currentsize=387.8m/406648160 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 30987ms, sequenceid=6834, compaction requested=true
2014-07-22 08:55:14,808 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:55:14,809 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 20 store files, 0 compacting, 20 eligible, 2000 blocking
2014-07-22 08:55:14,809 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 20 files from compaction candidates
2014-07-22 08:55:14,809 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 860.7m
2014-07-22 08:55:14,809 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:55:14,809 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:55:14,809 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 08:55:15,171 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:55:16,432 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:55:16,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36869 synced till here 36831
2014-07-22 08:55:16,957 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044513289 with entries=116, filesize=101.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044516433
2014-07-22 08:55:17,085 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:55:18,188 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:55:18,222 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044516433 with entries=86, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044518188
2014-07-22 08:55:20,450 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1635ms
GC pool 'ParNew' had collection(s): count=1 time=1689ms
2014-07-22 08:55:21,919 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:55:21,954 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37044 synced till here 37042
2014-07-22 08:55:22,001 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044518188 with entries=89, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044521920
2014-07-22 08:55:23,171 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8559, memsize=211.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/6e1b23e9331e42719653f1fbb42893be
2014-07-22 08:55:23,195 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/6e1b23e9331e42719653f1fbb42893be as hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/6e1b23e9331e42719653f1fbb42893be
2014-07-22 08:55:23,210 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/6e1b23e9331e42719653f1fbb42893be, entries=771040, sequenceid=8559, filesize=54.9m
2014-07-22 08:55:23,211 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~298.1m/312547760, currentsize=42.2m/44244160 for region usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. in 15030ms, sequenceid=8559, compaction requested=true
2014-07-22 08:55:23,211 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:55:23,211 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 2000 blocking
2014-07-22 08:55:23,211 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 937.1m
2014-07-22 08:55:23,211 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-22 08:55:23,211 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:55:23,212 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:55:23,212 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. because compaction request was cancelled
2014-07-22 08:55:23,523 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:55:23,549 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044521920 with entries=85, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044523524
2014-07-22 08:55:23,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044409945
2014-07-22 08:55:23,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044411321
2014-07-22 08:55:23,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044413011
2014-07-22 08:55:23,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044414209
2014-07-22 08:55:23,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044416207
2014-07-22 08:55:23,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044418226
2014-07-22 08:55:23,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044420184
2014-07-22 08:55:23,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044422127
2014-07-22 08:55:23,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044423412
2014-07-22 08:55:23,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044425190
2014-07-22 08:55:23,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044427227
2014-07-22 08:55:23,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044428367
2014-07-22 08:55:23,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044430390
2014-07-22 08:55:23,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044432587
2014-07-22 08:55:23,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044434072
2014-07-22 08:55:23,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044435863
2014-07-22 08:55:23,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044437610
2014-07-22 08:55:23,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044438945
2014-07-22 08:55:23,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044440454
2014-07-22 08:55:23,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044442520
2014-07-22 08:55:23,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044444393
2014-07-22 08:55:23,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044447044
2014-07-22 08:55:23,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044448085
2014-07-22 08:55:23,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044451432
2014-07-22 08:55:24,633 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:55:25,125 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:55:25,436 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37220 synced till here 37218
2014-07-22 08:55:25,475 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044523524 with entries=91, filesize=73.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044525125
2014-07-22 08:55:27,283 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:55:27,538 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37332 synced till here 37329
2014-07-22 08:55:27,575 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044525125 with entries=112, filesize=81.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044527283
2014-07-22 08:55:28,889 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:55:28,947 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37425 synced till here 37420
2014-07-22 08:55:29,002 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044527283 with entries=93, filesize=69.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044528890
2014-07-22 08:55:30,626 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:55:30,676 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37520 synced till here 37516
2014-07-22 08:55:30,735 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044528890 with entries=95, filesize=67.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044530627
2014-07-22 08:55:31,883 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:55:31,973 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37625 synced till here 37610
2014-07-22 08:55:32,074 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044530627 with entries=105, filesize=71.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044531884
2014-07-22 08:55:33,332 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:55:33,356 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37730 synced till here 37722
2014-07-22 08:55:33,446 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044531884 with entries=105, filesize=68.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044533333
2014-07-22 08:55:34,240 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:55:34,278 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37817 synced till here 37815
2014-07-22 08:55:34,338 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044533333 with entries=87, filesize=65.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044534241
2014-07-22 08:55:35,488 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:35,489 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:35,495 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:35,517 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:35,551 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:35,554 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:35,555 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:35,577 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:55:35,584 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:35,588 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:35,591 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:35,598 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:35,599 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:35,605 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:35,605 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:35,608 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:35,610 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:35,629 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044534241 with entries=82, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044535577
2014-07-22 08:55:35,653 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:35,690 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:35,729 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:35,769 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:35,813 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:35,860 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:35,915 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:35,976 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:35,976 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:35,977 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:36,007 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:36,079 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:36,094 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:36,829 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:36,829 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:36,830 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:36,956 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:36,957 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:36,957 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:37,114 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:37,252 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:37,360 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:37,427 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:37,477 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:37,525 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:37,528 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:37,573 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:37,574 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:37,574 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:37,578 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:37,578 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:37,579 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:37,579 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:37,634 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:55:38,971 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7061, memsize=327.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/9b21ea88d3594595b4584466d7800ad6
2014-07-22 08:55:38,994 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/9b21ea88d3594595b4584466d7800ad6 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/9b21ea88d3594595b4584466d7800ad6
2014-07-22 08:55:39,007 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/9b21ea88d3594595b4584466d7800ad6, entries=1191570, sequenceid=7061, filesize=84.9m
2014-07-22 08:55:39,008 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~890.5m/933737040, currentsize=350.1m/367098000 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 24199ms, sequenceid=7061, compaction requested=true
2014-07-22 08:55:39,008 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:55:39,008 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 2000 blocking
2014-07-22 08:55:39,008 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-22 08:55:39,008 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:55:39,009 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:55:39,009 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1376ms
2014-07-22 08:55:39,009 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 783.9m
2014-07-22 08:55:39,009 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,009 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 08:55:39,010 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1430ms
2014-07-22 08:55:39,010 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,010 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1432ms
2014-07-22 08:55:39,010 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,011 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1433ms
2014-07-22 08:55:39,011 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,011 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1434ms
2014-07-22 08:55:39,011 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,012 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1439ms
2014-07-22 08:55:39,012 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,012 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1439ms
2014-07-22 08:55:39,012 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,012 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1439ms
2014-07-22 08:55:39,012 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,013 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1485ms
2014-07-22 08:55:39,013 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,014 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1489ms
2014-07-22 08:55:39,014 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,014 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1537ms
2014-07-22 08:55:39,014 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,015 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1588ms
2014-07-22 08:55:39,015 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,017 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1658ms
2014-07-22 08:55:39,017 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,023 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1772ms
2014-07-22 08:55:39,023 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,026 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1911ms
2014-07-22 08:55:39,026 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,026 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2070ms
2014-07-22 08:55:39,026 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,027 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2071ms
2014-07-22 08:55:39,027 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,027 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2071ms
2014-07-22 08:55:39,027 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,027 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2197ms
2014-07-22 08:55:39,027 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,030 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2200ms
2014-07-22 08:55:39,031 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,031 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2202ms
2014-07-22 08:55:39,031 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,031 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2937ms
2014-07-22 08:55:39,031 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,031 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2952ms
2014-07-22 08:55:39,031 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,031 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3024ms
2014-07-22 08:55:39,031 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,032 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3055ms
2014-07-22 08:55:39,032 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,033 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3056ms
2014-07-22 08:55:39,033 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,036 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3060ms
2014-07-22 08:55:39,036 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,037 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3122ms
2014-07-22 08:55:39,037 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,037 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3177ms
2014-07-22 08:55:39,038 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,038 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3225ms
2014-07-22 08:55:39,038 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,043 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3274ms
2014-07-22 08:55:39,043 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,044 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3314ms
2014-07-22 08:55:39,044 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,045 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3354ms
2014-07-22 08:55:39,045 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,045 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3392ms
2014-07-22 08:55:39,046 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,048 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3438ms
2014-07-22 08:55:39,048 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,052 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3444ms
2014-07-22 08:55:39,052 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,053 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3447ms
2014-07-22 08:55:39,053 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,053 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3448ms
2014-07-22 08:55:39,053 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,053 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3454ms
2014-07-22 08:55:39,054 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,054 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3456ms
2014-07-22 08:55:39,054 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,054 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3463ms
2014-07-22 08:55:39,054 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,054 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3466ms
2014-07-22 08:55:39,054 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,054 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3470ms
2014-07-22 08:55:39,055 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,055 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3500ms
2014-07-22 08:55:39,055 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,060 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3506ms
2014-07-22 08:55:39,061 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,062 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3510ms
2014-07-22 08:55:39,062 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,062 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3545ms
2014-07-22 08:55:39,062 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,070 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3574ms
2014-07-22 08:55:39,070 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,070 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3582ms
2014-07-22 08:55:39,071 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,077 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3589ms
2014-07-22 08:55:39,077 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:55:39,354 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:55:41,150 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1322ms
GC pool 'ParNew' had collection(s): count=1 time=1584ms
2014-07-22 08:55:41,462 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:55:41,514 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:55:41,627 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38049 synced till here 38024
2014-07-22 08:55:42,428 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044535577 with entries=150, filesize=86.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044541462
2014-07-22 08:55:42,428 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044452557
2014-07-22 08:55:42,428 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044454516
2014-07-22 08:55:44,754 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:55:44,963 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38168 synced till here 38164
2014-07-22 08:55:45,039 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044541462 with entries=119, filesize=92.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044544754
2014-07-22 08:55:46,925 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10849,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044536076,"queuetimems":0,"class":"HRegionServer","responsesize":17302,"method":"Multi"}
2014-07-22 08:55:46,950 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11354,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044535596,"queuetimems":0,"class":"HRegionServer","responsesize":18713,"method":"Multi"}
2014-07-22 08:55:46,950 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10976,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044535974,"queuetimems":0,"class":"HRegionServer","responsesize":18211,"method":"Multi"}
2014-07-22 08:55:46,950 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11300,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044535650,"queuetimems":0,"class":"HRegionServer","responsesize":18969,"method":"Multi"}
2014-07-22 08:55:46,966 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11201,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044535765,"queuetimems":1,"class":"HRegionServer","responsesize":18243,"method":"Multi"}
2014-07-22 08:55:46,966 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11157,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044535809,"queuetimems":0,"class":"HRegionServer","responsesize":18328,"method":"Multi"}
2014-07-22 08:55:46,970 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11456,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044535513,"queuetimems":0,"class":"HRegionServer","responsesize":18277,"method":"Multi"}
2014-07-22 08:55:46,974 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11286,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044535687,"queuetimems":0,"class":"HRegionServer","responsesize":18362,"method":"Multi"}
2014-07-22 08:55:46,970 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11112,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044535857,"queuetimems":0,"class":"HRegionServer","responsesize":18235,"method":"Multi"}
2014-07-22 08:55:46,974 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11250,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044535724,"queuetimems":0,"class":"HRegionServer","responsesize":18295,"method":"Multi"}
2014-07-22 08:55:47,346 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:55:47,390 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11477,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044535912,"queuetimems":1,"class":"HRegionServer","responsesize":16712,"method":"Multi"}
2014-07-22 08:55:47,439 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38285 synced till here 38268
2014-07-22 08:55:48,748 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1042ms
GC pool 'ParNew' had collection(s): count=1 time=1138ms
2014-07-22 08:55:48,775 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044544754 with entries=117, filesize=89.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044547346
2014-07-22 08:55:51,027 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:55:51,056 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38426 synced till here 38395
2014-07-22 08:55:51,082 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7113, memsize=342.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/64db3a24b7d74b2ba5fcde302f5bb19d
2014-07-22 08:55:51,096 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/64db3a24b7d74b2ba5fcde302f5bb19d as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/64db3a24b7d74b2ba5fcde302f5bb19d
2014-07-22 08:55:51,107 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/64db3a24b7d74b2ba5fcde302f5bb19d, entries=1248620, sequenceid=7113, filesize=89.0m
2014-07-22 08:55:51,108 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~949.1m/995210080, currentsize=371.3m/389360000 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 27897ms, sequenceid=7113, compaction requested=true
2014-07-22 08:55:51,109 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:55:51,109 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 2000 blocking
2014-07-22 08:55:51,109 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-22 08:55:51,109 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 886.8m
2014-07-22 08:55:51,109 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:55:51,109 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:55:51,109 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 08:55:51,137 DEBUG [RpcServer.handler=30,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:55:51,198 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044547346 with entries=141, filesize=95.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044551027
2014-07-22 08:55:51,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044456504
2014-07-22 08:55:51,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044458354
2014-07-22 08:55:51,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044460598
2014-07-22 08:55:51,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044462052
2014-07-22 08:55:51,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044463466
2014-07-22 08:55:51,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044465019
2014-07-22 08:55:51,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044466361
2014-07-22 08:55:51,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044468856
2014-07-22 08:55:51,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044469925
2014-07-22 08:55:51,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044471463
2014-07-22 08:55:51,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044473999
2014-07-22 08:55:51,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044475896
2014-07-22 08:55:52,163 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:55:52,171 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:55:53,272 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38533 synced till here 38503
2014-07-22 08:55:53,560 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044551027 with entries=107, filesize=87.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044552171
2014-07-22 08:55:54,303 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:55:55,172 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38645 synced till here 38623
2014-07-22 08:55:55,447 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044552171 with entries=112, filesize=83.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044554303
2014-07-22 08:55:56,308 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:55:56,445 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38743 synced till here 38717
2014-07-22 08:55:58,057 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044554303 with entries=98, filesize=75.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044556309
2014-07-22 08:55:58,823 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:55:58,853 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38860 synced till here 38832
2014-07-22 08:56:00,277 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044556309 with entries=117, filesize=90.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044558823
2014-07-22 08:56:01,044 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:01,066 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38957 synced till here 38949
2014-07-22 08:56:01,120 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044558823 with entries=97, filesize=67.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044561044
2014-07-22 08:56:03,726 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:03,754 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39037 synced till here 39034
2014-07-22 08:56:03,818 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044561044 with entries=80, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044563726
2014-07-22 08:56:05,823 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:05,841 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39133 synced till here 39120
2014-07-22 08:56:05,923 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044563726 with entries=96, filesize=72.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044565823
2014-07-22 08:56:07,494 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:07,653 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7276, memsize=297.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/1446ce084d054860b3abb7de50336483
2014-07-22 08:56:07,661 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39256 synced till here 39238
2014-07-22 08:56:07,666 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/1446ce084d054860b3abb7de50336483 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/1446ce084d054860b3abb7de50336483
2014-07-22 08:56:07,697 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/1446ce084d054860b3abb7de50336483, entries=1083100, sequenceid=7276, filesize=77.1m
2014-07-22 08:56:07,698 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~783.9m/821991840, currentsize=425.3m/445970160 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 28689ms, sequenceid=7276, compaction requested=true
2014-07-22 08:56:07,699 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:56:07,699 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 2000 blocking
2014-07-22 08:56:07,699 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 770.2m
2014-07-22 08:56:07,700 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-22 08:56:07,700 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:56:07,700 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:56:07,700 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 08:56:07,849 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:56:07,860 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044565823 with entries=123, filesize=80.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044567638
2014-07-22 08:56:07,860 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044478306
2014-07-22 08:56:07,860 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044480418
2014-07-22 08:56:09,370 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:56:09,501 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:09,522 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39345 synced till here 39342
2014-07-22 08:56:09,575 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044567638 with entries=89, filesize=65.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044569501
2014-07-22 08:56:11,487 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:11,511 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39439 synced till here 39434
2014-07-22 08:56:11,551 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044569501 with entries=94, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044571488
2014-07-22 08:56:12,547 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:12,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39531 synced till here 39529
2014-07-22 08:56:12,602 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044571488 with entries=92, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044572547
2014-07-22 08:56:13,871 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:13,886 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39627 synced till here 39624
2014-07-22 08:56:13,935 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044572547 with entries=96, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044573871
2014-07-22 08:56:15,073 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:15,096 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39708 synced till here 39705
2014-07-22 08:56:15,148 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044573871 with entries=81, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044575073
2014-07-22 08:56:16,582 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:16,605 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39793 synced till here 39788
2014-07-22 08:56:17,053 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044575073 with entries=85, filesize=66.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044576583
2014-07-22 08:56:17,538 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7367, memsize=332.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/298b2a2d296349d7b9723f8009cc6a66
2014-07-22 08:56:18,086 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/298b2a2d296349d7b9723f8009cc6a66 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/298b2a2d296349d7b9723f8009cc6a66
2014-07-22 08:56:18,128 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/298b2a2d296349d7b9723f8009cc6a66, entries=1211830, sequenceid=7367, filesize=86.3m
2014-07-22 08:56:18,142 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~917.8m/962411600, currentsize=437.3m/458518560 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 27033ms, sequenceid=7367, compaction requested=true
2014-07-22 08:56:18,143 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 2000 blocking
2014-07-22 08:56:18,143 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-22 08:56:18,143 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:56:18,143 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:56:18,143 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 08:56:18,143 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:56:18,144 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 857.1m
2014-07-22 08:56:18,154 DEBUG [RpcServer.handler=6,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:56:18,481 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:18,506 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39898 synced till here 39879
2014-07-22 08:56:18,601 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044576583 with entries=105, filesize=76.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044578482
2014-07-22 08:56:18,601 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044482440
2014-07-22 08:56:18,601 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044484461
2014-07-22 08:56:18,601 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044487976
2014-07-22 08:56:18,601 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044490488
2014-07-22 08:56:18,602 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044491876
2014-07-22 08:56:18,602 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044493252
2014-07-22 08:56:18,602 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044495174
2014-07-22 08:56:18,602 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044496681
2014-07-22 08:56:18,648 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:56:19,079 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:56:20,001 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:20,018 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40000 synced till here 39993
2014-07-22 08:56:20,101 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044578482 with entries=102, filesize=69.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044580001
2014-07-22 08:56:20,101 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:56:21,304 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:21,329 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40082 synced till here 40080
2014-07-22 08:56:21,349 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044580001 with entries=82, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044581304
2014-07-22 08:56:21,350 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:56:22,143 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:22,907 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40182 synced till here 40172
2014-07-22 08:56:22,994 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044581304 with entries=100, filesize=71.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044582143
2014-07-22 08:56:22,994 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:56:23,433 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b.
2014-07-22 08:56:23,686 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:23,715 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40287 synced till here 40280
2014-07-22 08:56:23,821 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044582143 with entries=105, filesize=66.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044583687
2014-07-22 08:56:23,821 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:56:25,284 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:26,973 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1016ms
GC pool 'ParNew' had collection(s): count=1 time=1355ms
2014-07-22 08:56:26,996 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40416 synced till here 40404
2014-07-22 08:56:27,079 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044583687 with entries=129, filesize=88.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044585284
2014-07-22 08:56:27,079 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:56:28,268 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:28,290 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40515 synced till here 40512
2014-07-22 08:56:28,339 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044585284 with entries=99, filesize=65.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044588268
2014-07-22 08:56:28,339 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:56:29,891 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1417ms
GC pool 'ParNew' had collection(s): count=1 time=1454ms
2014-07-22 08:56:30,768 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:30,806 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40592 synced till here 40590
2014-07-22 08:56:30,868 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044588268 with entries=77, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044590768
2014-07-22 08:56:30,869 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:56:30,956 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:30,957 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:30,957 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:30,957 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:30,961 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:30,962 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:30,981 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:30,982 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:30,985 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:31,005 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:31,014 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:31,048 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:31,054 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:31,085 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:31,117 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:31,156 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:31,189 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7507, memsize=308.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/d2e5dfb232eb4288acb70dd3fcb4b708
2014-07-22 08:56:31,203 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:31,209 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/d2e5dfb232eb4288acb70dd3fcb4b708 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/d2e5dfb232eb4288acb70dd3fcb4b708
2014-07-22 08:56:32,092 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:32,092 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:32,093 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:32,093 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:32,094 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:32,094 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:32,094 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:32,095 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:32,095 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:32,096 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:32,099 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/d2e5dfb232eb4288acb70dd3fcb4b708, entries=1122440, sequenceid=7507, filesize=80.0m
2014-07-22 08:56:32,099 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~775.6m/813275600, currentsize=414.4m/434517120 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 24400ms, sequenceid=7507, compaction requested=true
2014-07-22 08:56:32,100 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:56:32,100 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 2000 blocking
2014-07-22 08:56:32,100 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-22 08:56:32,100 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:56:32,100 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4ms
2014-07-22 08:56:32,100 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:32,100 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:56:32,100 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5ms
2014-07-22 08:56:32,100 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 08:56:32,100 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:32,101 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6ms
2014-07-22 08:56:32,101 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:32,101 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 845.6m
2014-07-22 08:56:32,101 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7ms
2014-07-22 08:56:32,101 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:32,101 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7ms
2014-07-22 08:56:32,101 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:32,101 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8ms
2014-07-22 08:56:32,101 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:32,101 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9ms
2014-07-22 08:56:32,101 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:32,101 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9ms
2014-07-22 08:56:32,101 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:32,102 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10ms
2014-07-22 08:56:32,102 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:32,102 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10ms
2014-07-22 08:56:32,102 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:32,102 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 899ms
2014-07-22 08:56:32,102 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:32,102 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 946ms
2014-07-22 08:56:32,102 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:32,103 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 985ms
2014-07-22 08:56:32,103 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:32,103 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1018ms
2014-07-22 08:56:32,103 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:32,109 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1055ms
2014-07-22 08:56:32,109 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:32,109 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1061ms
2014-07-22 08:56:32,109 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:32,110 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1096ms
2014-07-22 08:56:32,110 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:32,112 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1108ms
2014-07-22 08:56:32,113 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:32,113 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1128ms
2014-07-22 08:56:32,113 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:32,113 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1131ms
2014-07-22 08:56:32,113 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:32,113 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1132ms
2014-07-22 08:56:32,113 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:32,119 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1157ms
2014-07-22 08:56:32,119 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:32,119 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1158ms
2014-07-22 08:56:32,120 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:32,120 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1163ms
2014-07-22 08:56:32,120 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:32,120 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1163ms
2014-07-22 08:56:32,120 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:32,120 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1163ms
2014-07-22 08:56:32,120 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:32,121 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1164ms
2014-07-22 08:56:32,121 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:32,316 DEBUG [RpcServer.handler=39,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:56:32,850 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:56:32,976 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:33,026 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40700 synced till here 40683
2014-07-22 08:56:33,176 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044590768 with entries=108, filesize=83.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044592976
2014-07-22 08:56:33,176 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:56:34,994 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:35,107 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40794 synced till here 40787
2014-07-22 08:56:35,144 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044592976 with entries=94, filesize=83.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044594995
2014-07-22 08:56:35,145 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:56:36,694 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:36,722 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40866 synced till here 40859
2014-07-22 08:56:36,753 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044594995 with entries=72, filesize=67.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044596695
2014-07-22 08:56:36,754 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=44, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:56:38,424 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:38,716 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044596695 with entries=103, filesize=73.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044598424
2014-07-22 08:56:38,716 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=45, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:56:40,382 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:40,407 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41056 synced till here 41047
2014-07-22 08:56:40,467 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044598424 with entries=87, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044600382
2014-07-22 08:56:40,467 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=46, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:56:42,074 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,074 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,075 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,075 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,075 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,076 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,076 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,077 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,079 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,102 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:42,109 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,112 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,129 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044600382 with entries=84, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044602102
2014-07-22 08:56:42,130 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=47, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:56:42,153 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,171 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,192 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,207 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,243 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,288 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,328 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,328 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,329 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,329 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,329 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,383 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,383 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,386 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,389 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,389 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,389 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,389 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,389 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,435 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,482 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,525 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,568 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,571 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,572 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,629 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,690 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,786 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,827 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,828 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,832 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,872 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,873 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,943 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:42,987 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:43,042 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:43,093 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:43,093 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:43,099 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:56:45,815 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1038ms
GC pool 'ParNew' had collection(s): count=1 time=1171ms
2014-07-22 08:56:46,916 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7604, memsize=413.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/8363035a49f64bc7966164c6ea966722
2014-07-22 08:56:46,933 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/8363035a49f64bc7966164c6ea966722 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/8363035a49f64bc7966164c6ea966722
2014-07-22 08:56:46,947 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/8363035a49f64bc7966164c6ea966722, entries=1506310, sequenceid=7604, filesize=107.3m
2014-07-22 08:56:46,948 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~862.2m/904062240, currentsize=414.5m/434595360 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 28804ms, sequenceid=7604, compaction requested=true
2014-07-22 08:56:46,948 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:56:46,948 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 2000 blocking
2014-07-22 08:56:46,949 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-22 08:56:46,949 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3850ms
2014-07-22 08:56:46,949 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:56:46,949 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:56:46,949 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,949 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 854.0m
2014-07-22 08:56:46,949 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 08:56:46,949 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3856ms
2014-07-22 08:56:46,949 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,950 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3858ms
2014-07-22 08:56:46,950 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,952 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3909ms
2014-07-22 08:56:46,952 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,953 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3967ms
2014-07-22 08:56:46,953 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,954 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4010ms
2014-07-22 08:56:46,954 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,954 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4081ms
2014-07-22 08:56:46,954 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,954 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4082ms
2014-07-22 08:56:46,955 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,955 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4123ms
2014-07-22 08:56:46,955 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,956 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4128ms
2014-07-22 08:56:46,956 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,956 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4129ms
2014-07-22 08:56:46,956 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,957 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4171ms
2014-07-22 08:56:46,957 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,957 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4267ms
2014-07-22 08:56:46,957 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,960 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4331ms
2014-07-22 08:56:46,960 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,960 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4388ms
2014-07-22 08:56:46,960 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,960 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4389ms
2014-07-22 08:56:46,960 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,967 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4399ms
2014-07-22 08:56:46,967 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,968 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4443ms
2014-07-22 08:56:46,968 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,975 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4492ms
2014-07-22 08:56:46,975 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,975 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4540ms
2014-07-22 08:56:46,975 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,975 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4586ms
2014-07-22 08:56:46,975 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,977 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4588ms
2014-07-22 08:56:46,978 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,978 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4589ms
2014-07-22 08:56:46,979 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,979 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4590ms
2014-07-22 08:56:46,979 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,984 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4594ms
2014-07-22 08:56:46,984 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,986 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4598ms
2014-07-22 08:56:46,986 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,987 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4603ms
2014-07-22 08:56:46,987 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,988 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4605ms
2014-07-22 08:56:46,988 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,989 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4659ms
2014-07-22 08:56:46,989 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,990 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4660ms
2014-07-22 08:56:46,990 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,990 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4662ms
2014-07-22 08:56:46,990 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,990 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4662ms
2014-07-22 08:56:46,990 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,991 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4662ms
2014-07-22 08:56:46,991 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,992 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4704ms
2014-07-22 08:56:46,992 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,992 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4750ms
2014-07-22 08:56:46,992 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,993 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4785ms
2014-07-22 08:56:46,993 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,995 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4803ms
2014-07-22 08:56:46,995 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,995 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4824ms
2014-07-22 08:56:46,995 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,995 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4842ms
2014-07-22 08:56:46,996 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,996 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4884ms
2014-07-22 08:56:46,996 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,996 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4887ms
2014-07-22 08:56:46,996 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,996 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4917ms
2014-07-22 08:56:46,996 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,997 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4920ms
2014-07-22 08:56:46,997 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:46,997 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4921ms
2014-07-22 08:56:46,997 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:47,005 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4930ms
2014-07-22 08:56:47,005 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:47,008 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4933ms
2014-07-22 08:56:47,008 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:47,008 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4933ms
2014-07-22 08:56:47,008 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:47,008 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4933ms
2014-07-22 08:56:47,008 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:47,010 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4935ms
2014-07-22 08:56:47,010 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:47,010 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4936ms
2014-07-22 08:56:47,010 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:56:47,423 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:56:49,003 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:49,009 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:56:49,054 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41286 synced till here 41253
2014-07-22 08:56:49,469 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044602102 with entries=146, filesize=97.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044609003
2014-07-22 08:56:49,471 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=48, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:56:51,200 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:51,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41402 synced till here 41364
2014-07-22 08:56:52,152 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044609003 with entries=116, filesize=96.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044611201
2014-07-22 08:56:52,152 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=49, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:56:52,582 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10103,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044602479,"queuetimems":1,"class":"HRegionServer","responsesize":15945,"method":"Multi"}
2014-07-22 08:56:53,634 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11090,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044602522,"queuetimems":1,"class":"HRegionServer","responsesize":17101,"method":"Multi"}
2014-07-22 08:56:53,967 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:53,977 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11775,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044602201,"queuetimems":0,"class":"HRegionServer","responsesize":18905,"method":"Multi"}
2014-07-22 08:56:54,064 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41532 synced till here 41513
2014-07-22 08:56:54,234 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044611201 with entries=130, filesize=85.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044613968
2014-07-22 08:56:54,234 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=50, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:56:56,341 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:56,361 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41646 synced till here 41615
2014-07-22 08:56:56,585 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044613968 with entries=114, filesize=98.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044616342
2014-07-22 08:56:56,586 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=51, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:56:58,305 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:56:58,489 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41757 synced till here 41721
2014-07-22 08:56:58,849 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044616342 with entries=111, filesize=86.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044618305
2014-07-22 08:56:58,849 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=52, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:57:00,069 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,069 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,069 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,070 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,070 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,071 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,071 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,071 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,072 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,072 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,072 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,072 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,073 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,074 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,075 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,076 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,077 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,079 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,081 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,081 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,081 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,081 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,082 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,083 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,083 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,084 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,084 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,085 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,085 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,086 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,086 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,092 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,171 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,173 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,190 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,595 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,655 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,702 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,708 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:00,709 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7756, memsize=379.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/ef897c034a6647d2b12478d29900d021
2014-07-22 08:57:00,723 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/ef897c034a6647d2b12478d29900d021 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/ef897c034a6647d2b12478d29900d021
2014-07-22 08:57:00,731 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/ef897c034a6647d2b12478d29900d021, entries=1382360, sequenceid=7756, filesize=98.4m
2014-07-22 08:57:00,731 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~845.6m/886632720, currentsize=416.9m/437155760 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 28631ms, sequenceid=7756, compaction requested=true
2014-07-22 08:57:00,732 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:57:00,732 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 2000 blocking
2014-07-22 08:57:00,732 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24ms
2014-07-22 08:57:00,732 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,732 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-22 08:57:00,732 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31ms
2014-07-22 08:57:00,732 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:57:00,732 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,733 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:57:00,733 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 79ms
2014-07-22 08:57:00,733 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,733 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 08:57:00,733 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 138ms
2014-07-22 08:57:00,733 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,733 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 543ms
2014-07-22 08:57:00,733 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,733 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 560ms
2014-07-22 08:57:00,733 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,733 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 562ms
2014-07-22 08:57:00,734 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,734 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 642ms
2014-07-22 08:57:00,734 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,735 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 649ms
2014-07-22 08:57:00,735 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,735 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 649ms
2014-07-22 08:57:00,735 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,735 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 651ms
2014-07-22 08:57:00,735 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,745 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 662ms
2014-07-22 08:57:00,745 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,745 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 661ms
2014-07-22 08:57:00,746 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,746 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 662ms
2014-07-22 08:57:00,746 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,746 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 664ms
2014-07-22 08:57:00,746 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,746 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 664ms
2014-07-22 08:57:00,746 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,746 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 664ms
2014-07-22 08:57:00,746 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,746 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 666ms
2014-07-22 08:57:00,746 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,746 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 666ms
2014-07-22 08:57:00,747 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,753 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b., current region memstore size 344.4m
2014-07-22 08:57:00,757 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 676ms
2014-07-22 08:57:00,757 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,760 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 681ms
2014-07-22 08:57:00,760 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,769 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 690ms
2014-07-22 08:57:00,769 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,769 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 692ms
2014-07-22 08:57:00,769 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,770 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 694ms
2014-07-22 08:57:00,794 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,794 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 719ms
2014-07-22 08:57:00,794 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,797 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 723ms
2014-07-22 08:57:00,797 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,805 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 732ms
2014-07-22 08:57:00,805 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,813 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 741ms
2014-07-22 08:57:00,813 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,813 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 741ms
2014-07-22 08:57:00,813 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,814 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 743ms
2014-07-22 08:57:00,814 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,825 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 754ms
2014-07-22 08:57:00,825 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,825 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 754ms
2014-07-22 08:57:00,825 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,826 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 756ms
2014-07-22 08:57:00,826 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,826 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 755ms
2014-07-22 08:57:00,826 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,831 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 761ms
2014-07-22 08:57:00,832 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,832 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 762ms
2014-07-22 08:57:00,832 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,832 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 763ms
2014-07-22 08:57:00,832 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,832 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 763ms
2014-07-22 08:57:00,832 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,832 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 764ms
2014-07-22 08:57:00,833 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:00,990 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:57:01,206 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:57:01,282 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:57:01,482 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41873 synced till here 41871
2014-07-22 08:57:01,548 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044618305 with entries=116, filesize=83.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044621282
2014-07-22 08:57:03,260 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:57:03,286 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41958 synced till here 41953
2014-07-22 08:57:03,343 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044621282 with entries=85, filesize=67.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044623260
2014-07-22 08:57:05,233 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:57:05,250 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42051 synced till here 42049
2014-07-22 08:57:05,259 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044623260 with entries=93, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044625234
2014-07-22 08:57:06,966 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:57:06,983 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42140 synced till here 42138
2014-07-22 08:57:07,014 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044625234 with entries=89, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044626966
2014-07-22 08:57:07,843 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:57:07,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42234 synced till here 42227
2014-07-22 08:57:07,925 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044626966 with entries=94, filesize=68.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044627843
2014-07-22 08:57:09,271 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:57:09,292 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044627843 with entries=92, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044629272
2014-07-22 08:57:10,719 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:57:10,737 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42426 synced till here 42424
2014-07-22 08:57:10,790 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044629272 with entries=100, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044630720
2014-07-22 08:57:10,857 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:10,878 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:10,886 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:10,901 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:10,910 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:10,910 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:10,913 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:10,914 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:10,915 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:10,919 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:10,943 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:10,996 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:10,996 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,005 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,006 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,008 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,075 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,140 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,141 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,143 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,145 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,222 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,279 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,283 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,341 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,384 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,387 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,391 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,391 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,392 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,459 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,511 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,561 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,562 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,616 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,618 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,619 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,620 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,680 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,771 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,829 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,836 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,836 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,898 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,899 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:11,902 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:12,826 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:12,827 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:12,828 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:12,829 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:12,912 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10032, memsize=200.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/261e74ebd1364d308584a3fd3e6ce5ed
2014-07-22 08:57:12,937 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/261e74ebd1364d308584a3fd3e6ce5ed as hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/261e74ebd1364d308584a3fd3e6ce5ed
2014-07-22 08:57:12,953 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/261e74ebd1364d308584a3fd3e6ce5ed, entries=730410, sequenceid=10032, filesize=52.1m
2014-07-22 08:57:12,954 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~344.6m/361360720, currentsize=38.1m/39968400 for region usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. in 12201ms, sequenceid=10032, compaction requested=true
2014-07-22 08:57:12,954 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:57:12,954 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 126ms
2014-07-22 08:57:12,954 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 2000 blocking
2014-07-22 08:57:12,954 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,954 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-22 08:57:12,955 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 127ms
2014-07-22 08:57:12,955 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:57:12,955 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 1004.3m
2014-07-22 08:57:12,955 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:57:12,955 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,955 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. because compaction request was cancelled
2014-07-22 08:57:12,955 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 128ms
2014-07-22 08:57:12,955 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,962 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 136ms
2014-07-22 08:57:12,962 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,962 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1060ms
2014-07-22 08:57:12,962 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,962 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1063ms
2014-07-22 08:57:12,962 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,962 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1064ms
2014-07-22 08:57:12,962 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,963 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1127ms
2014-07-22 08:57:12,963 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,965 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1129ms
2014-07-22 08:57:12,965 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,965 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1136ms
2014-07-22 08:57:12,965 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,966 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1195ms
2014-07-22 08:57:12,966 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,966 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1286ms
2014-07-22 08:57:12,966 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,973 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1353ms
2014-07-22 08:57:12,973 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,973 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1354ms
2014-07-22 08:57:12,973 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,974 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1356ms
2014-07-22 08:57:12,974 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,974 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1358ms
2014-07-22 08:57:12,974 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,974 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1412ms
2014-07-22 08:57:12,974 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,974 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1413ms
2014-07-22 08:57:12,974 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,975 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1464ms
2014-07-22 08:57:12,975 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,975 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1517ms
2014-07-22 08:57:12,975 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,978 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1586ms
2014-07-22 08:57:12,978 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,980 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1589ms
2014-07-22 08:57:12,980 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,980 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1590ms
2014-07-22 08:57:12,980 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,980 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1593ms
2014-07-22 08:57:12,980 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,980 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1596ms
2014-07-22 08:57:12,981 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,981 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1640ms
2014-07-22 08:57:12,981 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,981 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1698ms
2014-07-22 08:57:12,981 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,989 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1710ms
2014-07-22 08:57:12,989 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,989 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1767ms
2014-07-22 08:57:12,990 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,990 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1845ms
2014-07-22 08:57:12,990 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,990 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1847ms
2014-07-22 08:57:12,990 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,990 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1850ms
2014-07-22 08:57:12,990 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,990 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1850ms
2014-07-22 08:57:12,990 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,993 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1918ms
2014-07-22 08:57:12,993 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,993 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1985ms
2014-07-22 08:57:12,993 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,996 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1990ms
2014-07-22 08:57:12,996 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,996 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1991ms
2014-07-22 08:57:12,996 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,997 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2001ms
2014-07-22 08:57:12,997 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,997 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2002ms
2014-07-22 08:57:12,997 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:12,997 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2054ms
2014-07-22 08:57:12,997 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:13,004 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2085ms
2014-07-22 08:57:13,004 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:13,004 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2089ms
2014-07-22 08:57:13,004 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:13,005 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2091ms
2014-07-22 08:57:13,005 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:13,013 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2100ms
2014-07-22 08:57:13,013 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:13,013 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2103ms
2014-07-22 08:57:13,014 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:13,014 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2104ms
2014-07-22 08:57:13,014 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:13,014 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2114ms
2014-07-22 08:57:13,014 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:13,021 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2135ms
2014-07-22 08:57:13,021 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:13,021 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2143ms
2014-07-22 08:57:13,022 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:13,022 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2165ms
2014-07-22 08:57:13,022 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:13,752 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:57:14,929 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1111ms
GC pool 'ParNew' had collection(s): count=1 time=1119ms
2014-07-22 08:57:14,931 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42558 synced till here 42528
2014-07-22 08:57:15,221 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044630720 with entries=132, filesize=84.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044633753
2014-07-22 08:57:15,221 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044499004
2014-07-22 08:57:15,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044509137
2014-07-22 08:57:15,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044511239
2014-07-22 08:57:15,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044513289
2014-07-22 08:57:15,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044516433
2014-07-22 08:57:15,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044518188
2014-07-22 08:57:15,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044521920
2014-07-22 08:57:15,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044523524
2014-07-22 08:57:15,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044525125
2014-07-22 08:57:15,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044527283
2014-07-22 08:57:15,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044528890
2014-07-22 08:57:15,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044530627
2014-07-22 08:57:15,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044531884
2014-07-22 08:57:15,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044533333
2014-07-22 08:57:15,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044534241
2014-07-22 08:57:15,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044535577
2014-07-22 08:57:15,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044541462
2014-07-22 08:57:15,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044544754
2014-07-22 08:57:15,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044547346
2014-07-22 08:57:15,341 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:57:17,372 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:57:17,448 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42666 synced till here 42643
2014-07-22 08:57:17,719 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044633753 with entries=108, filesize=91.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044637372
2014-07-22 08:57:18,036 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,036 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,037 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,037 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,037 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,038 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,038 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,039 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,039 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,039 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,039 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,040 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,042 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,043 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,043 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,048 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,048 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,048 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,050 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,051 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,051 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,053 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,189 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,190 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,190 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,190 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,191 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,194 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,195 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,195 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,195 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,195 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,196 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,196 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,196 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,196 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,196 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,197 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,198 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,246 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,246 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,250 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,250 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,251 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,251 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,252 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,252 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,252 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,252 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:18,254 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:20,868 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7850, memsize=446.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/22b173f1d15e4f9c83171a9302e66ef1
2014-07-22 08:57:20,884 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/22b173f1d15e4f9c83171a9302e66ef1 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/22b173f1d15e4f9c83171a9302e66ef1
2014-07-22 08:57:20,965 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/22b173f1d15e4f9c83171a9302e66ef1, entries=1625600, sequenceid=7850, filesize=115.7m
2014-07-22 08:57:20,965 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~854.0m/895516960, currentsize=499.0m/523254240 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 34016ms, sequenceid=7850, compaction requested=true
2014-07-22 08:57:20,966 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:57:20,966 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 2000 blocking
2014-07-22 08:57:20,966 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-22 08:57:20,966 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:57:20,967 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2712ms
2014-07-22 08:57:20,967 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 904.0m
2014-07-22 08:57:20,967 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:57:20,967 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:20,967 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 08:57:20,968 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2715ms
2014-07-22 08:57:20,968 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:20,968 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2716ms
2014-07-22 08:57:20,968 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:20,969 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2717ms
2014-07-22 08:57:20,969 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:20,970 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2718ms
2014-07-22 08:57:20,970 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:20,971 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2720ms
2014-07-22 08:57:20,971 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:20,972 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2721ms
2014-07-22 08:57:20,972 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:20,972 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2722ms
2014-07-22 08:57:20,972 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:20,973 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2723ms
2014-07-22 08:57:20,973 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:20,974 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2728ms
2014-07-22 08:57:20,974 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:20,975 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2729ms
2014-07-22 08:57:20,975 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:20,978 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2780ms
2014-07-22 08:57:21,002 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,002 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2807ms
2014-07-22 08:57:21,002 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,002 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2807ms
2014-07-22 08:57:21,002 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,003 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2808ms
2014-07-22 08:57:21,003 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,004 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2807ms
2014-07-22 08:57:21,004 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,012 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2816ms
2014-07-22 08:57:21,012 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,013 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2818ms
2014-07-22 08:57:21,013 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,013 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2818ms
2014-07-22 08:57:21,013 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,013 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2818ms
2014-07-22 08:57:21,013 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,013 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2818ms
2014-07-22 08:57:21,014 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,025 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2830ms
2014-07-22 08:57:21,025 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,033 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2839ms
2014-07-22 08:57:21,033 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,042 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2851ms
2014-07-22 08:57:21,042 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,043 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2853ms
2014-07-22 08:57:21,043 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,043 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2854ms
2014-07-22 08:57:21,043 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,049 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2860ms
2014-07-22 08:57:21,049 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,050 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2861ms
2014-07-22 08:57:21,050 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,050 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2997ms
2014-07-22 08:57:21,050 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,050 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2999ms
2014-07-22 08:57:21,050 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,050 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2999ms
2014-07-22 08:57:21,051 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,062 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3011ms
2014-07-22 08:57:21,062 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,062 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3014ms
2014-07-22 08:57:21,062 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,063 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3015ms
2014-07-22 08:57:21,063 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,069 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3022ms
2014-07-22 08:57:21,069 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,077 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3034ms
2014-07-22 08:57:21,077 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,078 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3035ms
2014-07-22 08:57:21,078 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,078 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3036ms
2014-07-22 08:57:21,078 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,085 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3045ms
2014-07-22 08:57:21,086 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,086 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3047ms
2014-07-22 08:57:21,086 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,086 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3047ms
2014-07-22 08:57:21,086 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,092 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3052ms
2014-07-22 08:57:21,092 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,095 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3057ms
2014-07-22 08:57:21,095 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,096 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3058ms
2014-07-22 08:57:21,096 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,097 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3059ms
2014-07-22 08:57:21,098 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,098 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3061ms
2014-07-22 08:57:21,098 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,101 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3062ms
2014-07-22 08:57:21,103 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,103 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3066ms
2014-07-22 08:57:21,103 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,104 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3068ms
2014-07-22 08:57:21,104 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,104 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3068ms
2014-07-22 08:57:21,104 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:21,176 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:57:21,314 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42758 synced till here 42751
2014-07-22 08:57:22,448 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044637372 with entries=92, filesize=69.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044641176
2014-07-22 08:57:22,448 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044551027
2014-07-22 08:57:22,448 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044552171
2014-07-22 08:57:22,448 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044554303
2014-07-22 08:57:22,448 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044556309
2014-07-22 08:57:22,448 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044558823
2014-07-22 08:57:22,448 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044561044
2014-07-22 08:57:22,448 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044563726
2014-07-22 08:57:22,448 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044565823
2014-07-22 08:57:24,093 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1166ms
GC pool 'ParNew' had collection(s): count=1 time=1501ms
2014-07-22 08:57:24,401 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:57:24,405 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:57:24,946 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:57:24,980 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42868 synced till here 42842
2014-07-22 08:57:25,244 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044641176 with entries=110, filesize=92.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044644947
2014-07-22 08:57:27,308 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:57:27,343 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42972 synced till here 42942
2014-07-22 08:57:28,727 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10853,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044637872,"queuetimems":30,"class":"HRegionServer","responsesize":18865,"method":"Multi"}
2014-07-22 08:57:28,727 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11088,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044637637,"queuetimems":1928,"class":"HRegionServer","responsesize":11321,"method":"Multi"}
2014-07-22 08:57:28,881 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044644947 with entries=104, filesize=90.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044647308
2014-07-22 08:57:30,903 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:57:30,996 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43080 synced till here 43073
2014-07-22 08:57:31,082 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044647308 with entries=108, filesize=90.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044650903
2014-07-22 08:57:33,203 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:57:33,240 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43192 synced till here 43153
2014-07-22 08:57:33,545 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044650903 with entries=112, filesize=96.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044653205
2014-07-22 08:57:34,395 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:34,395 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:34,397 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:34,399 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:34,399 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:34,400 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:34,400 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:34,401 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:34,401 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:34,402 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:34,404 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:34,405 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:34,405 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:34,406 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,468 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:57:35,471 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,473 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,473 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,473 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,473 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,474 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,474 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,474 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,476 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,476 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,489 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43299 synced till here 43278
2014-07-22 08:57:35,622 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,622 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,623 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,623 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,624 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,625 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,626 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,627 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,627 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,627 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,628 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,628 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,628 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,628 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,628 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,630 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,631 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,631 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,633 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,633 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,635 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,635 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,635 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,635 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,635 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,635 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:57:35,639 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044653205 with entries=107, filesize=90.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044655468
2014-07-22 08:57:39,395 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:57:39,396 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:57:39,397 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:57:39,400 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:57:39,400 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:57:39,400 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:57:39,402 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:57:39,402 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 08:57:39,402 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:57:39,402 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5007ms
2014-07-22 08:57:39,404 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:57:39,405 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:57:39,405 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5004ms
2014-07-22 08:57:39,406 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:57:40,471 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:57:40,473 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:57:40,473 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:57:40,473 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:57:40,474 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:57:40,474 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:57:40,474 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:57:40,474 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:57:40,476 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:57:40,476 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:57:41,421 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5786ms
2014-07-22 08:57:41,422 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5792ms
2014-07-22 08:57:41,423 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5792ms
2014-07-22 08:57:41,423 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5792ms
2014-07-22 08:57:41,423 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5791ms
2014-07-22 08:57:41,424 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5791ms
2014-07-22 08:57:41,424 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5791ms
2014-07-22 08:57:41,424 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5791ms
2014-07-22 08:57:41,424 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5789ms
2014-07-22 08:57:41,425 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5792ms
2014-07-22 08:57:41,425 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5790ms
2014-07-22 08:57:41,425 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5804ms
2014-07-22 08:57:41,426 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5803ms
2014-07-22 08:57:41,426 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5803ms
2014-07-22 08:57:41,426 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5803ms
2014-07-22 08:57:41,427 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5802ms
2014-07-22 08:57:41,427 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5802ms
2014-07-22 08:57:41,428 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5802ms
2014-07-22 08:57:41,428 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5801ms
2014-07-22 08:57:41,429 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5801ms
2014-07-22 08:57:41,429 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5802ms
2014-07-22 08:57:41,429 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5802ms
2014-07-22 08:57:41,429 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5801ms
2014-07-22 08:57:41,429 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5801ms
2014-07-22 08:57:41,430 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5801ms
2014-07-22 08:57:41,430 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5802ms
2014-07-22 08:57:44,396 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 08:57:44,397 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 08:57:44,398 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 08:57:44,401 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 08:57:44,402 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 08:57:44,402 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 08:57:44,402 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 08:57:44,403 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 08:57:44,403 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10008ms
2014-07-22 08:57:44,404 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 08:57:44,405 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 08:57:44,405 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 08:57:44,406 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10004ms
2014-07-22 08:57:44,407 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 08:57:45,472 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 08:57:45,474 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 08:57:45,474 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 08:57:45,475 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 08:57:45,475 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 08:57:45,475 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 08:57:45,476 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 08:57:45,476 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-22 08:57:45,476 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 08:57:45,477 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 08:57:46,422 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10787ms
2014-07-22 08:57:46,423 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10793ms
2014-07-22 08:57:46,423 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10792ms
2014-07-22 08:57:46,424 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10793ms
2014-07-22 08:57:46,424 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10792ms
2014-07-22 08:57:46,424 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10791ms
2014-07-22 08:57:46,424 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10791ms
2014-07-22 08:57:46,425 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10792ms
2014-07-22 08:57:46,425 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10790ms
2014-07-22 08:57:46,426 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10792ms
2014-07-22 08:57:46,426 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10791ms
2014-07-22 08:57:46,427 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10803ms
2014-07-22 08:57:46,428 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10805ms
2014-07-22 08:57:46,428 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10807ms
2014-07-22 08:57:46,428 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10801ms
2014-07-22 08:57:46,429 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10803ms
2014-07-22 08:57:46,429 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10804ms
2014-07-22 08:57:46,429 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10805ms
2014-07-22 08:57:46,430 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10802ms
2014-07-22 08:57:46,430 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10807ms
2014-07-22 08:57:46,430 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10802ms
2014-07-22 08:57:46,430 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10802ms
2014-07-22 08:57:46,431 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10804ms
2014-07-22 08:57:46,431 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10804ms
2014-07-22 08:57:46,431 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10803ms
2014-07-22 08:57:46,431 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10803ms
2014-07-22 08:57:47,993 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8115, memsize=426.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/30706c3fc915403e8aacc798dbda10d5
2014-07-22 08:57:48,004 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/30706c3fc915403e8aacc798dbda10d5 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/30706c3fc915403e8aacc798dbda10d5
2014-07-22 08:57:48,018 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/30706c3fc915403e8aacc798dbda10d5, entries=1551620, sequenceid=8115, filesize=110.6m
2014-07-22 08:57:48,018 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~904.0m/947868480, currentsize=201.7m/211449440 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 27051ms, sequenceid=8115, compaction requested=true
2014-07-22 08:57:48,019 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:57:48,019 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 2000 blocking
2014-07-22 08:57:48,019 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12391ms
2014-07-22 08:57:48,019 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,019 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-22 08:57:48,020 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12391ms
2014-07-22 08:57:48,020 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,020 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:57:48,020 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 934.7m
2014-07-22 08:57:48,020 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:57:48,020 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 08:57:48,020 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12393ms
2014-07-22 08:57:48,021 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,021 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12394ms
2014-07-22 08:57:48,021 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,021 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12393ms
2014-07-22 08:57:48,021 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,021 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12393ms
2014-07-22 08:57:48,021 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,021 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12398ms
2014-07-22 08:57:48,021 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,021 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12394ms
2014-07-22 08:57:48,021 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,022 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12398ms
2014-07-22 08:57:48,022 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,022 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12397ms
2014-07-22 08:57:48,022 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,029 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12403ms
2014-07-22 08:57:48,029 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,030 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12403ms
2014-07-22 08:57:48,030 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,030 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12409ms
2014-07-22 08:57:48,030 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,034 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12412ms
2014-07-22 08:57:48,034 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,034 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12411ms
2014-07-22 08:57:48,034 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,034 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12399ms
2014-07-22 08:57:48,034 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,045 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12412ms
2014-07-22 08:57:48,045 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,045 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12410ms
2014-07-22 08:57:48,045 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,046 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12413ms
2014-07-22 08:57:48,046 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,046 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12413ms
2014-07-22 08:57:48,046 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,046 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12413ms
2014-07-22 08:57:48,046 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,046 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12414ms
2014-07-22 08:57:48,046 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,053 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12422ms
2014-07-22 08:57:48,053 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,053 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12422ms
2014-07-22 08:57:48,053 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,053 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12423ms
2014-07-22 08:57:48,053 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,057 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12422ms
2014-07-22 08:57:48,057 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,057 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12581ms
2014-07-22 08:57:48,058 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,060 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12584ms
2014-07-22 08:57:48,060 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,060 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12587ms
2014-07-22 08:57:48,060 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,065 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12591ms
2014-07-22 08:57:48,065 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,066 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12592ms
2014-07-22 08:57:48,066 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,066 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12593ms
2014-07-22 08:57:48,066 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,066 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12593ms
2014-07-22 08:57:48,066 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,066 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12593ms
2014-07-22 08:57:48,066 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,066 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12593ms
2014-07-22 08:57:48,066 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,082 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12612ms
2014-07-22 08:57:48,082 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,082 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12608,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044655474,"queuetimems":3905,"class":"HRegionServer","responsesize":183,"method":"Multi"}
2014-07-22 08:57:48,089 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13683ms
2014-07-22 08:57:48,089 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,089 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13688ms
2014-07-22 08:57:48,089 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,090 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13686ms
2014-07-22 08:57:48,090 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,090 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13686ms
2014-07-22 08:57:48,090 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,097 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13695ms
2014-07-22 08:57:48,097 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,097 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13702ms
2014-07-22 08:57:48,097 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,098 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13697ms
2014-07-22 08:57:48,098 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,103 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13703ms
2014-07-22 08:57:48,103 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,109 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13709ms
2014-07-22 08:57:48,109 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,109 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13710ms
2014-07-22 08:57:48,109 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,109 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13710ms
2014-07-22 08:57:48,110 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,117 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13720ms
2014-07-22 08:57:48,117 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,118 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13723ms
2014-07-22 08:57:48,118 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,118 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13724ms
2014-07-22 08:57:48,118 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:57:48,142 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12668,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044655473,"queuetimems":3905,"class":"HRegionServer","responsesize":1101,"method":"Multi"}
2014-07-22 08:57:48,144 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12674,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044655470,"queuetimems":3902,"class":"HRegionServer","responsesize":1089,"method":"Multi"}
2014-07-22 08:57:48,160 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13762,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044654397,"queuetimems":2872,"class":"HRegionServer","responsesize":32,"method":"Multi"}
2014-07-22 08:57:48,240 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15033,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653207,"queuetimems":6235,"class":"HRegionServer","responsesize":18777,"method":"Multi"}
2014-07-22 08:57:48,273 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15066,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653207,"queuetimems":6308,"class":"HRegionServer","responsesize":18615,"method":"Multi"}
2014-07-22 08:57:48,388 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8077, memsize=538.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/ee67a25b69174dc09c5da60cd2301397
2014-07-22 08:57:48,414 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/ee67a25b69174dc09c5da60cd2301397 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/ee67a25b69174dc09c5da60cd2301397
2014-07-22 08:57:48,425 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16762,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044651663,"queuetimems":4882,"class":"HRegionServer","responsesize":18818,"method":"Multi"}
2014-07-22 08:57:48,440 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/ee67a25b69174dc09c5da60cd2301397, entries=1960410, sequenceid=8077, filesize=139.7m
2014-07-22 08:57:48,457 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1004.3m/1053049200, currentsize=280.5m/294084880 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 35501ms, sequenceid=8077, compaction requested=true
2014-07-22 08:57:48,457 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 2000 blocking
2014-07-22 08:57:48,457 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-22 08:57:48,457 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:57:48,458 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:57:48,458 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 08:57:48,458 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:57:48,458 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 734.2m
2014-07-22 08:57:49,159 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:57:49,322 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:57:49,439 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43394 synced till here 43375
2014-07-22 08:57:49,536 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:57:49,557 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16052,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653505,"queuetimems":4537,"class":"HRegionServer","responsesize":13327,"method":"Multi"}
2014-07-22 08:57:49,557 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15835,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653722,"queuetimems":3929,"class":"HRegionServer","responsesize":18858,"method":"Multi"}
2014-07-22 08:57:49,557 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16072,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653485,"queuetimems":6150,"class":"HRegionServer","responsesize":18636,"method":"Multi"}
2014-07-22 08:57:49,557 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16055,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653502,"queuetimems":4559,"class":"HRegionServer","responsesize":15779,"method":"Multi"}
2014-07-22 08:57:49,561 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15844,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653714,"queuetimems":4032,"class":"HRegionServer","responsesize":18818,"method":"Multi"}
2014-07-22 08:57:49,886 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044655468 with entries=95, filesize=79.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044669322
2014-07-22 08:57:49,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044567638
2014-07-22 08:57:49,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044569501
2014-07-22 08:57:49,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044571488
2014-07-22 08:57:49,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044572547
2014-07-22 08:57:49,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044573871
2014-07-22 08:57:49,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044575073
2014-07-22 08:57:49,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044576583
2014-07-22 08:57:49,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044578482
2014-07-22 08:57:49,888 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044580001
2014-07-22 08:57:49,888 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044581304
2014-07-22 08:57:49,888 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044582143
2014-07-22 08:57:49,888 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044583687
2014-07-22 08:57:49,888 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044585284
2014-07-22 08:57:49,888 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044588268
2014-07-22 08:57:49,925 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:57:50,247 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16753,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653493,"queuetimems":4789,"class":"HRegionServer","responsesize":16223,"method":"Multi"}
2014-07-22 08:57:50,247 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16112,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044654134,"queuetimems":2777,"class":"HRegionServer","responsesize":18614,"method":"Multi"}
2014-07-22 08:57:50,247 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16783,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653463,"queuetimems":6330,"class":"HRegionServer","responsesize":18865,"method":"Multi"}
2014-07-22 08:57:50,247 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16750,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653496,"queuetimems":4737,"class":"HRegionServer","responsesize":12932,"method":"Multi"}
2014-07-22 08:57:50,254 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16751,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653502,"queuetimems":4652,"class":"HRegionServer","responsesize":11035,"method":"Multi"}
2014-07-22 08:57:50,254 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16540,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653714,"queuetimems":3966,"class":"HRegionServer","responsesize":18240,"method":"Multi"}
2014-07-22 08:57:50,254 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16100,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044654154,"queuetimems":2629,"class":"HRegionServer","responsesize":18574,"method":"Multi"}
2014-07-22 08:57:50,259 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16763,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653496,"queuetimems":4696,"class":"HRegionServer","responsesize":18660,"method":"Multi"}
2014-07-22 08:57:50,259 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16749,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653510,"queuetimems":4489,"class":"HRegionServer","responsesize":18344,"method":"Multi"}
2014-07-22 08:57:50,260 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16746,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653513,"queuetimems":4457,"class":"HRegionServer","responsesize":18709,"method":"Multi"}
2014-07-22 08:57:50,260 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16125,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044654134,"queuetimems":2919,"class":"HRegionServer","responsesize":18777,"method":"Multi"}
2014-07-22 08:57:50,260 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16773,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653486,"queuetimems":6097,"class":"HRegionServer","responsesize":18240,"method":"Multi"}
2014-07-22 08:57:50,259 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16125,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044654134,"queuetimems":2837,"class":"HRegionServer","responsesize":18983,"method":"Multi"}
2014-07-22 08:57:50,261 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16679,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653581,"queuetimems":4104,"class":"HRegionServer","responsesize":18687,"method":"Multi"}
2014-07-22 08:57:50,259 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16757,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653502,"queuetimems":4594,"class":"HRegionServer","responsesize":18964,"method":"Multi"}
2014-07-22 08:57:50,260 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16537,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653722,"queuetimems":2883,"class":"HRegionServer","responsesize":18734,"method":"Multi"}
2014-07-22 08:57:50,270 DEBUG [RpcServer.handler=23,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:57:50,270 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18432,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044651838,"queuetimems":4976,"class":"HRegionServer","responsesize":18614,"method":"Multi"}
2014-07-22 08:57:50,270 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16688,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653582,"queuetimems":3978,"class":"HRegionServer","responsesize":18636,"method":"Multi"}
2014-07-22 08:57:50,270 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16776,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653494,"queuetimems":4769,"class":"HRegionServer","responsesize":15820,"method":"Multi"}
2014-07-22 08:57:50,270 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18434,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044651836,"queuetimems":5011,"class":"HRegionServer","responsesize":18858,"method":"Multi"}
2014-07-22 08:57:50,272 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16549,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653722,"queuetimems":3873,"class":"HRegionServer","responsesize":18629,"method":"Multi"}
2014-07-22 08:57:50,273 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16365,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653908,"queuetimems":2818,"class":"HRegionServer","responsesize":18257,"method":"Multi"}
2014-07-22 08:57:50,281 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16558,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653722,"queuetimems":2871,"class":"HRegionServer","responsesize":18615,"method":"Multi"}
2014-07-22 08:57:50,281 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16377,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653904,"queuetimems":2994,"class":"HRegionServer","responsesize":18468,"method":"Multi"}
2014-07-22 08:57:50,281 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16383,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653898,"queuetimems":3023,"class":"HRegionServer","responsesize":18652,"method":"Multi"}
2014-07-22 08:57:50,281 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16779,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653502,"queuetimems":4677,"class":"HRegionServer","responsesize":11321,"method":"Multi"}
2014-07-22 08:57:50,294 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16788,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653505,"queuetimems":4519,"class":"HRegionServer","responsesize":9609,"method":"Multi"}
2014-07-22 08:57:50,294 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16819,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653474,"queuetimems":6258,"class":"HRegionServer","responsesize":18574,"method":"Multi"}
2014-07-22 08:57:50,294 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17087,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653207,"queuetimems":6272,"class":"HRegionServer","responsesize":18618,"method":"Multi"}
2014-07-22 08:57:50,295 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17085,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653209,"queuetimems":6153,"class":"HRegionServer","responsesize":18579,"method":"Multi"}
2014-07-22 08:57:50,302 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16168,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044654134,"queuetimems":2730,"class":"HRegionServer","responsesize":18579,"method":"Multi"}
2014-07-22 08:57:51,437 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17920,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653517,"queuetimems":4137,"class":"HRegionServer","responsesize":18618,"method":"Multi"}
2014-07-22 08:57:51,438 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15968,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044655470,"queuetimems":3903,"class":"HRegionServer","responsesize":18865,"method":"Multi"}
2014-07-22 08:57:51,466 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17992,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653474,"queuetimems":6299,"class":"HRegionServer","responsesize":18742,"method":"Multi"}
2014-07-22 08:57:51,494 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17356,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044654137,"queuetimems":2650,"class":"HRegionServer","responsesize":18865,"method":"Multi"}
2014-07-22 08:57:51,502 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17366,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044654135,"queuetimems":2685,"class":"HRegionServer","responsesize":18742,"method":"Multi"}
2014-07-22 08:57:51,510 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18024,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653485,"queuetimems":6222,"class":"HRegionServer","responsesize":18865,"method":"Multi"}
2014-07-22 08:57:51,581 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18373,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044653208,"queuetimems":6197,"class":"HRegionServer","responsesize":18734,"method":"Multi"}
2014-07-22 08:57:51,611 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:57:51,647 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43492 synced till here 43481
2014-07-22 08:57:51,764 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044669322 with entries=98, filesize=72.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044671611
2014-07-22 08:57:52,582 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:57:53,515 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43593 synced till here 43559
2014-07-22 08:57:53,805 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044671611 with entries=101, filesize=94.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044672582
2014-07-22 08:57:54,638 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:57:55,542 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43708 synced till here 43670
2014-07-22 08:57:55,789 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044672582 with entries=115, filesize=100.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044674638
2014-07-22 08:57:56,378 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:57:56,925 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044674638 with entries=101, filesize=80.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044676378
2014-07-22 08:57:59,527 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:57:59,662 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43919 synced till here 43918
2014-07-22 08:57:59,724 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044676378 with entries=110, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044679527
2014-07-22 08:58:01,582 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:01,607 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044679527 with entries=84, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044681583
2014-07-22 08:58:03,102 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8291, memsize=205.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/612b1d52da0f4f54b5a51b37d65f3354
2014-07-22 08:58:03,121 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/612b1d52da0f4f54b5a51b37d65f3354 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/612b1d52da0f4f54b5a51b37d65f3354
2014-07-22 08:58:03,138 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/612b1d52da0f4f54b5a51b37d65f3354, entries=746280, sequenceid=8291, filesize=53.2m
2014-07-22 08:58:03,138 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~767.9m/805242160, currentsize=212.8m/223144480 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 14680ms, sequenceid=8291, compaction requested=true
2014-07-22 08:58:03,138 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:58:03,139 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 2000 blocking
2014-07-22 08:58:03,139 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-22 08:58:03,139 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:58:03,139 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 565.7m
2014-07-22 08:58:03,139 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:58:03,139 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 08:58:03,372 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:03,413 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44090 synced till here 44086
2014-07-22 08:58:03,482 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044681583 with entries=87, filesize=65.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044683373
2014-07-22 08:58:03,642 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:58:04,752 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:04,769 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44174 synced till here 44172
2014-07-22 08:58:04,805 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044683373 with entries=84, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044684752
2014-07-22 08:58:04,956 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:58:06,162 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:06,185 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44266 synced till here 44261
2014-07-22 08:58:06,262 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044684752 with entries=92, filesize=66.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044686162
2014-07-22 08:58:08,504 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:08,529 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44346 synced till here 44345
2014-07-22 08:58:08,540 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044686162 with entries=80, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044688504
2014-07-22 08:58:09,225 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8294, memsize=288.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/d819f84823c1410eb84016700d5bb265
2014-07-22 08:58:09,430 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:10,419 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/d819f84823c1410eb84016700d5bb265 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/d819f84823c1410eb84016700d5bb265
2014-07-22 08:58:10,423 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44466 synced till here 44446
2014-07-22 08:58:10,437 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/d819f84823c1410eb84016700d5bb265, entries=1050740, sequenceid=8294, filesize=74.9m
2014-07-22 08:58:10,438 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~934.7m/980100000, currentsize=349.6m/366545040 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 22418ms, sequenceid=8294, compaction requested=true
2014-07-22 08:58:10,438 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:58:10,438 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 2000 blocking
2014-07-22 08:58:10,438 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-22 08:58:10,438 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 613.1m
2014-07-22 08:58:10,438 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:58:10,439 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:58:10,439 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 08:58:10,456 DEBUG [RpcServer.handler=24,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:58:10,498 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=6603, hits=3, hitRatio=0.04%, , cachingAccesses=5, cachingHits=3, cachingHitsRatio=60.00%, evictions=0, evicted=0, evictedPerRun=NaN
2014-07-22 08:58:10,625 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044688504 with entries=120, filesize=78.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044690329
2014-07-22 08:58:10,625 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044590768
2014-07-22 08:58:10,625 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044592976
2014-07-22 08:58:10,625 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044594995
2014-07-22 08:58:10,626 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044596695
2014-07-22 08:58:10,626 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044598424
2014-07-22 08:58:10,626 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044600382
2014-07-22 08:58:10,626 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044602102
2014-07-22 08:58:10,626 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044609003
2014-07-22 08:58:10,626 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044611201
2014-07-22 08:58:10,626 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044613968
2014-07-22 08:58:10,626 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044616342
2014-07-22 08:58:11,180 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:58:11,466 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:12,485 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44589 synced till here 44565
2014-07-22 08:58:12,579 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044690329 with entries=123, filesize=75.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044691466
2014-07-22 08:58:13,249 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8416, memsize=124.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/b6434a7341fa4ec7b5e4ffe745f7d62c
2014-07-22 08:58:13,265 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/b6434a7341fa4ec7b5e4ffe745f7d62c as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/b6434a7341fa4ec7b5e4ffe745f7d62c
2014-07-22 08:58:13,279 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/b6434a7341fa4ec7b5e4ffe745f7d62c, entries=452320, sequenceid=8416, filesize=32.2m
2014-07-22 08:58:13,279 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~572.6m/600431840, currentsize=179.7m/188425440 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 10140ms, sequenceid=8416, compaction requested=true
2014-07-22 08:58:13,280 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:58:13,280 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 2000 blocking
2014-07-22 08:58:13,280 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 395.3m
2014-07-22 08:58:13,280 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-22 08:58:13,280 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:58:13,280 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:58:13,280 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 08:58:13,333 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:13,362 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44674 synced till here 44673
2014-07-22 08:58:13,398 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044691466 with entries=85, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044693334
2014-07-22 08:58:14,111 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:58:14,737 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:14,764 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44754 synced till here 44751
2014-07-22 08:58:14,793 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044693334 with entries=80, filesize=65.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044694737
2014-07-22 08:58:16,249 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:16,311 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044694737 with entries=84, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044696250
2014-07-22 08:58:17,621 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:58:17,809 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:17,839 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44936 synced till here 44934
2014-07-22 08:58:17,939 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044696250 with entries=98, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044697809
2014-07-22 08:58:19,407 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:19,433 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45036 synced till here 45017
2014-07-22 08:58:19,830 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044697809 with entries=100, filesize=73.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044699408
2014-07-22 08:58:19,831 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:58:21,403 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:21,428 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45112 synced till here 45107
2014-07-22 08:58:21,493 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044699408 with entries=76, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044701404
2014-07-22 08:58:21,494 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:58:22,150 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:22,180 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044701404 with entries=82, filesize=61.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044702151
2014-07-22 08:58:22,180 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:58:23,520 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:24,557 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45305 synced till here 45302
2014-07-22 08:58:24,578 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044702151 with entries=111, filesize=89.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044703521
2014-07-22 08:58:24,579 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:58:25,428 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:25,604 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8481, memsize=184.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/9320f83778d449268e70cd71a3b37bc7
2014-07-22 08:58:25,623 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45411 synced till here 45408
2014-07-22 08:58:26,210 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/9320f83778d449268e70cd71a3b37bc7 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/9320f83778d449268e70cd71a3b37bc7
2014-07-22 08:58:26,239 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044703521 with entries=106, filesize=76.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044705429
2014-07-22 08:58:26,240 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:58:26,254 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/9320f83778d449268e70cd71a3b37bc7, entries=671580, sequenceid=8481, filesize=47.8m
2014-07-22 08:58:26,254 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~617.7m/647704800, currentsize=308.4m/323398640 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 15816ms, sequenceid=8481, compaction requested=true
2014-07-22 08:58:26,254 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:58:26,255 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 2000 blocking
2014-07-22 08:58:26,255 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-22 08:58:26,255 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 651.2m
2014-07-22 08:58:26,255 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:58:26,255 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:58:26,255 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 08:58:26,362 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:58:26,970 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:58:27,000 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:27,183 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8532, memsize=215.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/348ad7e8ac1441c4a47da256374f0956
2014-07-22 08:58:27,219 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/348ad7e8ac1441c4a47da256374f0956 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/348ad7e8ac1441c4a47da256374f0956
2014-07-22 08:58:27,228 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044705429 with entries=101, filesize=78.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044707000
2014-07-22 08:58:27,228 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:58:27,903 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/348ad7e8ac1441c4a47da256374f0956, entries=782960, sequenceid=8532, filesize=55.7m
2014-07-22 08:58:27,903 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~398.9m/418328560, currentsize=277.1m/290527520 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 14623ms, sequenceid=8532, compaction requested=true
2014-07-22 08:58:27,904 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:58:27,904 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 2000 blocking
2014-07-22 08:58:27,904 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-22 08:58:27,904 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 458.7m
2014-07-22 08:58:27,904 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:58:27,904 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:58:27,904 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 08:58:27,934 DEBUG [RpcServer.handler=37,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:58:28,302 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:58:28,625 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:28,789 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45614 synced till here 45613
2014-07-22 08:58:28,806 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044707000 with entries=102, filesize=72.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044708625
2014-07-22 08:58:28,807 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:58:30,267 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:30,302 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044708625 with entries=79, filesize=60.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044710267
2014-07-22 08:58:30,302 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:58:31,672 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:31,707 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45779 synced till here 45771
2014-07-22 08:58:31,759 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044710267 with entries=86, filesize=68.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044711673
2014-07-22 08:58:31,760 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:58:33,142 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:33,159 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45871 synced till here 45869
2014-07-22 08:58:33,385 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044711673 with entries=92, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044713143
2014-07-22 08:58:33,391 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:58:35,254 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:35,279 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45992 synced till here 45977
2014-07-22 08:58:35,426 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044713143 with entries=121, filesize=75.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044715254
2014-07-22 08:58:35,426 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:58:36,953 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:37,127 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46098 synced till here 46084
2014-07-22 08:58:37,283 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044715254 with entries=106, filesize=82.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044716954
2014-07-22 08:58:37,284 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=44, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:58:38,907 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:38,935 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46185 synced till here 46177
2014-07-22 08:58:39,030 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044716954 with entries=87, filesize=70.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044718907
2014-07-22 08:58:39,031 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=45, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:58:40,780 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:40,817 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46263 synced till here 46258
2014-07-22 08:58:40,870 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044718907 with entries=78, filesize=65.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044720781
2014-07-22 08:58:40,872 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=46, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:58:42,411 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:42,440 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46370 synced till here 46358
2014-07-22 08:58:42,517 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044720781 with entries=107, filesize=69.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044722411
2014-07-22 08:58:42,518 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=47, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:58:43,450 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:43,469 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46449 synced till here 46444
2014-07-22 08:58:43,554 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044722411 with entries=79, filesize=66.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044723451
2014-07-22 08:58:43,554 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=48, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:58:44,462 DEBUG [RpcServer.handler=46,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b.
2014-07-22 08:58:45,202 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:45,602 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044723451 with entries=97, filesize=77.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044725202
2014-07-22 08:58:45,603 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=49, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:58:47,040 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:47,190 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46636 synced till here 46633
2014-07-22 08:58:47,213 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044725202 with entries=90, filesize=69.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044727041
2014-07-22 08:58:47,213 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=50, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 08:58:48,848 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8689, memsize=291.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/8d2eefc5ccbe4653960687bf14e901bf
2014-07-22 08:58:49,060 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:49,071 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/8d2eefc5ccbe4653960687bf14e901bf as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/8d2eefc5ccbe4653960687bf14e901bf
2014-07-22 08:58:49,092 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/8d2eefc5ccbe4653960687bf14e901bf, entries=1060720, sequenceid=8689, filesize=75.5m
2014-07-22 08:58:49,093 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~463.6m/486161600, currentsize=386.1m/404870080 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 21189ms, sequenceid=8689, compaction requested=true
2014-07-22 08:58:49,093 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:58:49,094 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 2000 blocking
2014-07-22 08:58:49,094 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b., current region memstore size 267.6m
2014-07-22 08:58:49,094 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-22 08:58:49,094 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:58:49,094 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:58:49,094 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 08:58:49,121 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46720 synced till here 46717
2014-07-22 08:58:49,180 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044727041 with entries=84, filesize=66.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044729061
2014-07-22 08:58:49,249 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:58:50,335 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:58:50,802 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:50,857 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46809 synced till here 46794
2014-07-22 08:58:50,972 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044729061 with entries=89, filesize=72.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044730803
2014-07-22 08:58:52,988 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:53,032 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46933 synced till here 46914
2014-07-22 08:58:53,251 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044730803 with entries=124, filesize=77.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044732990
2014-07-22 08:58:53,301 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8682, memsize=368.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/5eb50d1382a44aff890bec4d8eb83eb5
2014-07-22 08:58:53,320 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/5eb50d1382a44aff890bec4d8eb83eb5 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/5eb50d1382a44aff890bec4d8eb83eb5
2014-07-22 08:58:53,353 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/5eb50d1382a44aff890bec4d8eb83eb5, entries=1342800, sequenceid=8682, filesize=95.6m
2014-07-22 08:58:53,354 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~660.5m/692609440, currentsize=494.5m/518532640 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 27099ms, sequenceid=8682, compaction requested=true
2014-07-22 08:58:53,355 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 2000 blocking
2014-07-22 08:58:53,355 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-22 08:58:53,355 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:58:53,355 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:58:53,355 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 08:58:53,357 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:58:53,358 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 792.4m
2014-07-22 08:58:53,506 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 08:58:54,914 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:54,936 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47019 synced till here 47011
2014-07-22 08:58:55,052 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044732990 with entries=86, filesize=69.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044734915
2014-07-22 08:58:55,132 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:58:56,805 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:56,969 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47122 synced till here 47118
2014-07-22 08:58:57,011 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044734915 with entries=103, filesize=93.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044736806
2014-07-22 08:58:58,594 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:58:58,640 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47205 synced till here 47200
2014-07-22 08:58:58,694 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044736806 with entries=83, filesize=67.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044738595
2014-07-22 08:59:00,193 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:59:00,432 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044738595 with entries=101, filesize=76.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044740193
2014-07-22 08:59:02,054 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:59:02,077 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47389 synced till here 47385
2014-07-22 08:59:02,136 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044740193 with entries=83, filesize=65.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044742055
2014-07-22 08:59:02,656 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11176, memsize=194.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/668c74b6404f4f02bd743cb3adc32b00
2014-07-22 08:59:02,671 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/668c74b6404f4f02bd743cb3adc32b00 as hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/668c74b6404f4f02bd743cb3adc32b00
2014-07-22 08:59:02,690 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/668c74b6404f4f02bd743cb3adc32b00, entries=706820, sequenceid=11176, filesize=50.4m
2014-07-22 08:59:02,690 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~268.1m/281110560, currentsize=36.2m/37920480 for region usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. in 13596ms, sequenceid=11176, compaction requested=true
2014-07-22 08:59:02,691 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 2000 blocking
2014-07-22 08:59:02,691 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-22 08:59:02,691 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:59:02,691 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:59:02,691 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. because compaction request was cancelled
2014-07-22 08:59:02,691 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:59:02,691 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 915.4m
2014-07-22 08:59:03,847 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:59:03,872 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47481 synced till here 47473
2014-07-22 08:59:03,945 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044742055 with entries=92, filesize=66.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044743848
2014-07-22 08:59:03,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044618305
2014-07-22 08:59:03,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044621282
2014-07-22 08:59:03,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044623260
2014-07-22 08:59:03,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044625234
2014-07-22 08:59:03,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044626966
2014-07-22 08:59:03,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044627843
2014-07-22 08:59:03,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044629272
2014-07-22 08:59:03,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044630720
2014-07-22 08:59:03,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044633753
2014-07-22 08:59:03,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044637372
2014-07-22 08:59:03,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044641176
2014-07-22 08:59:03,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044644947
2014-07-22 08:59:03,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044647308
2014-07-22 08:59:03,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044650903
2014-07-22 08:59:03,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044653205
2014-07-22 08:59:03,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044655468
2014-07-22 08:59:03,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044669322
2014-07-22 08:59:03,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044671611
2014-07-22 08:59:03,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044672582
2014-07-22 08:59:03,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044674638
2014-07-22 08:59:03,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044676378
2014-07-22 08:59:03,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044679527
2014-07-22 08:59:03,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044681583
2014-07-22 08:59:03,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044683373
2014-07-22 08:59:03,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044684752
2014-07-22 08:59:03,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044686162
2014-07-22 08:59:03,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044688504
2014-07-22 08:59:04,469 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:59:05,319 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:59:05,334 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47577 synced till here 47571
2014-07-22 08:59:05,405 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044743848 with entries=96, filesize=74.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044745320
2014-07-22 08:59:06,110 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:59:06,901 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47665 synced till here 47662
2014-07-22 08:59:06,928 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044745320 with entries=88, filesize=70.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044746111
2014-07-22 08:59:07,601 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:59:08,862 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47769 synced till here 47754
2014-07-22 08:59:08,945 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044746111 with entries=104, filesize=82.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044747602
2014-07-22 08:59:09,768 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:59:10,939 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1122ms
GC pool 'ParNew' had collection(s): count=1 time=1128ms
2014-07-22 08:59:10,960 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47872 synced till here 47853
2014-07-22 08:59:11,117 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044747602 with entries=103, filesize=77.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044749768
2014-07-22 08:59:13,009 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:59:13,078 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47979 synced till here 47958
2014-07-22 08:59:13,230 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044749768 with entries=107, filesize=84.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044753010
2014-07-22 08:59:13,654 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,657 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,658 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,658 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,668 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,673 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,730 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,733 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,736 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,739 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,781 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,812 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,813 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,813 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,813 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,816 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,817 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,818 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,819 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,820 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,837 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,837 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,838 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,840 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,842 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,849 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,849 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,851 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,852 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,852 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,853 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,899 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,940 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,966 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:13,995 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:14,928 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:14,940 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:14,950 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:14,989 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:15,021 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:15,052 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:15,053 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:15,053 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:15,055 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:15,055 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:15,092 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:15,121 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:15,156 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:15,192 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:15,226 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:18,654 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:59:18,657 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:59:18,658 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:59:18,658 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:59:18,668 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:59:18,673 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:59:18,731 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:59:18,733 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:59:18,736 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:59:18,740 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:59:18,781 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:59:18,812 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:59:18,813 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:59:18,813 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:59:18,814 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:59:18,816 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:59:18,817 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:59:18,819 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:59:18,820 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:59:18,821 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:59:18,837 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:59:18,838 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:59:18,839 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:59:18,841 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:59:18,843 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:59:18,849 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:59:18,850 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:59:18,852 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:59:18,852 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:59:18,853 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:59:18,854 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:59:18,899 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:59:18,940 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:59:18,967 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:59:18,996 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 08:59:20,215 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5023ms
2014-07-22 08:59:20,216 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5060ms
2014-07-22 08:59:20,217 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5267ms
2014-07-22 08:59:20,217 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5228ms
2014-07-22 08:59:20,218 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5197ms
2014-07-22 08:59:20,218 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5167ms
2014-07-22 08:59:20,219 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5167ms
2014-07-22 08:59:20,219 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5166ms
2014-07-22 08:59:20,219 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5164ms
2014-07-22 08:59:20,219 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5164ms
2014-07-22 08:59:20,219 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5098ms
2014-07-22 08:59:20,220 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5128ms
2014-07-22 08:59:20,221 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5280ms
2014-07-22 08:59:20,221 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5293ms
2014-07-22 08:59:20,226 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 08:59:22,190 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8943, memsize=410.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/979836cdda4749da82f392330eac026f
2014-07-22 08:59:22,211 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/979836cdda4749da82f392330eac026f as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/979836cdda4749da82f392330eac026f
2014-07-22 08:59:22,226 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/979836cdda4749da82f392330eac026f, entries=1496170, sequenceid=8943, filesize=106.5m
2014-07-22 08:59:22,227 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~803.3m/842351600, currentsize=373.1m/391229040 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 28869ms, sequenceid=8943, compaction requested=true
2014-07-22 08:59:22,227 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:59:22,227 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 2000 blocking
2014-07-22 08:59:22,227 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-22 08:59:22,227 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:59:22,228 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:59:22,228 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 08:59:22,228 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 840.1m
2014-07-22 08:59:22,228 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7002ms
2014-07-22 08:59:22,228 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,228 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7300ms
2014-07-22 08:59:22,228 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,229 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7288ms
2014-07-22 08:59:22,229 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,229 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7137ms
2014-07-22 08:59:22,229 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,230 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7108ms
2014-07-22 08:59:22,230 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,230 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7175ms
2014-07-22 08:59:22,230 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,245 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7190ms
2014-07-22 08:59:22,245 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,246 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7192ms
2014-07-22 08:59:22,246 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,257 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7205ms
2014-07-22 08:59:22,257 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,265 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7214ms
2014-07-22 08:59:22,265 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,266 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7244ms
2014-07-22 08:59:22,266 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,270 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7280ms
2014-07-22 08:59:22,270 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,278 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7327ms
2014-07-22 08:59:22,278 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,279 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7123ms
2014-07-22 08:59:22,280 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,280 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7088ms
2014-07-22 08:59:22,280 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,281 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8285ms
2014-07-22 08:59:22,281 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,289 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8323ms
2014-07-22 08:59:22,290 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,295 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8355ms
2014-07-22 08:59:22,295 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,301 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8402ms
2014-07-22 08:59:22,301 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,305 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8452ms
2014-07-22 08:59:22,305 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,306 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8453ms
2014-07-22 08:59:22,306 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,307 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8454ms
2014-07-22 08:59:22,307 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,307 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8456ms
2014-07-22 08:59:22,307 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,310 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8460ms
2014-07-22 08:59:22,310 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,311 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8462ms
2014-07-22 08:59:22,311 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,311 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8469ms
2014-07-22 08:59:22,311 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,312 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8471ms
2014-07-22 08:59:22,312 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,315 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8477ms
2014-07-22 08:59:22,315 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,315 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8478ms
2014-07-22 08:59:22,315 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,316 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8479ms
2014-07-22 08:59:22,316 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,317 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8496ms
2014-07-22 08:59:22,317 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,318 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8498ms
2014-07-22 08:59:22,319 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,319 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8501ms
2014-07-22 08:59:22,320 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,321 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8503ms
2014-07-22 08:59:22,321 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,321 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8505ms
2014-07-22 08:59:22,321 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,322 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8508ms
2014-07-22 08:59:22,322 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,322 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8509ms
2014-07-22 08:59:22,323 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,323 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8511ms
2014-07-22 08:59:22,323 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,326 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8514ms
2014-07-22 08:59:22,326 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,326 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8545ms
2014-07-22 08:59:22,326 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,329 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8589ms
2014-07-22 08:59:22,329 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,330 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8593ms
2014-07-22 08:59:22,330 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,332 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8598ms
2014-07-22 08:59:22,332 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,332 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8602ms
2014-07-22 08:59:22,332 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,338 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8664ms
2014-07-22 08:59:22,338 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,338 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8670ms
2014-07-22 08:59:22,395 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:59:22,405 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,414 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8755ms
2014-07-22 08:59:22,415 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,419 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8762ms
2014-07-22 08:59:22,419 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,425 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8769ms
2014-07-22 08:59:22,425 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,426 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8771ms
2014-07-22 08:59:22,426 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:22,454 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11094,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044751360,"queuetimems":1,"class":"HRegionServer","responsesize":18792,"method":"Multi"}
2014-07-22 08:59:22,554 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48073 synced till here 48061
2014-07-22 08:59:22,715 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044753010 with entries=94, filesize=71.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044762406
2014-07-22 08:59:22,716 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044690329
2014-07-22 08:59:23,197 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 08:59:23,199 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11620,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044751578,"queuetimems":0,"class":"HRegionServer","responsesize":18531,"method":"Multi"}
2014-07-22 08:59:23,199 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10279,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044752919,"queuetimems":4,"class":"HRegionServer","responsesize":15564,"method":"Multi"}
2014-07-22 08:59:23,199 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11539,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044751659,"queuetimems":0,"class":"HRegionServer","responsesize":18681,"method":"Multi"}
2014-07-22 08:59:23,206 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10214,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044752992,"queuetimems":0,"class":"HRegionServer","responsesize":15282,"method":"Multi"}
2014-07-22 08:59:23,209 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11441,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044751767,"queuetimems":1,"class":"HRegionServer","responsesize":18490,"method":"Multi"}
2014-07-22 08:59:23,209 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11510,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044751697,"queuetimems":0,"class":"HRegionServer","responsesize":18738,"method":"Multi"}
2014-07-22 08:59:23,210 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10143,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044753065,"queuetimems":1,"class":"HRegionServer","responsesize":18750,"method":"Multi"}
2014-07-22 08:59:23,223 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10043,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044753179,"queuetimems":0,"class":"HRegionServer","responsesize":18991,"method":"Multi"}
2014-07-22 08:59:23,223 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10111,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044753111,"queuetimems":0,"class":"HRegionServer","responsesize":18522,"method":"Multi"}
2014-07-22 08:59:23,302 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:59:23,453 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10501,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044752952,"queuetimems":1,"class":"HRegionServer","responsesize":18792,"method":"Multi"}
2014-07-22 08:59:23,462 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11661,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044751800,"queuetimems":1,"class":"HRegionServer","responsesize":16974,"method":"Multi"}
2014-07-22 08:59:23,470 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10209,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044753260,"queuetimems":0,"class":"HRegionServer","responsesize":18490,"method":"Multi"}
2014-07-22 08:59:24,661 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11061,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044753600,"queuetimems":0,"class":"HRegionServer","responsesize":13005,"method":"Multi"}
2014-07-22 08:59:24,666 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11134,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044753532,"queuetimems":1,"class":"HRegionServer","responsesize":15680,"method":"Multi"}
2014-07-22 08:59:24,838 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:59:24,861 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48195 synced till here 48165
2014-07-22 08:59:25,039 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11572,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044753466,"queuetimems":0,"class":"HRegionServer","responsesize":13354,"method":"Multi"}
2014-07-22 08:59:25,039 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10100,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044754938,"queuetimems":0,"class":"HRegionServer","responsesize":18979,"method":"Multi"}
2014-07-22 08:59:25,141 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044762406 with entries=122, filesize=101.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044764838
2014-07-22 08:59:25,687 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10636,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044755051,"queuetimems":0,"class":"HRegionServer","responsesize":17878,"method":"Multi"}
2014-07-22 08:59:25,687 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11724,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044753962,"queuetimems":0,"class":"HRegionServer","responsesize":10901,"method":"Multi"}
2014-07-22 08:59:25,687 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11694,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044753992,"queuetimems":0,"class":"HRegionServer","responsesize":13079,"method":"Multi"}
2014-07-22 08:59:25,688 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10569,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044755118,"queuetimems":0,"class":"HRegionServer","responsesize":15380,"method":"Multi"}
2014-07-22 08:59:25,688 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10740,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044754948,"queuetimems":0,"class":"HRegionServer","responsesize":13005,"method":"Multi"}
2014-07-22 08:59:25,688 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11795,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044753893,"queuetimems":0,"class":"HRegionServer","responsesize":18533,"method":"Multi"}
2014-07-22 08:59:25,687 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10597,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044755090,"queuetimems":1,"class":"HRegionServer","responsesize":18309,"method":"Multi"}
2014-07-22 08:59:25,695 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10675,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044755019,"queuetimems":1,"class":"HRegionServer","responsesize":18738,"method":"Multi"}
2014-07-22 08:59:25,687 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10464,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044755223,"queuetimems":0,"class":"HRegionServer","responsesize":18750,"method":"Multi"}
2014-07-22 08:59:25,688 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10704,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044754984,"queuetimems":0,"class":"HRegionServer","responsesize":18681,"method":"Multi"}
2014-07-22 08:59:25,688 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10534,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044755154,"queuetimems":1,"class":"HRegionServer","responsesize":18531,"method":"Multi"}
2014-07-22 08:59:26,607 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12828,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044753778,"queuetimems":0,"class":"HRegionServer","responsesize":15564,"method":"Multi"}
2014-07-22 08:59:26,608 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12941,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044753667,"queuetimems":0,"class":"HRegionServer","responsesize":18979,"method":"Multi"}
2014-07-22 08:59:26,734 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:59:26,750 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11561,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044755188,"queuetimems":0,"class":"HRegionServer","responsesize":18522,"method":"Multi"}
2014-07-22 08:59:26,757 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12920,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044753836,"queuetimems":0,"class":"HRegionServer","responsesize":18490,"method":"Multi"}
2014-07-22 08:59:26,772 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48299 synced till here 48267
2014-07-22 08:59:26,894 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12957,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044753937,"queuetimems":0,"class":"HRegionServer","responsesize":18742,"method":"Multi"}
2014-07-22 08:59:27,003 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044764838 with entries=104, filesize=87.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044766735
2014-07-22 08:59:27,214 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13484,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044753729,"queuetimems":0,"class":"HRegionServer","responsesize":16974,"method":"Multi"}
2014-07-22 08:59:29,378 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:59:29,442 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48432 synced till here 48402
2014-07-22 08:59:29,710 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044766735 with entries=133, filesize=102.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044769378
2014-07-22 08:59:31,291 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:59:31,346 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48543 synced till here 48520
2014-07-22 08:59:31,472 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044769378 with entries=111, filesize=92.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044771292
2014-07-22 08:59:31,712 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:31,715 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:31,716 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:31,734 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:31,746 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:31,761 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:31,779 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:31,792 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:31,793 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:31,793 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:31,794 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:31,795 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:31,805 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:31,808 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:31,914 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:31,914 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:31,957 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:31,969 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:31,988 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:32,549 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:32,550 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:32,587 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:32,602 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:32,772 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:32,772 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:33,263 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:33,309 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:33,338 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:33,343 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:34,288 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 08:59:34,932 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9054, memsize=444.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/297bdc54b1ec4ef3871f387ea7738112
2014-07-22 08:59:34,948 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/297bdc54b1ec4ef3871f387ea7738112 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/297bdc54b1ec4ef3871f387ea7738112
2014-07-22 08:59:34,956 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/297bdc54b1ec4ef3871f387ea7738112, entries=1618560, sequenceid=9054, filesize=115.3m
2014-07-22 08:59:34,956 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~919.1m/963762000, currentsize=406.2m/425982480 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 32265ms, sequenceid=9054, compaction requested=true
2014-07-22 08:59:34,957 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:59:34,957 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 2000 blocking
2014-07-22 08:59:34,957 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 669ms
2014-07-22 08:59:34,957 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-22 08:59:34,957 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:34,957 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 1.0g
2014-07-22 08:59:34,957 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1615ms
2014-07-22 08:59:34,957 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:34,957 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1619ms
2014-07-22 08:59:34,957 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:34,957 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1648ms
2014-07-22 08:59:34,957 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:34,958 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1695ms
2014-07-22 08:59:34,958 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:34,958 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2186ms
2014-07-22 08:59:34,958 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:34,957 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:59:34,958 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:59:34,958 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 08:59:34,958 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2186ms
2014-07-22 08:59:34,959 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:34,961 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2359ms
2014-07-22 08:59:34,962 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:34,962 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2376ms
2014-07-22 08:59:34,962 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:34,967 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2417ms
2014-07-22 08:59:34,967 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:34,969 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2420ms
2014-07-22 08:59:34,969 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:34,969 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2982ms
2014-07-22 08:59:34,969 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:34,969 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3000ms
2014-07-22 08:59:34,969 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:34,969 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3012ms
2014-07-22 08:59:34,969 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:34,969 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3055ms
2014-07-22 08:59:34,970 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:34,972 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3058ms
2014-07-22 08:59:34,972 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:34,972 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3164ms
2014-07-22 08:59:34,972 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:34,972 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3167ms
2014-07-22 08:59:34,972 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:34,973 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3178ms
2014-07-22 08:59:34,973 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:34,973 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3179ms
2014-07-22 08:59:34,973 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:34,974 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3181ms
2014-07-22 08:59:34,974 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:34,987 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3194ms
2014-07-22 08:59:34,987 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:34,997 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3205ms
2014-07-22 08:59:34,997 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:34,997 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3218ms
2014-07-22 08:59:34,997 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:34,998 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3236ms
2014-07-22 08:59:34,998 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:34,998 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3253ms
2014-07-22 08:59:34,998 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:35,001 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3267ms
2014-07-22 08:59:35,001 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:35,001 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3285ms
2014-07-22 08:59:35,002 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:35,002 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3287ms
2014-07-22 08:59:35,002 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:35,002 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3291ms
2014-07-22 08:59:35,002 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 08:59:35,134 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 08:59:35,290 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:59:35,326 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48641 synced till here 48631
2014-07-22 08:59:35,417 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044771292 with entries=98, filesize=69.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044775290
2014-07-22 08:59:35,417 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044691466
2014-07-22 08:59:35,418 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044693334
2014-07-22 08:59:35,418 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044694737
2014-07-22 08:59:35,418 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044696250
2014-07-22 08:59:35,418 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044697809
2014-07-22 08:59:35,418 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044699408
2014-07-22 08:59:35,418 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044701404
2014-07-22 08:59:35,418 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044702151
2014-07-22 08:59:35,418 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044703521
2014-07-22 08:59:36,109 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:59:38,579 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:59:38,830 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044775290 with entries=95, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044778580
2014-07-22 08:59:40,639 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:59:40,659 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48819 synced till here 48816
2014-07-22 08:59:40,706 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044778580 with entries=83, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044780639
2014-07-22 08:59:41,625 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:59:42,239 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48926 synced till here 48923
2014-07-22 08:59:42,313 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044780639 with entries=107, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044781626
2014-07-22 08:59:43,422 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9176, memsize=329.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/f174825b038d4ce6a322c65aa03ee341
2014-07-22 08:59:43,435 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/f174825b038d4ce6a322c65aa03ee341 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/f174825b038d4ce6a322c65aa03ee341
2014-07-22 08:59:43,443 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/f174825b038d4ce6a322c65aa03ee341, entries=1199480, sequenceid=9176, filesize=85.5m
2014-07-22 08:59:43,444 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~840.1m/880932560, currentsize=311.1m/326194160 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 21216ms, sequenceid=9176, compaction requested=true
2014-07-22 08:59:43,444 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:59:43,445 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 2000 blocking
2014-07-22 08:59:43,445 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-22 08:59:43,445 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 696.9m
2014-07-22 08:59:43,445 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:59:43,445 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:59:43,445 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 08:59:43,581 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 08:59:43,813 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:59:44,803 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044781626 with entries=120, filesize=69.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044783813
2014-07-22 08:59:44,885 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 08:59:45,740 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:59:45,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49130 synced till here 49128
2014-07-22 08:59:45,854 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044783813 with entries=84, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044785742
2014-07-22 08:59:47,444 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:59:48,332 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49248 synced till here 49244
2014-07-22 08:59:48,372 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044785742 with entries=118, filesize=92.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044787444
2014-07-22 08:59:49,121 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:59:49,150 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49355 synced till here 49342
2014-07-22 08:59:49,352 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044787444 with entries=107, filesize=67.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044789122
2014-07-22 08:59:51,242 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:59:51,275 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49457 synced till here 49447
2014-07-22 08:59:51,757 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044789122 with entries=102, filesize=75.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044791242
2014-07-22 08:59:53,251 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:59:53,272 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49534 synced till here 49531
2014-07-22 08:59:53,854 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044791242 with entries=77, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044793252
2014-07-22 08:59:55,195 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:59:55,223 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49621 synced till here 49620
2014-07-22 08:59:55,238 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044793252 with entries=87, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044795196
2014-07-22 08:59:56,614 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:59:56,819 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49715 synced till here 49713
2014-07-22 08:59:56,831 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044795196 with entries=94, filesize=76.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044796615
2014-07-22 08:59:58,583 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 08:59:58,600 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49799 synced till here 49798
2014-07-22 08:59:58,622 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044796615 with entries=84, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044798584
2014-07-22 08:59:59,199 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9307, memsize=341.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/ed2891e2abe44f1893da2868304688b9
2014-07-22 08:59:59,233 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/ed2891e2abe44f1893da2868304688b9 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/ed2891e2abe44f1893da2868304688b9
2014-07-22 08:59:59,251 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/ed2891e2abe44f1893da2868304688b9, entries=1241830, sequenceid=9307, filesize=88.5m
2014-07-22 08:59:59,252 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.0g/1120126800, currentsize=387.0m/405805440 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 24295ms, sequenceid=9307, compaction requested=true
2014-07-22 08:59:59,252 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 08:59:59,252 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 2000 blocking
2014-07-22 08:59:59,252 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-22 08:59:59,252 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 787.1m
2014-07-22 08:59:59,252 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 08:59:59,252 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 08:59:59,253 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 08:59:59,301 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:00:00,618 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:00:00,637 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49879 synced till here 49876
2014-07-22 09:00:00,684 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044798584 with entries=80, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044800619
2014-07-22 09:00:00,684 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044705429
2014-07-22 09:00:00,684 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044707000
2014-07-22 09:00:00,684 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044708625
2014-07-22 09:00:00,684 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044710267
2014-07-22 09:00:00,684 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044711673
2014-07-22 09:00:00,684 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044713143
2014-07-22 09:00:00,684 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044715254
2014-07-22 09:00:00,684 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044716954
2014-07-22 09:00:00,684 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044718907
2014-07-22 09:00:00,685 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044720781
2014-07-22 09:00:00,685 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044722411
2014-07-22 09:00:00,685 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044723451
2014-07-22 09:00:00,685 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044725202
2014-07-22 09:00:00,685 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044727041
2014-07-22 09:00:00,922 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:00:01,360 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:00:01,376 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49961 synced till here 49959
2014-07-22 09:00:01,413 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044800619 with entries=82, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044801360
2014-07-22 09:00:01,413 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:00:02,704 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1188ms
GC pool 'ParNew' had collection(s): count=1 time=1273ms
2014-07-22 09:00:03,018 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9355, memsize=244.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/abd9e39b914f4e589f86b934c6beaacc
2014-07-22 09:00:03,043 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/abd9e39b914f4e589f86b934c6beaacc as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/abd9e39b914f4e589f86b934c6beaacc
2014-07-22 09:00:03,056 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/abd9e39b914f4e589f86b934c6beaacc, entries=891020, sequenceid=9355, filesize=63.4m
2014-07-22 09:00:03,057 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~696.9m/730724720, currentsize=318.6m/334111920 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 19612ms, sequenceid=9355, compaction requested=true
2014-07-22 09:00:03,057 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:00:03,058 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 2000 blocking
2014-07-22 09:00:03,058 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 637.1m
2014-07-22 09:00:03,058 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-22 09:00:03,058 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:00:03,058 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:00:03,058 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:00:03,074 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:00:03,501 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:00:03,523 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50052 synced till here 50046
2014-07-22 09:00:03,611 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044801360 with entries=91, filesize=66.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044803501
2014-07-22 09:00:03,612 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:00:04,380 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:00:04,961 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:00:04,980 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50156 synced till here 50141
2014-07-22 09:00:05,092 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044803501 with entries=104, filesize=69.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044804961
2014-07-22 09:00:05,097 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:00:06,379 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:00:06,414 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50243 synced till here 50242
2014-07-22 09:00:06,449 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044804961 with entries=87, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044806380
2014-07-22 09:00:06,450 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:00:08,542 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:00:08,580 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50335 synced till here 50331
2014-07-22 09:00:08,642 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044806380 with entries=92, filesize=67.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044808543
2014-07-22 09:00:08,643 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:00:10,145 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:00:10,162 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50436 synced till here 50429
2014-07-22 09:00:10,207 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044808543 with entries=101, filesize=69.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044810146
2014-07-22 09:00:10,207 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:00:11,734 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:00:11,749 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50522 synced till here 50515
2014-07-22 09:00:11,822 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044810146 with entries=86, filesize=70.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044811734
2014-07-22 09:00:11,826 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:00:12,521 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:00:13,152 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50621 synced till here 50619
2014-07-22 09:00:13,187 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044811734 with entries=99, filesize=68.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044812521
2014-07-22 09:00:13,187 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:00:14,157 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:00:14,189 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50703 synced till here 50701
2014-07-22 09:00:14,239 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044812521 with entries=82, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044814157
2014-07-22 09:00:14,240 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:00:15,708 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:00:16,831 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50843 synced till here 50839
2014-07-22 09:00:16,884 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044814157 with entries=140, filesize=90.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044815709
2014-07-22 09:00:16,885 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:00:17,816 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:00:19,093 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50947 synced till here 50930
2014-07-22 09:00:19,169 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044815709 with entries=104, filesize=79.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044817817
2014-07-22 09:00:19,170 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:00:19,899 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:00:19,979 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51027 synced till here 51015
2014-07-22 09:00:21,006 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044817817 with entries=80, filesize=70.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044819899
2014-07-22 09:00:21,006 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=44, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:00:21,869 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:00:23,060 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1076ms
GC pool 'ParNew' had collection(s): count=1 time=1103ms
2014-07-22 09:00:23,105 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51128 synced till here 51122
2014-07-22 09:00:23,220 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044819899 with entries=101, filesize=87.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044823060
2014-07-22 09:00:23,221 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=45, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:00:24,122 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:00:25,340 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51217 synced till here 51200
2014-07-22 09:00:25,363 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,364 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,365 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,368 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,369 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,369 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,371 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,372 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,403 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,433 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,433 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,433 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,434 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,438 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,439 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,442 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,469 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,472 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,474 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044823060 with entries=89, filesize=82.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044824123
2014-07-22 09:00:25,475 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=46, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:00:25,495 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,509 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,516 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,518 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,537 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,578 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,606 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,645 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,687 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,719 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,719 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,721 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,762 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,803 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,845 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,883 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:25,883 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:26,484 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9525, memsize=308.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/3a868d0ec2344e779d7827ff7ce59d36
2014-07-22 09:00:26,504 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/3a868d0ec2344e779d7827ff7ce59d36 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/3a868d0ec2344e779d7827ff7ce59d36
2014-07-22 09:00:26,525 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/3a868d0ec2344e779d7827ff7ce59d36, entries=1124480, sequenceid=9525, filesize=80.1m
2014-07-22 09:00:26,526 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~790.2m/828533200, currentsize=458.6m/480896480 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 27274ms, sequenceid=9525, compaction requested=true
2014-07-22 09:00:26,526 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:00:26,527 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 2000 blocking
2014-07-22 09:00:26,527 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-22 09:00:26,527 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 644ms
2014-07-22 09:00:26,527 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:00:26,527 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 854.1m
2014-07-22 09:00:26,527 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,527 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:00:26,527 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:00:26,530 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 646ms
2014-07-22 09:00:26,530 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,530 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 686ms
2014-07-22 09:00:26,530 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,531 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 728ms
2014-07-22 09:00:26,531 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,534 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 772ms
2014-07-22 09:00:26,535 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,535 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 814ms
2014-07-22 09:00:26,535 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,535 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 816ms
2014-07-22 09:00:26,536 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,536 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 817ms
2014-07-22 09:00:26,536 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,536 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 849ms
2014-07-22 09:00:26,536 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,537 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 892ms
2014-07-22 09:00:26,537 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,540 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 935ms
2014-07-22 09:00:26,540 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,542 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 963ms
2014-07-22 09:00:26,542 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,542 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1005ms
2014-07-22 09:00:26,542 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,542 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1024ms
2014-07-22 09:00:26,542 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,557 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1042ms
2014-07-22 09:00:26,557 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,558 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1049ms
2014-07-22 09:00:26,558 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,558 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1063ms
2014-07-22 09:00:26,558 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,558 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1086ms
2014-07-22 09:00:26,558 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,562 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1093ms
2014-07-22 09:00:26,562 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,563 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1120ms
2014-07-22 09:00:26,563 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,564 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1124ms
2014-07-22 09:00:26,564 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,565 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1128ms
2014-07-22 09:00:26,565 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,577 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1143ms
2014-07-22 09:00:26,577 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,589 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1156ms
2014-07-22 09:00:26,589 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,590 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1156ms
2014-07-22 09:00:26,590 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,590 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1158ms
2014-07-22 09:00:26,590 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,590 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1187ms
2014-07-22 09:00:26,590 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,590 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1218ms
2014-07-22 09:00:26,590 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,592 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1223ms
2014-07-22 09:00:26,592 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,597 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1228ms
2014-07-22 09:00:26,597 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,597 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1228ms
2014-07-22 09:00:26,597 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,605 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1237ms
2014-07-22 09:00:26,605 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,613 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1248ms
2014-07-22 09:00:26,613 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,614 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1250ms
2014-07-22 09:00:26,614 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,615 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1252ms
2014-07-22 09:00:26,615 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:26,710 DEBUG [RpcServer.handler=37,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:00:28,216 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9558, memsize=305.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/e92fb42bdc62482e94a05962b000770d
2014-07-22 09:00:28,229 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/e92fb42bdc62482e94a05962b000770d as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/e92fb42bdc62482e94a05962b000770d
2014-07-22 09:00:28,249 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/e92fb42bdc62482e94a05962b000770d, entries=1111370, sequenceid=9558, filesize=79.2m
2014-07-22 09:00:28,250 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~642.5m/673726400, currentsize=407.1m/426882400 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 25192ms, sequenceid=9558, compaction requested=true
2014-07-22 09:00:28,250 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:00:28,250 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 2000 blocking
2014-07-22 09:00:28,250 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-22 09:00:28,250 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:00:28,250 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b., current region memstore size 231.2m
2014-07-22 09:00:28,250 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:00:28,251 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:00:28,314 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:00:28,319 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:00:28,329 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51295 synced till here 51283
2014-07-22 09:00:28,429 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:00:28,430 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:00:28,571 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044824123 with entries=78, filesize=73.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044828314
2014-07-22 09:00:29,497 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:00:29,510 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51371 synced till here 51360
2014-07-22 09:00:30,691 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044828314 with entries=76, filesize=75.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044829498
2014-07-22 09:00:31,475 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:00:31,647 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51450 synced till here 51448
2014-07-22 09:00:31,684 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044829498 with entries=79, filesize=73.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044831476
2014-07-22 09:00:33,321 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:00:33,558 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51523 synced till here 51521
2014-07-22 09:00:33,608 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044831476 with entries=73, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044833321
2014-07-22 09:00:35,392 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:00:36,468 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51635 synced till here 51630
2014-07-22 09:00:36,518 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044833321 with entries=112, filesize=89.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044835393
2014-07-22 09:00:37,597 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:00:37,601 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12181, memsize=163.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/bd0bddcf732a4349912cf78b7d718c96
2014-07-22 09:00:38,699 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/bd0bddcf732a4349912cf78b7d718c96 as hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/bd0bddcf732a4349912cf78b7d718c96
2014-07-22 09:00:38,714 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51743 synced till here 51724
2014-07-22 09:00:38,717 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/bd0bddcf732a4349912cf78b7d718c96, entries=593930, sequenceid=12181, filesize=42.3m
2014-07-22 09:00:38,717 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~231.2m/242465040, currentsize=18.5m/19440560 for region usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. in 10467ms, sequenceid=12181, compaction requested=true
2014-07-22 09:00:38,718 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:00:38,718 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 2000 blocking
2014-07-22 09:00:38,718 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-22 09:00:38,718 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:00:38,718 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 911.9m
2014-07-22 09:00:38,718 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:00:38,718 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. because compaction request was cancelled
2014-07-22 09:00:38,864 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044835393 with entries=108, filesize=76.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044837598
2014-07-22 09:00:38,864 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044729061
2014-07-22 09:00:38,864 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044730803
2014-07-22 09:00:38,864 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044732990
2014-07-22 09:00:38,864 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044734915
2014-07-22 09:00:38,864 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044736806
2014-07-22 09:00:38,865 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044738595
2014-07-22 09:00:38,865 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044740193
2014-07-22 09:00:38,865 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044742055
2014-07-22 09:00:38,865 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044743848
2014-07-22 09:00:38,865 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044745320
2014-07-22 09:00:38,865 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044746111
2014-07-22 09:00:38,865 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044747602
2014-07-22 09:00:38,865 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044749768
2014-07-22 09:00:38,865 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044753010
2014-07-22 09:00:38,865 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044762406
2014-07-22 09:00:38,865 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044764838
2014-07-22 09:00:38,865 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044766735
2014-07-22 09:00:38,865 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044769378
2014-07-22 09:00:39,562 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:00:41,190 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1507ms
GC pool 'ParNew' had collection(s): count=1 time=1521ms
2014-07-22 09:00:41,238 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51849 synced till here 51823
2014-07-22 09:00:41,466 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:00:41,551 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044837598 with entries=106, filesize=86.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044839562
2014-07-22 09:00:43,469 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:00:43,513 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51941 synced till here 51921
2014-07-22 09:00:43,817 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044839562 with entries=92, filesize=82.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044843470
2014-07-22 09:00:45,510 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1126ms
GC pool 'ParNew' had collection(s): count=1 time=1145ms
2014-07-22 09:00:45,823 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:00:45,873 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52041 synced till here 52018
2014-07-22 09:00:46,524 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044843470 with entries=100, filesize=90.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044845823
2014-07-22 09:00:48,408 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:00:48,445 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52141 synced till here 52110
2014-07-22 09:00:48,723 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044845823 with entries=100, filesize=89.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044848408
2014-07-22 09:00:50,312 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:00:50,348 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52224 synced till here 52208
2014-07-22 09:00:50,946 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044848408 with entries=83, filesize=74.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044850312
2014-07-22 09:00:51,343 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:51,345 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:51,345 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:51,350 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:51,352 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:51,352 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:51,353 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:51,353 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:51,358 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:51,383 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:51,384 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:51,384 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:51,411 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:51,414 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:51,415 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:51,422 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:51,422 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:51,428 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:51,478 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:52,197 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:52,246 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:52,263 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:53,188 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:53,194 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:53,209 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:53,231 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:53,270 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:53,301 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:53,331 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:53,368 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:53,403 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:55,436 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:55,481 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:55,530 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:55,582 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:55,633 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:55,681 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:55,735 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:55,813 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:55,870 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:56,343 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:00:56,345 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:00:56,346 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:00:56,350 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:00:56,352 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:00:56,353 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:00:56,353 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:00:56,354 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:00:56,358 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:00:56,383 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:00:56,384 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:00:56,384 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:00:56,412 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:00:56,415 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:00:56,415 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:00:56,422 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:00:56,423 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:00:56,428 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:00:56,479 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:00:57,162 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:57,198 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:00:57,246 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:00:57,263 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:00:57,390 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:57,430 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:57,456 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:57,518 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:57,576 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:57,626 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:57,664 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9806, memsize=433.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/4704272443944fc08102bf22de5dbe8a
2014-07-22 09:00:57,674 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/4704272443944fc08102bf22de5dbe8a as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/4704272443944fc08102bf22de5dbe8a
2014-07-22 09:00:57,682 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:00:57,682 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/4704272443944fc08102bf22de5dbe8a, entries=1576420, sequenceid=9806, filesize=112.3m
2014-07-22 09:00:57,683 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~854.1m/895568400, currentsize=405.0m/424659760 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 31156ms, sequenceid=9806, compaction requested=true
2014-07-22 09:00:57,683 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:00:57,683 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 28 store files, 0 compacting, 28 eligible, 2000 blocking
2014-07-22 09:00:57,684 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2ms
2014-07-22 09:00:57,684 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,684 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 28 files from compaction candidates
2014-07-22 09:00:57,684 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 58ms
2014-07-22 09:00:57,684 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,684 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:00:57,684 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:00:57,684 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 867.5m
2014-07-22 09:00:57,684 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:00:57,685 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 109ms
2014-07-22 09:00:57,685 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,685 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 167ms
2014-07-22 09:00:57,685 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,686 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 230ms
2014-07-22 09:00:57,686 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,686 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 256ms
2014-07-22 09:00:57,686 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,686 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 296ms
2014-07-22 09:00:57,686 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,686 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5423ms
2014-07-22 09:00:57,686 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,686 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5440ms
2014-07-22 09:00:57,686 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,686 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5489ms
2014-07-22 09:00:57,687 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,687 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 525ms
2014-07-22 09:00:57,687 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,687 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6209ms
2014-07-22 09:00:57,687 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,688 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6260ms
2014-07-22 09:00:57,688 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,688 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6266ms
2014-07-22 09:00:57,688 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,693 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6271ms
2014-07-22 09:00:57,693 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,701 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6287ms
2014-07-22 09:00:57,701 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,701 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6287ms
2014-07-22 09:00:57,701 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,702 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6291ms
2014-07-22 09:00:57,702 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,702 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6318ms
2014-07-22 09:00:57,702 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,728 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6345ms
2014-07-22 09:00:57,728 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,729 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6346ms
2014-07-22 09:00:57,729 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,730 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6372ms
2014-07-22 09:00:57,730 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,733 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6380ms
2014-07-22 09:00:57,734 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,741 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6388ms
2014-07-22 09:00:57,741 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,743 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6391ms
2014-07-22 09:00:57,743 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,743 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6391ms
2014-07-22 09:00:57,743 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,744 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6393ms
2014-07-22 09:00:57,744 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,744 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6399ms
2014-07-22 09:00:57,744 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,751 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6406ms
2014-07-22 09:00:57,751 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,753 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6410ms
2014-07-22 09:00:57,753 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,753 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1883ms
2014-07-22 09:00:57,754 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,757 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1945ms
2014-07-22 09:00:57,757 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,757 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2023ms
2014-07-22 09:00:57,758 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,769 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2088ms
2014-07-22 09:00:57,769 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,777 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2144ms
2014-07-22 09:00:57,777 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,777 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2195ms
2014-07-22 09:00:57,777 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,778 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2248ms
2014-07-22 09:00:57,778 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,783 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2302ms
2014-07-22 09:00:57,783 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,785 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2349ms
2014-07-22 09:00:57,785 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,785 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4382ms
2014-07-22 09:00:57,785 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,785 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4417ms
2014-07-22 09:00:57,785 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,786 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4455ms
2014-07-22 09:00:57,786 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,788 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4487ms
2014-07-22 09:00:57,788 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,788 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4518ms
2014-07-22 09:00:57,788 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,789 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4557ms
2014-07-22 09:00:57,789 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,789 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4580ms
2014-07-22 09:00:57,789 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,790 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4595ms
2014-07-22 09:00:57,790 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,790 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4602ms
2014-07-22 09:00:57,790 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:00:57,884 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:00:59,338 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1154ms
GC pool 'ParNew' had collection(s): count=1 time=1171ms
2014-07-22 09:00:59,798 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:00:59,861 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:00:59,877 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52334 synced till here 52322
2014-07-22 09:00:59,925 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044850312 with entries=110, filesize=98.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044859799
2014-07-22 09:00:59,925 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044771292
2014-07-22 09:00:59,925 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044775290
2014-07-22 09:00:59,925 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044778580
2014-07-22 09:00:59,925 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044780639
2014-07-22 09:01:00,148 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10004,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044850143,"queuetimems":0,"class":"HRegionServer","responsesize":18956,"method":"Multi"}
2014-07-22 09:01:00,362 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10015,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044850346,"queuetimems":0,"class":"HRegionServer","responsesize":18769,"method":"Multi"}
2014-07-22 09:01:00,362 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10155,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044850207,"queuetimems":0,"class":"HRegionServer","responsesize":18118,"method":"Multi"}
2014-07-22 09:01:00,362 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10189,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044850172,"queuetimems":0,"class":"HRegionServer","responsesize":15236,"method":"Multi"}
2014-07-22 09:01:01,884 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:01,924 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52431 synced till here 52407
2014-07-22 09:01:02,143 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10732,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044851410,"queuetimems":1,"class":"HRegionServer","responsesize":18532,"method":"Multi"}
2014-07-22 09:01:02,254 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044859799 with entries=97, filesize=86.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044861885
2014-07-22 09:01:02,588 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10327,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044852261,"queuetimems":1,"class":"HRegionServer","responsesize":9633,"method":"Multi"}
2014-07-22 09:01:02,588 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10348,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044852240,"queuetimems":0,"class":"HRegionServer","responsesize":18372,"method":"Multi"}
2014-07-22 09:01:02,588 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11115,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044851473,"queuetimems":0,"class":"HRegionServer","responsesize":18881,"method":"Multi"}
2014-07-22 09:01:02,590 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10396,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044852193,"queuetimems":0,"class":"HRegionServer","responsesize":17951,"method":"Multi"}
2014-07-22 09:01:02,979 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:02,997 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52504 synced till here 52501
2014-07-22 09:01:03,041 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044861885 with entries=73, filesize=65.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044862979
2014-07-22 09:01:04,902 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:04,925 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52591 synced till here 52589
2014-07-22 09:01:04,972 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044862979 with entries=87, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044864903
2014-07-22 09:01:06,736 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9889, memsize=387.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/319f1ffb595647bb8c75970da6be031c
2014-07-22 09:01:06,748 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/319f1ffb595647bb8c75970da6be031c as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/319f1ffb595647bb8c75970da6be031c
2014-07-22 09:01:06,761 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/319f1ffb595647bb8c75970da6be031c, entries=1411930, sequenceid=9889, filesize=100.6m
2014-07-22 09:01:06,762 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~916.7m/961244080, currentsize=363.3m/380980800 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 28043ms, sequenceid=9889, compaction requested=true
2014-07-22 09:01:06,763 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:01:06,763 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 2000 blocking
2014-07-22 09:01:06,763 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 950.7m
2014-07-22 09:01:06,763 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-22 09:01:06,763 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:01:06,763 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:01:06,763 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:01:06,845 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:01:07,146 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:07,207 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044864903 with entries=83, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044867146
2014-07-22 09:01:07,208 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044781626
2014-07-22 09:01:07,208 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044783813
2014-07-22 09:01:07,208 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044785742
2014-07-22 09:01:07,208 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044787444
2014-07-22 09:01:07,208 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044789122
2014-07-22 09:01:07,208 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044791242
2014-07-22 09:01:07,208 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044793252
2014-07-22 09:01:07,208 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044795196
2014-07-22 09:01:07,208 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044796615
2014-07-22 09:01:07,742 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:01:08,894 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:08,918 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52768 synced till here 52763
2014-07-22 09:01:08,969 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044867146 with entries=94, filesize=66.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044868895
2014-07-22 09:01:10,585 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:10,607 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52858 synced till here 52854
2014-07-22 09:01:10,669 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044868895 with entries=90, filesize=65.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044870585
2014-07-22 09:01:12,046 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:12,064 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52945 synced till here 52943
2014-07-22 09:01:12,102 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044870585 with entries=87, filesize=68.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044872047
2014-07-22 09:01:13,002 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:13,617 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044872047 with entries=72, filesize=61.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044873002
2014-07-22 09:01:14,583 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:14,600 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53093 synced till here 53090
2014-07-22 09:01:14,648 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044873002 with entries=76, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044874584
2014-07-22 09:01:15,845 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10032, memsize=258.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/97474eb97b9a4d80a08bff1286603820
2014-07-22 09:01:15,861 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/97474eb97b9a4d80a08bff1286603820 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/97474eb97b9a4d80a08bff1286603820
2014-07-22 09:01:15,872 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/97474eb97b9a4d80a08bff1286603820, entries=941420, sequenceid=10032, filesize=67.0m
2014-07-22 09:01:15,873 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~867.5m/909654160, currentsize=303.7m/318501440 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 18189ms, sequenceid=10032, compaction requested=true
2014-07-22 09:01:15,873 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:01:15,873 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 2000 blocking
2014-07-22 09:01:15,873 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 700.8m
2014-07-22 09:01:15,873 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-22 09:01:15,874 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:01:15,874 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:01:15,874 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:01:16,039 DEBUG [RpcServer.handler=37,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:01:16,151 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:16,181 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53177 synced till here 53175
2014-07-22 09:01:16,242 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044874584 with entries=84, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044876151
2014-07-22 09:01:16,242 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044798584
2014-07-22 09:01:16,242 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044800619
2014-07-22 09:01:16,599 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:01:17,479 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:17,723 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044876151 with entries=93, filesize=74.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044877479
2014-07-22 09:01:19,041 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:19,742 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53392 synced till here 53391
2014-07-22 09:01:20,258 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044877479 with entries=122, filesize=107.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044879042
2014-07-22 09:01:23,517 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:24,181 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10115, memsize=274.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/1abe910859e94abdbe8f7cd78a21634c
2014-07-22 09:01:24,193 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/1abe910859e94abdbe8f7cd78a21634c as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/1abe910859e94abdbe8f7cd78a21634c
2014-07-22 09:01:24,205 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/1abe910859e94abdbe8f7cd78a21634c, entries=998850, sequenceid=10115, filesize=71.1m
2014-07-22 09:01:24,205 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~952.6m/998834880, currentsize=288.8m/302791440 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 17442ms, sequenceid=10115, compaction requested=true
2014-07-22 09:01:24,206 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:01:24,206 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 28 store files, 0 compacting, 28 eligible, 2000 blocking
2014-07-22 09:01:24,206 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 654.4m
2014-07-22 09:01:24,206 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 28 files from compaction candidates
2014-07-22 09:01:24,207 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:01:24,207 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:01:24,207 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:01:24,278 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:01:24,665 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:01:24,728 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53526 synced till here 53522
2014-07-22 09:01:24,810 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044879042 with entries=134, filesize=100.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044883517
2014-07-22 09:01:24,810 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044801360
2014-07-22 09:01:24,810 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044803501
2014-07-22 09:01:24,811 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044804961
2014-07-22 09:01:24,811 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044806380
2014-07-22 09:01:24,811 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044808543
2014-07-22 09:01:24,811 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044810146
2014-07-22 09:01:24,811 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044811734
2014-07-22 09:01:24,811 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044812521
2014-07-22 09:01:24,811 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044814157
2014-07-22 09:01:24,811 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044815709
2014-07-22 09:01:24,811 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044817817
2014-07-22 09:01:24,811 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044819899
2014-07-22 09:01:24,811 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044823060
2014-07-22 09:01:29,480 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10218, memsize=292.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/478969233f9749859676d47d49d21895
2014-07-22 09:01:29,495 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/478969233f9749859676d47d49d21895 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/478969233f9749859676d47d49d21895
2014-07-22 09:01:29,506 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/478969233f9749859676d47d49d21895, entries=1065600, sequenceid=10218, filesize=75.9m
2014-07-22 09:01:29,506 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~704.1m/738301360, currentsize=146.7m/153860240 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 13633ms, sequenceid=10218, compaction requested=true
2014-07-22 09:01:29,507 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:01:29,507 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 2000 blocking
2014-07-22 09:01:29,507 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 452.6m
2014-07-22 09:01:29,507 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-22 09:01:29,507 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:01:29,507 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:01:29,507 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:01:29,869 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:29,931 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:01:30,082 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53650 synced till here 53648
2014-07-22 09:01:30,138 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044883517 with entries=124, filesize=72.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044889869
2014-07-22 09:01:30,138 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044824123
2014-07-22 09:01:30,138 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044828314
2014-07-22 09:01:31,282 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:31,457 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53727 synced till here 53726
2014-07-22 09:01:31,487 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044889869 with entries=77, filesize=67.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044891282
2014-07-22 09:01:32,907 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:32,943 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53818 synced till here 53817
2014-07-22 09:01:32,978 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044891282 with entries=91, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044892907
2014-07-22 09:01:34,371 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:34,538 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53904 synced till here 53901
2014-07-22 09:01:34,871 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044892907 with entries=86, filesize=71.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044894371
2014-07-22 09:01:37,803 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:01:38,146 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:38,184 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044894371 with entries=97, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044898147
2014-07-22 09:01:38,558 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10276, memsize=340.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/e114edf40ec142979d081946ff948b24
2014-07-22 09:01:38,571 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/e114edf40ec142979d081946ff948b24 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/e114edf40ec142979d081946ff948b24
2014-07-22 09:01:38,583 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/e114edf40ec142979d081946ff948b24, entries=1239380, sequenceid=10276, filesize=88.2m
2014-07-22 09:01:38,583 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~654.4m/686182800, currentsize=154.6m/162096000 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 14377ms, sequenceid=10276, compaction requested=true
2014-07-22 09:01:38,584 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:01:38,584 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 28 store files, 0 compacting, 28 eligible, 2000 blocking
2014-07-22 09:01:38,584 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 28 files from compaction candidates
2014-07-22 09:01:38,584 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 448.5m
2014-07-22 09:01:38,584 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:01:38,584 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:01:38,584 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:01:39,462 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:39,492 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54081 synced till here 54079
2014-07-22 09:01:39,524 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044898147 with entries=80, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044899463
2014-07-22 09:01:39,576 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:01:41,513 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:41,527 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54156 synced till here 54152
2014-07-22 09:01:41,583 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044899463 with entries=75, filesize=65.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044901514
2014-07-22 09:01:44,535 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:44,561 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044901514 with entries=74, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044904535
2014-07-22 09:01:45,247 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10303, memsize=368.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/684d54056ea54fa8abae516c07d628a7
2014-07-22 09:01:45,263 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/684d54056ea54fa8abae516c07d628a7 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/684d54056ea54fa8abae516c07d628a7
2014-07-22 09:01:45,277 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/684d54056ea54fa8abae516c07d628a7, entries=1343240, sequenceid=10303, filesize=95.6m
2014-07-22 09:01:45,277 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~454.4m/476424880, currentsize=209.2m/219360000 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 15770ms, sequenceid=10303, compaction requested=true
2014-07-22 09:01:45,278 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:01:45,278 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 28 store files, 0 compacting, 28 eligible, 2000 blocking
2014-07-22 09:01:45,278 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 358.3m
2014-07-22 09:01:45,278 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 28 files from compaction candidates
2014-07-22 09:01:45,278 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:01:45,278 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:01:45,278 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:01:45,576 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:01:45,768 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:45,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54306 synced till here 54304
2014-07-22 09:01:45,812 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044904535 with entries=76, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044905769
2014-07-22 09:01:46,802 DEBUG [RpcServer.handler=46,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:01:47,574 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:47,622 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54404 synced till here 54389
2014-07-22 09:01:47,692 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044905769 with entries=98, filesize=69.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044907575
2014-07-22 09:01:47,692 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:01:49,170 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:01:49,986 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:51,811 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1149ms
GC pool 'ParNew' had collection(s): count=1 time=1595ms
2014-07-22 09:01:51,812 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54528 synced till here 54517
2014-07-22 09:01:51,871 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044907575 with entries=124, filesize=83.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044909987
2014-07-22 09:01:51,872 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:01:52,753 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:52,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54615 synced till here 54612
2014-07-22 09:01:52,821 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044909987 with entries=87, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044912754
2014-07-22 09:01:52,821 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:01:54,807 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:54,932 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54712 synced till here 54703
2014-07-22 09:01:54,971 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044912754 with entries=97, filesize=73.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044914808
2014-07-22 09:01:54,972 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:01:56,206 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:56,338 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54830 synced till here 54818
2014-07-22 09:01:56,435 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044914808 with entries=118, filesize=77.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044916206
2014-07-22 09:01:56,436 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:01:57,988 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:58,020 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54914 synced till here 54906
2014-07-22 09:01:58,086 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044916206 with entries=84, filesize=69.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044917988
2014-07-22 09:01:58,086 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:01:59,401 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:01:59,436 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55000 synced till here 54988
2014-07-22 09:01:59,572 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044917988 with entries=86, filesize=69.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044919402
2014-07-22 09:01:59,572 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:02:00,997 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1015ms
GC pool 'ParNew' had collection(s): count=1 time=1078ms
2014-07-22 09:02:01,411 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:01,522 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55089 synced till here 55079
2014-07-22 09:02:01,603 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044919402 with entries=89, filesize=71.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044921411
2014-07-22 09:02:01,603 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:02:03,202 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:03,224 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55187 synced till here 55182
2014-07-22 09:02:03,291 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044921411 with entries=98, filesize=68.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044923202
2014-07-22 09:02:03,291 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:02:03,682 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10388, memsize=437.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/132180f477054a8191f0fe37c0a9a118
2014-07-22 09:02:03,695 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/132180f477054a8191f0fe37c0a9a118 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/132180f477054a8191f0fe37c0a9a118
2014-07-22 09:02:04,760 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/132180f477054a8191f0fe37c0a9a118, entries=1592530, sequenceid=10388, filesize=113.3m
2014-07-22 09:02:04,767 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~450.0m/471892640, currentsize=387.5m/406301840 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 26183ms, sequenceid=10388, compaction requested=true
2014-07-22 09:02:04,768 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 2000 blocking
2014-07-22 09:02:04,768 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-22 09:02:04,768 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:02:04,768 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:02:04,768 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:02:04,769 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:02:04,769 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 541.1m
2014-07-22 09:02:04,778 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:02:04,956 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:05,010 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55279 synced till here 55263
2014-07-22 09:02:05,207 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044923202 with entries=92, filesize=76.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044924957
2014-07-22 09:02:05,208 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:02:05,602 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:02:07,082 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:07,119 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55372 synced till here 55368
2014-07-22 09:02:07,151 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044924957 with entries=93, filesize=65.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044927082
2014-07-22 09:02:07,152 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:02:08,071 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:08,098 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55447 synced till here 55445
2014-07-22 09:02:08,621 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044927082 with entries=75, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044928072
2014-07-22 09:02:08,621 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=44, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:02:08,863 DEBUG [RpcServer.handler=41,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b.
2014-07-22 09:02:09,182 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10445, memsize=352.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/43da8f321f514e58a9148d2e3df231ff
2014-07-22 09:02:09,195 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/43da8f321f514e58a9148d2e3df231ff as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/43da8f321f514e58a9148d2e3df231ff
2014-07-22 09:02:09,205 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/43da8f321f514e58a9148d2e3df231ff, entries=1284120, sequenceid=10445, filesize=91.4m
2014-07-22 09:02:09,205 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~361.7m/379247680, currentsize=404.8m/424430640 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 23927ms, sequenceid=10445, compaction requested=true
2014-07-22 09:02:09,206 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:02:09,206 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 30 store files, 0 compacting, 30 eligible, 2000 blocking
2014-07-22 09:02:09,206 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b., current region memstore size 258.2m
2014-07-22 09:02:09,206 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 30 files from compaction candidates
2014-07-22 09:02:09,206 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:02:09,206 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:02:09,206 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:02:09,212 DEBUG [RpcServer.handler=30,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:02:09,356 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:09,374 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55540 synced till here 55531
2014-07-22 09:02:09,417 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:02:09,442 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044928072 with entries=93, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044929357
2014-07-22 09:02:10,585 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:10,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55646 synced till here 55630
2014-07-22 09:02:10,854 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044929357 with entries=106, filesize=72.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044930585
2014-07-22 09:02:12,320 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1230ms
GC pool 'ParNew' had collection(s): count=1 time=1282ms
2014-07-22 09:02:13,016 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:13,040 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55745 synced till here 55738
2014-07-22 09:02:13,104 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044930585 with entries=99, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044933016
2014-07-22 09:02:14,323 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:14,353 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55828 synced till here 55823
2014-07-22 09:02:14,415 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044933016 with entries=83, filesize=68.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044934323
2014-07-22 09:02:16,001 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:16,018 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55906 synced till here 55905
2014-07-22 09:02:16,027 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044934323 with entries=78, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044936001
2014-07-22 09:02:17,519 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:17,537 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55984 synced till here 55983
2014-07-22 09:02:17,552 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044936001 with entries=78, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044937519
2014-07-22 09:02:18,500 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:18,996 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56069 synced till here 56059
2014-07-22 09:02:19,055 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044937519 with entries=85, filesize=68.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044938501
2014-07-22 09:02:19,713 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:19,732 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56149 synced till here 56146
2014-07-22 09:02:19,793 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044938501 with entries=80, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044939713
2014-07-22 09:02:20,069 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12959, memsize=216.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/fde3ded693054d61aadb61d57a8f1824
2014-07-22 09:02:20,089 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/fde3ded693054d61aadb61d57a8f1824 as hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/fde3ded693054d61aadb61d57a8f1824
2014-07-22 09:02:20,118 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/fde3ded693054d61aadb61d57a8f1824, entries=787190, sequenceid=12959, filesize=56.1m
2014-07-22 09:02:20,119 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~258.2m/270770640, currentsize=43.3m/45453840 for region usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. in 10913ms, sequenceid=12959, compaction requested=true
2014-07-22 09:02:20,119 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:02:20,120 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 2000 blocking
2014-07-22 09:02:20,120 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 830.5m
2014-07-22 09:02:20,120 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-22 09:02:20,120 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:02:20,120 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:02:20,120 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. because compaction request was cancelled
2014-07-22 09:02:20,942 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:02:21,799 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:21,817 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56227 synced till here 56226
2014-07-22 09:02:21,860 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044939713 with entries=78, filesize=62.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044941800
2014-07-22 09:02:21,860 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044829498
2014-07-22 09:02:21,862 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044831476
2014-07-22 09:02:21,862 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044833321
2014-07-22 09:02:21,862 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044835393
2014-07-22 09:02:21,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044837598
2014-07-22 09:02:21,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044839562
2014-07-22 09:02:21,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044843470
2014-07-22 09:02:21,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044845823
2014-07-22 09:02:21,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044848408
2014-07-22 09:02:21,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044850312
2014-07-22 09:02:21,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044859799
2014-07-22 09:02:21,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044861885
2014-07-22 09:02:21,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044862979
2014-07-22 09:02:21,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044864903
2014-07-22 09:02:21,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044867146
2014-07-22 09:02:21,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044868895
2014-07-22 09:02:21,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044870585
2014-07-22 09:02:21,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044872047
2014-07-22 09:02:21,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044873002
2014-07-22 09:02:21,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044874584
2014-07-22 09:02:21,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044876151
2014-07-22 09:02:21,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044877479
2014-07-22 09:02:22,827 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10612, memsize=326.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/4c2cce457c2741659dca0f6341e70c73
2014-07-22 09:02:22,847 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/4c2cce457c2741659dca0f6341e70c73 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/4c2cce457c2741659dca0f6341e70c73
2014-07-22 09:02:23,025 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/4c2cce457c2741659dca0f6341e70c73, entries=1187680, sequenceid=10612, filesize=84.6m
2014-07-22 09:02:23,025 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~549.1m/575803680, currentsize=344.2m/360952800 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 18256ms, sequenceid=10612, compaction requested=true
2014-07-22 09:02:23,026 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:02:23,026 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 2000 blocking
2014-07-22 09:02:23,027 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 735.8m
2014-07-22 09:02:23,027 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-22 09:02:23,027 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:02:23,027 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:02:23,027 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:02:23,074 DEBUG [RpcServer.handler=19,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:02:23,160 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:23,180 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56313 synced till here 56306
2014-07-22 09:02:23,258 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044941800 with entries=86, filesize=70.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044943160
2014-07-22 09:02:23,258 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044879042
2014-07-22 09:02:23,935 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:02:25,286 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:25,322 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044943160 with entries=79, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044945286
2014-07-22 09:02:26,669 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:27,061 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044945286 with entries=116, filesize=89.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044946670
2014-07-22 09:02:28,262 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:28,286 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56579 synced till here 56577
2014-07-22 09:02:28,316 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044946670 with entries=71, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044948262
2014-07-22 09:02:30,567 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:30,581 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56670 synced till here 56662
2014-07-22 09:02:30,654 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044948262 with entries=91, filesize=70.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044950567
2014-07-22 09:02:34,251 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:34,288 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56757 synced till here 56752
2014-07-22 09:02:34,343 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044950567 with entries=87, filesize=65.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044954252
2014-07-22 09:02:35,849 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:36,126 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56859 synced till here 56858
2014-07-22 09:02:36,147 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044954252 with entries=102, filesize=79.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044955850
2014-07-22 09:02:37,678 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:37,942 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56979 synced till here 56970
2014-07-22 09:02:38,070 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044955850 with entries=120, filesize=84.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044957678
2014-07-22 09:02:39,671 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:39,827 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57095 synced till here 57076
2014-07-22 09:02:39,926 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044957678 with entries=116, filesize=74.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044959672
2014-07-22 09:02:41,723 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1143ms
GC pool 'ParNew' had collection(s): count=1 time=1488ms
2014-07-22 09:02:42,322 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:42,351 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57188 synced till here 57176
2014-07-22 09:02:42,467 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044959672 with entries=93, filesize=72.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044962323
2014-07-22 09:02:44,092 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:44,115 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57277 synced till here 57271
2014-07-22 09:02:44,192 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044962323 with entries=89, filesize=69.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044964092
2014-07-22 09:02:44,411 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,411 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,412 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,412 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,413 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,413 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,413 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,415 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,417 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,419 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,429 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,429 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,433 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,457 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,458 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,458 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,468 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,469 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,471 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,477 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,508 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,533 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,553 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,589 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,589 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,630 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,669 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,710 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,745 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,779 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,814 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,814 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,815 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:44,853 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:46,220 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:46,231 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:46,254 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:46,285 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:46,319 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:46,356 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:46,395 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:46,396 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:46,397 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:46,398 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:46,400 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:46,401 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:46,401 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:46,456 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:46,525 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:46,587 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:02:47,506 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10820, memsize=403.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/b032e43769cc484899690b7b9d7845c3
2014-07-22 09:02:47,535 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/b032e43769cc484899690b7b9d7845c3 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/b032e43769cc484899690b7b9d7845c3
2014-07-22 09:02:47,554 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/b032e43769cc484899690b7b9d7845c3, entries=1467970, sequenceid=10820, filesize=104.6m
2014-07-22 09:02:47,555 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~744.5m/780667680, currentsize=322.6m/338270560 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 24528ms, sequenceid=10820, compaction requested=true
2014-07-22 09:02:47,555 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:02:47,555 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 30 store files, 0 compacting, 30 eligible, 2000 blocking
2014-07-22 09:02:47,555 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 968ms
2014-07-22 09:02:47,556 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 30 files from compaction candidates
2014-07-22 09:02:47,556 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:02:47,556 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,556 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:02:47,556 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1031ms
2014-07-22 09:02:47,556 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:02:47,556 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 1000.8m
2014-07-22 09:02:47,556 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,557 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1101ms
2014-07-22 09:02:47,557 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,557 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1156ms
2014-07-22 09:02:47,557 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,558 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1158ms
2014-07-22 09:02:47,558 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,558 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1158ms
2014-07-22 09:02:47,558 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,558 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1160ms
2014-07-22 09:02:47,558 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,558 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1161ms
2014-07-22 09:02:47,558 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,558 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1162ms
2014-07-22 09:02:47,558 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,559 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1164ms
2014-07-22 09:02:47,559 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,563 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1207ms
2014-07-22 09:02:47,563 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,563 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1244ms
2014-07-22 09:02:47,564 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,564 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1280ms
2014-07-22 09:02:47,564 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,564 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1311ms
2014-07-22 09:02:47,564 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,564 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1333ms
2014-07-22 09:02:47,564 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,565 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1346ms
2014-07-22 09:02:47,565 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,565 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2712ms
2014-07-22 09:02:47,565 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,565 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2750ms
2014-07-22 09:02:47,565 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,567 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2753ms
2014-07-22 09:02:47,568 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,569 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2755ms
2014-07-22 09:02:47,569 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,569 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2790ms
2014-07-22 09:02:47,569 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,570 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2825ms
2014-07-22 09:02:47,570 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,570 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2860ms
2014-07-22 09:02:47,570 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,570 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2902ms
2014-07-22 09:02:47,570 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,570 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2940ms
2014-07-22 09:02:47,570 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,570 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2981ms
2014-07-22 09:02:47,570 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,570 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2982ms
2014-07-22 09:02:47,571 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,573 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3020ms
2014-07-22 09:02:47,573 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,573 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3040ms
2014-07-22 09:02:47,574 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,575 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3067ms
2014-07-22 09:02:47,576 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,576 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3099ms
2014-07-22 09:02:47,576 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,576 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3105ms
2014-07-22 09:02:47,576 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,576 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3108ms
2014-07-22 09:02:47,576 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,576 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3108ms
2014-07-22 09:02:47,577 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,582 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3124ms
2014-07-22 09:02:47,582 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,583 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3126ms
2014-07-22 09:02:47,583 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,583 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3126ms
2014-07-22 09:02:47,583 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,583 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3150ms
2014-07-22 09:02:47,583 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,583 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3154ms
2014-07-22 09:02:47,583 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,583 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3154ms
2014-07-22 09:02:47,584 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,584 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3165ms
2014-07-22 09:02:47,584 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,584 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3167ms
2014-07-22 09:02:47,584 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,589 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3174ms
2014-07-22 09:02:47,589 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,589 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3176ms
2014-07-22 09:02:47,589 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,590 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3178ms
2014-07-22 09:02:47,590 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,594 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3182ms
2014-07-22 09:02:47,594 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,594 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3182ms
2014-07-22 09:02:47,594 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,595 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3182ms
2014-07-22 09:02:47,595 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,601 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3190ms
2014-07-22 09:02:47,601 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,601 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3190ms
2014-07-22 09:02:47,601 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:02:47,666 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10792, memsize=490.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/60e3367a8f784ac89366a1cbadb1300a
2014-07-22 09:02:47,693 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/60e3367a8f784ac89366a1cbadb1300a as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/60e3367a8f784ac89366a1cbadb1300a
2014-07-22 09:02:47,722 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/60e3367a8f784ac89366a1cbadb1300a, entries=1786800, sequenceid=10792, filesize=127.3m
2014-07-22 09:02:47,728 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~830.5m/870806160, currentsize=375.4m/393685840 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 27608ms, sequenceid=10792, compaction requested=true
2014-07-22 09:02:47,728 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 2000 blocking
2014-07-22 09:02:47,729 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-22 09:02:47,729 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:02:47,729 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:02:47,729 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:02:47,729 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:02:47,729 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 668.4m
2014-07-22 09:02:47,964 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:02:50,092 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1878ms
GC pool 'ParNew' had collection(s): count=1 time=1975ms
2014-07-22 09:02:50,214 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:50,218 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:02:50,370 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57378 synced till here 57376
2014-07-22 09:02:50,418 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044964092 with entries=101, filesize=81.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044970214
2014-07-22 09:02:50,419 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044883517
2014-07-22 09:02:50,419 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044889869
2014-07-22 09:02:50,419 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044891282
2014-07-22 09:02:50,419 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044892907
2014-07-22 09:02:50,419 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044894371
2014-07-22 09:02:50,419 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044898147
2014-07-22 09:02:50,419 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044899463
2014-07-22 09:02:50,419 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044901514
2014-07-22 09:02:50,764 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:02:51,020 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:02:52,903 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1308ms
GC pool 'ParNew' had collection(s): count=1 time=1632ms
2014-07-22 09:02:53,029 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:53,073 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57482 synced till here 57449
2014-07-22 09:02:53,387 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044970214 with entries=104, filesize=92.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044973029
2014-07-22 09:02:54,996 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10446,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044964549,"queuetimems":0,"class":"HRegionServer","responsesize":18785,"method":"Multi"}
2014-07-22 09:02:54,997 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10570,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044964427,"queuetimems":1,"class":"HRegionServer","responsesize":18425,"method":"Multi"}
2014-07-22 09:02:54,997 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10220,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044964777,"queuetimems":0,"class":"HRegionServer","responsesize":18112,"method":"Multi"}
2014-07-22 09:02:55,006 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10155,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044964851,"queuetimems":1,"class":"HRegionServer","responsesize":18771,"method":"Multi"}
2014-07-22 09:02:55,006 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10341,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044964665,"queuetimems":1,"class":"HRegionServer","responsesize":17789,"method":"Multi"}
2014-07-22 09:02:55,007 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10539,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044964468,"queuetimems":0,"class":"HRegionServer","responsesize":18394,"method":"Multi"}
2014-07-22 09:02:55,010 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10267,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044964743,"queuetimems":1,"class":"HRegionServer","responsesize":18442,"method":"Multi"}
2014-07-22 09:02:55,250 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:55,458 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57607 synced till here 57559
2014-07-22 09:02:55,555 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10742,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044964812,"queuetimems":0,"class":"HRegionServer","responsesize":18288,"method":"Multi"}
2014-07-22 09:02:55,559 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10930,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044964627,"queuetimems":0,"class":"HRegionServer","responsesize":18346,"method":"Multi"}
2014-07-22 09:02:55,562 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11057,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044964505,"queuetimems":0,"class":"HRegionServer","responsesize":16488,"method":"Multi"}
2014-07-22 09:02:55,866 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11279,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406044964586,"queuetimems":0,"class":"HRegionServer","responsesize":17033,"method":"Multi"}
2014-07-22 09:02:55,913 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044973029 with entries=125, filesize=108.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044975250
2014-07-22 09:02:57,747 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:02:58,331 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57729 synced till here 57691
2014-07-22 09:02:59,618 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044975250 with entries=122, filesize=107.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044977748
2014-07-22 09:03:00,647 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:03:01,688 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57821 synced till here 57802
2014-07-22 09:03:01,832 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044977748 with entries=92, filesize=77.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044980648
2014-07-22 09:03:02,625 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:03:03,642 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57921 synced till here 57895
2014-07-22 09:03:03,890 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044980648 with entries=100, filesize=86.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044982626
2014-07-22 09:03:04,748 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:03:04,834 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58005 synced till here 57992
2014-07-22 09:03:05,041 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044982626 with entries=84, filesize=80.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044984749
2014-07-22 09:03:06,558 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:03:06,603 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58096 synced till here 58075
2014-07-22 09:03:06,767 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044984749 with entries=91, filesize=78.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044986559
2014-07-22 09:03:08,606 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:03:08,645 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044986559 with entries=72, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044988606
2014-07-22 09:03:10,498 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=6603, hits=3, hitRatio=0.04%, , cachingAccesses=5, cachingHits=3, cachingHitsRatio=60.00%, evictions=0, evicted=0, evictedPerRun=NaN
2014-07-22 09:03:12,618 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:03:12,722 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044988606 with entries=91, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044992618
2014-07-22 09:03:13,593 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:13,605 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:13,606 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:13,639 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:13,650 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:14,058 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:14,082 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:14,085 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:14,125 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:14,126 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:14,126 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:14,127 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:14,129 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:14,132 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:14,170 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:14,206 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:14,241 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:14,270 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:14,311 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:14,354 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:14,840 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:14,889 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:15,771 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:15,788 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:15,855 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:15,906 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:15,948 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:15,991 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:16,023 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:16,025 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:16,026 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:17,771 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:17,781 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:17,782 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:17,782 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:17,786 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:17,787 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:17,844 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:17,893 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:17,923 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:17,968 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:18,250 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:18,306 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:18,335 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11000, memsize=493.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/8bf05d8e0b3f441a806f4e4716b2584b
2014-07-22 09:03:18,351 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:03:18,358 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/8bf05d8e0b3f441a806f4e4716b2584b as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/8bf05d8e0b3f441a806f4e4716b2584b
2014-07-22 09:03:18,373 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/8bf05d8e0b3f441a806f4e4716b2584b, entries=1798000, sequenceid=11000, filesize=128.0m
2014-07-22 09:03:18,373 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~676.9m/709807040, currentsize=377.8m/396150880 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 30644ms, sequenceid=11000, compaction requested=true
2014-07-22 09:03:18,373 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:03:18,374 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 30 store files, 0 compacting, 30 eligible, 2000 blocking
2014-07-22 09:03:18,374 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23ms
2014-07-22 09:03:18,374 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 30 files from compaction candidates
2014-07-22 09:03:18,374 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,374 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:03:18,374 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 749.7m
2014-07-22 09:03:18,374 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:03:18,374 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:03:18,377 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 71ms
2014-07-22 09:03:18,377 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,377 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 127ms
2014-07-22 09:03:18,378 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,378 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 410ms
2014-07-22 09:03:18,378 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,378 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 455ms
2014-07-22 09:03:18,378 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,378 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 485ms
2014-07-22 09:03:18,378 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,378 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 534ms
2014-07-22 09:03:18,379 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,379 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 592ms
2014-07-22 09:03:18,379 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,381 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 595ms
2014-07-22 09:03:18,381 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,382 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 600ms
2014-07-22 09:03:18,382 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,383 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 601ms
2014-07-22 09:03:18,383 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,383 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 602ms
2014-07-22 09:03:18,383 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,383 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 612ms
2014-07-22 09:03:18,383 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,383 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2357ms
2014-07-22 09:03:18,383 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,383 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2358ms
2014-07-22 09:03:18,384 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,387 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2364ms
2014-07-22 09:03:18,387 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,387 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2396ms
2014-07-22 09:03:18,387 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,387 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2439ms
2014-07-22 09:03:18,387 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,391 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2486ms
2014-07-22 09:03:18,391 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,391 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2536ms
2014-07-22 09:03:18,391 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,397 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2609ms
2014-07-22 09:03:18,397 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,397 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2626ms
2014-07-22 09:03:18,398 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,398 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3509ms
2014-07-22 09:03:18,398 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,398 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3558ms
2014-07-22 09:03:18,398 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,399 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4045ms
2014-07-22 09:03:18,399 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,399 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4088ms
2014-07-22 09:03:18,399 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,402 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4132ms
2014-07-22 09:03:18,402 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,402 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4161ms
2014-07-22 09:03:18,402 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,402 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4196ms
2014-07-22 09:03:18,402 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,406 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4236ms
2014-07-22 09:03:18,406 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,406 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4274ms
2014-07-22 09:03:18,406 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,406 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4277ms
2014-07-22 09:03:18,406 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,406 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4279ms
2014-07-22 09:03:18,407 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,407 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4281ms
2014-07-22 09:03:18,407 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,407 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4282ms
2014-07-22 09:03:18,407 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,407 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4282ms
2014-07-22 09:03:18,407 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,408 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4324ms
2014-07-22 09:03:18,408 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,408 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4326ms
2014-07-22 09:03:18,408 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,408 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4350ms
2014-07-22 09:03:18,408 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,408 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4758ms
2014-07-22 09:03:18,408 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,408 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4769ms
2014-07-22 09:03:18,408 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,409 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4803ms
2014-07-22 09:03:18,409 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,409 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4804ms
2014-07-22 09:03:18,409 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,412 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4819ms
2014-07-22 09:03:18,412 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:03:18,495 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:03:19,640 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:03:19,653 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58389 synced till here 58378
2014-07-22 09:03:19,797 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044992618 with entries=130, filesize=81.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044999640
2014-07-22 09:03:19,932 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:03:20,569 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:03:21,428 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58484 synced till here 58459
2014-07-22 09:03:21,598 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044999640 with entries=95, filesize=83.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045000569
2014-07-22 09:03:22,331 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:03:22,574 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58563 synced till here 58562
2014-07-22 09:03:22,593 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045000569 with entries=79, filesize=69.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045002331
2014-07-22 09:03:23,605 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11024, memsize=556.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/a8844666a089479d9534303e9452ae4d
2014-07-22 09:03:23,617 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/a8844666a089479d9534303e9452ae4d as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/a8844666a089479d9534303e9452ae4d
2014-07-22 09:03:23,628 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/a8844666a089479d9534303e9452ae4d, entries=2026980, sequenceid=11024, filesize=144.3m
2014-07-22 09:03:23,629 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1000.8m/1049432720, currentsize=467.9m/490584480 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 36073ms, sequenceid=11024, compaction requested=true
2014-07-22 09:03:23,629 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:03:23,629 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 2000 blocking
2014-07-22 09:03:23,629 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 806.9m
2014-07-22 09:03:23,629 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-22 09:03:23,630 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:03:23,630 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:03:23,630 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:03:23,714 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:03:24,090 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:03:24,126 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045002331 with entries=98, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045004091
2014-07-22 09:03:24,126 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044904535
2014-07-22 09:03:24,126 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044905769
2014-07-22 09:03:24,126 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044907575
2014-07-22 09:03:24,126 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044909987
2014-07-22 09:03:24,126 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044912754
2014-07-22 09:03:24,126 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044914808
2014-07-22 09:03:24,126 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044916206
2014-07-22 09:03:24,126 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044917988
2014-07-22 09:03:24,126 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044919402
2014-07-22 09:03:24,126 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044921411
2014-07-22 09:03:24,126 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044923202
2014-07-22 09:03:24,126 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044924957
2014-07-22 09:03:24,126 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044927082
2014-07-22 09:03:24,236 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:03:24,450 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:03:26,578 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:03:27,007 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58788 synced till here 58781
2014-07-22 09:03:27,053 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045004091 with entries=127, filesize=79.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045006578
2014-07-22 09:03:27,056 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:03:29,344 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:03:29,405 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58868 synced till here 58866
2014-07-22 09:03:29,439 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045006578 with entries=80, filesize=65.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045009345
2014-07-22 09:03:29,440 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:03:30,778 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:03:30,803 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58970 synced till here 58968
2014-07-22 09:03:30,846 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045009345 with entries=102, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045010778
2014-07-22 09:03:30,847 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:03:32,196 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:03:32,214 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59047 synced till here 59045
2014-07-22 09:03:32,268 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045010778 with entries=77, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045012196
2014-07-22 09:03:32,268 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:03:33,883 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:03:34,575 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59178 synced till here 59170
2014-07-22 09:03:34,689 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045012196 with entries=131, filesize=104.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045013884
2014-07-22 09:03:34,690 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:03:36,164 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1036ms
GC pool 'ParNew' had collection(s): count=1 time=1077ms
2014-07-22 09:03:36,645 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:03:36,668 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59275 synced till here 59268
2014-07-22 09:03:36,733 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045013884 with entries=97, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045016646
2014-07-22 09:03:36,734 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:03:38,452 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:03:38,457 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11227, memsize=308.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/e0038fb5e0a844e0ab07d32c88727b24
2014-07-22 09:03:38,470 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59372 synced till here 59359
2014-07-22 09:03:38,475 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/e0038fb5e0a844e0ab07d32c88727b24 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/e0038fb5e0a844e0ab07d32c88727b24
2014-07-22 09:03:38,486 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/e0038fb5e0a844e0ab07d32c88727b24, entries=1121420, sequenceid=11227, filesize=79.8m
2014-07-22 09:03:38,486 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~749.7m/786143520, currentsize=327.0m/342842000 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 20112ms, sequenceid=11227, compaction requested=true
2014-07-22 09:03:38,487 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:03:38,487 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 30 store files, 0 compacting, 30 eligible, 2000 blocking
2014-07-22 09:03:38,487 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 700.6m
2014-07-22 09:03:38,487 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 30 files from compaction candidates
2014-07-22 09:03:38,487 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:03:38,487 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:03:38,487 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:03:38,550 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:03:38,603 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045016646 with entries=97, filesize=77.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045018452
2014-07-22 09:03:38,603 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:03:40,276 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:03:40,716 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:03:40,793 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59485 synced till here 59464
2014-07-22 09:03:40,868 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045018452 with entries=113, filesize=88.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045020716
2014-07-22 09:03:40,869 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:03:42,495 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:03:42,764 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59589 synced till here 59579
2014-07-22 09:03:43,715 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045020716 with entries=104, filesize=89.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045022496
2014-07-22 09:03:43,716 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:03:45,609 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:03:45,923 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59704 synced till here 59673
2014-07-22 09:03:47,288 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1017ms
GC pool 'ParNew' had collection(s): count=1 time=1240ms
2014-07-22 09:03:47,356 DEBUG [RpcServer.handler=15,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b.
2014-07-22 09:03:47,534 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045022496 with entries=115, filesize=89.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045025609
2014-07-22 09:03:47,534 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=44, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:03:48,609 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:03:49,904 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1115ms
GC pool 'ParNew' had collection(s): count=1 time=1282ms
2014-07-22 09:03:49,967 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59794 synced till here 59774
2014-07-22 09:03:50,463 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11282, memsize=298.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/0297fff64bee407280ac7057c8d59925
2014-07-22 09:03:50,494 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045025609 with entries=90, filesize=84.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045028610
2014-07-22 09:03:50,494 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=45, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:03:50,547 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/0297fff64bee407280ac7057c8d59925 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/0297fff64bee407280ac7057c8d59925
2014-07-22 09:03:50,556 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/0297fff64bee407280ac7057c8d59925, entries=1085650, sequenceid=11282, filesize=77.3m
2014-07-22 09:03:50,556 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~808.6m/847901200, currentsize=383.6m/402243840 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 26927ms, sequenceid=11282, compaction requested=true
2014-07-22 09:03:50,557 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:03:50,557 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 2000 blocking
2014-07-22 09:03:50,557 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 878.4m
2014-07-22 09:03:50,557 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-22 09:03:50,557 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:03:50,557 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:03:50,557 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:03:50,591 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:03:52,043 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1137ms
GC pool 'ParNew' had collection(s): count=1 time=1159ms
2014-07-22 09:03:52,201 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:03:52,300 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59897 synced till here 59870
2014-07-22 09:03:52,510 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045028610 with entries=103, filesize=82.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045032202
2014-07-22 09:03:52,510 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=46, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:03:53,320 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:03:54,656 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1111ms
GC pool 'ParNew' had collection(s): count=1 time=1295ms
2014-07-22 09:03:55,219 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:03:55,365 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60028 synced till here 59999
2014-07-22 09:03:55,717 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045032202 with entries=131, filesize=109.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045035220
2014-07-22 09:03:55,718 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=47, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:03:57,688 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1030ms
GC pool 'ParNew' had collection(s): count=1 time=1160ms
2014-07-22 09:03:58,132 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:03:58,183 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60117 synced till here 60099
2014-07-22 09:03:58,559 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045035220 with entries=89, filesize=79.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045038133
2014-07-22 09:03:58,560 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=48, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:04:00,288 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:04:00,308 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60204 synced till here 60194
2014-07-22 09:04:00,386 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045038133 with entries=87, filesize=75.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045040288
2014-07-22 09:04:00,388 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=49, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:04:02,203 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:04:02,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60292 synced till here 60285
2014-07-22 09:04:02,340 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045040288 with entries=88, filesize=68.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045042204
2014-07-22 09:04:02,340 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=50, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:04:03,761 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:04:05,329 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1239ms
GC pool 'ParNew' had collection(s): count=1 time=1547ms
2014-07-22 09:04:05,334 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60382 synced till here 60378
2014-07-22 09:04:05,405 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045042204 with entries=90, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045043761
2014-07-22 09:04:05,409 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=51, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:04:06,298 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:04:06,329 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045043761 with entries=90, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045046298
2014-07-22 09:04:06,329 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=52, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:04:06,437 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11401, memsize=276.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/5be8d69ae8454be9b55e644944110aba
2014-07-22 09:04:06,449 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/5be8d69ae8454be9b55e644944110aba as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/5be8d69ae8454be9b55e644944110aba
2014-07-22 09:04:06,466 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/5be8d69ae8454be9b55e644944110aba, entries=1004730, sequenceid=11401, filesize=71.5m
2014-07-22 09:04:06,467 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~702.4m/736564320, currentsize=403.0m/422570640 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 27980ms, sequenceid=11401, compaction requested=true
2014-07-22 09:04:06,468 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:04:06,468 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 2000 blocking
2014-07-22 09:04:06,468 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-22 09:04:06,468 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:04:06,468 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b., current region memstore size 294.4m
2014-07-22 09:04:06,468 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:04:06,468 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:04:06,511 DEBUG [RpcServer.handler=15,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:04:07,071 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:04:07,792 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:04:07,817 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60575 synced till here 60572
2014-07-22 09:04:07,955 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045046298 with entries=103, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045047793
2014-07-22 09:04:09,542 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:04:09,880 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60699 synced till here 60689
2014-07-22 09:04:09,957 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045047793 with entries=124, filesize=82.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045049543
2014-07-22 09:04:11,707 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:04:11,784 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045049543 with entries=85, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045051707
2014-07-22 09:04:13,424 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:04:13,438 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60885 synced till here 60875
2014-07-22 09:04:13,543 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045051707 with entries=101, filesize=66.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045053424
2014-07-22 09:04:15,758 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:04:15,783 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60977 synced till here 60971
2014-07-22 09:04:15,821 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045053424 with entries=92, filesize=66.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045055758
2014-07-22 09:04:17,324 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:04:17,368 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61057 synced till here 61050
2014-07-22 09:04:17,446 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045055758 with entries=80, filesize=67.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045057326
2014-07-22 09:04:18,513 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:04:18,541 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61133 synced till here 61132
2014-07-22 09:04:18,576 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045057326 with entries=76, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045058514
2014-07-22 09:04:19,532 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11537, memsize=288.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/3f6552fe22e24ceeabc973f1623a7447
2014-07-22 09:04:19,558 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/3f6552fe22e24ceeabc973f1623a7447 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/3f6552fe22e24ceeabc973f1623a7447
2014-07-22 09:04:19,582 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/3f6552fe22e24ceeabc973f1623a7447, entries=1051170, sequenceid=11537, filesize=74.8m
2014-07-22 09:04:19,582 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~909.3m/953487840, currentsize=408.6m/428449360 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 29025ms, sequenceid=11537, compaction requested=true
2014-07-22 09:04:19,583 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 32 store files, 0 compacting, 32 eligible, 2000 blocking
2014-07-22 09:04:19,583 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 32 files from compaction candidates
2014-07-22 09:04:19,584 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:04:19,584 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:04:19,584 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:04:19,585 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:04:19,586 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 941.4m
2014-07-22 09:04:19,627 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:04:20,112 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:04:20,140 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61220 synced till here 61212
2014-07-22 09:04:20,232 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045058514 with entries=87, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045060113
2014-07-22 09:04:21,165 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14067, memsize=211.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/4223b4502e3748039daef991e468fb2a
2014-07-22 09:04:21,184 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/4223b4502e3748039daef991e468fb2a as hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/4223b4502e3748039daef991e468fb2a
2014-07-22 09:04:21,185 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:04:21,199 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/4223b4502e3748039daef991e468fb2a, entries=769930, sequenceid=14067, filesize=54.8m
2014-07-22 09:04:21,199 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~294.4m/308676480, currentsize=51.7m/54169760 for region usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. in 14731ms, sequenceid=14067, compaction requested=true
2014-07-22 09:04:21,199 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:04:21,200 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 2000 blocking
2014-07-22 09:04:21,200 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-22 09:04:21,200 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:04:21,200 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 877.7m
2014-07-22 09:04:21,200 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:04:21,200 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. because compaction request was cancelled
2014-07-22 09:04:21,531 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:04:21,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61310 synced till here 61303
2014-07-22 09:04:21,614 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045060113 with entries=90, filesize=68.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045061532
2014-07-22 09:04:21,615 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044928072
2014-07-22 09:04:21,615 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044929357
2014-07-22 09:04:21,615 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044930585
2014-07-22 09:04:21,615 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044933016
2014-07-22 09:04:21,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044934323
2014-07-22 09:04:21,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044936001
2014-07-22 09:04:21,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044937519
2014-07-22 09:04:21,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044938501
2014-07-22 09:04:21,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044939713
2014-07-22 09:04:21,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044941800
2014-07-22 09:04:21,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044943160
2014-07-22 09:04:21,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044945286
2014-07-22 09:04:21,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044946670
2014-07-22 09:04:21,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044948262
2014-07-22 09:04:21,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044950567
2014-07-22 09:04:21,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044954252
2014-07-22 09:04:21,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044955850
2014-07-22 09:04:21,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044957678
2014-07-22 09:04:21,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044959672
2014-07-22 09:04:21,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044962323
2014-07-22 09:04:21,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044964092
2014-07-22 09:04:21,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044970214
2014-07-22 09:04:21,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044973029
2014-07-22 09:04:21,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044975250
2014-07-22 09:04:21,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044977748
2014-07-22 09:04:21,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044980648
2014-07-22 09:04:21,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044982626
2014-07-22 09:04:21,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044984749
2014-07-22 09:04:21,617 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044986559
2014-07-22 09:04:21,617 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044988606
2014-07-22 09:04:22,061 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:04:23,236 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:04:23,260 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61394 synced till here 61393
2014-07-22 09:04:23,288 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045061532 with entries=84, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045063236
2014-07-22 09:04:25,042 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:04:25,063 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61491 synced till here 61484
2014-07-22 09:04:25,127 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045063236 with entries=97, filesize=70.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045065043
2014-07-22 09:04:26,638 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:04:26,655 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61594 synced till here 61584
2014-07-22 09:04:26,736 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045065043 with entries=103, filesize=70.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045066638
2014-07-22 09:04:28,500 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:04:28,674 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61699 synced till here 61691
2014-07-22 09:04:28,783 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045066638 with entries=105, filesize=78.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045068501
2014-07-22 09:04:30,234 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:04:30,256 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61784 synced till here 61777
2014-07-22 09:04:30,318 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045068501 with entries=85, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045070234
2014-07-22 09:04:31,276 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:04:31,922 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61889 synced till here 61888
2014-07-22 09:04:31,950 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045070234 with entries=105, filesize=71.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045071276
2014-07-22 09:04:33,352 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:04:34,463 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61998 synced till here 61996
2014-07-22 09:04:34,489 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045071276 with entries=109, filesize=88.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045073353
2014-07-22 09:04:35,024 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:35,045 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:35,048 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:35,057 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:35,108 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:35,108 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:35,119 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:35,122 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:35,158 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:35,194 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:35,231 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:35,269 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:35,309 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:35,345 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:35,382 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:35,417 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:35,454 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:35,454 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:35,455 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:35,456 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:35,457 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,207 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,221 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,240 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,242 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,277 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,306 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,335 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,335 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,336 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,344 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,357 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,395 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,406 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,406 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,406 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,407 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,425 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,426 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,427 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,466 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,489 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,530 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,565 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,594 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,628 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,632 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,666 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,666 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:36,668 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:04:40,298 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5029ms
2014-07-22 09:04:40,299 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5191ms
2014-07-22 09:04:40,299 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5191ms
2014-07-22 09:04:40,299 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1602ms
GC pool 'ParNew' had collection(s): count=1 time=1872ms
2014-07-22 09:04:40,300 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5181ms
2014-07-22 09:04:40,300 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5106ms
2014-07-22 09:04:40,300 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5178ms
2014-07-22 09:04:40,301 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5143ms
2014-07-22 09:04:40,302 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5070ms
2014-07-22 09:04:40,302 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5278ms
2014-07-22 09:04:40,303 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5258ms
2014-07-22 09:04:40,303 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5255ms
2014-07-22 09:04:40,304 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5246ms
2014-07-22 09:04:40,310 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:04:40,345 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:04:40,383 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:04:40,417 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:04:40,454 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:04:40,455 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:04:40,455 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:04:40,456 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:04:40,457 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:04:41,207 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:04:41,221 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:04:41,240 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:04:41,242 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:04:41,278 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:04:41,307 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:04:41,335 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:04:41,336 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:04:41,337 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:04:41,345 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:04:41,363 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5006ms
2014-07-22 09:04:41,396 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:04:41,406 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:04:41,407 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:04:41,407 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:04:41,408 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 09:04:41,425 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:04:41,427 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:04:41,427 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:04:41,467 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:04:41,489 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:04:41,531 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:04:41,566 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:04:41,594 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:04:41,629 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:04:41,632 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:04:41,666 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:04:41,667 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:04:41,669 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:04:45,300 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10030ms
2014-07-22 09:04:45,300 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10106ms
2014-07-22 09:04:45,300 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10181ms
2014-07-22 09:04:45,301 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10192ms
2014-07-22 09:04:45,301 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10193ms
2014-07-22 09:04:45,302 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10179ms
2014-07-22 09:04:45,303 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10144ms
2014-07-22 09:04:45,303 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10279ms
2014-07-22 09:04:45,304 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10073ms
2014-07-22 09:04:45,304 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10256ms
2014-07-22 09:04:45,304 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10259ms
2014-07-22 09:04:45,304 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10247ms
2014-07-22 09:04:45,310 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 09:04:45,345 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 09:04:45,384 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 09:04:45,417 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 09:04:45,455 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 09:04:45,455 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 09:04:45,456 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 09:04:45,457 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 09:04:45,458 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 09:04:46,814 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10146ms
2014-07-22 09:04:46,815 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10326ms
2014-07-22 09:04:46,816 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1013ms
GC pool 'ParNew' had collection(s): count=1 time=1153ms
2014-07-22 09:04:46,816 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10409ms
2014-07-22 09:04:46,816 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10286ms
2014-07-22 09:04:46,817 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10252ms
2014-07-22 09:04:46,817 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10223ms
2014-07-22 09:04:46,817 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10189ms
2014-07-22 09:04:46,817 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10185ms
2014-07-22 09:04:46,818 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10152ms
2014-07-22 09:04:46,818 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10153ms
2014-07-22 09:04:46,819 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10612ms
2014-07-22 09:04:46,819 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10599ms
2014-07-22 09:04:46,819 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10579ms
2014-07-22 09:04:46,820 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10577ms
2014-07-22 09:04:46,820 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10543ms
2014-07-22 09:04:46,820 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10514ms
2014-07-22 09:04:46,820 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10485ms
2014-07-22 09:04:46,821 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10486ms
2014-07-22 09:04:46,821 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10485ms
2014-07-22 09:04:46,821 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10477ms
2014-07-22 09:04:46,822 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10465ms
2014-07-22 09:04:46,822 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10427ms
2014-07-22 09:04:46,822 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10416ms
2014-07-22 09:04:46,822 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10416ms
2014-07-22 09:04:46,822 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10416ms
2014-07-22 09:04:46,823 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10397ms
2014-07-22 09:04:46,823 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10396ms
2014-07-22 09:04:46,834 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10408ms
2014-07-22 09:04:46,834 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10368ms
2014-07-22 09:04:47,193 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11777, memsize=397.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/09c7d8dd5d41487db2af6966d0fcaa3e
2014-07-22 09:04:47,206 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/09c7d8dd5d41487db2af6966d0fcaa3e as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/09c7d8dd5d41487db2af6966d0fcaa3e
2014-07-22 09:04:47,221 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/09c7d8dd5d41487db2af6966d0fcaa3e, entries=1448030, sequenceid=11777, filesize=103.1m
2014-07-22 09:04:47,222 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~881.2m/924045120, currentsize=252.2m/264473040 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 26022ms, sequenceid=11777, compaction requested=true
2014-07-22 09:04:47,222 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:04:47,222 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 32 store files, 0 compacting, 32 eligible, 2000 blocking
2014-07-22 09:04:47,223 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10757ms
2014-07-22 09:04:47,223 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 32 files from compaction candidates
2014-07-22 09:04:47,223 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,223 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:04:47,223 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 904.7m
2014-07-22 09:04:47,223 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:04:47,223 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:04:47,225 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10799ms
2014-07-22 09:04:47,225 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,226 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10799ms
2014-07-22 09:04:47,226 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,226 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10801ms
2014-07-22 09:04:47,226 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,226 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10820ms
2014-07-22 09:04:47,226 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,229 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10823ms
2014-07-22 09:04:47,229 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,230 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10824ms
2014-07-22 09:04:47,230 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,230 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10835ms
2014-07-22 09:04:47,230 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,230 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10873ms
2014-07-22 09:04:47,230 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,230 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10886ms
2014-07-22 09:04:47,231 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,231 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10895ms
2014-07-22 09:04:47,231 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,232 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10897ms
2014-07-22 09:04:47,232 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,233 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10898ms
2014-07-22 09:04:47,233 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,233 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10927ms
2014-07-22 09:04:47,233 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,233 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10956ms
2014-07-22 09:04:47,233 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,235 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10993ms
2014-07-22 09:04:47,235 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,235 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10995ms
2014-07-22 09:04:47,235 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,235 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11015ms
2014-07-22 09:04:47,235 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,236 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11030ms
2014-07-22 09:04:47,236 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,236 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10571ms
2014-07-22 09:04:47,236 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,236 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10570ms
2014-07-22 09:04:47,236 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,236 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10604ms
2014-07-22 09:04:47,236 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,240 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10612ms
2014-07-22 09:04:47,240 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,240 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10646ms
2014-07-22 09:04:47,240 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,241 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10675ms
2014-07-22 09:04:47,241 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,241 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10711ms
2014-07-22 09:04:47,241 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,242 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10836ms
2014-07-22 09:04:47,242 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,242 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10753ms
2014-07-22 09:04:47,242 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,242 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10574ms
2014-07-22 09:04:47,242 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,242 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11785ms
2014-07-22 09:04:47,242 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,249 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11793ms
2014-07-22 09:04:47,249 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,249 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11794ms
2014-07-22 09:04:47,249 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,252 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11798ms
2014-07-22 09:04:47,252 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,252 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11798ms
2014-07-22 09:04:47,252 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,253 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11835ms
2014-07-22 09:04:47,253 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,254 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11872ms
2014-07-22 09:04:47,254 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,254 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11909ms
2014-07-22 09:04:47,255 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,255 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11946ms
2014-07-22 09:04:47,255 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,257 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12200ms
2014-07-22 09:04:47,257 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,257 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12212ms
2014-07-22 09:04:47,258 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,258 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12210ms
2014-07-22 09:04:47,258 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,258 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12027ms
2014-07-22 09:04:47,258 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,264 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12240ms
2014-07-22 09:04:47,264 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,264 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12106ms
2014-07-22 09:04:47,265 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,265 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12143ms
2014-07-22 09:04:47,265 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,272 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12164ms
2014-07-22 09:04:47,272 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,272 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12164ms
2014-07-22 09:04:47,272 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,277 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12158ms
2014-07-22 09:04:47,277 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,277 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12083ms
2014-07-22 09:04:47,277 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,278 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12009ms
2014-07-22 09:04:47,278 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:04:47,282 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10855,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076426,"queuetimems":0,"class":"HRegionServer","responsesize":1054,"method":"Multi"}
2014-07-22 09:04:47,286 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10862,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076424,"queuetimems":0,"class":"HRegionServer","responsesize":1105,"method":"Multi"}
2014-07-22 09:04:47,288 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10881,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076406,"queuetimems":0,"class":"HRegionServer","responsesize":207,"method":"Multi"}
2014-07-22 09:04:47,288 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10621,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076666,"queuetimems":1,"class":"HRegionServer","responsesize":141,"method":"Multi"}
2014-07-22 09:04:47,295 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10667,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076628,"queuetimems":0,"class":"HRegionServer","responsesize":8,"method":"Multi"}
2014-07-22 09:04:47,296 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10631,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076665,"queuetimems":0,"class":"HRegionServer","responsesize":26,"method":"Multi"}
2014-07-22 09:04:47,296 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10890,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076406,"queuetimems":0,"class":"HRegionServer","responsesize":14,"method":"Multi"}
2014-07-22 09:04:47,296 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10889,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076406,"queuetimems":0,"class":"HRegionServer","responsesize":159,"method":"Multi"}
2014-07-22 09:04:47,295 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11055,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076240,"queuetimems":0,"class":"HRegionServer","responsesize":122,"method":"Multi"}
2014-07-22 09:04:47,296 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10961,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076335,"queuetimems":0,"class":"HRegionServer","responsesize":20,"method":"Multi"}
2014-07-22 09:04:47,296 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10961,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076335,"queuetimems":0,"class":"HRegionServer","responsesize":44,"method":"Multi"}
2014-07-22 09:04:47,302 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11846,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045075455,"queuetimems":0,"class":"HRegionServer","responsesize":1212,"method":"Multi"}
2014-07-22 09:04:47,305 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11849,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045075456,"queuetimems":1,"class":"HRegionServer","responsesize":128,"method":"Multi"}
2014-07-22 09:04:47,305 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11851,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045075454,"queuetimems":0,"class":"HRegionServer","responsesize":177,"method":"Multi"}
2014-07-22 09:04:47,305 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12186,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045075119,"queuetimems":0,"class":"HRegionServer","responsesize":141,"method":"Multi"}
2014-07-22 09:04:47,306 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11852,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045075454,"queuetimems":0,"class":"HRegionServer","responsesize":1324,"method":"Multi"}
2014-07-22 09:04:47,358 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12604,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045074753,"queuetimems":0,"class":"HRegionServer","responsesize":18534,"method":"Multi"}
2014-07-22 09:04:47,526 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12711,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045074814,"queuetimems":1,"class":"HRegionServer","responsesize":19090,"method":"Multi"}
2014-07-22 09:04:47,544 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11763, memsize=417.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/8aa3f874ffa24dbfbc772cf07a58cbe8
2014-07-22 09:04:47,560 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:04:47,602 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 62105 synced till here 62094
2014-07-22 09:04:47,610 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/8aa3f874ffa24dbfbc772cf07a58cbe8 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/8aa3f874ffa24dbfbc772cf07a58cbe8
2014-07-22 09:04:47,620 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/8aa3f874ffa24dbfbc772cf07a58cbe8, entries=1519660, sequenceid=11763, filesize=108.1m
2014-07-22 09:04:47,621 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~949.8m/995945760, currentsize=285.7m/299588480 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 28035ms, sequenceid=11763, compaction requested=true
2014-07-22 09:04:47,622 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 2000 blocking
2014-07-22 09:04:47,622 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-22 09:04:47,622 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:04:47,622 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:04:47,623 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:04:47,633 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:04:47,634 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 694.5m
2014-07-22 09:04:47,650 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12770,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045074879,"queuetimems":0,"class":"HRegionServer","responsesize":18288,"method":"Multi"}
2014-07-22 09:04:47,767 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045073353 with entries=107, filesize=71.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045087561
2014-07-22 09:04:47,767 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044992618
2014-07-22 09:04:47,768 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406044999640
2014-07-22 09:04:47,768 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045000569
2014-07-22 09:04:47,768 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045002331
2014-07-22 09:04:47,768 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045004091
2014-07-22 09:04:47,768 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045006578
2014-07-22 09:04:47,768 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045009345
2014-07-22 09:04:47,768 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045010778
2014-07-22 09:04:47,769 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045012196
2014-07-22 09:04:47,769 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045013884
2014-07-22 09:04:47,769 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045016646
2014-07-22 09:04:48,225 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:04:48,228 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:04:48,286 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13312,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045074974,"queuetimems":0,"class":"HRegionServer","responsesize":15421,"method":"Multi"}
2014-07-22 09:04:49,734 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:04:49,772 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14822,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045074938,"queuetimems":0,"class":"HRegionServer","responsesize":17912,"method":"Multi"}
2014-07-22 09:04:49,799 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13443,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076356,"queuetimems":0,"class":"HRegionServer","responsesize":6898,"method":"Multi"}
2014-07-22 09:04:49,808 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13464,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076344,"queuetimems":0,"class":"HRegionServer","responsesize":5437,"method":"Multi"}
2014-07-22 09:04:49,978 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13554,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076423,"queuetimems":0,"class":"HRegionServer","responsesize":9394,"method":"Multi"}
2014-07-22 09:04:50,156 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:04:50,159 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13766,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076392,"queuetimems":0,"class":"HRegionServer","responsesize":18269,"method":"Multi"}
2014-07-22 09:04:50,159 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13854,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076304,"queuetimems":0,"class":"HRegionServer","responsesize":15380,"method":"Multi"}
2014-07-22 09:04:50,171 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 62220 synced till here 62188
2014-07-22 09:04:50,435 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14099,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076335,"queuetimems":1,"class":"HRegionServer","responsesize":16046,"method":"Multi"}
2014-07-22 09:04:50,437 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14161,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076275,"queuetimems":0,"class":"HRegionServer","responsesize":18565,"method":"Multi"}
2014-07-22 09:04:50,437 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13973,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076463,"queuetimems":0,"class":"HRegionServer","responsesize":17013,"method":"Multi"}
2014-07-22 09:04:50,547 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:04:50,547 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045087561 with entries=115, filesize=92.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045090156
2014-07-22 09:04:50,591 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14103,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076487,"queuetimems":1,"class":"HRegionServer","responsesize":10989,"method":"Multi"}
2014-07-22 09:04:50,686 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14094,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076591,"queuetimems":0,"class":"HRegionServer","responsesize":13352,"method":"Multi"}
2014-07-22 09:04:50,686 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14468,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076217,"queuetimems":0,"class":"HRegionServer","responsesize":19117,"method":"Multi"}
2014-07-22 09:04:50,739 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14500,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076239,"queuetimems":0,"class":"HRegionServer","responsesize":18177,"method":"Multi"}
2014-07-22 09:04:50,805 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14140,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076665,"queuetimems":0,"class":"HRegionServer","responsesize":16838,"method":"Multi"}
2014-07-22 09:04:52,311 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:04:52,845 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 62325 synced till here 62305
2014-07-22 09:04:52,938 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17630,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045075307,"queuetimems":0,"class":"HRegionServer","responsesize":18803,"method":"Multi"}
2014-07-22 09:04:53,019 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045090156 with entries=105, filesize=77.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045092311
2014-07-22 09:04:54,553 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1215ms
GC pool 'ParNew' had collection(s): count=1 time=1240ms
2014-07-22 09:04:54,701 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19434,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045075266,"queuetimems":0,"class":"HRegionServer","responsesize":19090,"method":"Multi"}
2014-07-22 09:04:54,714 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19372,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045075342,"queuetimems":0,"class":"HRegionServer","responsesize":18563,"method":"Multi"}
2014-07-22 09:04:54,738 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19285,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045075452,"queuetimems":0,"class":"HRegionServer","responsesize":18710,"method":"Multi"}
2014-07-22 09:04:54,745 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19553,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045075192,"queuetimems":0,"class":"HRegionServer","responsesize":18288,"method":"Multi"}
2014-07-22 09:04:54,746 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18341,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076405,"queuetimems":0,"class":"HRegionServer","responsesize":7587,"method":"Multi"}
2014-07-22 09:04:54,750 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19705,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045075045,"queuetimems":0,"class":"HRegionServer","responsesize":18118,"method":"Multi"}
2014-07-22 09:04:54,770 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19390,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045075380,"queuetimems":0,"class":"HRegionServer","responsesize":17912,"method":"Multi"}
2014-07-22 09:04:54,771 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19614,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045075156,"queuetimems":0,"class":"HRegionServer","responsesize":18702,"method":"Multi"}
2014-07-22 09:04:54,775 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19361,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045075414,"queuetimems":0,"class":"HRegionServer","responsesize":18118,"method":"Multi"}
2014-07-22 09:04:54,775 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18571,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076204,"queuetimems":1,"class":"HRegionServer","responsesize":15421,"method":"Multi"}
2014-07-22 09:04:54,834 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18263,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076563,"queuetimems":1,"class":"HRegionServer","responsesize":17048,"method":"Multi"}
2014-07-22 09:04:54,875 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18247,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076628,"queuetimems":0,"class":"HRegionServer","responsesize":17250,"method":"Multi"}
2014-07-22 09:04:54,878 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18353,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045076524,"queuetimems":0,"class":"HRegionServer","responsesize":16937,"method":"Multi"}
2014-07-22 09:04:55,183 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:04:55,184 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20065,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045075119,"queuetimems":0,"class":"HRegionServer","responsesize":18432,"method":"Multi"}
2014-07-22 09:04:55,210 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19980,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045075229,"queuetimems":0,"class":"HRegionServer","responsesize":18534,"method":"Multi"}
2014-07-22 09:04:55,287 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 62436 synced till here 62433
2014-07-22 09:04:55,323 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045092311 with entries=111, filesize=76.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045095184
2014-07-22 09:04:56,602 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1047ms
GC pool 'ParNew' had collection(s): count=1 time=1087ms
2014-07-22 09:04:57,255 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:04:57,314 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 62524 synced till here 62503
2014-07-22 09:04:57,497 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045095184 with entries=88, filesize=75.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045097256
2014-07-22 09:04:59,714 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:04:59,736 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 62632 synced till here 62611
2014-07-22 09:04:59,905 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045097256 with entries=108, filesize=87.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045099714
2014-07-22 09:05:01,302 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:05:01,356 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 62754 synced till here 62747
2014-07-22 09:05:01,421 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045099714 with entries=122, filesize=66.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045101303
2014-07-22 09:05:03,527 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:05:03,557 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 62854 synced till here 62831
2014-07-22 09:05:03,802 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045101303 with entries=100, filesize=83.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045103528
2014-07-22 09:05:04,841 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:05:05,987 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 62959 synced till here 62954
2014-07-22 09:05:06,032 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045103528 with entries=105, filesize=68.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045104842
2014-07-22 09:05:07,024 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:05:07,040 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 63066 synced till here 63039
2014-07-22 09:05:07,235 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045104842 with entries=107, filesize=91.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045107024
2014-07-22 09:05:08,749 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:05:08,834 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 63164 synced till here 63162
2014-07-22 09:05:08,904 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045107024 with entries=98, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045108750
2014-07-22 09:05:10,753 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:05:10,766 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 63296 synced till here 63265
2014-07-22 09:05:11,130 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045108750 with entries=132, filesize=93.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045110753
2014-07-22 09:05:12,613 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:05:12,672 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 63391 synced till here 63381
2014-07-22 09:05:12,767 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045110753 with entries=95, filesize=70.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045112613
2014-07-22 09:05:13,198 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,200 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,201 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,202 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,202 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,202 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,202 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,207 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,207 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,208 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,208 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,208 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,210 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,269 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,310 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,396 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,396 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,499 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:05:13,524 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 63472 synced till here 63461
2014-07-22 09:05:13,540 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,581 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,582 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,582 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,582 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,582 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,583 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,583 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,584 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,585 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,585 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,589 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,594 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045112613 with entries=81, filesize=71.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045113500
2014-07-22 09:05:13,612 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,647 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,675 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,683 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,719 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,755 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,785 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,816 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,850 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,852 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,888 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,922 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,923 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,924 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,924 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,927 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,927 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:13,928 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:16,291 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:16,292 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:16,295 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:05:18,198 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:05:18,200 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:05:18,202 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:05:18,203 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:05:18,204 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5003ms
2014-07-22 09:05:18,204 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 09:05:18,207 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:05:18,208 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:05:18,208 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:05:18,208 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5006ms
2014-07-22 09:05:18,208 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:05:18,208 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5006ms
2014-07-22 09:05:18,211 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:05:18,269 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:05:18,310 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:05:18,396 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:05:18,397 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:05:18,540 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:05:18,581 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:05:18,582 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:05:18,582 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:05:18,582 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:05:18,582 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:05:18,583 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:05:18,584 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:05:18,585 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:05:18,585 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:05:18,585 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:05:18,589 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:05:18,612 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:05:18,648 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:05:18,990 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5062ms
2014-07-22 09:05:18,990 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5102ms
2014-07-22 09:05:18,990 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5174ms
2014-07-22 09:05:18,990 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5138ms
2014-07-22 09:05:18,991 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5141ms
2014-07-22 09:05:18,991 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5064ms
2014-07-22 09:05:18,991 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5069ms
2014-07-22 09:05:18,991 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5068ms
2014-07-22 09:05:18,992 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5067ms
2014-07-22 09:05:18,992 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5068ms
2014-07-22 09:05:18,992 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5065ms
2014-07-22 09:05:18,993 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5317ms
2014-07-22 09:05:18,993 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5310ms
2014-07-22 09:05:18,993 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5274ms
2014-07-22 09:05:18,993 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5238ms
2014-07-22 09:05:18,993 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5208ms
2014-07-22 09:05:20,530 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11914, memsize=416.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/68d9720bbc994f928683a75d0863f532
2014-07-22 09:05:20,540 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/68d9720bbc994f928683a75d0863f532 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/68d9720bbc994f928683a75d0863f532
2014-07-22 09:05:20,547 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/68d9720bbc994f928683a75d0863f532, entries=1516260, sequenceid=11914, filesize=108.0m
2014-07-22 09:05:20,548 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~904.7m/948656880, currentsize=449.3m/471129360 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 33325ms, sequenceid=11914, compaction requested=true
2014-07-22 09:05:20,548 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 32 store files, 0 compacting, 32 eligible, 2000 blocking
2014-07-22 09:05:20,549 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 32 files from compaction candidates
2014-07-22 09:05:20,549 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:05:20,549 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:05:20,549 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:05:20,549 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:05:20,549 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6764ms
2014-07-22 09:05:20,549 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,549 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 723.2m
2014-07-22 09:05:20,549 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6794ms
2014-07-22 09:05:20,549 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,550 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6831ms
2014-07-22 09:05:20,550 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,560 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6877ms
2014-07-22 09:05:20,560 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,560 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6885ms
2014-07-22 09:05:20,560 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,562 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6633ms
2014-07-22 09:05:20,562 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,562 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6638ms
2014-07-22 09:05:20,562 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,562 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6638ms
2014-07-22 09:05:20,562 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,563 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6639ms
2014-07-22 09:05:20,563 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,563 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6641ms
2014-07-22 09:05:20,563 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11972, memsize=449.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/929bfc4a45d148959419b1cc207c5eb2
2014-07-22 09:05:20,563 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,567 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6640ms
2014-07-22 09:05:20,567 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,569 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6719ms
2014-07-22 09:05:20,569 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,569 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6717ms
2014-07-22 09:05:20,569 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,569 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6753ms
2014-07-22 09:05:20,569 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,570 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6682ms
2014-07-22 09:05:20,570 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,570 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6642ms
2014-07-22 09:05:20,570 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,570 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6923ms
2014-07-22 09:05:20,570 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,571 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6959ms
2014-07-22 09:05:20,571 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,571 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6982ms
2014-07-22 09:05:20,571 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,572 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6987ms
2014-07-22 09:05:20,572 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,581 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6996ms
2014-07-22 09:05:20,581 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,581 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6997ms
2014-07-22 09:05:20,581 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,582 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6999ms
2014-07-22 09:05:20,582 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,589 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7006ms
2014-07-22 09:05:20,589 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,597 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7015ms
2014-07-22 09:05:20,597 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,597 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7016ms
2014-07-22 09:05:20,597 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,598 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7015ms
2014-07-22 09:05:20,598 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,599 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/929bfc4a45d148959419b1cc207c5eb2 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/929bfc4a45d148959419b1cc207c5eb2
2014-07-22 09:05:20,609 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7028ms
2014-07-22 09:05:20,609 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,609 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7028ms
2014-07-22 09:05:20,609 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,610 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7070ms
2014-07-22 09:05:20,610 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,610 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7214ms
2014-07-22 09:05:20,610 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,610 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7215ms
2014-07-22 09:05:20,611 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,613 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7303ms
2014-07-22 09:05:20,613 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,613 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7344ms
2014-07-22 09:05:20,613 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,613 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7403ms
2014-07-22 09:05:20,613 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,614 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/929bfc4a45d148959419b1cc207c5eb2, entries=1638080, sequenceid=11972, filesize=116.6m
2014-07-22 09:05:20,614 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7412ms
2014-07-22 09:05:20,614 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,615 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7407ms
2014-07-22 09:05:20,615 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,616 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7414ms
2014-07-22 09:05:20,616 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,618 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~761.2m/798180640, currentsize=394.2m/413369040 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 32984ms, sequenceid=11972, compaction requested=true
2014-07-22 09:05:20,618 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 33 store files, 0 compacting, 33 eligible, 2000 blocking
2014-07-22 09:05:20,619 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 33 files from compaction candidates
2014-07-22 09:05:20,619 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:05:20,619 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:05:20,619 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:05:20,619 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:05:20,619 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 744.6m
2014-07-22 09:05:20,625 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7417ms
2014-07-22 09:05:20,625 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,669 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7462ms
2014-07-22 09:05:20,669 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,670 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7464ms
2014-07-22 09:05:20,670 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,670 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7468ms
2014-07-22 09:05:20,670 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,670 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7469ms
2014-07-22 09:05:20,670 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,670 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7468ms
2014-07-22 09:05:20,670 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,670 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7468ms
2014-07-22 09:05:20,670 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,670 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7470ms
2014-07-22 09:05:20,670 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,674 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7476ms
2014-07-22 09:05:20,674 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,674 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4379ms
2014-07-22 09:05:20,674 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,675 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4384ms
2014-07-22 09:05:20,675 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,675 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4384ms
2014-07-22 09:05:20,675 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:05:20,690 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11319,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045109371,"queuetimems":11649,"class":"HRegionServer","responsesize":18534,"method":"Multi"}
2014-07-22 09:05:20,690 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11361,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045109329,"queuetimems":12162,"class":"HRegionServer","responsesize":18473,"method":"Multi"}
2014-07-22 09:05:20,747 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11058,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045109688,"queuetimems":5731,"class":"HRegionServer","responsesize":17013,"method":"Multi"}
2014-07-22 09:05:20,754 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11391,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045109362,"queuetimems":11795,"class":"HRegionServer","responsesize":17283,"method":"Multi"}
2014-07-22 09:05:20,756 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11350,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045109406,"queuetimems":8134,"class":"HRegionServer","responsesize":18177,"method":"Multi"}
2014-07-22 09:05:20,756 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11080,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045109676,"queuetimems":5796,"class":"HRegionServer","responsesize":18034,"method":"Multi"}
2014-07-22 09:05:20,757 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11205,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045109551,"queuetimems":5901,"class":"HRegionServer","responsesize":16937,"method":"Multi"}
2014-07-22 09:05:20,756 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11387,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045109369,"queuetimems":11721,"class":"HRegionServer","responsesize":17048,"method":"Multi"}
2014-07-22 09:05:20,782 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10266,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045110515,"queuetimems":6454,"class":"HRegionServer","responsesize":15579,"method":"Multi"}
2014-07-22 09:05:20,808 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:05:20,993 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:05:21,049 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11535,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045109513,"queuetimems":5895,"class":"HRegionServer","responsesize":18269,"method":"Multi"}
2014-07-22 09:05:21,050 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11544,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045109506,"queuetimems":5923,"class":"HRegionServer","responsesize":19117,"method":"Multi"}
2014-07-22 09:05:21,097 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11598,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045109498,"queuetimems":7918,"class":"HRegionServer","responsesize":18118,"method":"Multi"}
2014-07-22 09:05:21,218 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11522,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045109695,"queuetimems":5690,"class":"HRegionServer","responsesize":16838,"method":"Multi"}
2014-07-22 09:05:21,401 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:05:21,404 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12041,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045109362,"queuetimems":11760,"class":"HRegionServer","responsesize":15579,"method":"Multi"}
2014-07-22 09:05:21,406 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12035,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045109370,"queuetimems":11684,"class":"HRegionServer","responsesize":18269,"method":"Multi"}
2014-07-22 09:05:22,308 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12964,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045109343,"queuetimems":12016,"class":"HRegionServer","responsesize":16838,"method":"Multi"}
2014-07-22 09:05:22,447 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:05:22,483 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 63585 synced till here 63573
2014-07-22 09:05:22,495 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:05:22,564 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045113500 with entries=113, filesize=70.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045122448
2014-07-22 09:05:22,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045018452
2014-07-22 09:05:22,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045020716
2014-07-22 09:05:22,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045022496
2014-07-22 09:05:22,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045025609
2014-07-22 09:05:22,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045028610
2014-07-22 09:05:22,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045032202
2014-07-22 09:05:22,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045035220
2014-07-22 09:05:22,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045038133
2014-07-22 09:05:22,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045040288
2014-07-22 09:05:22,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045042204
2014-07-22 09:05:22,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045043761
2014-07-22 09:05:23,227 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:05:23,326 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045122448 with entries=80, filesize=71.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045123227
2014-07-22 09:05:24,406 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11198,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045113208,"queuetimems":652,"class":"HRegionServer","responsesize":18186,"method":"Multi"}
2014-07-22 09:05:24,409 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10799,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045113610,"queuetimems":0,"class":"HRegionServer","responsesize":15380,"method":"Multi"}
2014-07-22 09:05:24,460 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10873,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045113587,"queuetimems":1,"class":"HRegionServer","responsesize":15421,"method":"Multi"}
2014-07-22 09:05:24,460 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10610,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045113850,"queuetimems":0,"class":"HRegionServer","responsesize":18930,"method":"Multi"}
2014-07-22 09:05:24,461 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11258,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045113203,"queuetimems":2615,"class":"HRegionServer","responsesize":18803,"method":"Multi"}
2014-07-22 09:05:24,495 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10575,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045113920,"queuetimems":0,"class":"HRegionServer","responsesize":18177,"method":"Multi"}
2014-07-22 09:05:24,583 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11384,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045113199,"queuetimems":2662,"class":"HRegionServer","responsesize":18710,"method":"Multi"}
2014-07-22 09:05:24,598 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11394,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045113203,"queuetimems":2558,"class":"HRegionServer","responsesize":18534,"method":"Multi"}
2014-07-22 09:05:25,749 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:05:25,782 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045123227 with entries=90, filesize=61.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045125750
2014-07-22 09:05:25,783 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:05:27,296 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:05:27,627 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 63884 synced till here 63882
2014-07-22 09:05:27,648 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045125750 with entries=129, filesize=80.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045127297
2014-07-22 09:05:27,651 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:05:29,364 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:05:29,400 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045127297 with entries=90, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045129364
2014-07-22 09:05:29,402 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:05:31,128 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:05:31,149 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64096 synced till here 64070
2014-07-22 09:05:31,283 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045129364 with entries=122, filesize=71.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045131128
2014-07-22 09:05:31,283 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:05:32,841 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:05:33,094 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64207 synced till here 64205
2014-07-22 09:05:33,127 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045131128 with entries=111, filesize=83.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045132842
2014-07-22 09:05:33,128 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:05:34,824 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:05:34,839 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64292 synced till here 64285
2014-07-22 09:05:34,912 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045132842 with entries=85, filesize=65.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045134824
2014-07-22 09:05:34,912 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:05:36,396 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12223, memsize=220.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/fea01e5b2b0545b1a68922bcba06719e
2014-07-22 09:05:36,414 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/fea01e5b2b0545b1a68922bcba06719e as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/fea01e5b2b0545b1a68922bcba06719e
2014-07-22 09:05:36,426 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/fea01e5b2b0545b1a68922bcba06719e, entries=803500, sequenceid=12223, filesize=57.2m
2014-07-22 09:05:36,427 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~723.2m/758352240, currentsize=265.2m/278119280 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 15878ms, sequenceid=12223, compaction requested=true
2014-07-22 09:05:36,427 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:05:36,427 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 33 store files, 0 compacting, 33 eligible, 2000 blocking
2014-07-22 09:05:36,428 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 33 files from compaction candidates
2014-07-22 09:05:36,428 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 659.7m
2014-07-22 09:05:36,428 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:05:36,428 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:05:36,428 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:05:36,464 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:05:36,465 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:05:36,481 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64373 synced till here 64371
2014-07-22 09:05:36,513 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045134824 with entries=81, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045136466
2014-07-22 09:05:36,513 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:05:37,034 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:05:37,488 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:05:38,061 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64461 synced till here 64456
2014-07-22 09:05:38,082 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045136466 with entries=88, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045137489
2014-07-22 09:05:38,083 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:05:38,088 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12224, memsize=247.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/3786c4adde754455bd0d82379365bce0
2014-07-22 09:05:38,102 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/3786c4adde754455bd0d82379365bce0 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/3786c4adde754455bd0d82379365bce0
2014-07-22 09:05:38,114 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/3786c4adde754455bd0d82379365bce0, entries=902120, sequenceid=12224, filesize=64.2m
2014-07-22 09:05:38,115 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~744.6m/780812800, currentsize=291.9m/306102320 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 17496ms, sequenceid=12224, compaction requested=true
2014-07-22 09:05:38,115 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:05:38,115 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 32 store files, 0 compacting, 32 eligible, 2000 blocking
2014-07-22 09:05:38,116 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 758.1m
2014-07-22 09:05:38,116 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 32 files from compaction candidates
2014-07-22 09:05:38,116 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:05:38,116 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:05:38,116 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:05:38,182 DEBUG [RpcServer.handler=46,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:05:38,835 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:05:38,849 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:05:38,864 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64549 synced till here 64548
2014-07-22 09:05:38,887 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045137489 with entries=88, filesize=62.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045138849
2014-07-22 09:05:38,888 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:05:40,408 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:05:40,518 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64640 synced till here 64632
2014-07-22 09:05:40,668 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045138849 with entries=91, filesize=70.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045140408
2014-07-22 09:05:40,668 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:05:42,057 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:05:42,094 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045140408 with entries=88, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045142058
2014-07-22 09:05:42,095 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:05:43,440 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:05:43,839 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64826 synced till here 64825
2014-07-22 09:05:43,894 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045142058 with entries=98, filesize=81.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045143440
2014-07-22 09:05:43,895 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=44, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:05:45,558 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:05:45,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64926 synced till here 64914
2014-07-22 09:05:45,675 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045143440 with entries=100, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045145558
2014-07-22 09:05:45,676 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=45, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:05:46,078 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b.
2014-07-22 09:05:47,287 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:05:47,313 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65035 synced till here 65014
2014-07-22 09:05:47,401 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045145558 with entries=109, filesize=71.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045147287
2014-07-22 09:05:47,403 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=46, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:05:48,707 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:05:48,728 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65131 synced till here 65125
2014-07-22 09:05:48,798 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045147287 with entries=96, filesize=66.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045148708
2014-07-22 09:05:48,800 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=47, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:05:50,163 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:05:50,200 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65235 synced till here 65215
2014-07-22 09:05:50,266 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045148708 with entries=104, filesize=67.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045150164
2014-07-22 09:05:50,267 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=48, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:05:50,361 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12394, memsize=192.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/54708a2465964163980f785659bb0e32
2014-07-22 09:05:50,377 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/54708a2465964163980f785659bb0e32 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/54708a2465964163980f785659bb0e32
2014-07-22 09:05:50,393 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/54708a2465964163980f785659bb0e32, entries=699920, sequenceid=12394, filesize=49.9m
2014-07-22 09:05:50,394 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~663.4m/695625120, currentsize=268.1m/281158720 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 13966ms, sequenceid=12394, compaction requested=true
2014-07-22 09:05:50,395 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:05:50,395 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 34 store files, 0 compacting, 34 eligible, 2000 blocking
2014-07-22 09:05:50,395 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b., current region memstore size 280.1m
2014-07-22 09:05:50,395 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 34 files from compaction candidates
2014-07-22 09:05:50,395 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:05:50,395 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:05:50,395 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:05:50,443 DEBUG [RpcServer.handler=15,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:05:50,685 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:06:04,122 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13955,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150166,"queuetimems":1,"class":"HRegionServer","responsesize":16081,"method":"Multi"}
2014-07-22 09:06:04,122 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13416,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150705,"queuetimems":2,"class":"HRegionServer","responsesize":14,"method":"Multi"}
2014-07-22 09:06:04,122 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46187 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,122 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13418,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150703,"queuetimems":0,"class":"HRegionServer","responsesize":214,"method":"Multi"}
2014-07-22 09:06:04,122 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13417,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150705,"queuetimems":0,"class":"HRegionServer","responsesize":171,"method":"Multi"}
2014-07-22 09:06:04,123 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13417,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150705,"queuetimems":0,"class":"HRegionServer","responsesize":1153,"method":"Multi"}
2014-07-22 09:06:04,123 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13416,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150707,"queuetimems":0,"class":"HRegionServer","responsesize":1256,"method":"Multi"}
2014-07-22 09:06:04,124 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,124 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46288 service: ClientService methodName: Multi size: 228.6k connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,124 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,125 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46295 service: ClientService methodName: Multi size: 209.8k connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,125 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13373,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150750,"queuetimems":0,"class":"HRegionServer","responsesize":1152,"method":"Multi"}
2014-07-22 09:06:04,125 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46294 service: ClientService methodName: Multi size: 35.0k connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,125 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,125 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,125 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46284 service: ClientService methodName: Multi size: 209.8k connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,125 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,125 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46297 service: ClientService methodName: Multi size: 43.8k connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,125 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,125 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46296 service: ClientService methodName: Multi size: 2.6k connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,126 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,130 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14615,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045149508,"queuetimems":0,"class":"HRegionServer","responsesize":18753,"method":"Multi"}
2014-07-22 09:06:04,131 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46205 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,131 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,131 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13333,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150798,"queuetimems":1,"class":"HRegionServer","responsesize":1488,"method":"Multi"}
2014-07-22 09:06:04,132 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13315,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150816,"queuetimems":0,"class":"HRegionServer","responsesize":3132,"method":"Multi"}
2014-07-22 09:06:04,132 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46280 service: ClientService methodName: Multi size: 271.0k connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,132 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,132 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13212,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150920,"queuetimems":1,"class":"HRegionServer","responsesize":5194,"method":"Multi"}
2014-07-22 09:06:04,132 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46277 service: ClientService methodName: Multi size: 563.2k connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,132 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,132 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46264 service: ClientService methodName: Multi size: 932.8k connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,132 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,134 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13312,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150821,"queuetimems":0,"class":"HRegionServer","responsesize":2122,"method":"Multi"}
2014-07-22 09:06:04,134 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12937,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045151196,"queuetimems":0,"class":"HRegionServer","responsesize":155,"method":"Multi"}
2014-07-22 09:06:04,134 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13317,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150817,"queuetimems":1,"class":"HRegionServer","responsesize":396,"method":"Multi"}
2014-07-22 09:06:04,134 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46272 service: ClientService methodName: Multi size: 382.2k connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,134 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,135 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46274 service: ClientService methodName: Multi size: 75.0k connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,135 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,135 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46251 service: ClientService methodName: Multi size: 31.3k connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,135 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,139 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12855,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045151284,"queuetimems":0,"class":"HRegionServer","responsesize":8,"method":"Multi"}
2014-07-22 09:06:04,140 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46366 service: ClientService methodName: Multi size: 1.3k connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,140 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,140 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12856,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045151284,"queuetimems":0,"class":"HRegionServer","responsesize":8,"method":"Multi"}
2014-07-22 09:06:04,140 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46364 service: ClientService methodName: Multi size: 1.3k connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,140 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,141 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12667,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045151473,"queuetimems":0,"class":"HRegionServer","responsesize":116,"method":"Multi"}
2014-07-22 09:06:04,141 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46355 service: ClientService methodName: Multi size: 23.8k connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,141 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,142 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12669,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045151473,"queuetimems":1,"class":"HRegionServer","responsesize":1301,"method":"Multi"}
2014-07-22 09:06:04,143 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46356 service: ClientService methodName: Multi size: 237.3k connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,143 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,143 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12668,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045151475,"queuetimems":2,"class":"HRegionServer","responsesize":20,"method":"Multi"}
2014-07-22 09:06:04,143 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46353 service: ClientService methodName: Multi size: 3.8k connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,143 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,143 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12670,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045151473,"queuetimems":0,"class":"HRegionServer","responsesize":177,"method":"Multi"}
2014-07-22 09:06:04,143 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46354 service: ClientService methodName: Multi size: 36.3k connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,143 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,140 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12669,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045151471,"queuetimems":0,"class":"HRegionServer","responsesize":44,"method":"Multi"}
2014-07-22 09:06:04,145 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46357 service: ClientService methodName: Multi size: 8.8k connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,145 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,177 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:06:04,179 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13968,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150210,"queuetimems":0,"class":"HRegionServer","responsesize":18851,"method":"Multi"}
2014-07-22 09:06:04,179 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46182 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,179 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,179 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13808,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150370,"queuetimems":1,"class":"HRegionServer","responsesize":12690,"method":"Multi"}
2014-07-22 09:06:04,210 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46180 service: ClientService methodName: Multi size: 2.2m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,210 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,253 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65338 synced till here 65333
2014-07-22 09:06:04,292 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13993,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150299,"queuetimems":1,"class":"HRegionServer","responsesize":18696,"method":"Multi"}
2014-07-22 09:06:04,292 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46181 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,293 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,305 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045150164 with entries=103, filesize=66.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045164178
2014-07-22 09:06:04,332 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13887,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150445,"queuetimems":0,"class":"HRegionServer","responsesize":18429,"method":"Multi"}
2014-07-22 09:06:04,333 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46332 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,333 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,622 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13969,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150652,"queuetimems":1,"class":"HRegionServer","responsesize":4669,"method":"Multi"}
2014-07-22 09:06:04,622 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46300 service: ClientService methodName: Multi size: 847.0k connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,622 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,689 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14159,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150530,"queuetimems":0,"class":"HRegionServer","responsesize":18848,"method":"Multi"}
2014-07-22 09:06:04,689 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14110,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150579,"queuetimems":0,"class":"HRegionServer","responsesize":18723,"method":"Multi"}
2014-07-22 09:06:04,690 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46331 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,690 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,690 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46330 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,690 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,781 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14078,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150703,"queuetimems":0,"class":"HRegionServer","responsesize":16081,"method":"Multi"}
2014-07-22 09:06:04,782 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13799,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150982,"queuetimems":0,"class":"HRegionServer","responsesize":11365,"method":"Multi"}
2014-07-22 09:06:04,782 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46298 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,782 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13834,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150948,"queuetimems":0,"class":"HRegionServer","responsesize":9379,"method":"Multi"}
2014-07-22 09:06:04,782 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,782 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46261 service: ClientService methodName: Multi size: 2.0m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,782 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,782 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46263 service: ClientService methodName: Multi size: 1.6m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,782 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:04,782 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14150,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150631,"queuetimems":1,"class":"HRegionServer","responsesize":18448,"method":"Multi"}
2014-07-22 09:06:04,783 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46320 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:04,783 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:05,095 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13988,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045151107,"queuetimems":0,"class":"HRegionServer","responsesize":13328,"method":"Multi"}
2014-07-22 09:06:05,096 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46257 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:05,096 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:05,166 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14356,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150810,"queuetimems":0,"class":"HRegionServer","responsesize":5625,"method":"Multi"}
2014-07-22 09:06:05,167 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46279 service: ClientService methodName: Multi size: 1015.6k connection: 9.1.143.53:54552: output error
2014-07-22 09:06:05,167 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:06,033 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:06:06,035 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14960,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045151075,"queuetimems":0,"class":"HRegionServer","responsesize":18735,"method":"Multi"}
2014-07-22 09:06:06,035 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15168,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150867,"queuetimems":0,"class":"HRegionServer","responsesize":18553,"method":"Multi"}
2014-07-22 09:06:06,036 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15148,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150887,"queuetimems":0,"class":"HRegionServer","responsesize":6812,"method":"Multi"}
2014-07-22 09:06:06,036 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15131,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150904,"queuetimems":0,"class":"HRegionServer","responsesize":8019,"method":"Multi"}
2014-07-22 09:06:06,036 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46258 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:06,036 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14565,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045151471,"queuetimems":0,"class":"HRegionServer","responsesize":7699,"method":"Multi"}
2014-07-22 09:06:06,036 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:06,036 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46267 service: ClientService methodName: Multi size: 1.4m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:06,036 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:06,036 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46269 service: ClientService methodName: Multi size: 1.2m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:06,036 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:06,037 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46271 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:06,037 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:06,037 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46358 service: ClientService methodName: Multi size: 1.4m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:06,037 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:06,052 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65454 synced till here 65418
2014-07-22 09:06:06,358 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15073,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045151284,"queuetimems":0,"class":"HRegionServer","responsesize":18696,"method":"Multi"}
2014-07-22 09:06:06,359 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14906,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045151452,"queuetimems":0,"class":"HRegionServer","responsesize":15723,"method":"Multi"}
2014-07-22 09:06:06,360 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14961,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045151398,"queuetimems":0,"class":"HRegionServer","responsesize":18322,"method":"Multi"}
2014-07-22 09:06:06,360 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15006,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045151354,"queuetimems":1,"class":"HRegionServer","responsesize":18578,"method":"Multi"}
2014-07-22 09:06:06,361 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15123,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045151238,"queuetimems":0,"class":"HRegionServer","responsesize":15715,"method":"Multi"}
2014-07-22 09:06:06,361 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15208,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045151153,"queuetimems":0,"class":"HRegionServer","responsesize":18867,"method":"Multi"}
2014-07-22 09:06:06,361 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15050,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045151311,"queuetimems":0,"class":"HRegionServer","responsesize":12690,"method":"Multi"}
2014-07-22 09:06:06,359 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46367 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:06,362 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:06,362 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46362 service: ClientService methodName: Multi size: 2.2m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:06,362 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:06,367 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15572,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150794,"queuetimems":0,"class":"HRegionServer","responsesize":18560,"method":"Multi"}
2014-07-22 09:06:06,367 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46283 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:06,367 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:06,369 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15621,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045150748,"queuetimems":0,"class":"HRegionServer","responsesize":18710,"method":"Multi"}
2014-07-22 09:06:06,370 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46286 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:06,370 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:06,371 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46359 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:06,371 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:06,380 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46255 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:06,381 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:06,381 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46369 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:06,381 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:06,381 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46361 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:06,381 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:06,381 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46360 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:06,381 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:06,474 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15278,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045151195,"queuetimems":0,"class":"HRegionServer","responsesize":17422,"method":"Multi"}
2014-07-22 09:06:06,474 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46253 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:06,474 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:06,507 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045164178 with entries=116, filesize=88.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045166034
2014-07-22 09:06:06,859 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12399, memsize=214.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/ae162d94f31144d0a6d7fac1fb4f3fb0
2014-07-22 09:06:06,898 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/ae162d94f31144d0a6d7fac1fb4f3fb0 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/ae162d94f31144d0a6d7fac1fb4f3fb0
2014-07-22 09:06:06,918 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/ae162d94f31144d0a6d7fac1fb4f3fb0, entries=780610, sequenceid=12399, filesize=55.6m
2014-07-22 09:06:06,918 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~763.4m/800434640, currentsize=290.2m/304303280 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 28802ms, sequenceid=12399, compaction requested=true
2014-07-22 09:06:06,919 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:06:06,919 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 33 store files, 0 compacting, 33 eligible, 2000 blocking
2014-07-22 09:06:06,919 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 33 files from compaction candidates
2014-07-22 09:06:06,919 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 590.9m
2014-07-22 09:06:06,919 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:06:06,920 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:06:06,920 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:06:06,999 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:06:07,035 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16009,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54552","starttimems":1406045151025,"queuetimems":0,"class":"HRegionServer","responsesize":18377,"method":"Multi"}
2014-07-22 09:06:07,035 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46259 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54552: output error
2014-07-22 09:06:07,035 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:06:08,885 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:06:08,970 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:06:09,017 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65579 synced till here 65552
2014-07-22 09:06:09,258 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045166034 with entries=125, filesize=79.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045168970
2014-07-22 09:06:09,259 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045112613
2014-07-22 09:06:11,394 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:06:11,416 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65696 synced till here 65666
2014-07-22 09:06:11,733 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045168970 with entries=117, filesize=89.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045171394
2014-07-22 09:06:13,466 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:06:13,547 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=15217, memsize=184.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/957e8d8f0eeb4f9691c51770e70dbaa7
2014-07-22 09:06:13,558 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65843 synced till here 65813
2014-07-22 09:06:13,578 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/957e8d8f0eeb4f9691c51770e70dbaa7 as hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/957e8d8f0eeb4f9691c51770e70dbaa7
2014-07-22 09:06:13,597 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/957e8d8f0eeb4f9691c51770e70dbaa7, entries=672770, sequenceid=15217, filesize=47.9m
2014-07-22 09:06:13,598 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~280.1m/293722960, currentsize=30.2m/31706160 for region usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. in 23203ms, sequenceid=15217, compaction requested=true
2014-07-22 09:06:13,598 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:06:13,598 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 2000 blocking
2014-07-22 09:06:13,598 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 701.9m
2014-07-22 09:06:13,598 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-22 09:06:13,598 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:06:13,599 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:06:13,599 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. because compaction request was cancelled
2014-07-22 09:06:13,775 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045171394 with entries=147, filesize=91.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045173466
2014-07-22 09:06:13,775 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045046298
2014-07-22 09:06:13,775 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045047793
2014-07-22 09:06:13,775 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045049543
2014-07-22 09:06:13,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045051707
2014-07-22 09:06:13,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045053424
2014-07-22 09:06:13,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045055758
2014-07-22 09:06:13,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045057326
2014-07-22 09:06:13,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045058514
2014-07-22 09:06:13,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045060113
2014-07-22 09:06:13,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045061532
2014-07-22 09:06:13,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045063236
2014-07-22 09:06:13,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045065043
2014-07-22 09:06:13,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045066638
2014-07-22 09:06:13,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045068501
2014-07-22 09:06:13,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045070234
2014-07-22 09:06:13,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045071276
2014-07-22 09:06:13,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045073353
2014-07-22 09:06:13,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045087561
2014-07-22 09:06:13,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045090156
2014-07-22 09:06:13,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045092311
2014-07-22 09:06:13,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045095184
2014-07-22 09:06:13,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045097256
2014-07-22 09:06:13,777 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045099714
2014-07-22 09:06:13,777 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045101303
2014-07-22 09:06:13,777 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045103528
2014-07-22 09:06:13,777 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045104842
2014-07-22 09:06:13,777 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045107024
2014-07-22 09:06:13,777 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045108750
2014-07-22 09:06:13,777 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045110753
2014-07-22 09:06:15,269 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:06:15,607 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:06:15,689 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65987 synced till here 65949
2014-07-22 09:06:16,889 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045173466 with entries=144, filesize=102.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045175607
2014-07-22 09:06:17,738 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:06:17,802 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 66078 synced till here 66069
2014-07-22 09:06:18,043 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045175607 with entries=91, filesize=72.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045177739
2014-07-22 09:06:20,579 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:06:20,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 66174 synced till here 66164
2014-07-22 09:06:20,804 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045177739 with entries=96, filesize=72.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045180579
2014-07-22 09:06:22,467 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:06:22,693 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 66296 synced till here 66290
2014-07-22 09:06:22,768 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045180579 with entries=122, filesize=80.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045182467
2014-07-22 09:06:24,098 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:06:24,314 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 66397 synced till here 66387
2014-07-22 09:06:24,543 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045182467 with entries=101, filesize=72.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045184098
2014-07-22 09:06:25,949 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:06:25,977 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 66481 synced till here 66480
2014-07-22 09:06:25,991 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045184098 with entries=84, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045185949
2014-07-22 09:06:27,469 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:06:27,494 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 66572 synced till here 66564
2014-07-22 09:06:27,673 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045185949 with entries=91, filesize=67.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045187469
2014-07-22 09:06:28,801 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12587, memsize=367.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/d1c509be43d646d8b83328b286616f3f
2014-07-22 09:06:28,826 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/d1c509be43d646d8b83328b286616f3f as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/d1c509be43d646d8b83328b286616f3f
2014-07-22 09:06:28,860 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/d1c509be43d646d8b83328b286616f3f, entries=1337840, sequenceid=12587, filesize=95.2m
2014-07-22 09:06:28,861 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~598.7m/627772880, currentsize=350.1m/367121520 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 21942ms, sequenceid=12587, compaction requested=true
2014-07-22 09:06:28,861 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:06:28,862 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 34 store files, 0 compacting, 34 eligible, 2000 blocking
2014-07-22 09:06:28,862 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 34 files from compaction candidates
2014-07-22 09:06:28,862 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 686.6m
2014-07-22 09:06:28,862 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:06:28,862 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:06:28,862 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:06:29,028 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:06:29,618 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:06:29,775 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:06:29,798 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 66683 synced till here 66672
2014-07-22 09:06:29,984 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045187469 with entries=111, filesize=84.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045189618
2014-07-22 09:06:31,904 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:06:33,198 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 66809 synced till here 66791
2014-07-22 09:06:33,393 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045189618 with entries=126, filesize=90.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045191905
2014-07-22 09:06:34,562 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12668, memsize=367.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/5db211685e80486a944d98f36b120047
2014-07-22 09:06:34,573 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:06:34,579 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/5db211685e80486a944d98f36b120047 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/5db211685e80486a944d98f36b120047
2014-07-22 09:06:34,594 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/5db211685e80486a944d98f36b120047, entries=1336370, sequenceid=12668, filesize=95.1m
2014-07-22 09:06:34,595 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~713.4m/748091600, currentsize=326.3m/342111040 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 20997ms, sequenceid=12668, compaction requested=true
2014-07-22 09:06:35,523 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 33 store files, 0 compacting, 33 eligible, 2000 blocking
2014-07-22 09:06:35,524 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 33 files from compaction candidates
2014-07-22 09:06:35,524 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:06:35,524 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:06:35,524 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:06:35,524 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:06:35,524 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 724.5m
2014-07-22 09:06:35,547 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 66908 synced till here 66890
2014-07-22 09:06:35,682 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:06:35,729 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045191905 with entries=99, filesize=79.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045194573
2014-07-22 09:06:35,729 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045113500
2014-07-22 09:06:35,729 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045122448
2014-07-22 09:06:35,729 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045123227
2014-07-22 09:06:35,729 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045125750
2014-07-22 09:06:35,729 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045127297
2014-07-22 09:06:35,729 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045129364
2014-07-22 09:06:35,729 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045131128
2014-07-22 09:06:35,729 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045132842
2014-07-22 09:06:35,729 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045134824
2014-07-22 09:06:37,386 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:06:37,474 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:06:37,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 67006 synced till here 66981
2014-07-22 09:06:37,734 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045194573 with entries=98, filesize=87.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045197474
2014-07-22 09:06:38,411 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:06:39,080 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 67085 synced till here 67068
2014-07-22 09:06:39,220 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045197474 with entries=79, filesize=77.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045198412
2014-07-22 09:06:40,156 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:06:40,187 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 67160 synced till here 67150
2014-07-22 09:06:40,885 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045198412 with entries=75, filesize=69.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045200157
2014-07-22 09:06:42,109 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:06:42,126 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 67270 synced till here 67264
2014-07-22 09:06:42,185 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045200157 with entries=110, filesize=72.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045202110
2014-07-22 09:06:43,792 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:06:43,833 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045202110 with entries=83, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045203792
2014-07-22 09:06:45,211 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:06:45,241 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 67441 synced till here 67435
2014-07-22 09:06:45,383 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045203792 with entries=88, filesize=69.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045205212
2014-07-22 09:06:50,227 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1303ms
GC pool 'ParNew' had collection(s): count=1 time=1252ms
2014-07-22 09:06:50,238 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:06:50,356 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 67578 synced till here 67560
2014-07-22 09:06:50,397 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12826, memsize=312.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/0ac0cfc586164331998616aabcbdbdd9
2014-07-22 09:06:50,423 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/0ac0cfc586164331998616aabcbdbdd9 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/0ac0cfc586164331998616aabcbdbdd9
2014-07-22 09:06:50,454 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/0ac0cfc586164331998616aabcbdbdd9, entries=1136080, sequenceid=12826, filesize=80.9m
2014-07-22 09:06:50,471 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~695.8m/729569280, currentsize=337.9m/354295920 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 21609ms, sequenceid=12826, compaction requested=true
2014-07-22 09:06:50,471 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 35 store files, 0 compacting, 35 eligible, 2000 blocking
2014-07-22 09:06:50,472 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 35 files from compaction candidates
2014-07-22 09:06:50,472 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:06:50,472 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:06:50,472 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:06:50,476 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045205212 with entries=137, filesize=88.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045210239
2014-07-22 09:06:50,476 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045136466
2014-07-22 09:06:50,514 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:06:50,514 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 669.9m
2014-07-22 09:06:50,613 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:06:52,490 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:06:52,515 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 67693 synced till here 67661
2014-07-22 09:06:52,807 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:06:53,098 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045210239 with entries=115, filesize=97.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045212490
2014-07-22 09:06:54,673 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:06:54,860 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 67795 synced till here 67772
2014-07-22 09:06:55,134 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045212490 with entries=102, filesize=89.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045214674
2014-07-22 09:06:57,113 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:06:57,179 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 67897 synced till here 67877
2014-07-22 09:06:57,358 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045214674 with entries=102, filesize=87.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045217114
2014-07-22 09:06:59,354 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:06:59,433 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 67998 synced till here 67981
2014-07-22 09:06:59,672 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045217114 with entries=101, filesize=94.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045219355
2014-07-22 09:07:01,177 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12854, memsize=324.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/e3303339a0bc4a2d83295afefb192628
2014-07-22 09:07:01,203 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/e3303339a0bc4a2d83295afefb192628 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/e3303339a0bc4a2d83295afefb192628
2014-07-22 09:07:01,235 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/e3303339a0bc4a2d83295afefb192628, entries=1179780, sequenceid=12854, filesize=84.0m
2014-07-22 09:07:01,238 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~737.4m/773194640, currentsize=399.7m/419153040 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 25714ms, sequenceid=12854, compaction requested=true
2014-07-22 09:07:01,241 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 34 store files, 0 compacting, 34 eligible, 2000 blocking
2014-07-22 09:07:01,241 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 34 files from compaction candidates
2014-07-22 09:07:01,242 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:07:01,242 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:07:01,242 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:07:01,242 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:07:01,242 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 757.1m
2014-07-22 09:07:01,477 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:01,478 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:07:01,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 68084 synced till here 68069
2014-07-22 09:07:01,621 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045219355 with entries=86, filesize=82.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045221477
2014-07-22 09:07:01,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045137489
2014-07-22 09:07:01,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045138849
2014-07-22 09:07:01,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045140408
2014-07-22 09:07:01,622 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045142058
2014-07-22 09:07:01,622 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045143440
2014-07-22 09:07:01,622 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045145558
2014-07-22 09:07:01,622 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045147287
2014-07-22 09:07:01,624 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045148708
2014-07-22 09:07:03,037 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:07:03,610 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:03,874 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 68213 synced till here 68197
2014-07-22 09:07:03,990 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045221477 with entries=129, filesize=102.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045223611
2014-07-22 09:07:05,672 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:06,232 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 68330 synced till here 68325
2014-07-22 09:07:06,885 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045223611 with entries=117, filesize=91.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045225672
2014-07-22 09:07:07,846 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:07,873 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 68427 synced till here 68426
2014-07-22 09:07:07,895 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045225672 with entries=97, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045227847
2014-07-22 09:07:09,514 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:09,532 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 68526 synced till here 68525
2014-07-22 09:07:09,560 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045227847 with entries=99, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045229514
2014-07-22 09:07:11,490 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:11,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 68637 synced till here 68611
2014-07-22 09:07:11,709 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045229514 with entries=111, filesize=84.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045231526
2014-07-22 09:07:13,650 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:13,677 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 68726 synced till here 68720
2014-07-22 09:07:13,760 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045231526 with entries=89, filesize=68.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045233652
2014-07-22 09:07:13,760 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:07:15,078 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12997, memsize=324.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/5608b8f4c935474ca22ee1144ca992de
2014-07-22 09:07:15,101 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/5608b8f4c935474ca22ee1144ca992de as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/5608b8f4c935474ca22ee1144ca992de
2014-07-22 09:07:15,112 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/5608b8f4c935474ca22ee1144ca992de, entries=1180720, sequenceid=12997, filesize=84.1m
2014-07-22 09:07:15,112 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~687.3m/720662240, currentsize=414.3m/434400400 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 24598ms, sequenceid=12997, compaction requested=true
2014-07-22 09:07:15,113 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:07:15,113 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 35 store files, 0 compacting, 35 eligible, 2000 blocking
2014-07-22 09:07:15,113 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 733.0m
2014-07-22 09:07:15,113 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 35 files from compaction candidates
2014-07-22 09:07:15,113 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:07:15,114 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:07:15,114 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:07:15,141 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:07:15,619 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:15,638 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 68808 synced till here 68805
2014-07-22 09:07:15,654 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045233652 with entries=82, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045235619
2014-07-22 09:07:15,656 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:07:15,812 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:07:17,245 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:17,355 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 68903 synced till here 68898
2014-07-22 09:07:17,497 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045235619 with entries=95, filesize=67.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045237246
2014-07-22 09:07:17,497 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:07:19,500 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:19,521 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 68994 synced till here 68993
2014-07-22 09:07:19,550 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045237246 with entries=91, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045239501
2014-07-22 09:07:19,550 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:07:21,281 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:21,801 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 69137 synced till here 69130
2014-07-22 09:07:21,895 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045239501 with entries=143, filesize=109.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045241281
2014-07-22 09:07:21,895 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:07:23,178 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13115, memsize=329.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/b451b6a1b6f14ed9a2603a89f9467101
2014-07-22 09:07:23,194 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/b451b6a1b6f14ed9a2603a89f9467101 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/b451b6a1b6f14ed9a2603a89f9467101
2014-07-22 09:07:23,205 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/b451b6a1b6f14ed9a2603a89f9467101, entries=1200570, sequenceid=13115, filesize=85.5m
2014-07-22 09:07:23,206 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~772.6m/810165280, currentsize=338.9m/355348960 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 21964ms, sequenceid=13115, compaction requested=true
2014-07-22 09:07:23,207 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:07:23,207 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 34 store files, 0 compacting, 34 eligible, 2000 blocking
2014-07-22 09:07:23,207 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 34 files from compaction candidates
2014-07-22 09:07:23,207 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 781.0m
2014-07-22 09:07:23,207 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:07:23,207 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:07:23,207 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:07:23,277 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:07:23,772 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:23,803 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 69235 synced till here 69222
2014-07-22 09:07:23,953 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045241281 with entries=98, filesize=73.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045243773
2014-07-22 09:07:23,954 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:07:24,107 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:07:26,174 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:26,207 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 69337 synced till here 69321
2014-07-22 09:07:26,582 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045243773 with entries=102, filesize=77.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045246175
2014-07-22 09:07:26,583 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:07:28,195 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:28,234 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 69413 synced till here 69408
2014-07-22 09:07:28,298 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045246175 with entries=76, filesize=68.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045248196
2014-07-22 09:07:28,299 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:07:29,929 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:29,988 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 69480 synced till here 69479
2014-07-22 09:07:30,010 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045248196 with entries=67, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045249930
2014-07-22 09:07:30,011 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:07:32,109 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:32,144 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045249930 with entries=90, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045252109
2014-07-22 09:07:32,144 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:07:32,955 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:32,980 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 69651 synced till here 69650
2014-07-22 09:07:32,996 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045252109 with entries=81, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045252955
2014-07-22 09:07:32,996 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:07:33,411 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13246, memsize=294.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/afd9f39e799441608332d5b2c832a06b
2014-07-22 09:07:33,437 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/afd9f39e799441608332d5b2c832a06b as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/afd9f39e799441608332d5b2c832a06b
2014-07-22 09:07:33,461 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/afd9f39e799441608332d5b2c832a06b, entries=1070420, sequenceid=13246, filesize=76.2m
2014-07-22 09:07:33,462 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~742.3m/778365440, currentsize=286.8m/300706960 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 18349ms, sequenceid=13246, compaction requested=true
2014-07-22 09:07:33,464 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:07:33,464 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 36 store files, 0 compacting, 36 eligible, 2000 blocking
2014-07-22 09:07:33,464 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b., current region memstore size 227.8m
2014-07-22 09:07:33,464 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 36 files from compaction candidates
2014-07-22 09:07:33,464 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:07:33,464 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:07:33,465 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:07:34,187 DEBUG [RpcServer.handler=19,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:07:34,390 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:07:35,609 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:36,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 69750 synced till here 69746
2014-07-22 09:07:36,639 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045252955 with entries=99, filesize=72.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045255609
2014-07-22 09:07:37,340 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:38,456 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 69859 synced till here 69829
2014-07-22 09:07:38,558 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045255609 with entries=109, filesize=80.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045257340
2014-07-22 09:07:39,335 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:40,258 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 69976 synced till here 69954
2014-07-22 09:07:40,432 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045257340 with entries=117, filesize=78.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045259336
2014-07-22 09:07:40,842 DEBUG [RpcServer.handler=30,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b.
2014-07-22 09:07:41,320 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:41,451 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 70067 synced till here 70054
2014-07-22 09:07:42,628 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045259336 with entries=91, filesize=77.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045261320
2014-07-22 09:07:43,472 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:43,728 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045261320 with entries=79, filesize=68.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045263473
2014-07-22 09:07:45,078 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16213, memsize=190.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/43ae64bfa7c749a39937f44811b0cceb
2014-07-22 09:07:45,086 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:45,093 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/43ae64bfa7c749a39937f44811b0cceb as hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/43ae64bfa7c749a39937f44811b0cceb
2014-07-22 09:07:45,105 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 70243 synced till here 70228
2014-07-22 09:07:45,109 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/43ae64bfa7c749a39937f44811b0cceb, entries=692290, sequenceid=16213, filesize=49.3m
2014-07-22 09:07:45,110 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~227.8m/238855840, currentsize=40.5m/42441200 for region usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. in 11646ms, sequenceid=16213, compaction requested=true
2014-07-22 09:07:45,110 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:07:45,110 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 2000 blocking
2014-07-22 09:07:45,110 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-22 09:07:45,110 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 896.0m
2014-07-22 09:07:45,110 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:07:45,111 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:07:45,111 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. because compaction request was cancelled
2014-07-22 09:07:45,531 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045263473 with entries=97, filesize=69.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045265086
2014-07-22 09:07:45,531 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045150164
2014-07-22 09:07:45,531 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045164178
2014-07-22 09:07:45,531 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045166034
2014-07-22 09:07:45,531 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045168970
2014-07-22 09:07:45,531 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045171394
2014-07-22 09:07:45,531 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045173466
2014-07-22 09:07:45,531 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045175607
2014-07-22 09:07:45,532 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045177739
2014-07-22 09:07:45,532 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045180579
2014-07-22 09:07:45,532 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045182467
2014-07-22 09:07:45,532 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045184098
2014-07-22 09:07:45,533 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045185949
2014-07-22 09:07:45,533 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045187469
2014-07-22 09:07:45,533 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045189618
2014-07-22 09:07:45,533 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045191905
2014-07-22 09:07:47,332 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:47,350 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 70339 synced till here 70334
2014-07-22 09:07:47,430 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045265086 with entries=96, filesize=69.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045267333
2014-07-22 09:07:47,448 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:07:49,374 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13299, memsize=385.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/e98428a9415640c5a470266ae7c97de0
2014-07-22 09:07:49,375 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:49,415 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/e98428a9415640c5a470266ae7c97de0 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/e98428a9415640c5a470266ae7c97de0
2014-07-22 09:07:49,428 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 70413 synced till here 70412
2014-07-22 09:07:49,444 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/e98428a9415640c5a470266ae7c97de0, entries=1405200, sequenceid=13299, filesize=100.0m
2014-07-22 09:07:49,445 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~786.2m/824417200, currentsize=426.0m/446684560 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 26238ms, sequenceid=13299, compaction requested=true
2014-07-22 09:07:49,446 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:07:49,446 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 35 store files, 0 compacting, 35 eligible, 2000 blocking
2014-07-22 09:07:49,446 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 754.6m
2014-07-22 09:07:49,446 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 35 files from compaction candidates
2014-07-22 09:07:49,446 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:07:49,446 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:07:49,446 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:07:49,452 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045267333 with entries=74, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045269375
2014-07-22 09:07:49,452 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045194573
2014-07-22 09:07:49,452 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045197474
2014-07-22 09:07:49,452 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045198412
2014-07-22 09:07:49,452 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045200157
2014-07-22 09:07:49,452 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045202110
2014-07-22 09:07:49,452 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045203792
2014-07-22 09:07:49,452 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045205212
2014-07-22 09:07:49,766 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:07:51,262 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:07:51,419 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:51,452 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 70494 synced till here 70491
2014-07-22 09:07:51,546 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045269375 with entries=81, filesize=65.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045271420
2014-07-22 09:07:52,921 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:52,947 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 70582 synced till here 70573
2014-07-22 09:07:52,984 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045271420 with entries=88, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045272922
2014-07-22 09:07:54,882 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:55,147 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 70733 synced till here 70728
2014-07-22 09:07:55,187 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045272922 with entries=151, filesize=98.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045274882
2014-07-22 09:07:56,263 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:56,277 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 70818 synced till here 70814
2014-07-22 09:07:56,320 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045274882 with entries=85, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045276263
2014-07-22 09:07:57,795 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:57,841 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 70905 synced till here 70893
2014-07-22 09:07:57,997 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045276263 with entries=87, filesize=73.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045277796
2014-07-22 09:07:59,569 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:07:59,591 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 70990 synced till here 70985
2014-07-22 09:07:59,639 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045277796 with entries=85, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045279569
2014-07-22 09:08:00,365 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:08:01,319 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 71078 synced till here 71059
2014-07-22 09:08:01,454 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045279569 with entries=88, filesize=75.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045280366
2014-07-22 09:08:03,152 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:08:03,289 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 71190 synced till here 71168
2014-07-22 09:08:03,490 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045280366 with entries=112, filesize=88.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045283152
2014-07-22 09:08:05,338 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:08:05,393 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 71276 synced till here 71266
2014-07-22 09:08:05,557 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045283152 with entries=86, filesize=70.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045285338
2014-07-22 09:08:07,846 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:08:07,854 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:07,855 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:07,855 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:07,855 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:07,856 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:07,860 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:07,861 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:07,862 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:07,862 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:07,865 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:07,866 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:07,868 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:07,869 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:07,871 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:07,872 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:07,872 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:07,873 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:07,874 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:07,874 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:07,874 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:07,875 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:07,876 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:07,876 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:07,878 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:07,937 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:07,994 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:08,028 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:08,029 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:08,030 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:08,032 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:08,032 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:08,033 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:08,033 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:08,034 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:08,034 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 71391 synced till here 71389
2014-07-22 09:08:08,038 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:08,049 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:08,049 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:08,049 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:08,057 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045285338 with entries=115, filesize=96.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045287847
2014-07-22 09:08:08,087 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:08,089 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:08,144 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:08,146 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:08,199 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:08,230 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:08,265 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:08,310 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:08,354 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:08,409 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:08,432 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:08,481 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:11,146 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1179ms
GC pool 'ParNew' had collection(s): count=1 time=1202ms
2014-07-22 09:08:11,146 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=6603, hits=3, hitRatio=0.04%, , cachingAccesses=5, cachingHits=3, cachingHitsRatio=60.00%, evictions=0, evicted=0, evictedPerRun=NaN
2014-07-22 09:08:12,855 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:12,856 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:12,856 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:12,856 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:12,857 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:12,860 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:12,861 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:12,862 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:12,863 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:12,866 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:12,900 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5022ms
2014-07-22 09:08:12,900 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5024ms
2014-07-22 09:08:12,901 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5025ms
2014-07-22 09:08:12,901 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5026ms
2014-07-22 09:08:12,902 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5027ms
2014-07-22 09:08:12,902 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5028ms
2014-07-22 09:08:12,902 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5028ms
2014-07-22 09:08:12,902 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5029ms
2014-07-22 09:08:12,903 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5031ms
2014-07-22 09:08:12,903 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5031ms
2014-07-22 09:08:12,903 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5032ms
2014-07-22 09:08:12,903 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5034ms
2014-07-22 09:08:12,904 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5036ms
2014-07-22 09:08:12,904 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5038ms
2014-07-22 09:08:12,938 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:12,995 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:13,029 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:13,030 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:13,031 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:13,032 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:13,033 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:13,033 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:13,034 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:13,034 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:13,038 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:13,049 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:13,050 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:13,050 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:13,087 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:13,090 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:13,144 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:13,147 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:13,200 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:13,231 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:13,266 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:13,310 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:13,354 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:13,409 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:13,432 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:13,482 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:15,154 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13510, memsize=426.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/4c324b97be3641a9bfec1444dc4903b9
2014-07-22 09:08:15,173 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/4c324b97be3641a9bfec1444dc4903b9 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/4c324b97be3641a9bfec1444dc4903b9
2014-07-22 09:08:15,183 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/4c324b97be3641a9bfec1444dc4903b9, entries=1552310, sequenceid=13510, filesize=110.6m
2014-07-22 09:08:15,184 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~902.8m/946688720, currentsize=384.4m/403066800 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 30074ms, sequenceid=13510, compaction requested=true
2014-07-22 09:08:15,185 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:08:15,185 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 36 store files, 0 compacting, 36 eligible, 2000 blocking
2014-07-22 09:08:15,185 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6704ms
2014-07-22 09:08:15,186 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,186 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 36 files from compaction candidates
2014-07-22 09:08:15,186 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6754ms
2014-07-22 09:08:15,186 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,186 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 899.4m
2014-07-22 09:08:15,189 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6781ms
2014-07-22 09:08:15,186 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:08:15,190 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,190 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:08:15,190 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6836ms
2014-07-22 09:08:15,190 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:08:15,190 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,191 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6881ms
2014-07-22 09:08:15,191 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,191 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6926ms
2014-07-22 09:08:15,191 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,191 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6961ms
2014-07-22 09:08:15,191 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,191 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6992ms
2014-07-22 09:08:15,192 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,192 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7046ms
2014-07-22 09:08:15,192 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,196 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7052ms
2014-07-22 09:08:15,196 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,197 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7108ms
2014-07-22 09:08:15,197 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,197 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7110ms
2014-07-22 09:08:15,197 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,197 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7148ms
2014-07-22 09:08:15,197 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,198 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7148ms
2014-07-22 09:08:15,198 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,200 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7151ms
2014-07-22 09:08:15,201 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,201 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7164ms
2014-07-22 09:08:15,201 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,203 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7170ms
2014-07-22 09:08:15,203 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,203 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7170ms
2014-07-22 09:08:15,203 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,206 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7173ms
2014-07-22 09:08:15,206 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,207 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7175ms
2014-07-22 09:08:15,207 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,207 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7175ms
2014-07-22 09:08:15,207 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,221 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7191ms
2014-07-22 09:08:15,221 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,221 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7192ms
2014-07-22 09:08:15,222 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,222 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7194ms
2014-07-22 09:08:15,222 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,222 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7228ms
2014-07-22 09:08:15,222 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,222 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7285ms
2014-07-22 09:08:15,222 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,222 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7356ms
2014-07-22 09:08:15,222 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,222 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7354ms
2014-07-22 09:08:15,223 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,223 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7354ms
2014-07-22 09:08:15,223 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,223 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7352ms
2014-07-22 09:08:15,223 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,228 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7356ms
2014-07-22 09:08:15,228 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,231 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7359ms
2014-07-22 09:08:15,231 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,231 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7358ms
2014-07-22 09:08:15,231 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,232 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7358ms
2014-07-22 09:08:15,232 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,232 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7358ms
2014-07-22 09:08:15,232 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,232 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7358ms
2014-07-22 09:08:15,232 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,232 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7357ms
2014-07-22 09:08:15,232 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,233 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7356ms
2014-07-22 09:08:15,233 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,233 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7357ms
2014-07-22 09:08:15,233 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,241 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7363ms
2014-07-22 09:08:15,241 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,244 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7379ms
2014-07-22 09:08:15,244 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,244 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7382ms
2014-07-22 09:08:15,244 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,244 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7382ms
2014-07-22 09:08:15,245 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,252 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7391ms
2014-07-22 09:08:15,252 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,252 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7392ms
2014-07-22 09:08:15,252 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,261 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7405ms
2014-07-22 09:08:15,261 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,269 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7414ms
2014-07-22 09:08:15,269 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,277 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7422ms
2014-07-22 09:08:15,277 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,277 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7422ms
2014-07-22 09:08:15,278 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,278 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7424ms
2014-07-22 09:08:15,278 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:15,300 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11476,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045283823,"queuetimems":0,"class":"HRegionServer","responsesize":17289,"method":"Multi"}
2014-07-22 09:08:16,624 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:08:16,819 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12637,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045284181,"queuetimems":0,"class":"HRegionServer","responsesize":18915,"method":"Multi"}
2014-07-22 09:08:16,819 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11186,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045285632,"queuetimems":0,"class":"HRegionServer","responsesize":18651,"method":"Multi"}
2014-07-22 09:08:16,819 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11534,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045285284,"queuetimems":0,"class":"HRegionServer","responsesize":19049,"method":"Multi"}
2014-07-22 09:08:16,819 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11282,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045285536,"queuetimems":0,"class":"HRegionServer","responsesize":15692,"method":"Multi"}
2014-07-22 09:08:16,826 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11250,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045285576,"queuetimems":0,"class":"HRegionServer","responsesize":11212,"method":"Multi"}
2014-07-22 09:08:16,826 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11329,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045285497,"queuetimems":0,"class":"HRegionServer","responsesize":15869,"method":"Multi"}
2014-07-22 09:08:16,830 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11398,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045285431,"queuetimems":0,"class":"HRegionServer","responsesize":19020,"method":"Multi"}
2014-07-22 09:08:16,831 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11161,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045285667,"queuetimems":0,"class":"HRegionServer","responsesize":15916,"method":"Multi"}
2014-07-22 09:08:17,159 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11331,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045285827,"queuetimems":0,"class":"HRegionServer","responsesize":13478,"method":"Multi"}
2014-07-22 09:08:17,159 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11220,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045285939,"queuetimems":0,"class":"HRegionServer","responsesize":18918,"method":"Multi"}
2014-07-22 09:08:17,160 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11175,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045285984,"queuetimems":1,"class":"HRegionServer","responsesize":18452,"method":"Multi"}
2014-07-22 09:08:17,160 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11454,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045285706,"queuetimems":0,"class":"HRegionServer","responsesize":18573,"method":"Multi"}
2014-07-22 09:08:17,161 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11302,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045285858,"queuetimems":0,"class":"HRegionServer","responsesize":12961,"method":"Multi"}
2014-07-22 09:08:17,174 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11298,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045285875,"queuetimems":0,"class":"HRegionServer","responsesize":11028,"method":"Multi"}
2014-07-22 09:08:17,185 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11389,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045285795,"queuetimems":0,"class":"HRegionServer","responsesize":13254,"method":"Multi"}
2014-07-22 09:08:17,457 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:08:17,459 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12068,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045285391,"queuetimems":0,"class":"HRegionServer","responsesize":18525,"method":"Multi"}
2014-07-22 09:08:17,522 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 71506 synced till here 71480
2014-07-22 09:08:17,729 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:08:17,774 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045287847 with entries=115, filesize=96.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045297458
2014-07-22 09:08:17,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045210239
2014-07-22 09:08:17,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045212490
2014-07-22 09:08:17,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045214674
2014-07-22 09:08:17,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045217114
2014-07-22 09:08:19,449 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13377,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045286072,"queuetimems":0,"class":"HRegionServer","responsesize":18501,"method":"Multi"}
2014-07-22 09:08:19,449 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11762,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045287687,"queuetimems":1,"class":"HRegionServer","responsesize":18681,"method":"Multi"}
2014-07-22 09:08:19,449 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11020,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045288429,"queuetimems":0,"class":"HRegionServer","responsesize":11212,"method":"Multi"}
2014-07-22 09:08:19,449 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11881,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045287568,"queuetimems":0,"class":"HRegionServer","responsesize":18215,"method":"Multi"}
2014-07-22 09:08:19,450 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13299,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045286150,"queuetimems":0,"class":"HRegionServer","responsesize":18720,"method":"Multi"}
2014-07-22 09:08:19,458 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11933,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045287524,"queuetimems":0,"class":"HRegionServer","responsesize":18495,"method":"Multi"}
2014-07-22 09:08:19,459 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11983,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045287476,"queuetimems":0,"class":"HRegionServer","responsesize":18599,"method":"Multi"}
2014-07-22 09:08:19,460 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13431,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045286028,"queuetimems":1,"class":"HRegionServer","responsesize":18660,"method":"Multi"}
2014-07-22 09:08:19,466 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11959,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045287506,"queuetimems":0,"class":"HRegionServer","responsesize":18641,"method":"Multi"}
2014-07-22 09:08:19,474 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13362,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045286111,"queuetimems":0,"class":"HRegionServer","responsesize":18568,"method":"Multi"}
2014-07-22 09:08:19,474 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13276,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045286197,"queuetimems":0,"class":"HRegionServer","responsesize":18915,"method":"Multi"}
2014-07-22 09:08:19,985 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:08:19,987 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13752,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045286235,"queuetimems":0,"class":"HRegionServer","responsesize":17289,"method":"Multi"}
2014-07-22 09:08:19,987 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12368,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045287619,"queuetimems":0,"class":"HRegionServer","responsesize":16969,"method":"Multi"}
2014-07-22 09:08:19,987 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13714,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045286273,"queuetimems":0,"class":"HRegionServer","responsesize":16898,"method":"Multi"}
2014-07-22 09:08:20,002 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11526,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045288476,"queuetimems":0,"class":"HRegionServer","responsesize":18573,"method":"Multi"}
2014-07-22 09:08:20,171 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11823,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045288347,"queuetimems":0,"class":"HRegionServer","responsesize":15916,"method":"Multi"}
2014-07-22 09:08:20,171 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11768,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045288403,"queuetimems":0,"class":"HRegionServer","responsesize":19020,"method":"Multi"}
2014-07-22 09:08:20,172 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12185,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045287986,"queuetimems":1,"class":"HRegionServer","responsesize":18625,"method":"Multi"}
2014-07-22 09:08:20,906 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12643,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045288262,"queuetimems":0,"class":"HRegionServer","responsesize":15692,"method":"Multi"}
2014-07-22 09:08:20,921 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12725,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045288196,"queuetimems":1,"class":"HRegionServer","responsesize":18452,"method":"Multi"}
2014-07-22 09:08:20,922 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12696,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045288225,"queuetimems":0,"class":"HRegionServer","responsesize":12961,"method":"Multi"}
2014-07-22 09:08:20,922 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 71628 synced till here 71605
2014-07-22 09:08:20,931 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12631,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045288300,"queuetimems":1,"class":"HRegionServer","responsesize":15692,"method":"Multi"}
2014-07-22 09:08:20,938 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12795,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045288142,"queuetimems":1,"class":"HRegionServer","responsesize":19049,"method":"Multi"}
2014-07-22 09:08:20,947 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12901,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045288045,"queuetimems":0,"class":"HRegionServer","responsesize":18918,"method":"Multi"}
2014-07-22 09:08:20,970 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13543, memsize=451.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/b038b92482fc4d4695ef23a3e57129eb
2014-07-22 09:08:20,992 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/b038b92482fc4d4695ef23a3e57129eb as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/b038b92482fc4d4695ef23a3e57129eb
2014-07-22 09:08:21,004 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/b038b92482fc4d4695ef23a3e57129eb, entries=1642290, sequenceid=13543, filesize=117.0m
2014-07-22 09:08:21,005 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~760.3m/797217280, currentsize=420.7m/441117680 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 31559ms, sequenceid=13543, compaction requested=true
2014-07-22 09:08:21,005 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 35 store files, 0 compacting, 35 eligible, 2000 blocking
2014-07-22 09:08:21,006 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 35 files from compaction candidates
2014-07-22 09:08:21,006 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:08:21,006 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:08:21,006 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:08:21,007 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:08:21,007 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b., current region memstore size 99.5m
2014-07-22 09:08:21,082 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12996,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045288086,"queuetimems":0,"class":"HRegionServer","responsesize":18660,"method":"Multi"}
2014-07-22 09:08:21,082 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13207,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045287875,"queuetimems":3,"class":"HRegionServer","responsesize":18651,"method":"Multi"}
2014-07-22 09:08:21,090 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:08:21,150 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045297458 with entries=122, filesize=106.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045299985
2014-07-22 09:08:21,150 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045219355
2014-07-22 09:08:21,150 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045221477
2014-07-22 09:08:21,150 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045223611
2014-07-22 09:08:21,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045225672
2014-07-22 09:08:21,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045227847
2014-07-22 09:08:21,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045229514
2014-07-22 09:08:21,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045231526
2014-07-22 09:08:21,188 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13266,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045287922,"queuetimems":1,"class":"HRegionServer","responsesize":18525,"method":"Multi"}
2014-07-22 09:08:21,553 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:08:21,963 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:08:21,988 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 71735 synced till here 71707
2014-07-22 09:08:22,746 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045299985 with entries=107, filesize=86.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045301963
2014-07-22 09:08:23,725 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:08:23,795 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 71839 synced till here 71809
2014-07-22 09:08:23,947 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16632, memsize=51.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/457df7f58b37415ea0af0ef522a441cc
2014-07-22 09:08:23,960 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/457df7f58b37415ea0af0ef522a441cc as hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/457df7f58b37415ea0af0ef522a441cc
2014-07-22 09:08:23,970 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/457df7f58b37415ea0af0ef522a441cc, entries=188300, sequenceid=16632, filesize=13.4m
2014-07-22 09:08:23,971 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~100.6m/105488160, currentsize=2.9m/3084240 for region usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. in 2964ms, sequenceid=16632, compaction requested=true
2014-07-22 09:08:23,971 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:08:23,972 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 2000 blocking
2014-07-22 09:08:23,972 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 885.7m
2014-07-22 09:08:23,972 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-22 09:08:23,972 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:08:23,972 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:08:23,972 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. because compaction request was cancelled
2014-07-22 09:08:24,194 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045301963 with entries=104, filesize=89.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045303726
2014-07-22 09:08:25,393 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:08:25,728 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:08:26,769 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 71999 synced till here 71991
2014-07-22 09:08:26,818 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045303726 with entries=160, filesize=124.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045305394
2014-07-22 09:08:27,624 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:08:27,669 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045305394 with entries=91, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045307624
2014-07-22 09:08:29,426 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:08:29,778 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 72204 synced till here 72201
2014-07-22 09:08:29,818 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045307624 with entries=114, filesize=81.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045309427
2014-07-22 09:08:31,722 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:08:31,769 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 72294 synced till here 72283
2014-07-22 09:08:31,825 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045309427 with entries=90, filesize=67.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045311723
2014-07-22 09:08:33,206 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:08:33,224 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 72363 synced till here 72360
2014-07-22 09:08:33,255 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045311723 with entries=69, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045313206
2014-07-22 09:08:34,943 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:08:34,980 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 72464 synced till here 72457
2014-07-22 09:08:35,061 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045313206 with entries=101, filesize=70.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045314943
2014-07-22 09:08:36,891 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:08:36,930 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 72568 synced till here 72565
2014-07-22 09:08:36,970 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045314943 with entries=104, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045316892
2014-07-22 09:08:37,402 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,408 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,408 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,409 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,410 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,420 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,420 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,495 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,503 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,542 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,544 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,591 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,633 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,685 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,722 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,725 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,754 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,755 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,755 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,755 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,756 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,758 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,758 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,799 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,833 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,836 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,836 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,838 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,840 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:37,840 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:40,423 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:40,425 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:40,426 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:40,427 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:40,427 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:40,427 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:40,427 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:40,428 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:40,442 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:40,482 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:40,520 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:40,558 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:40,596 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:40,641 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:40,690 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:40,739 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:40,783 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:40,832 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:40,890 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:40,939 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:08:42,402 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:42,409 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:42,410 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:42,410 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:42,411 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:42,420 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:42,420 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:42,495 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:42,503 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:42,543 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:42,545 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:42,591 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:42,634 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:42,685 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:42,723 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:42,726 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:42,755 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:42,755 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:42,756 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:42,756 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:42,756 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:42,758 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:42,758 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:42,800 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:42,834 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:42,836 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:42,836 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:42,839 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:08:42,840 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:42,840 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:08:43,407 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13761, memsize=399.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/9662b42bde33474b9920d30b960c0e32
2014-07-22 09:08:43,427 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/9662b42bde33474b9920d30b960c0e32 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/9662b42bde33474b9920d30b960c0e32
2014-07-22 09:08:43,455 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/9662b42bde33474b9920d30b960c0e32, entries=1452830, sequenceid=13761, filesize=103.5m
2014-07-22 09:08:43,455 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~902.3m/946149440, currentsize=386.4m/405174720 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 28269ms, sequenceid=13761, compaction requested=true
2014-07-22 09:08:43,456 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:08:43,456 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 37 store files, 0 compacting, 37 eligible, 2000 blocking
2014-07-22 09:08:43,456 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5616ms
2014-07-22 09:08:43,456 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 37 files from compaction candidates
2014-07-22 09:08:43,456 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,456 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:08:43,457 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5617ms
2014-07-22 09:08:43,457 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,456 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 808.7m
2014-07-22 09:08:43,457 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5619ms
2014-07-22 09:08:43,457 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,457 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5621ms
2014-07-22 09:08:43,457 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,461 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5625ms
2014-07-22 09:08:43,461 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,461 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5628ms
2014-07-22 09:08:43,462 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,462 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5663ms
2014-07-22 09:08:43,462 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,462 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5704ms
2014-07-22 09:08:43,462 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,457 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:08:43,465 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:08:43,468 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5710ms
2014-07-22 09:08:43,468 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,469 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5712ms
2014-07-22 09:08:43,469 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,469 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5714ms
2014-07-22 09:08:43,469 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,469 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5714ms
2014-07-22 09:08:43,469 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,470 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5715ms
2014-07-22 09:08:43,470 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,472 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5717ms
2014-07-22 09:08:43,472 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,472 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5747ms
2014-07-22 09:08:43,472 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,472 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5750ms
2014-07-22 09:08:43,472 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,473 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5788ms
2014-07-22 09:08:43,474 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,477 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5843ms
2014-07-22 09:08:43,477 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,478 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5886ms
2014-07-22 09:08:43,478 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,478 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5934ms
2014-07-22 09:08:43,478 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,485 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5943ms
2014-07-22 09:08:43,485 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,486 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5983ms
2014-07-22 09:08:43,487 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,488 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5992ms
2014-07-22 09:08:43,488 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,488 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6069ms
2014-07-22 09:08:43,488 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,489 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6069ms
2014-07-22 09:08:43,490 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,490 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6080ms
2014-07-22 09:08:43,490 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,491 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6081ms
2014-07-22 09:08:43,491 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,493 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6085ms
2014-07-22 09:08:43,493 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,494 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6086ms
2014-07-22 09:08:43,494 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,495 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6092ms
2014-07-22 09:08:43,495 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,496 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2556ms
2014-07-22 09:08:43,496 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,497 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2606ms
2014-07-22 09:08:43,497 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,502 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2669ms
2014-07-22 09:08:43,502 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,504 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2721ms
2014-07-22 09:08:43,504 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,509 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2770ms
2014-07-22 09:08:43,510 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,511 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2820ms
2014-07-22 09:08:43,511 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,511 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2870ms
2014-07-22 09:08:43,511 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,517 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2921ms
2014-07-22 09:08:43,517 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,519 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2960ms
2014-07-22 09:08:43,519 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,519 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2999ms
2014-07-22 09:08:43,519 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,519 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3037ms
2014-07-22 09:08:43,519 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,520 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3078ms
2014-07-22 09:08:43,520 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,520 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3092ms
2014-07-22 09:08:43,520 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,525 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3098ms
2014-07-22 09:08:43,525 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,527 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3099ms
2014-07-22 09:08:43,527 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,528 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3101ms
2014-07-22 09:08:43,528 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,528 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3102ms
2014-07-22 09:08:43,528 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,528 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3102ms
2014-07-22 09:08:43,529 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,530 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3105ms
2014-07-22 09:08:43,530 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,532 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3109ms
2014-07-22 09:08:43,532 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:08:43,668 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:08:45,143 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:08:45,253 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 72706 synced till here 72688
2014-07-22 09:08:45,480 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:08:45,483 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045316892 with entries=138, filesize=94.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045325143
2014-07-22 09:08:45,484 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045233652
2014-07-22 09:08:45,484 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045235619
2014-07-22 09:08:45,484 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045237246
2014-07-22 09:08:45,484 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045239501
2014-07-22 09:08:47,055 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13826, memsize=296.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/cad819d4a53b425abf1db33f83093d20
2014-07-22 09:08:47,069 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/cad819d4a53b425abf1db33f83093d20 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/cad819d4a53b425abf1db33f83093d20
2014-07-22 09:08:47,081 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/cad819d4a53b425abf1db33f83093d20, entries=1081000, sequenceid=13826, filesize=77.0m
2014-07-22 09:08:47,081 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~911.2m/955507600, currentsize=274.4m/287698240 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 23109ms, sequenceid=13826, compaction requested=true
2014-07-22 09:08:47,082 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:08:47,082 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 36 store files, 0 compacting, 36 eligible, 2000 blocking
2014-07-22 09:08:47,082 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 816.6m
2014-07-22 09:08:47,082 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 36 files from compaction candidates
2014-07-22 09:08:47,082 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:08:47,082 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:08:47,082 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:08:47,280 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:08:47,282 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:08:47,308 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 72819 synced till here 72782
2014-07-22 09:08:47,603 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045325143 with entries=113, filesize=101.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045327280
2014-07-22 09:08:47,603 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045241281
2014-07-22 09:08:47,603 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045243773
2014-07-22 09:08:47,603 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045246175
2014-07-22 09:08:47,603 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045248196
2014-07-22 09:08:47,603 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045249930
2014-07-22 09:08:47,603 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045252109
2014-07-22 09:08:47,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045252955
2014-07-22 09:08:47,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045255609
2014-07-22 09:08:47,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045257340
2014-07-22 09:08:47,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045259336
2014-07-22 09:08:47,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045261320
2014-07-22 09:08:47,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045263473
2014-07-22 09:08:49,023 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11624,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045317398,"queuetimems":0,"class":"HRegionServer","responsesize":18833,"method":"Multi"}
2014-07-22 09:08:49,037 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:08:49,223 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:08:49,288 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045327280 with entries=86, filesize=77.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045329224
2014-07-22 09:08:50,128 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:08:50,858 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045329224 with entries=72, filesize=69.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045330130
2014-07-22 09:08:51,666 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:08:51,821 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73071 synced till here 73067
2014-07-22 09:08:52,390 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045330130 with entries=94, filesize=69.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045331666
2014-07-22 09:08:53,198 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:08:53,219 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73178 synced till here 73171
2014-07-22 09:08:53,292 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045331666 with entries=107, filesize=67.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045333199
2014-07-22 09:08:54,745 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:08:54,787 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73287 synced till here 73276
2014-07-22 09:08:55,326 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045333199 with entries=109, filesize=70.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045334746
2014-07-22 09:08:57,024 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:08:57,095 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73403 synced till here 73400
2014-07-22 09:08:57,161 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045334746 with entries=116, filesize=78.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045337024
2014-07-22 09:08:58,622 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:08:58,662 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73475 synced till here 73467
2014-07-22 09:08:59,336 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045337024 with entries=72, filesize=69.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045338623
2014-07-22 09:09:00,742 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:09:00,771 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73550 synced till here 73549
2014-07-22 09:09:00,794 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045338623 with entries=75, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045340742
2014-07-22 09:09:02,224 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:09:02,246 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73642 synced till here 73634
2014-07-22 09:09:02,477 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045340742 with entries=92, filesize=67.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045342224
2014-07-22 09:09:02,687 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13980, memsize=246.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/d2a783f0cd534c02a89925a2ee2df26f
2014-07-22 09:09:02,715 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/d2a783f0cd534c02a89925a2ee2df26f as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/d2a783f0cd534c02a89925a2ee2df26f
2014-07-22 09:09:02,736 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/d2a783f0cd534c02a89925a2ee2df26f, entries=898530, sequenceid=13980, filesize=64.0m
2014-07-22 09:09:02,736 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~808.7m/847996080, currentsize=354.7m/371948720 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 19280ms, sequenceid=13980, compaction requested=true
2014-07-22 09:09:02,737 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:09:02,737 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 37 store files, 0 compacting, 37 eligible, 2000 blocking
2014-07-22 09:09:02,737 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 751.7m
2014-07-22 09:09:02,738 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 37 files from compaction candidates
2014-07-22 09:09:02,738 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:09:02,738 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:09:02,738 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:09:02,924 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:09:03,899 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:09:03,924 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:09:03,930 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73718 synced till here 73712
2014-07-22 09:09:03,997 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045342224 with entries=76, filesize=68.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045343900
2014-07-22 09:09:03,999 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045265086
2014-07-22 09:09:03,999 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045267333
2014-07-22 09:09:05,012 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:09:05,512 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14027, memsize=218.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/4e48e84ca3044b0cbc45a66ef36cddbc
2014-07-22 09:09:05,513 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73796 synced till here 73784
2014-07-22 09:09:05,566 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/4e48e84ca3044b0cbc45a66ef36cddbc as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/4e48e84ca3044b0cbc45a66ef36cddbc
2014-07-22 09:09:05,578 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/4e48e84ca3044b0cbc45a66ef36cddbc, entries=794730, sequenceid=14027, filesize=56.6m
2014-07-22 09:09:05,578 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~832.9m/873386080, currentsize=322.0m/337631200 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 18496ms, sequenceid=14027, compaction requested=true
2014-07-22 09:09:05,579 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:09:05,579 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 36 store files, 0 compacting, 36 eligible, 2000 blocking
2014-07-22 09:09:05,579 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 36 files from compaction candidates
2014-07-22 09:09:05,579 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 669.8m
2014-07-22 09:09:05,579 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:09:05,579 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:09:05,579 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:09:05,580 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:09:05,594 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045343900 with entries=78, filesize=69.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045345012
2014-07-22 09:09:05,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045269375
2014-07-22 09:09:05,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045271420
2014-07-22 09:09:05,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045272922
2014-07-22 09:09:05,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045274882
2014-07-22 09:09:05,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045276263
2014-07-22 09:09:05,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045277796
2014-07-22 09:09:05,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045279569
2014-07-22 09:09:05,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045280366
2014-07-22 09:09:05,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045283152
2014-07-22 09:09:05,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045285338
2014-07-22 09:09:06,170 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:09:06,997 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:09:07,014 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73870 synced till here 73868
2014-07-22 09:09:07,283 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045345012 with entries=74, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045346997
2014-07-22 09:09:08,829 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:09:08,871 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73949 synced till here 73944
2014-07-22 09:09:08,978 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045346997 with entries=79, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045348830
2014-07-22 09:09:10,320 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:09:10,337 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 74037 synced till here 74030
2014-07-22 09:09:10,422 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045348830 with entries=88, filesize=71.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045350320
2014-07-22 09:09:11,756 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:09:11,783 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 74128 synced till here 74119
2014-07-22 09:09:11,929 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045350320 with entries=91, filesize=69.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045351756
2014-07-22 09:09:14,423 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:09:14,440 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 74209 synced till here 74205
2014-07-22 09:09:14,517 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045351756 with entries=81, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045354423
2014-07-22 09:09:16,178 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:09:16,553 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 74332 synced till here 74321
2014-07-22 09:09:16,672 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045354423 with entries=123, filesize=94.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045356179
2014-07-22 09:09:19,955 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1178ms
GC pool 'ParNew' had collection(s): count=1 time=1417ms
2014-07-22 09:09:20,255 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:09:20,337 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 74463 synced till here 74429
2014-07-22 09:09:20,684 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045356179 with entries=131, filesize=92.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045360255
2014-07-22 09:09:20,829 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14199, memsize=302.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/9e17db2e2a2f45f298f766977c56b899
2014-07-22 09:09:20,842 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/9e17db2e2a2f45f298f766977c56b899 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/9e17db2e2a2f45f298f766977c56b899
2014-07-22 09:09:20,852 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/9e17db2e2a2f45f298f766977c56b899, entries=1101160, sequenceid=14199, filesize=78.4m
2014-07-22 09:09:20,853 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~751.7m/788191680, currentsize=292.9m/307157360 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 18116ms, sequenceid=14199, compaction requested=true
2014-07-22 09:09:20,854 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:09:20,854 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 38 store files, 0 compacting, 38 eligible, 2000 blocking
2014-07-22 09:09:20,854 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 626.5m
2014-07-22 09:09:20,854 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 38 files from compaction candidates
2014-07-22 09:09:20,854 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:09:20,855 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:09:20,855 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:09:20,905 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:09:22,689 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:09:22,709 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 74581 synced till here 74547
2014-07-22 09:09:23,003 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:09:23,258 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045360255 with entries=118, filesize=92.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045362689
2014-07-22 09:09:23,258 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045287847
2014-07-22 09:09:23,258 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045297458
2014-07-22 09:09:24,759 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14210, memsize=326.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/66b45685ed294516bbc95eac208fef9b
2014-07-22 09:09:24,772 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/66b45685ed294516bbc95eac208fef9b as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/66b45685ed294516bbc95eac208fef9b
2014-07-22 09:09:24,783 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/66b45685ed294516bbc95eac208fef9b, entries=1189530, sequenceid=14210, filesize=84.7m
2014-07-22 09:09:24,783 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~671.6m/704260640, currentsize=235.4m/246859920 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 19204ms, sequenceid=14210, compaction requested=true
2014-07-22 09:09:24,784 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:09:24,784 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 37 store files, 0 compacting, 37 eligible, 2000 blocking
2014-07-22 09:09:24,784 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 37 files from compaction candidates
2014-07-22 09:09:24,784 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 596.7m
2014-07-22 09:09:24,784 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:09:24,784 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:09:24,784 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:09:24,962 DEBUG [RpcServer.handler=6,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:09:25,479 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:09:25,555 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:09:25,580 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 74689 synced till here 74663
2014-07-22 09:09:26,995 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1108ms
GC pool 'ParNew' had collection(s): count=1 time=1135ms
2014-07-22 09:09:27,043 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045362689 with entries=108, filesize=83.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045365480
2014-07-22 09:09:29,412 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:09:29,545 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 74826 synced till here 74801
2014-07-22 09:09:30,003 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045365480 with entries=137, filesize=101.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045369413
2014-07-22 09:09:32,658 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:09:32,762 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 74958 synced till here 74931
2014-07-22 09:09:34,233 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1176ms
GC pool 'ParNew' had collection(s): count=1 time=1359ms
2014-07-22 09:09:34,332 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045369413 with entries=132, filesize=102.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045372659
2014-07-22 09:09:34,333 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:09:36,535 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:09:36,596 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 75069 synced till here 75051
2014-07-22 09:09:36,798 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045372659 with entries=111, filesize=84.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045376535
2014-07-22 09:09:36,798 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:09:37,724 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:09:38,698 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 75180 synced till here 75143
2014-07-22 09:09:38,992 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045376535 with entries=111, filesize=87.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045377725
2014-07-22 09:09:38,992 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:09:39,828 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:09:40,400 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 75302 synced till here 75279
2014-07-22 09:09:40,674 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045377725 with entries=122, filesize=91.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045379828
2014-07-22 09:09:40,675 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:09:43,103 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:09:43,124 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 75422 synced till here 75392
2014-07-22 09:09:43,611 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045379828 with entries=120, filesize=92.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045383103
2014-07-22 09:09:43,612 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:09:45,403 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:09:45,514 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 75525 synced till here 75496
2014-07-22 09:09:45,765 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045383103 with entries=103, filesize=82.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045385404
2014-07-22 09:09:45,765 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:09:46,591 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:09:46,660 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 75635 synced till here 75619
2014-07-22 09:09:46,880 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045385404 with entries=110, filesize=79.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045386591
2014-07-22 09:09:46,881 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:09:48,220 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:09:48,244 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 75718 synced till here 75712
2014-07-22 09:09:48,318 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045386591 with entries=83, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045388221
2014-07-22 09:09:48,318 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:09:49,803 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14344, memsize=375.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/4b313a1dd00549e58f65007d7956ec9a
2014-07-22 09:09:49,820 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/4b313a1dd00549e58f65007d7956ec9a as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/4b313a1dd00549e58f65007d7956ec9a
2014-07-22 09:09:49,842 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/4b313a1dd00549e58f65007d7956ec9a, entries=1367000, sequenceid=14344, filesize=97.3m
2014-07-22 09:09:49,843 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~640.0m/671073680, currentsize=421.3m/441771360 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 28989ms, sequenceid=14344, compaction requested=true
2014-07-22 09:09:49,843 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:09:49,843 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 38 store files, 0 compacting, 38 eligible, 2000 blocking
2014-07-22 09:09:49,843 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 38 files from compaction candidates
2014-07-22 09:09:49,843 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 690.9m
2014-07-22 09:09:49,843 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:09:49,843 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:09:49,843 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:09:49,872 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:09:50,370 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:09:51,641 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14382, memsize=394.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/1f05e3e72abc4c0586b5c37bc3c1bf47
2014-07-22 09:09:51,660 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/1f05e3e72abc4c0586b5c37bc3c1bf47 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/1f05e3e72abc4c0586b5c37bc3c1bf47
2014-07-22 09:09:51,670 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/1f05e3e72abc4c0586b5c37bc3c1bf47, entries=1436210, sequenceid=14382, filesize=102.2m
2014-07-22 09:09:51,670 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~615.0m/644893200, currentsize=370.9m/388912160 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 26886ms, sequenceid=14382, compaction requested=true
2014-07-22 09:09:51,671 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:09:51,671 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 37 store files, 0 compacting, 37 eligible, 2000 blocking
2014-07-22 09:09:51,671 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 37 files from compaction candidates
2014-07-22 09:09:51,671 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 665.4m
2014-07-22 09:09:51,671 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:09:51,671 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:09:51,671 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:09:52,001 DEBUG [RpcServer.handler=39,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:09:52,179 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:09:52,575 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:09:52,638 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045388221 with entries=101, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045392575
2014-07-22 09:09:52,639 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:09:54,753 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:09:54,958 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 75935 synced till here 75931
2014-07-22 09:09:54,979 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045392575 with entries=116, filesize=79.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045394753
2014-07-22 09:09:54,980 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:09:56,287 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b.
2014-07-22 09:09:56,389 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:09:56,451 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 76033 synced till here 76031
2014-07-22 09:09:56,509 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045394753 with entries=98, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045396389
2014-07-22 09:09:56,510 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:09:58,078 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:09:58,109 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 76122 synced till here 76115
2014-07-22 09:09:58,179 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045396389 with entries=89, filesize=69.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045398079
2014-07-22 09:09:58,180 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=44, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:10:00,194 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:00,243 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 76245 synced till here 76224
2014-07-22 09:10:00,371 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045398079 with entries=123, filesize=84.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045400195
2014-07-22 09:10:00,372 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=45, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:10:02,140 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:02,164 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 76364 synced till here 76350
2014-07-22 09:10:02,226 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045400195 with entries=119, filesize=75.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045402140
2014-07-22 09:10:02,226 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=46, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:10:04,669 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14621, memsize=257.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/68eb27e74099421fb1f34c714bd11a2a
2014-07-22 09:10:04,693 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/68eb27e74099421fb1f34c714bd11a2a as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/68eb27e74099421fb1f34c714bd11a2a
2014-07-22 09:10:04,706 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/68eb27e74099421fb1f34c714bd11a2a, entries=935910, sequenceid=14621, filesize=66.7m
2014-07-22 09:10:04,708 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~690.9m/724459040, currentsize=195.9m/205384720 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 14864ms, sequenceid=14621, compaction requested=true
2014-07-22 09:10:04,708 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 39 store files, 0 compacting, 39 eligible, 2000 blocking
2014-07-22 09:10:04,709 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 39 files from compaction candidates
2014-07-22 09:10:04,709 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:10:04,709 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:10:04,709 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:10:04,709 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:10:04,709 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b., current region memstore size 283.8m
2014-07-22 09:10:04,941 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:05,021 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 76454 synced till here 76441
2014-07-22 09:10:05,046 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:10:05,139 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045402140 with entries=90, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045404941
2014-07-22 09:10:05,486 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14617, memsize=230.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/6d652cdc99c7428bb2ae5b22101cc896
2014-07-22 09:10:05,499 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/6d652cdc99c7428bb2ae5b22101cc896 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/6d652cdc99c7428bb2ae5b22101cc896
2014-07-22 09:10:05,517 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/6d652cdc99c7428bb2ae5b22101cc896, entries=837630, sequenceid=14617, filesize=59.7m
2014-07-22 09:10:05,518 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~665.4m/697766320, currentsize=198.6m/208226320 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 13847ms, sequenceid=14617, compaction requested=true
2014-07-22 09:10:05,635 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:10:05,635 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 38 store files, 0 compacting, 38 eligible, 2000 blocking
2014-07-22 09:10:05,636 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 38 files from compaction candidates
2014-07-22 09:10:05,636 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:10:05,636 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 630.3m
2014-07-22 09:10:05,636 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:10:05,636 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:10:06,170 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:10:07,490 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:07,504 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 76542 synced till here 76541
2014-07-22 09:10:07,516 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045404941 with entries=88, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045407490
2014-07-22 09:10:08,263 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:10:08,364 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:08,381 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 76635 synced till here 76633
2014-07-22 09:10:08,837 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045407490 with entries=93, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045408364
2014-07-22 09:10:09,379 DEBUG [RpcServer.handler=39,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:10:09,770 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:10,549 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 76732 synced till here 76713
2014-07-22 09:10:10,695 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045408364 with entries=97, filesize=79.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045409771
2014-07-22 09:10:11,736 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:11,775 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 76839 synced till here 76832
2014-07-22 09:10:11,920 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045409771 with entries=107, filesize=71.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045411736
2014-07-22 09:10:13,826 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:13,856 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 76949 synced till here 76925
2014-07-22 09:10:14,882 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045411736 with entries=110, filesize=76.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045413826
2014-07-22 09:10:15,778 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:15,801 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 77048 synced till here 77037
2014-07-22 09:10:15,850 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=17678, memsize=182.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/fb9c61b606f948bbab7a26bcb5314aab
2014-07-22 09:10:15,868 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/fb9c61b606f948bbab7a26bcb5314aab as hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/fb9c61b606f948bbab7a26bcb5314aab
2014-07-22 09:10:15,884 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/fb9c61b606f948bbab7a26bcb5314aab, entries=665960, sequenceid=17678, filesize=47.5m
2014-07-22 09:10:15,885 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~287.5m/301498320, currentsize=40.1m/42031760 for region usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. in 11176ms, sequenceid=17678, compaction requested=true
2014-07-22 09:10:15,885 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 17 store files, 0 compacting, 17 eligible, 2000 blocking
2014-07-22 09:10:15,885 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:10:15,886 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 17 files from compaction candidates
2014-07-22 09:10:15,886 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045413826 with entries=99, filesize=72.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045415778
2014-07-22 09:10:15,886 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:10:15,886 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:10:15,886 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 751.3m
2014-07-22 09:10:15,886 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045299985
2014-07-22 09:10:15,886 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. because compaction request was cancelled
2014-07-22 09:10:15,886 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045301963
2014-07-22 09:10:15,886 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045303726
2014-07-22 09:10:15,886 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045305394
2014-07-22 09:10:15,886 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045307624
2014-07-22 09:10:15,886 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045309427
2014-07-22 09:10:15,886 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045311723
2014-07-22 09:10:15,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045313206
2014-07-22 09:10:15,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045314943
2014-07-22 09:10:15,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045316892
2014-07-22 09:10:15,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045325143
2014-07-22 09:10:15,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045327280
2014-07-22 09:10:15,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045329224
2014-07-22 09:10:15,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045330130
2014-07-22 09:10:15,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045331666
2014-07-22 09:10:15,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045333199
2014-07-22 09:10:15,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045334746
2014-07-22 09:10:15,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045337024
2014-07-22 09:10:15,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045338623
2014-07-22 09:10:15,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045340742
2014-07-22 09:10:15,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045342224
2014-07-22 09:10:15,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045343900
2014-07-22 09:10:15,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045345012
2014-07-22 09:10:15,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045346997
2014-07-22 09:10:15,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045348830
2014-07-22 09:10:15,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045350320
2014-07-22 09:10:15,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045351756
2014-07-22 09:10:15,888 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045354423
2014-07-22 09:10:15,888 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045356179
2014-07-22 09:10:17,382 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:17,424 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:10:17,492 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 77151 synced till here 77145
2014-07-22 09:10:17,562 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045415778 with entries=103, filesize=69.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045417383
2014-07-22 09:10:19,609 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:19,624 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 77243 synced till here 77231
2014-07-22 09:10:19,772 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045417383 with entries=92, filesize=70.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045419609
2014-07-22 09:10:19,983 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14747, memsize=179.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/ad256535c6f14db29f8709577865635b
2014-07-22 09:10:20,001 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/ad256535c6f14db29f8709577865635b as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/ad256535c6f14db29f8709577865635b
2014-07-22 09:10:20,021 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/ad256535c6f14db29f8709577865635b, entries=652770, sequenceid=14747, filesize=46.5m
2014-07-22 09:10:20,022 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~643.1m/674348960, currentsize=229.1m/240275520 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 14386ms, sequenceid=14747, compaction requested=true
2014-07-22 09:10:20,023 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:10:20,023 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 39 store files, 0 compacting, 39 eligible, 2000 blocking
2014-07-22 09:10:20,023 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 39 files from compaction candidates
2014-07-22 09:10:20,023 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:10:20,023 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:10:20,023 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:10:20,024 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 461.7m
2014-07-22 09:10:21,038 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:10:21,258 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:21,280 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 77344 synced till here 77334
2014-07-22 09:10:21,471 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045419609 with entries=101, filesize=73.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045421258
2014-07-22 09:10:21,471 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045360255
2014-07-22 09:10:21,624 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:10:22,923 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:23,145 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 77448 synced till here 77439
2014-07-22 09:10:23,234 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045421258 with entries=104, filesize=76.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045422924
2014-07-22 09:10:24,811 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:24,827 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 77555 synced till here 77546
2014-07-22 09:10:24,944 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045422924 with entries=107, filesize=72.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045424812
2014-07-22 09:10:27,021 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:27,120 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 77656 synced till here 77644
2014-07-22 09:10:27,272 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045424812 with entries=101, filesize=90.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045427021
2014-07-22 09:10:29,150 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:29,171 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 77759 synced till here 77736
2014-07-22 09:10:30,563 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1137ms
GC pool 'ParNew' had collection(s): count=1 time=1176ms
2014-07-22 09:10:30,608 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045427021 with entries=103, filesize=80.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045429151
2014-07-22 09:10:31,566 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:31,615 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 77853 synced till here 77831
2014-07-22 09:10:32,691 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045429151 with entries=94, filesize=87.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045431567
2014-07-22 09:10:33,490 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:33,512 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 77943 synced till here 77929
2014-07-22 09:10:34,688 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045431567 with entries=90, filesize=84.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045433490
2014-07-22 09:10:35,627 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:35,681 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 78031 synced till here 78020
2014-07-22 09:10:36,588 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045433490 with entries=88, filesize=84.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045435628
2014-07-22 09:10:37,552 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:37,763 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 78113 synced till here 78111
2014-07-22 09:10:37,780 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14847, memsize=268.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/56397badc29243f7b6d09b7f7996ff92
2014-07-22 09:10:37,789 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045435628 with entries=82, filesize=76.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045437552
2014-07-22 09:10:37,835 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/56397badc29243f7b6d09b7f7996ff92 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/56397badc29243f7b6d09b7f7996ff92
2014-07-22 09:10:37,847 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/56397badc29243f7b6d09b7f7996ff92, entries=976020, sequenceid=14847, filesize=69.5m
2014-07-22 09:10:37,847 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~756.8m/793552880, currentsize=375.2m/393420720 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 21961ms, sequenceid=14847, compaction requested=true
2014-07-22 09:10:37,847 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:10:37,848 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 38 store files, 0 compacting, 38 eligible, 2000 blocking
2014-07-22 09:10:37,848 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 38 files from compaction candidates
2014-07-22 09:10:37,848 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 761.4m
2014-07-22 09:10:37,848 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:10:37,848 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:10:37,848 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:10:37,923 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:10:38,826 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:10:39,093 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:39,231 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 78209 synced till here 78204
2014-07-22 09:10:39,258 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045437552 with entries=96, filesize=72.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045439093
2014-07-22 09:10:39,258 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045362689
2014-07-22 09:10:39,259 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045365480
2014-07-22 09:10:39,260 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045369413
2014-07-22 09:10:39,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045372659
2014-07-22 09:10:39,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045376535
2014-07-22 09:10:39,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045377725
2014-07-22 09:10:39,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045379828
2014-07-22 09:10:39,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045383103
2014-07-22 09:10:39,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045385404
2014-07-22 09:10:39,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045386591
2014-07-22 09:10:40,593 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:40,723 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14898, memsize=313.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/6aaca43d0f6c440ba5fc44f6091e9fca
2014-07-22 09:10:40,780 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/6aaca43d0f6c440ba5fc44f6091e9fca as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/6aaca43d0f6c440ba5fc44f6091e9fca
2014-07-22 09:10:40,906 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 78298 synced till here 78297
2014-07-22 09:10:40,925 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/6aaca43d0f6c440ba5fc44f6091e9fca, entries=1141640, sequenceid=14898, filesize=81.3m
2014-07-22 09:10:40,925 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~465.3m/487929600, currentsize=369.9m/387904080 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 20902ms, sequenceid=14898, compaction requested=true
2014-07-22 09:10:40,926 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:10:40,926 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 40 store files, 0 compacting, 40 eligible, 2000 blocking
2014-07-22 09:10:40,927 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 608.9m
2014-07-22 09:10:40,927 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 40 files from compaction candidates
2014-07-22 09:10:40,927 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:10:40,927 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:10:40,927 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:10:40,930 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045439093 with entries=89, filesize=76.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045440594
2014-07-22 09:10:41,702 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:10:42,159 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:10:42,395 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:42,432 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 78397 synced till here 78385
2014-07-22 09:10:42,466 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045440594 with entries=99, filesize=72.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045442395
2014-07-22 09:10:43,727 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:43,768 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 78478 synced till here 78474
2014-07-22 09:10:44,030 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045442395 with entries=81, filesize=65.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045443727
2014-07-22 09:10:45,992 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:46,327 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 78592 synced till here 78586
2014-07-22 09:10:46,359 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045443727 with entries=114, filesize=88.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045445992
2014-07-22 09:10:48,014 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:48,029 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 78690 synced till here 78682
2014-07-22 09:10:48,150 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045445992 with entries=98, filesize=73.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045448014
2014-07-22 09:10:49,637 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:49,665 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 78796 synced till here 78773
2014-07-22 09:10:49,917 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045448014 with entries=106, filesize=75.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045449637
2014-07-22 09:10:52,265 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:53,250 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 78929 synced till here 78902
2014-07-22 09:10:53,771 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045449637 with entries=133, filesize=87.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045452266
2014-07-22 09:10:54,656 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:54,676 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 79013 synced till here 79004
2014-07-22 09:10:55,569 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045452266 with entries=84, filesize=76.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045454656
2014-07-22 09:10:56,345 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:56,407 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 79084 synced till here 79079
2014-07-22 09:10:56,497 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045454656 with entries=71, filesize=68.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045456346
2014-07-22 09:10:58,357 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:58,374 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 79169 synced till here 79167
2014-07-22 09:10:58,411 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045456346 with entries=85, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045458358
2014-07-22 09:10:59,510 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=15100, memsize=265.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/79374238866e4be0b28c2fa8bbb59654
2014-07-22 09:10:59,531 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/79374238866e4be0b28c2fa8bbb59654 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/79374238866e4be0b28c2fa8bbb59654
2014-07-22 09:10:59,543 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/79374238866e4be0b28c2fa8bbb59654, entries=964880, sequenceid=15100, filesize=68.7m
2014-07-22 09:10:59,544 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~614.1m/643892240, currentsize=305.3m/320122880 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 18617ms, sequenceid=15100, compaction requested=true
2014-07-22 09:10:59,544 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:10:59,544 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 40 store files, 0 compacting, 40 eligible, 2000 blocking
2014-07-22 09:10:59,544 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 752.7m
2014-07-22 09:10:59,544 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 40 files from compaction candidates
2014-07-22 09:10:59,545 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:10:59,545 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:10:59,545 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:10:59,548 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:10:59,784 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:10:59,803 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 79244 synced till here 79242
2014-07-22 09:10:59,828 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045458358 with entries=75, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045459784
2014-07-22 09:11:00,270 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:11:01,833 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:01,857 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 79331 synced till here 79326
2014-07-22 09:11:01,938 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045459784 with entries=87, filesize=68.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045461833
2014-07-22 09:11:02,035 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=15060, memsize=356.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/d0df07ce5d6f4e219c5693b04d98b291
2014-07-22 09:11:02,049 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/d0df07ce5d6f4e219c5693b04d98b291 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/d0df07ce5d6f4e219c5693b04d98b291
2014-07-22 09:11:02,090 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/d0df07ce5d6f4e219c5693b04d98b291, entries=1298510, sequenceid=15060, filesize=92.4m
2014-07-22 09:11:02,091 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~761.4m/798358000, currentsize=411.2m/431172000 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 24243ms, sequenceid=15060, compaction requested=true
2014-07-22 09:11:02,091 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 39 store files, 0 compacting, 39 eligible, 2000 blocking
2014-07-22 09:11:02,091 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 39 files from compaction candidates
2014-07-22 09:11:02,091 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:11:02,091 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:11:02,092 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:11:02,092 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:11:02,092 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 726.0m
2014-07-22 09:11:02,179 DEBUG [RpcServer.handler=23,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:11:03,270 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:03,309 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 79425 synced till here 79416
2014-07-22 09:11:03,416 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045461833 with entries=94, filesize=68.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045463275
2014-07-22 09:11:03,416 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045388221
2014-07-22 09:11:03,416 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045392575
2014-07-22 09:11:03,416 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045394753
2014-07-22 09:11:03,416 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045396389
2014-07-22 09:11:03,416 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045398079
2014-07-22 09:11:03,416 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045400195
2014-07-22 09:11:03,583 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:11:05,204 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:05,252 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 79540 synced till here 79514
2014-07-22 09:11:05,536 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045463275 with entries=115, filesize=78.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045465205
2014-07-22 09:11:05,537 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:11:07,418 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:07,479 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 79643 synced till here 79633
2014-07-22 09:11:07,626 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045465205 with entries=103, filesize=71.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045467419
2014-07-22 09:11:07,626 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:11:08,504 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:08,527 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 79721 synced till here 79710
2014-07-22 09:11:09,521 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045467419 with entries=78, filesize=75.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045468504
2014-07-22 09:11:09,522 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:11:10,254 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:11,231 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 79812 synced till here 79790
2014-07-22 09:11:11,363 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045468504 with entries=91, filesize=79.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045470254
2014-07-22 09:11:11,363 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:11:12,275 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:12,300 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 79895 synced till here 79887
2014-07-22 09:11:13,335 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045470254 with entries=83, filesize=67.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045472275
2014-07-22 09:11:13,336 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:11:14,261 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:14,287 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 80008 synced till here 79995
2014-07-22 09:11:15,136 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045472275 with entries=113, filesize=78.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045474262
2014-07-22 09:11:15,137 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:11:16,894 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:16,922 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 80108 synced till here 80089
2014-07-22 09:11:18,286 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045474262 with entries=100, filesize=79.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045476895
2014-07-22 09:11:18,287 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:11:20,395 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:20,434 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 80202 synced till here 80182
2014-07-22 09:11:20,566 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045476895 with entries=94, filesize=84.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045480396
2014-07-22 09:11:20,567 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:11:22,442 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1047ms
GC pool 'ParNew' had collection(s): count=1 time=1083ms
2014-07-22 09:11:22,470 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:22,508 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 80294 synced till here 80279
2014-07-22 09:11:22,635 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045480396 with entries=92, filesize=81.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045482471
2014-07-22 09:11:22,636 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:11:23,476 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:23,518 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 80369 synced till here 80361
2014-07-22 09:11:24,657 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045482471 with entries=75, filesize=70.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045483477
2014-07-22 09:11:24,658 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:11:25,334 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:25,351 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 80439 synced till here 80436
2014-07-22 09:11:25,395 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=15271, memsize=284.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/96194155d9fb4855b84768469ea74c42
2014-07-22 09:11:25,416 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045483477 with entries=70, filesize=66.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045485334
2014-07-22 09:11:25,417 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:11:25,418 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/96194155d9fb4855b84768469ea74c42 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/96194155d9fb4855b84768469ea74c42
2014-07-22 09:11:25,431 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/96194155d9fb4855b84768469ea74c42, entries=1036670, sequenceid=15271, filesize=73.9m
2014-07-22 09:11:25,432 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~756.4m/793095040, currentsize=435.3m/456432160 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 25888ms, sequenceid=15271, compaction requested=true
2014-07-22 09:11:25,432 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:11:25,432 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 39 store files, 0 compacting, 39 eligible, 2000 blocking
2014-07-22 09:11:25,432 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 39 files from compaction candidates
2014-07-22 09:11:25,433 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 750.2m
2014-07-22 09:11:25,433 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:11:25,433 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:11:25,433 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:11:25,665 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:11:25,762 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=15312, memsize=264.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/e39a9219eef441099449af93878762d2
2014-07-22 09:11:25,776 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/e39a9219eef441099449af93878762d2 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/e39a9219eef441099449af93878762d2
2014-07-22 09:11:26,690 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/e39a9219eef441099449af93878762d2, entries=964390, sequenceid=15312, filesize=68.7m
2014-07-22 09:11:26,691 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~731.4m/766980240, currentsize=395.2m/414365920 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 24599ms, sequenceid=15312, compaction requested=true
2014-07-22 09:11:26,691 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:11:26,692 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 41 store files, 0 compacting, 41 eligible, 2000 blocking
2014-07-22 09:11:26,692 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 816.0m
2014-07-22 09:11:26,692 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 41 files from compaction candidates
2014-07-22 09:11:26,692 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:11:26,692 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:11:26,692 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:11:26,768 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:11:26,987 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:11:27,379 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:27,563 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 80534 synced till here 80527
2014-07-22 09:11:27,635 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045485334 with entries=95, filesize=76.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045487380
2014-07-22 09:11:27,636 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=44, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:11:27,797 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:11:29,239 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:29,254 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 80632 synced till here 80620
2014-07-22 09:11:29,362 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045487380 with entries=98, filesize=74.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045489239
2014-07-22 09:11:29,362 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=45, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:11:31,197 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:31,218 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 80727 synced till here 80720
2014-07-22 09:11:31,314 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045489239 with entries=95, filesize=70.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045491197
2014-07-22 09:11:31,315 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=46, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:11:32,760 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:32,782 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 80810 synced till here 80803
2014-07-22 09:11:32,908 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045491197 with entries=83, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045492761
2014-07-22 09:11:32,908 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=47, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:11:34,524 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:34,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 80910 synced till here 80898
2014-07-22 09:11:34,695 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045492761 with entries=100, filesize=78.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045494525
2014-07-22 09:11:34,697 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=48, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:11:36,471 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:36,757 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 81037 synced till here 81028
2014-07-22 09:11:36,869 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045494525 with entries=127, filesize=103.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045496472
2014-07-22 09:11:36,870 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=49, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:11:38,478 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:38,535 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 81118 synced till here 81105
2014-07-22 09:11:38,903 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045496472 with entries=81, filesize=72.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045498479
2014-07-22 09:11:38,904 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=50, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:11:40,519 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:40,565 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 81194 synced till here 81192
2014-07-22 09:11:40,593 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045498479 with entries=76, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045500520
2014-07-22 09:11:40,593 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=51, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:11:42,038 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:42,050 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 81269 synced till here 81264
2014-07-22 09:11:42,168 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045500520 with entries=75, filesize=66.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045502038
2014-07-22 09:11:42,169 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=52, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:11:43,707 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:43,731 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 81349 synced till here 81345
2014-07-22 09:11:43,802 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045502038 with entries=80, filesize=67.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045503707
2014-07-22 09:11:43,802 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=53, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:11:44,672 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:45,441 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 81433 synced till here 81426
2014-07-22 09:11:45,551 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045503707 with entries=84, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045504672
2014-07-22 09:11:45,552 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=54, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:11:45,762 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:45,763 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:45,766 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:45,769 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:45,769 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:45,801 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:45,801 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:45,803 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:45,853 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:45,855 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:45,855 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:45,856 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:45,863 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:45,873 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:45,899 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:45,911 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:45,911 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:45,918 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:45,964 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:46,002 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:46,049 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:46,093 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:46,093 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:46,094 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:46,094 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:46,095 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:46,132 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:46,135 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:46,179 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:46,223 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:46,224 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:46,226 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:46,267 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:46,268 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:46,269 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:46,269 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:46,308 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:46,346 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:46,383 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:46,417 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:46,417 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:46,418 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:11:46,446 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=15521, memsize=237.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/bf534dd052ec4b63939cd9de01674f04
2014-07-22 09:11:46,473 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/bf534dd052ec4b63939cd9de01674f04 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/bf534dd052ec4b63939cd9de01674f04
2014-07-22 09:11:46,499 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/bf534dd052ec4b63939cd9de01674f04, entries=865710, sequenceid=15521, filesize=61.7m
2014-07-22 09:11:46,500 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~750.2m/786628880, currentsize=355.4m/372636960 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 21068ms, sequenceid=15521, compaction requested=true
2014-07-22 09:11:46,500 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:11:46,500 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 41 store files, 0 compacting, 41 eligible, 2000 blocking
2014-07-22 09:11:46,501 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 83ms
2014-07-22 09:11:46,501 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,501 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 41 files from compaction candidates
2014-07-22 09:11:46,501 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 84ms
2014-07-22 09:11:46,501 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,501 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:11:46,501 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b., current region memstore size 252.0m
2014-07-22 09:11:46,501 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:11:46,501 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 84ms
2014-07-22 09:11:46,501 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:11:46,501 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,502 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 119ms
2014-07-22 09:11:46,502 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,502 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 156ms
2014-07-22 09:11:46,502 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,502 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 194ms
2014-07-22 09:11:46,502 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,502 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 233ms
2014-07-22 09:11:46,502 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,503 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 235ms
2014-07-22 09:11:46,503 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,503 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 235ms
2014-07-22 09:11:46,503 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,503 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 236ms
2014-07-22 09:11:46,503 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,503 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 277ms
2014-07-22 09:11:46,503 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,503 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 280ms
2014-07-22 09:11:46,503 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,503 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 280ms
2014-07-22 09:11:46,504 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,504 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 325ms
2014-07-22 09:11:46,504 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,504 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 369ms
2014-07-22 09:11:46,504 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,505 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 373ms
2014-07-22 09:11:46,505 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,513 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 418ms
2014-07-22 09:11:46,513 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,513 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 419ms
2014-07-22 09:11:46,514 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,514 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 420ms
2014-07-22 09:11:46,514 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,514 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 421ms
2014-07-22 09:11:46,514 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,514 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 421ms
2014-07-22 09:11:46,514 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,514 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 465ms
2014-07-22 09:11:46,514 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,514 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 512ms
2014-07-22 09:11:46,514 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,514 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 550ms
2014-07-22 09:11:46,515 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,517 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 599ms
2014-07-22 09:11:46,517 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,517 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 606ms
2014-07-22 09:11:46,518 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,518 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 607ms
2014-07-22 09:11:46,518 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,518 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 619ms
2014-07-22 09:11:46,518 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,520 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 647ms
2014-07-22 09:11:46,520 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,521 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 658ms
2014-07-22 09:11:46,521 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,521 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 666ms
2014-07-22 09:11:46,521 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,522 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 667ms
2014-07-22 09:11:46,522 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,545 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 690ms
2014-07-22 09:11:46,545 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,545 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 692ms
2014-07-22 09:11:46,545 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,552 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 749ms
2014-07-22 09:11:46,552 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,552 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 751ms
2014-07-22 09:11:46,553 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,553 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 753ms
2014-07-22 09:11:46,553 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,561 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 792ms
2014-07-22 09:11:46,561 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,562 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 793ms
2014-07-22 09:11:46,562 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,562 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 796ms
2014-07-22 09:11:46,562 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,569 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 806ms
2014-07-22 09:11:46,569 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,577 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 815ms
2014-07-22 09:11:46,577 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:11:46,637 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:11:46,881 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:11:48,281 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:48,358 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 81528 synced till here 81523
2014-07-22 09:11:48,441 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045504672 with entries=95, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045508281
2014-07-22 09:11:50,595 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:50,641 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 81617 synced till here 81599
2014-07-22 09:11:51,102 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045508281 with entries=89, filesize=85.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045510596
2014-07-22 09:11:51,769 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:51,773 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b.
2014-07-22 09:11:53,407 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1321ms
GC pool 'ParNew' had collection(s): count=1 time=1530ms
2014-07-22 09:11:53,426 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 81704 synced till here 81682
2014-07-22 09:11:53,726 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045510596 with entries=87, filesize=82.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045511770
2014-07-22 09:11:54,497 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=15521, memsize=302.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/10a173a2017446369b5fb1629b8ac6cb
2014-07-22 09:11:54,509 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/10a173a2017446369b5fb1629b8ac6cb as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/10a173a2017446369b5fb1629b8ac6cb
2014-07-22 09:11:54,520 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/10a173a2017446369b5fb1629b8ac6cb, entries=1100760, sequenceid=15521, filesize=78.4m
2014-07-22 09:11:54,520 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~821.4m/861295440, currentsize=437.6m/458884640 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 27828ms, sequenceid=15521, compaction requested=true
2014-07-22 09:11:54,521 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:11:54,521 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 40 store files, 0 compacting, 40 eligible, 2000 blocking
2014-07-22 09:11:54,521 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 40 files from compaction candidates
2014-07-22 09:11:54,521 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 916.4m
2014-07-22 09:11:54,521 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:11:54,521 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:11:54,521 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:11:54,613 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:11:54,760 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:56,025 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1115ms
GC pool 'ParNew' had collection(s): count=1 time=1254ms
2014-07-22 09:11:56,075 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 81788 synced till here 81772
2014-07-22 09:11:56,322 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045511770 with entries=84, filesize=80.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045514760
2014-07-22 09:11:56,834 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:11:57,256 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:57,280 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045514760 with entries=66, filesize=61.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045517257
2014-07-22 09:11:59,297 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:11:59,329 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 81926 synced till here 81925
2014-07-22 09:11:59,354 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045517257 with entries=72, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045519298
2014-07-22 09:12:00,506 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=18756, memsize=188.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/a77b7c4255374cdbbf0d6912abd78428
2014-07-22 09:12:00,520 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/a77b7c4255374cdbbf0d6912abd78428 as hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/a77b7c4255374cdbbf0d6912abd78428
2014-07-22 09:12:00,530 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/a77b7c4255374cdbbf0d6912abd78428, entries=687990, sequenceid=18756, filesize=49.0m
2014-07-22 09:12:00,531 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~252.1m/264300560, currentsize=10.5m/10992000 for region usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. in 14030ms, sequenceid=18756, compaction requested=true
2014-07-22 09:12:00,532 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:12:00,532 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 18 store files, 0 compacting, 18 eligible, 2000 blocking
2014-07-22 09:12:00,532 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 18 files from compaction candidates
2014-07-22 09:12:00,532 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 954.3m
2014-07-22 09:12:00,532 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:12:00,532 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:12:00,532 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. because compaction request was cancelled
2014-07-22 09:12:00,990 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:12:01,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 82016 synced till here 82006
2014-07-22 09:12:01,093 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045519298 with entries=90, filesize=72.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045520991
2014-07-22 09:12:01,141 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045402140
2014-07-22 09:12:01,141 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045404941
2014-07-22 09:12:01,141 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045407490
2014-07-22 09:12:01,141 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045408364
2014-07-22 09:12:01,141 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045409771
2014-07-22 09:12:01,141 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045411736
2014-07-22 09:12:01,143 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045413826
2014-07-22 09:12:01,143 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045415778
2014-07-22 09:12:01,143 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045417383
2014-07-22 09:12:01,144 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045419609
2014-07-22 09:12:01,144 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045421258
2014-07-22 09:12:01,144 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045422924
2014-07-22 09:12:01,144 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045424812
2014-07-22 09:12:01,144 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045427021
2014-07-22 09:12:01,144 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045429151
2014-07-22 09:12:01,144 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045431567
2014-07-22 09:12:01,144 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045433490
2014-07-22 09:12:01,144 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045435628
2014-07-22 09:12:01,144 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045437552
2014-07-22 09:12:01,144 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045439093
2014-07-22 09:12:01,144 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045440594
2014-07-22 09:12:01,144 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045442395
2014-07-22 09:12:01,144 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045443727
2014-07-22 09:12:01,144 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045445992
2014-07-22 09:12:01,144 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045448014
2014-07-22 09:12:01,144 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045449637
2014-07-22 09:12:01,144 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045452266
2014-07-22 09:12:01,144 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045454656
2014-07-22 09:12:01,145 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045456346
2014-07-22 09:12:02,727 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:12:02,787 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:12:02,801 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 82126 synced till here 82111
2014-07-22 09:12:03,007 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045520991 with entries=110, filesize=76.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045522787
2014-07-22 09:12:04,828 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:12:04,856 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 82204 synced till here 82203
2014-07-22 09:12:04,878 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045522787 with entries=78, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045524828
2014-07-22 09:12:06,414 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:12:06,431 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 82292 synced till here 82290
2014-07-22 09:12:06,514 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045524828 with entries=88, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045526415
2014-07-22 09:12:07,461 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:12:07,482 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 82380 synced till here 82379
2014-07-22 09:12:07,516 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045526415 with entries=88, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045527462
2014-07-22 09:12:08,714 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:12:08,732 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 82479 synced till here 82476
2014-07-22 09:12:08,794 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045527462 with entries=99, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045528714
2014-07-22 09:12:10,226 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:12:10,389 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 82557 synced till here 82555
2014-07-22 09:12:10,438 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045528714 with entries=78, filesize=71.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045530227
2014-07-22 09:12:11,937 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:12:11,971 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 82638 synced till here 82637
2014-07-22 09:12:11,982 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045530227 with entries=81, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045531937
2014-07-22 09:12:13,527 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:13,557 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:13,557 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:13,558 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:13,558 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:13,558 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:13,558 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:13,559 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:13,562 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:13,563 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:13,572 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:13,573 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:13,579 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:13,580 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:13,668 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:13,669 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:13,742 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:13,808 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:13,878 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:13,886 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:13,887 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:13,952 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:13,954 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:14,131 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:14,136 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:14,137 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:14,138 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:14,141 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:14,142 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:14,146 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:14,151 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:14,153 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:12:14,426 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=15792, memsize=289.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/6e42b187ac584cb2bd93a71924518ad2
2014-07-22 09:12:14,438 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/6e42b187ac584cb2bd93a71924518ad2 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/6e42b187ac584cb2bd93a71924518ad2
2014-07-22 09:12:14,451 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/6e42b187ac584cb2bd93a71924518ad2, entries=1055140, sequenceid=15792, filesize=75.1m
2014-07-22 09:12:14,452 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~921.4m/966176320, currentsize=308.0m/322936160 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 19931ms, sequenceid=15792, compaction requested=true
2014-07-22 09:12:14,452 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:12:14,453 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 40 store files, 0 compacting, 40 eligible, 2000 blocking
2014-07-22 09:12:14,453 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 300ms
2014-07-22 09:12:14,453 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,453 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 40 files from compaction candidates
2014-07-22 09:12:14,453 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 796.9m
2014-07-22 09:12:14,453 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 302ms
2014-07-22 09:12:14,453 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,453 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:12:14,453 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 307ms
2014-07-22 09:12:14,454 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,453 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:12:14,454 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 312ms
2014-07-22 09:12:14,454 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,454 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:12:14,454 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 313ms
2014-07-22 09:12:14,454 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,457 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 320ms
2014-07-22 09:12:14,457 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,458 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 321ms
2014-07-22 09:12:14,458 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,458 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 322ms
2014-07-22 09:12:14,458 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,458 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 327ms
2014-07-22 09:12:14,458 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,458 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 504ms
2014-07-22 09:12:14,459 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,459 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 507ms
2014-07-22 09:12:14,459 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,461 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 574ms
2014-07-22 09:12:14,461 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,461 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 575ms
2014-07-22 09:12:14,461 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,469 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 591ms
2014-07-22 09:12:14,469 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,469 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 661ms
2014-07-22 09:12:14,470 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,470 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 728ms
2014-07-22 09:12:14,470 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,471 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 801ms
2014-07-22 09:12:14,471 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,472 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 803ms
2014-07-22 09:12:14,472 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,473 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 893ms
2014-07-22 09:12:14,473 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,473 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 894ms
2014-07-22 09:12:14,473 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,473 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 900ms
2014-07-22 09:12:14,474 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,482 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 909ms
2014-07-22 09:12:14,482 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,482 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 919ms
2014-07-22 09:12:14,482 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,483 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 921ms
2014-07-22 09:12:14,483 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,484 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 926ms
2014-07-22 09:12:14,484 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,485 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 926ms
2014-07-22 09:12:14,485 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,485 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 928ms
2014-07-22 09:12:14,485 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,485 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 928ms
2014-07-22 09:12:14,485 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,485 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 927ms
2014-07-22 09:12:14,485 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,486 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 928ms
2014-07-22 09:12:14,486 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,486 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 929ms
2014-07-22 09:12:14,486 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,487 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 961ms
2014-07-22 09:12:14,487 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:12:14,556 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:12:14,958 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:12:14,987 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045531937 with entries=105, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045534958
2014-07-22 09:12:14,987 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045458358
2014-07-22 09:12:14,987 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045459784
2014-07-22 09:12:15,222 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:12:17,459 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:12:17,484 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 82817 synced till here 82814
2014-07-22 09:12:17,774 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045534958 with entries=74, filesize=65.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045537459
2014-07-22 09:12:18,186 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=15853, memsize=282.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/5f5e1f50783942f9bc78f8b786b29925
2014-07-22 09:12:18,198 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/5f5e1f50783942f9bc78f8b786b29925 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/5f5e1f50783942f9bc78f8b786b29925
2014-07-22 09:12:18,208 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/5f5e1f50783942f9bc78f8b786b29925, entries=1027360, sequenceid=15853, filesize=73.2m
2014-07-22 09:12:18,208 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~961.7m/1008443120, currentsize=276.7m/290124560 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 17676ms, sequenceid=15853, compaction requested=true
2014-07-22 09:12:18,208 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:12:18,209 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 42 store files, 0 compacting, 42 eligible, 2000 blocking
2014-07-22 09:12:18,209 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b., current region memstore size 81.8m
2014-07-22 09:12:18,209 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 42 files from compaction candidates
2014-07-22 09:12:18,209 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:12:18,209 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:12:18,209 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:12:19,067 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:12:19,140 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:12:19,690 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:12:20,033 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 82960 synced till here 82927
2014-07-22 09:12:20,270 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045537459 with entries=143, filesize=97.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045539690
2014-07-22 09:12:20,270 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045461833
2014-07-22 09:12:20,270 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045463275
2014-07-22 09:12:20,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045465205
2014-07-22 09:12:20,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045467419
2014-07-22 09:12:20,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045468504
2014-07-22 09:12:20,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045470254
2014-07-22 09:12:20,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045472275
2014-07-22 09:12:20,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045474262
2014-07-22 09:12:20,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045476895
2014-07-22 09:12:20,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045480396
2014-07-22 09:12:20,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045482471
2014-07-22 09:12:20,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045483477
2014-07-22 09:12:22,077 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:12:22,124 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 83053 synced till here 83038
2014-07-22 09:12:22,256 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045539690 with entries=93, filesize=71.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045542077
2014-07-22 09:12:24,162 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=19051, memsize=75.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/1305b7a1cc154c85bfde855ea7d3e243
2014-07-22 09:12:24,183 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/1305b7a1cc154c85bfde855ea7d3e243 as hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/1305b7a1cc154c85bfde855ea7d3e243
2014-07-22 09:12:24,201 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/1305b7a1cc154c85bfde855ea7d3e243, entries=275850, sequenceid=19051, filesize=19.7m
2014-07-22 09:12:24,202 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~82.8m/86798720, currentsize=11.6m/12130000 for region usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. in 5992ms, sequenceid=19051, compaction requested=true
2014-07-22 09:12:24,202 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:12:24,202 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 19 store files, 0 compacting, 19 eligible, 2000 blocking
2014-07-22 09:12:24,203 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 897.1m
2014-07-22 09:12:24,203 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 19 files from compaction candidates
2014-07-22 09:12:24,203 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:12:24,203 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:12:24,203 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. because compaction request was cancelled
2014-07-22 09:12:24,693 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:12:24,706 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 83138 synced till here 83123
2014-07-22 09:12:24,898 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045542077 with entries=85, filesize=78.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045544693
2014-07-22 09:12:26,269 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:12:27,007 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:12:28,388 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 83271 synced till here 83255
2014-07-22 09:12:28,508 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045544693 with entries=133, filesize=93.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045547007
2014-07-22 09:12:30,568 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1262ms
GC pool 'ParNew' had collection(s): count=1 time=1365ms
2014-07-22 09:12:30,831 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:12:30,896 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 83377 synced till here 83368
2014-07-22 09:12:31,028 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045547007 with entries=106, filesize=90.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045550831
2014-07-22 09:12:33,099 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:12:33,314 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 83493 synced till here 83449
2014-07-22 09:12:33,689 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045550831 with entries=116, filesize=95.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045553099
2014-07-22 09:12:36,273 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:12:36,399 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 83614 synced till here 83612
2014-07-22 09:12:36,487 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045553099 with entries=121, filesize=96.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045556274
2014-07-22 09:12:38,675 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:12:38,718 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 83715 synced till here 83679
2014-07-22 09:12:40,320 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045556274 with entries=101, filesize=94.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045558676
2014-07-22 09:12:42,661 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1162ms
GC pool 'ParNew' had collection(s): count=1 time=1232ms
2014-07-22 09:12:43,403 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:12:43,490 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 83845 synced till here 83818
2014-07-22 09:12:43,642 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045558676 with entries=130, filesize=100.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045563403
2014-07-22 09:12:45,714 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:12:45,797 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 83951 synced till here 83921
2014-07-22 09:12:46,106 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045563403 with entries=106, filesize=89.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045565715
2014-07-22 09:12:47,554 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:12:47,598 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 84055 synced till here 84028
2014-07-22 09:12:47,942 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=15974, memsize=371.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/078fd72e6477485598c0763a54bb95e2
2014-07-22 09:12:47,946 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045565715 with entries=104, filesize=85.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045567555
2014-07-22 09:12:47,961 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/078fd72e6477485598c0763a54bb95e2 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/078fd72e6477485598c0763a54bb95e2
2014-07-22 09:12:47,975 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/078fd72e6477485598c0763a54bb95e2, entries=1352790, sequenceid=15974, filesize=96.3m
2014-07-22 09:12:47,976 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~796.9m/835588080, currentsize=463.4m/485873040 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 33523ms, sequenceid=15974, compaction requested=true
2014-07-22 09:12:47,976 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:12:47,976 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 42 store files, 0 compacting, 42 eligible, 2000 blocking
2014-07-22 09:12:47,977 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 42 files from compaction candidates
2014-07-22 09:12:47,977 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 798.9m
2014-07-22 09:12:47,977 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:12:47,977 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:12:47,977 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:12:48,211 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:12:50,131 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:12:50,135 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:12:50,171 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 84163 synced till here 84141
2014-07-22 09:12:50,388 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045567555 with entries=108, filesize=95.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045570131
2014-07-22 09:12:52,340 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:12:52,364 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045570131 with entries=74, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045572340
2014-07-22 09:12:58,264 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:12:58,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 84332 synced till here 84331
2014-07-22 09:12:58,335 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045572340 with entries=95, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045578264
2014-07-22 09:12:59,907 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16037, memsize=455.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/f152e4364fee497bbc98b390934ba644
2014-07-22 09:12:59,921 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/f152e4364fee497bbc98b390934ba644 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/f152e4364fee497bbc98b390934ba644
2014-07-22 09:12:59,935 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/f152e4364fee497bbc98b390934ba644, entries=1657720, sequenceid=16037, filesize=118.1m
2014-07-22 09:12:59,936 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~904.1m/948055840, currentsize=467.3m/490031360 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 35733ms, sequenceid=16037, compaction requested=true
2014-07-22 09:12:59,936 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:12:59,936 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 41 store files, 0 compacting, 41 eligible, 2000 blocking
2014-07-22 09:12:59,937 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 800.5m
2014-07-22 09:12:59,937 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 41 files from compaction candidates
2014-07-22 09:12:59,937 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:12:59,937 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:12:59,937 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:12:59,977 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:13:00,909 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:13:00,931 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 84420 synced till here 84410
2014-07-22 09:13:01,053 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045578264 with entries=88, filesize=72.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045580910
2014-07-22 09:13:01,053 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045485334
2014-07-22 09:13:01,053 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045487380
2014-07-22 09:13:01,053 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045489239
2014-07-22 09:13:01,053 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045491197
2014-07-22 09:13:01,053 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045492761
2014-07-22 09:13:01,053 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045494525
2014-07-22 09:13:01,053 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045496472
2014-07-22 09:13:01,053 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045498479
2014-07-22 09:13:01,054 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045500520
2014-07-22 09:13:01,054 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045502038
2014-07-22 09:13:01,054 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045503707
2014-07-22 09:13:01,054 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045504672
2014-07-22 09:13:01,054 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045508281
2014-07-22 09:13:01,054 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045510596
2014-07-22 09:13:01,260 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:13:02,353 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:13:02,560 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 84502 synced till here 84489
2014-07-22 09:13:02,867 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045580910 with entries=82, filesize=71.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045582353
2014-07-22 09:13:04,212 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:13:04,239 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 84585 synced till here 84584
2014-07-22 09:13:04,262 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045582353 with entries=83, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045584212
2014-07-22 09:13:05,926 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:13:05,949 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 84676 synced till here 84666
2014-07-22 09:13:06,027 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045584212 with entries=91, filesize=68.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045585926
2014-07-22 09:13:07,539 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16259, memsize=335.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/199d6bc1a0304b5ca7290ff26ecdd16c
2014-07-22 09:13:07,554 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/199d6bc1a0304b5ca7290ff26ecdd16c as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/199d6bc1a0304b5ca7290ff26ecdd16c
2014-07-22 09:13:07,568 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/199d6bc1a0304b5ca7290ff26ecdd16c, entries=1220160, sequenceid=16259, filesize=86.8m
2014-07-22 09:13:07,569 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~802.1m/841011920, currentsize=217.6m/228214240 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 19592ms, sequenceid=16259, compaction requested=true
2014-07-22 09:13:07,569 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:13:07,570 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 41 store files, 0 compacting, 41 eligible, 2000 blocking
2014-07-22 09:13:07,570 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 710.6m
2014-07-22 09:13:07,570 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 41 files from compaction candidates
2014-07-22 09:13:07,570 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:13:07,570 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:13:07,570 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:13:07,904 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:13:08,801 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 84793 synced till here 84781
2014-07-22 09:13:08,884 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045585926 with entries=117, filesize=88.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045587904
2014-07-22 09:13:08,884 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045511770
2014-07-22 09:13:08,884 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045514760
2014-07-22 09:13:08,884 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045517257
2014-07-22 09:13:09,023 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:13:09,477 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:13:09,676 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:13:10,404 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 84894 synced till here 84879
2014-07-22 09:13:10,498 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=6603, hits=3, hitRatio=0.04%, , cachingAccesses=5, cachingHits=3, cachingHitsRatio=60.00%, evictions=0, evicted=0, evictedPerRun=NaN
2014-07-22 09:13:10,501 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045587904 with entries=101, filesize=76.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045589677
2014-07-22 09:13:11,420 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:13:11,450 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 84987 synced till here 84977
2014-07-22 09:13:11,531 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045589677 with entries=93, filesize=68.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045591421
2014-07-22 09:13:12,909 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:13:13,224 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045591421 with entries=116, filesize=79.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045592910
2014-07-22 09:13:14,662 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:13:14,688 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 85185 synced till here 85178
2014-07-22 09:13:14,774 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045592910 with entries=82, filesize=67.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045594663
2014-07-22 09:13:16,558 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:13:16,627 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 85302 synced till here 85276
2014-07-22 09:13:18,298 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1080ms
GC pool 'ParNew' had collection(s): count=1 time=1383ms
2014-07-22 09:13:18,322 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045594663 with entries=117, filesize=77.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045596559
2014-07-22 09:13:19,232 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:13:19,358 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 85402 synced till here 85378
2014-07-22 09:13:20,530 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045596559 with entries=100, filesize=85.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045599232
2014-07-22 09:13:22,782 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1357ms
GC pool 'ParNew' had collection(s): count=1 time=1440ms
2014-07-22 09:13:22,843 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:13:22,884 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 85512 synced till here 85491
2014-07-22 09:13:23,089 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045599232 with entries=110, filesize=83.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045602844
2014-07-22 09:13:23,571 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16325, memsize=313.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/344150b47e3f4ad4962ff2965bf07ea8
2014-07-22 09:13:23,586 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/344150b47e3f4ad4962ff2965bf07ea8 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/344150b47e3f4ad4962ff2965bf07ea8
2014-07-22 09:13:23,596 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/344150b47e3f4ad4962ff2965bf07ea8, entries=1142850, sequenceid=16325, filesize=81.3m
2014-07-22 09:13:23,597 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~805.2m/844326000, currentsize=406.6m/426353120 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 23660ms, sequenceid=16325, compaction requested=true
2014-07-22 09:13:23,597 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:13:23,597 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 43 store files, 0 compacting, 43 eligible, 2000 blocking
2014-07-22 09:13:23,598 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 845.0m
2014-07-22 09:13:23,598 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 43 files from compaction candidates
2014-07-22 09:13:23,598 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:13:23,598 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:13:23,598 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:13:23,783 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:13:24,046 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 85599 synced till here 85586
2014-07-22 09:13:24,187 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:13:24,964 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045602844 with entries=87, filesize=75.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045603783
2014-07-22 09:13:24,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045519298
2014-07-22 09:13:24,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045520991
2014-07-22 09:13:24,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045522787
2014-07-22 09:13:24,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045524828
2014-07-22 09:13:24,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045526415
2014-07-22 09:13:24,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045527462
2014-07-22 09:13:24,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045528714
2014-07-22 09:13:24,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045530227
2014-07-22 09:13:25,245 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:13:25,632 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16388, memsize=205.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/45e3628ab34a4a569c218ad4f833fd8d
2014-07-22 09:13:25,644 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/45e3628ab34a4a569c218ad4f833fd8d as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/45e3628ab34a4a569c218ad4f833fd8d
2014-07-22 09:13:25,656 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/45e3628ab34a4a569c218ad4f833fd8d, entries=749450, sequenceid=16388, filesize=53.4m
2014-07-22 09:13:25,656 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~719.2m/754121120, currentsize=297.8m/312262240 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 18086ms, sequenceid=16388, compaction requested=true
2014-07-22 09:13:25,657 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:13:25,657 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 43 store files, 0 compacting, 43 eligible, 2000 blocking
2014-07-22 09:13:25,657 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 531.2m
2014-07-22 09:13:25,657 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 43 files from compaction candidates
2014-07-22 09:13:25,657 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:13:25,657 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:13:25,657 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:13:25,796 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:13:25,796 DEBUG [RpcServer.handler=19,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:13:26,859 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 85705 synced till here 85686
2014-07-22 09:13:26,998 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045603783 with entries=106, filesize=79.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045605796
2014-07-22 09:13:26,999 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045531937
2014-07-22 09:13:26,999 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045534958
2014-07-22 09:13:27,484 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:13:29,843 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1515ms
GC pool 'ParNew' had collection(s): count=1 time=2005ms
2014-07-22 09:13:29,899 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:13:30,010 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 85826 synced till here 85792
2014-07-22 09:13:30,310 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045605796 with entries=121, filesize=83.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045609899
2014-07-22 09:13:32,162 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:13:32,193 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 85925 synced till here 85893
2014-07-22 09:13:32,720 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045609899 with entries=99, filesize=90.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045612163
2014-07-22 09:13:34,500 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:13:34,617 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 86043 synced till here 86030
2014-07-22 09:13:34,781 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045612163 with entries=118, filesize=90.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045614500
2014-07-22 09:13:36,019 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1048ms
GC pool 'ParNew' had collection(s): count=1 time=1047ms
2014-07-22 09:13:36,902 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:13:36,922 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 86169 synced till here 86139
2014-07-22 09:13:38,365 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045614500 with entries=126, filesize=97.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045616903
2014-07-22 09:13:39,104 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:13:39,381 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 86297 synced till here 86264
2014-07-22 09:13:40,428 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045616903 with entries=128, filesize=98.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045619105
2014-07-22 09:13:40,428 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:13:41,246 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:13:42,528 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 86410 synced till here 86389
2014-07-22 09:13:42,705 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045619105 with entries=113, filesize=95.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045621247
2014-07-22 09:13:42,706 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:13:43,383 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:13:44,286 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 86518 synced till here 86493
2014-07-22 09:13:44,466 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045621247 with entries=108, filesize=85.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045623383
2014-07-22 09:13:44,466 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:13:45,541 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:13:45,572 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 86598 synced till here 86597
2014-07-22 09:13:45,594 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045623383 with entries=80, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045625541
2014-07-22 09:13:45,595 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:13:49,561 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:13:49,607 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 86692 synced till here 86691
2014-07-22 09:13:49,637 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045625541 with entries=94, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045629562
2014-07-22 09:13:49,637 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:13:51,177 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16572, memsize=327.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/2759a5fba188438d83a840018556e5c2
2014-07-22 09:13:51,482 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16534, memsize=326.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/c373c4e936d14dccb14596d135fb63c4
2014-07-22 09:13:51,632 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/2759a5fba188438d83a840018556e5c2 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/2759a5fba188438d83a840018556e5c2
2014-07-22 09:13:51,633 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/c373c4e936d14dccb14596d135fb63c4 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/c373c4e936d14dccb14596d135fb63c4
2014-07-22 09:13:51,643 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/2759a5fba188438d83a840018556e5c2, entries=1192070, sequenceid=16572, filesize=84.9m
2014-07-22 09:13:51,644 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~541.9m/568242160, currentsize=343.0m/359611520 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 25987ms, sequenceid=16572, compaction requested=true
2014-07-22 09:13:51,644 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:13:51,645 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 42 store files, 0 compacting, 42 eligible, 2000 blocking
2014-07-22 09:13:51,645 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 42 files from compaction candidates
2014-07-22 09:13:51,645 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 790.6m
2014-07-22 09:13:51,645 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:13:51,645 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:13:51,645 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:13:51,683 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/c373c4e936d14dccb14596d135fb63c4, entries=1189420, sequenceid=16534, filesize=84.7m
2014-07-22 09:13:51,684 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~857.6m/899272080, currentsize=407.7m/427501200 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 28086ms, sequenceid=16534, compaction requested=true
2014-07-22 09:13:51,684 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:13:51,684 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 42 store files, 0 compacting, 42 eligible, 2000 blocking
2014-07-22 09:13:51,684 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 670.8m
2014-07-22 09:13:51,684 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 42 files from compaction candidates
2014-07-22 09:13:51,684 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:13:51,684 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:13:51,684 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:13:51,775 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:13:51,878 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:13:52,206 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:13:52,355 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:13:53,588 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:13:53,810 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 86802 synced till here 86797
2014-07-22 09:13:53,876 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045629562 with entries=110, filesize=75.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045633589
2014-07-22 09:13:53,877 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:13:55,646 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:13:55,676 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 86886 synced till here 86881
2014-07-22 09:13:56,005 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045633589 with entries=84, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045635646
2014-07-22 09:13:56,005 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:13:57,299 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:13:57,438 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 86996 synced till here 86984
2014-07-22 09:13:57,517 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045635646 with entries=110, filesize=78.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045637299
2014-07-22 09:13:57,518 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:13:58,987 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:13:59,313 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 87121 synced till here 87119
2014-07-22 09:13:59,698 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045637299 with entries=125, filesize=89.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045638988
2014-07-22 09:13:59,699 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:14:01,336 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:01,354 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 87202 synced till here 87201
2014-07-22 09:14:01,372 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045638988 with entries=81, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045641337
2014-07-22 09:14:01,372 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:14:02,771 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:02,791 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 87291 synced till here 87286
2014-07-22 09:14:02,868 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045641337 with entries=89, filesize=67.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045642772
2014-07-22 09:14:02,869 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:14:04,017 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:04,039 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 87367 synced till here 87363
2014-07-22 09:14:04,110 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045642772 with entries=76, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045644018
2014-07-22 09:14:04,110 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=44, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:14:05,324 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16791, memsize=197.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/0ab5bfbeb5c842a195b3f746c9799431
2014-07-22 09:14:05,338 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/0ab5bfbeb5c842a195b3f746c9799431 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/0ab5bfbeb5c842a195b3f746c9799431
2014-07-22 09:14:05,359 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/0ab5bfbeb5c842a195b3f746c9799431, entries=717600, sequenceid=16791, filesize=51.1m
2014-07-22 09:14:05,360 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~670.8m/703428720, currentsize=234.7m/246114560 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 13676ms, sequenceid=16791, compaction requested=true
2014-07-22 09:14:05,361 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:14:05,361 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 44 store files, 0 compacting, 44 eligible, 2000 blocking
2014-07-22 09:14:05,362 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 44 files from compaction candidates
2014-07-22 09:14:05,362 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:14:05,362 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:14:05,362 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b., current region memstore size 200.4m
2014-07-22 09:14:05,362 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:14:05,370 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:05,391 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 87446 synced till here 87443
2014-07-22 09:14:05,439 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045644018 with entries=79, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045645370
2014-07-22 09:14:05,578 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:14:06,203 DEBUG [RpcServer.handler=24,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:14:07,061 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:07,081 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 87535 synced till here 87527
2014-07-22 09:14:07,180 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045645370 with entries=89, filesize=70.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045647061
2014-07-22 09:14:08,085 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:08,478 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 87611 synced till here 87609
2014-07-22 09:14:08,507 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045647061 with entries=76, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045648085
2014-07-22 09:14:10,221 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:10,258 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16798, memsize=292.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/8eb0ba5848bb45f2847d97a0bb35a26e
2014-07-22 09:14:10,263 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045648085 with entries=82, filesize=60.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045650222
2014-07-22 09:14:10,288 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/8eb0ba5848bb45f2847d97a0bb35a26e as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/8eb0ba5848bb45f2847d97a0bb35a26e
2014-07-22 09:14:10,312 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/8eb0ba5848bb45f2847d97a0bb35a26e, entries=1063810, sequenceid=16798, filesize=75.8m
2014-07-22 09:14:10,313 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~792.4m/830881920, currentsize=321.6m/337241440 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 18668ms, sequenceid=16798, compaction requested=true
2014-07-22 09:14:10,314 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:14:10,314 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 44 store files, 0 compacting, 44 eligible, 2000 blocking
2014-07-22 09:14:10,314 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 663.5m
2014-07-22 09:14:10,314 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 44 files from compaction candidates
2014-07-22 09:14:10,315 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:14:10,315 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:14:10,315 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:14:10,370 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:14:11,338 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:14:11,608 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:11,656 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045650222 with entries=82, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045651608
2014-07-22 09:14:13,442 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:13,454 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=19949, memsize=149.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/db307704d892476589d185ddac860aba
2014-07-22 09:14:13,469 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 87866 synced till here 87865
2014-07-22 09:14:13,471 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/db307704d892476589d185ddac860aba as hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/db307704d892476589d185ddac860aba
2014-07-22 09:14:13,489 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/db307704d892476589d185ddac860aba, entries=542490, sequenceid=19949, filesize=38.6m
2014-07-22 09:14:13,489 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~200.9m/210648320, currentsize=39.1m/41031600 for region usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. in 8127ms, sequenceid=19949, compaction requested=true
2014-07-22 09:14:13,490 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:14:13,490 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 20 store files, 0 compacting, 20 eligible, 2000 blocking
2014-07-22 09:14:13,490 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 771.8m
2014-07-22 09:14:13,490 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 20 files from compaction candidates
2014-07-22 09:14:13,491 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:14:13,491 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:14:13,491 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. because compaction request was cancelled
2014-07-22 09:14:13,499 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045651608 with entries=91, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045653443
2014-07-22 09:14:13,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045537459
2014-07-22 09:14:13,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045539690
2014-07-22 09:14:13,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045542077
2014-07-22 09:14:13,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045544693
2014-07-22 09:14:13,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045547007
2014-07-22 09:14:13,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045550831
2014-07-22 09:14:13,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045553099
2014-07-22 09:14:13,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045556274
2014-07-22 09:14:13,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045558676
2014-07-22 09:14:13,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045563403
2014-07-22 09:14:13,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045565715
2014-07-22 09:14:13,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045567555
2014-07-22 09:14:13,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045570131
2014-07-22 09:14:13,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045572340
2014-07-22 09:14:13,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045578264
2014-07-22 09:14:13,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045580910
2014-07-22 09:14:13,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045582353
2014-07-22 09:14:13,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045584212
2014-07-22 09:14:13,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045585926
2014-07-22 09:14:13,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045587904
2014-07-22 09:14:13,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045589677
2014-07-22 09:14:13,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045591421
2014-07-22 09:14:13,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045592910
2014-07-22 09:14:13,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045594663
2014-07-22 09:14:13,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045596559
2014-07-22 09:14:13,502 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045599232
2014-07-22 09:14:14,860 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:14:14,927 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:14,984 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 87953 synced till here 87940
2014-07-22 09:14:15,441 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045653443 with entries=87, filesize=72.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045654928
2014-07-22 09:14:17,197 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1145ms
GC pool 'ParNew' had collection(s): count=1 time=1174ms
2014-07-22 09:14:17,644 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:17,802 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 88093 synced till here 88057
2014-07-22 09:14:18,015 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045654928 with entries=140, filesize=85.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045657645
2014-07-22 09:14:19,984 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:20,078 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 88197 synced till here 88180
2014-07-22 09:14:20,216 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045657645 with entries=104, filesize=80.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045659984
2014-07-22 09:14:22,005 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:22,036 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 88309 synced till here 88286
2014-07-22 09:14:22,214 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045659984 with entries=112, filesize=88.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045662006
2014-07-22 09:14:24,347 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:25,428 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 88402 synced till here 88391
2014-07-22 09:14:25,567 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045662006 with entries=93, filesize=87.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045664347
2014-07-22 09:14:26,517 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:26,541 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 88492 synced till here 88475
2014-07-22 09:14:27,636 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045664347 with entries=90, filesize=84.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045666517
2014-07-22 09:14:28,404 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:28,526 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 88570 synced till here 88563
2014-07-22 09:14:28,561 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045666517 with entries=78, filesize=72.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045668404
2014-07-22 09:14:30,099 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:30,122 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 88645 synced till here 88634
2014-07-22 09:14:30,222 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045668404 with entries=75, filesize=69.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045670099
2014-07-22 09:14:31,784 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:32,083 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 88762 synced till here 88754
2014-07-22 09:14:32,137 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045670099 with entries=117, filesize=83.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045671784
2014-07-22 09:14:33,590 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:33,846 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 88876 synced till here 88875
2014-07-22 09:14:33,870 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045671784 with entries=114, filesize=77.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045673590
2014-07-22 09:14:35,227 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:35,247 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 88963 synced till here 88957
2014-07-22 09:14:35,349 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045673590 with entries=87, filesize=68.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045675228
2014-07-22 09:14:35,553 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16970, memsize=335.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/f8b071d951194f8a81a3a10acf319ce7
2014-07-22 09:14:35,588 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/f8b071d951194f8a81a3a10acf319ce7 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/f8b071d951194f8a81a3a10acf319ce7
2014-07-22 09:14:35,615 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/f8b071d951194f8a81a3a10acf319ce7, entries=1222240, sequenceid=16970, filesize=87.0m
2014-07-22 09:14:35,616 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~665.3m/697655200, currentsize=444.2m/465749520 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 25302ms, sequenceid=16970, compaction requested=true
2014-07-22 09:14:35,616 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 43 store files, 0 compacting, 43 eligible, 2000 blocking
2014-07-22 09:14:35,617 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 43 files from compaction candidates
2014-07-22 09:14:35,617 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:14:35,617 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:14:35,617 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:14:35,617 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:14:35,617 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 762.0m
2014-07-22 09:14:36,701 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:14:37,024 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:37,077 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045675228 with entries=84, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045677024
2014-07-22 09:14:37,359 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:14:38,840 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:38,893 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 89130 synced till here 89126
2014-07-22 09:14:38,964 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045677024 with entries=83, filesize=66.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045678840
2014-07-22 09:14:40,559 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:40,577 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 89220 synced till here 89214
2014-07-22 09:14:40,604 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045678840 with entries=90, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045680560
2014-07-22 09:14:41,244 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16999, memsize=367.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/e9254675fd9d438eb893abb7ea51687c
2014-07-22 09:14:41,267 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/e9254675fd9d438eb893abb7ea51687c as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/e9254675fd9d438eb893abb7ea51687c
2014-07-22 09:14:41,324 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/e9254675fd9d438eb893abb7ea51687c, entries=1338480, sequenceid=16999, filesize=95.3m
2014-07-22 09:14:41,325 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~779.3m/817117120, currentsize=477.6m/500771920 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 27835ms, sequenceid=16999, compaction requested=true
2014-07-22 09:14:41,326 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:14:41,326 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 43 store files, 0 compacting, 43 eligible, 2000 blocking
2014-07-22 09:14:41,326 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 43 files from compaction candidates
2014-07-22 09:14:41,326 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 860.1m
2014-07-22 09:14:41,327 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:14:41,327 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:14:41,327 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:14:41,405 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:14:42,363 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:42,392 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 89314 synced till here 89301
2014-07-22 09:14:42,511 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045680560 with entries=94, filesize=74.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045682363
2014-07-22 09:14:42,511 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045602844
2014-07-22 09:14:42,511 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045603783
2014-07-22 09:14:42,511 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045605796
2014-07-22 09:14:42,511 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045609899
2014-07-22 09:14:42,512 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045612163
2014-07-22 09:14:42,512 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045614500
2014-07-22 09:14:42,512 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045616903
2014-07-22 09:14:42,512 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045619105
2014-07-22 09:14:42,512 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045621247
2014-07-22 09:14:42,512 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045623383
2014-07-22 09:14:42,512 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045625541
2014-07-22 09:14:43,101 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:14:43,466 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:44,476 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 89430 synced till here 89414
2014-07-22 09:14:44,547 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045682363 with entries=116, filesize=78.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045683467
2014-07-22 09:14:45,303 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:45,328 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 89525 synced till here 89504
2014-07-22 09:14:46,453 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045683467 with entries=95, filesize=76.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045685303
2014-07-22 09:14:47,226 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:47,256 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 89617 synced till here 89607
2014-07-22 09:14:48,287 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045685303 with entries=92, filesize=68.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045687227
2014-07-22 09:14:48,983 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:49,601 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 89705 synced till here 89695
2014-07-22 09:14:49,706 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045687227 with entries=88, filesize=73.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045688984
2014-07-22 09:14:51,222 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:51,272 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 89787 synced till here 89782
2014-07-22 09:14:51,356 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045688984 with entries=82, filesize=68.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045691223
2014-07-22 09:14:53,379 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:53,476 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 89894 synced till here 89880
2014-07-22 09:14:53,530 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045691223 with entries=107, filesize=82.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045693379
2014-07-22 09:14:55,305 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:55,340 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 89986 synced till here 89974
2014-07-22 09:14:55,444 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045693379 with entries=92, filesize=70.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045695306
2014-07-22 09:14:56,782 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1028ms
GC pool 'ParNew' had collection(s): count=1 time=1087ms
2014-07-22 09:14:57,505 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:14:57,556 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 90086 synced till here 90072
2014-07-22 09:14:57,661 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045695306 with entries=100, filesize=78.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045697506
2014-07-22 09:14:58,910 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:14:58,913 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:14:58,943 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:14:58,976 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:14:58,979 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:14:58,985 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:14:58,990 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:14:59,014 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:14:59,019 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:14:59,034 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:14:59,055 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:14:59,089 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:14:59,118 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:14:59,147 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:14:59,182 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:14:59,198 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:14:59,233 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:14:59,266 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:14:59,297 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:14:59,658 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:14:59,931 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:14:59,967 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:00,002 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:00,212 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:00,254 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:00,264 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:01,164 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:01,210 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:01,251 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:01,299 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:01,341 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:01,404 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:01,463 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:01,510 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:02,908 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:02,972 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:03,010 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:03,059 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:03,129 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:03,129 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:03,130 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:04,467 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5169ms
2014-07-22 09:15:04,469 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5558ms
2014-07-22 09:15:04,469 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5556ms
2014-07-22 09:15:04,470 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5527ms
2014-07-22 09:15:04,470 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5494ms
2014-07-22 09:15:04,470 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5491ms
2014-07-22 09:15:04,470 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5480ms
2014-07-22 09:15:04,471 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5487ms
2014-07-22 09:15:04,471 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5457ms
2014-07-22 09:15:04,471 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5382ms
2014-07-22 09:15:04,472 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5453ms
2014-07-22 09:15:04,472 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5438ms
2014-07-22 09:15:04,473 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5417ms
2014-07-22 09:15:04,473 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5355ms
2014-07-22 09:15:04,474 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5326ms
2014-07-22 09:15:04,476 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5242ms
2014-07-22 09:15:04,476 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5294ms
2014-07-22 09:15:04,477 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5279ms
2014-07-22 09:15:04,477 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5211ms
2014-07-22 09:15:04,490 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:04,512 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:04,659 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:15:04,884 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:04,931 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:15:04,967 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:15:05,002 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:15:05,065 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:05,117 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:05,152 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:05,171 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:05,213 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:15:05,219 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:05,255 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:15:05,265 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:15:05,272 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:15:05,665 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=17223, memsize=434.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/6e19a1d2bc164e85a6110601a0813167
2014-07-22 09:15:05,685 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/6e19a1d2bc164e85a6110601a0813167 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/6e19a1d2bc164e85a6110601a0813167
2014-07-22 09:15:05,696 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/6e19a1d2bc164e85a6110601a0813167, entries=1581100, sequenceid=17223, filesize=112.6m
2014-07-22 09:15:05,697 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~763.7m/800831200, currentsize=395.6m/414836800 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 30080ms, sequenceid=17223, compaction requested=true
2014-07-22 09:15:05,697 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:15:05,697 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 45 store files, 0 compacting, 45 eligible, 2000 blocking
2014-07-22 09:15:05,698 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 426ms
2014-07-22 09:15:05,698 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 45 files from compaction candidates
2014-07-22 09:15:05,698 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,698 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:15:05,698 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5434ms
2014-07-22 09:15:05,698 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,698 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 836.0m
2014-07-22 09:15:05,698 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5444ms
2014-07-22 09:15:05,698 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,698 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 479ms
2014-07-22 09:15:05,698 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,698 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5486ms
2014-07-22 09:15:05,699 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,699 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 528ms
2014-07-22 09:15:05,699 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,699 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 547ms
2014-07-22 09:15:05,699 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,699 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 582ms
2014-07-22 09:15:05,699 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,699 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 634ms
2014-07-22 09:15:05,698 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:15:05,699 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,700 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:15:05,700 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5698ms
2014-07-22 09:15:05,700 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,700 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5734ms
2014-07-22 09:15:05,700 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,700 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5770ms
2014-07-22 09:15:05,700 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,710 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 826ms
2014-07-22 09:15:05,710 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,711 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6052ms
2014-07-22 09:15:05,711 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,712 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1200ms
2014-07-22 09:15:05,712 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,714 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1224ms
2014-07-22 09:15:05,714 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,717 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6451ms
2014-07-22 09:15:05,718 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,721 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6523ms
2014-07-22 09:15:05,721 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,722 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6539ms
2014-07-22 09:15:05,722 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,723 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6490ms
2014-07-22 09:15:05,723 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,723 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6576ms
2014-07-22 09:15:05,723 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,724 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6605ms
2014-07-22 09:15:05,724 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,724 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6669ms
2014-07-22 09:15:05,724 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,725 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6691ms
2014-07-22 09:15:05,725 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,725 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6706ms
2014-07-22 09:15:05,725 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,726 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6636ms
2014-07-22 09:15:05,726 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,727 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6713ms
2014-07-22 09:15:05,727 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,733 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6748ms
2014-07-22 09:15:05,733 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,733 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6743ms
2014-07-22 09:15:05,733 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,734 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6754ms
2014-07-22 09:15:05,734 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,735 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6758ms
2014-07-22 09:15:05,735 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,738 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6794ms
2014-07-22 09:15:05,738 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,738 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6825ms
2014-07-22 09:15:05,738 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,742 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6832ms
2014-07-22 09:15:05,742 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,743 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6445ms
2014-07-22 09:15:05,743 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,743 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2613ms
2014-07-22 09:15:05,743 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,743 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2614ms
2014-07-22 09:15:05,743 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,744 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2616ms
2014-07-22 09:15:05,744 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,744 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2685ms
2014-07-22 09:15:05,744 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,747 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2737ms
2014-07-22 09:15:05,755 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,755 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2783ms
2014-07-22 09:15:05,755 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,755 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2847ms
2014-07-22 09:15:05,755 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,755 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4245ms
2014-07-22 09:15:05,755 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,765 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4302ms
2014-07-22 09:15:05,765 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,769 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4365ms
2014-07-22 09:15:05,769 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,771 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4430ms
2014-07-22 09:15:05,771 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,772 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4472ms
2014-07-22 09:15:05,772 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,773 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4522ms
2014-07-22 09:15:05,773 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,773 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4563ms
2014-07-22 09:15:05,773 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,774 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4609ms
2014-07-22 09:15:05,774 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:15:05,949 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:15:05,975 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:06,053 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 90176 synced till here 90167
2014-07-22 09:15:06,151 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045697506 with entries=90, filesize=70.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045705976
2014-07-22 09:15:06,606 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:15:07,973 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:08,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 90282 synced till here 90273
2014-07-22 09:15:08,114 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045705976 with entries=106, filesize=89.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045707974
2014-07-22 09:15:09,264 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10083,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045699180,"queuetimems":1,"class":"HRegionServer","responsesize":18651,"method":"Multi"}
2014-07-22 09:15:09,548 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:09,572 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 90374 synced till here 90356
2014-07-22 09:15:10,158 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11217,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045698940,"queuetimems":0,"class":"HRegionServer","responsesize":18722,"method":"Multi"}
2014-07-22 09:15:10,160 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10962,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045699197,"queuetimems":1,"class":"HRegionServer","responsesize":9405,"method":"Multi"}
2014-07-22 09:15:10,162 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10899,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045699263,"queuetimems":0,"class":"HRegionServer","responsesize":17086,"method":"Multi"}
2014-07-22 09:15:10,163 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10932,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045699231,"queuetimems":0,"class":"HRegionServer","responsesize":17898,"method":"Multi"}
2014-07-22 09:15:10,170 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11082,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045699087,"queuetimems":1,"class":"HRegionServer","responsesize":19286,"method":"Multi"}
2014-07-22 09:15:10,205 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045707974 with entries=92, filesize=80.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045709548
2014-07-22 09:15:10,231 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10574,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045699656,"queuetimems":0,"class":"HRegionServer","responsesize":17084,"method":"Multi"}
2014-07-22 09:15:10,232 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11215,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045699017,"queuetimems":0,"class":"HRegionServer","responsesize":16952,"method":"Multi"}
2014-07-22 09:15:10,233 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10937,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045699295,"queuetimems":0,"class":"HRegionServer","responsesize":17111,"method":"Multi"}
2014-07-22 09:15:10,357 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11211,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045699145,"queuetimems":0,"class":"HRegionServer","responsesize":15285,"method":"Multi"}
2014-07-22 09:15:12,110 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:12,130 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 90473 synced till here 90472
2014-07-22 09:15:12,143 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045709548 with entries=99, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045712110
2014-07-22 09:15:13,785 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:13,838 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 90566 synced till here 90565
2014-07-22 09:15:13,852 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045712110 with entries=93, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045713786
2014-07-22 09:15:14,210 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=17291, memsize=490.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/a237fc1c499d441c99ee93a801e210bd
2014-07-22 09:15:14,226 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/a237fc1c499d441c99ee93a801e210bd as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/a237fc1c499d441c99ee93a801e210bd
2014-07-22 09:15:14,236 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/a237fc1c499d441c99ee93a801e210bd, entries=1785760, sequenceid=17291, filesize=127.2m
2014-07-22 09:15:14,237 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~868.7m/910934160, currentsize=428.1m/448878320 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 32911ms, sequenceid=17291, compaction requested=true
2014-07-22 09:15:14,238 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:15:14,238 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 45 store files, 0 compacting, 45 eligible, 2000 blocking
2014-07-22 09:15:14,238 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 925.7m
2014-07-22 09:15:14,238 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 45 files from compaction candidates
2014-07-22 09:15:14,238 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:15:14,238 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:15:14,238 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:15:14,250 DEBUG [RpcServer.handler=5,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:15:15,509 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:15,612 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:15:15,883 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 90704 synced till here 90695
2014-07-22 09:15:16,010 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045713786 with entries=138, filesize=90.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045715510
2014-07-22 09:15:16,011 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045629562
2014-07-22 09:15:16,563 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045633589
2014-07-22 09:15:16,563 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045635646
2014-07-22 09:15:16,563 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045637299
2014-07-22 09:15:16,563 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045638988
2014-07-22 09:15:16,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045641337
2014-07-22 09:15:16,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045642772
2014-07-22 09:15:16,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045644018
2014-07-22 09:15:16,626 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:15:17,546 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:17,590 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045715510 with entries=89, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045717546
2014-07-22 09:15:17,590 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:15:19,274 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:19,328 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045717546 with entries=80, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045719276
2014-07-22 09:15:19,329 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:15:21,060 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:21,093 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045719276 with entries=67, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045721060
2014-07-22 09:15:21,094 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:15:22,608 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:22,644 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 91045 synced till here 91037
2014-07-22 09:15:22,758 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045721060 with entries=105, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045722608
2014-07-22 09:15:22,759 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:15:24,709 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:24,732 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 91132 synced till here 91123
2014-07-22 09:15:24,826 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045722608 with entries=87, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045724709
2014-07-22 09:15:24,827 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:15:25,661 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:26,915 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 91235 synced till here 91218
2014-07-22 09:15:27,257 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045724709 with entries=103, filesize=72.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045725661
2014-07-22 09:15:27,257 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:15:27,355 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=17463, memsize=252.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/4faf511191154a9cb65eb24739c9f6ff
2014-07-22 09:15:27,368 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/4faf511191154a9cb65eb24739c9f6ff as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/4faf511191154a9cb65eb24739c9f6ff
2014-07-22 09:15:27,380 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/4faf511191154a9cb65eb24739c9f6ff, entries=917720, sequenceid=17463, filesize=65.4m
2014-07-22 09:15:27,380 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~836.0m/876604800, currentsize=348.3m/365238640 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 21682ms, sequenceid=17463, compaction requested=true
2014-07-22 09:15:27,381 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 44 store files, 0 compacting, 44 eligible, 2000 blocking
2014-07-22 09:15:27,381 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:15:27,381 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 44 files from compaction candidates
2014-07-22 09:15:27,382 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:15:27,382 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:15:27,382 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:15:27,382 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 747.2m
2014-07-22 09:15:27,493 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:15:27,834 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:28,868 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 91347 synced till here 91337
2014-07-22 09:15:28,987 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045725661 with entries=112, filesize=71.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045727835
2014-07-22 09:15:28,987 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:15:29,012 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:15:31,020 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:31,056 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 91449 synced till here 91429
2014-07-22 09:15:31,200 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045727835 with entries=102, filesize=90.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045731021
2014-07-22 09:15:31,201 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:15:33,531 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1547ms
GC pool 'ParNew' had collection(s): count=1 time=1658ms
2014-07-22 09:15:33,814 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:33,879 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 91557 synced till here 91536
2014-07-22 09:15:34,098 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045731021 with entries=108, filesize=91.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045733814
2014-07-22 09:15:34,118 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:15:36,189 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:36,216 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 91665 synced till here 91647
2014-07-22 09:15:36,508 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045733814 with entries=108, filesize=96.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045736190
2014-07-22 09:15:36,509 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=44, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:15:38,293 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:38,380 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 91758 synced till here 91735
2014-07-22 09:15:38,608 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045736190 with entries=93, filesize=85.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045738293
2014-07-22 09:15:38,609 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=45, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:15:40,925 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:41,020 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 91860 synced till here 91852
2014-07-22 09:15:41,122 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045738293 with entries=102, filesize=94.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045740926
2014-07-22 09:15:41,122 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=46, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:15:41,208 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=17547, memsize=262.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/06bb839cfe764f42ac70f8efada49412
2014-07-22 09:15:41,225 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/06bb839cfe764f42ac70f8efada49412 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/06bb839cfe764f42ac70f8efada49412
2014-07-22 09:15:41,235 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/06bb839cfe764f42ac70f8efada49412, entries=956100, sequenceid=17547, filesize=68.1m
2014-07-22 09:15:41,236 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~927.7m/972788160, currentsize=412.8m/432842720 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 26998ms, sequenceid=17547, compaction requested=true
2014-07-22 09:15:41,238 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 44 store files, 0 compacting, 44 eligible, 2000 blocking
2014-07-22 09:15:41,238 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 44 files from compaction candidates
2014-07-22 09:15:41,238 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:15:41,238 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:15:41,238 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:15:41,238 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:15:41,239 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 889.1m
2014-07-22 09:15:41,256 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:15:42,531 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:42,609 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:15:42,720 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 91946 synced till here 91929
2014-07-22 09:15:42,806 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045740926 with entries=86, filesize=80.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045742532
2014-07-22 09:15:42,806 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=47, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:15:44,227 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:44,250 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 92022 synced till here 92019
2014-07-22 09:15:44,307 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045742532 with entries=76, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045744228
2014-07-22 09:15:44,308 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=48, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:15:45,797 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:45,813 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 92123 synced till here 92112
2014-07-22 09:15:45,927 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045744228 with entries=101, filesize=68.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045745797
2014-07-22 09:15:45,930 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=49, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:15:47,485 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:47,511 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 92212 synced till here 92208
2014-07-22 09:15:47,552 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045745797 with entries=89, filesize=66.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045747485
2014-07-22 09:15:47,553 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=50, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:15:48,512 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:49,397 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 92294 synced till here 92291
2014-07-22 09:15:49,436 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045747485 with entries=82, filesize=65.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045748513
2014-07-22 09:15:49,438 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=51, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:15:50,298 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b.
2014-07-22 09:15:50,398 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:51,345 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 92409 synced till here 92403
2014-07-22 09:15:51,403 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045748513 with entries=115, filesize=84.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045750399
2014-07-22 09:15:51,404 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=52, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:15:52,276 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:53,200 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 92524 synced till here 92519
2014-07-22 09:15:53,234 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045750399 with entries=115, filesize=73.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045752276
2014-07-22 09:15:53,235 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=53, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:15:53,552 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=17670, memsize=307.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/3c44d788c3e14d95a71a76f91ecbde3f
2014-07-22 09:15:53,568 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/3c44d788c3e14d95a71a76f91ecbde3f as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/3c44d788c3e14d95a71a76f91ecbde3f
2014-07-22 09:15:53,639 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/3c44d788c3e14d95a71a76f91ecbde3f, entries=1120860, sequenceid=17670, filesize=79.8m
2014-07-22 09:15:53,641 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~747.2m/783529120, currentsize=454.5m/476547360 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 26259ms, sequenceid=17670, compaction requested=true
2014-07-22 09:15:53,641 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 46 store files, 0 compacting, 46 eligible, 2000 blocking
2014-07-22 09:15:53,642 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 46 files from compaction candidates
2014-07-22 09:15:53,642 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:15:53,642 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:15:53,642 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:15:53,642 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:15:53,642 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b., current region memstore size 267.5m
2014-07-22 09:15:53,776 DEBUG [RpcServer.handler=6,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:15:53,902 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:15:54,153 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:54,188 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045752276 with entries=70, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045754153
2014-07-22 09:15:55,664 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:55,682 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 92688 synced till here 92685
2014-07-22 09:15:55,717 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045754153 with entries=94, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045755664
2014-07-22 09:15:57,222 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:57,241 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 92773 synced till here 92767
2014-07-22 09:15:57,309 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045755664 with entries=85, filesize=67.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045757222
2014-07-22 09:15:59,010 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:15:59,033 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 92862 synced till here 92857
2014-07-22 09:15:59,098 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045757222 with entries=89, filesize=67.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045759010
2014-07-22 09:16:00,732 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:16:00,758 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 92950 synced till here 92949
2014-07-22 09:16:00,778 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045759010 with entries=88, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045760733
2014-07-22 09:16:02,725 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:16:02,765 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 93038 synced till here 93036
2014-07-22 09:16:02,786 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045760733 with entries=88, filesize=70.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045762726
2014-07-22 09:16:03,755 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=17817, memsize=268.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/ee818b0c3e8440a8bc791f81c3f847d3
2014-07-22 09:16:03,768 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/ee818b0c3e8440a8bc791f81c3f847d3 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/ee818b0c3e8440a8bc791f81c3f847d3
2014-07-22 09:16:03,779 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/ee818b0c3e8440a8bc791f81c3f847d3, entries=976520, sequenceid=17817, filesize=69.6m
2014-07-22 09:16:03,780 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~889.1m/932294800, currentsize=386.1m/404816560 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 22541ms, sequenceid=17817, compaction requested=true
2014-07-22 09:16:03,780 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:16:03,781 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 46 store files, 0 compacting, 46 eligible, 2000 blocking
2014-07-22 09:16:03,781 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 984.7m
2014-07-22 09:16:03,781 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 46 files from compaction candidates
2014-07-22 09:16:03,781 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:16:03,781 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:16:03,781 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:16:03,938 DEBUG [RpcServer.handler=7,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:16:04,443 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:16:04,462 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 93109 synced till here 93106
2014-07-22 09:16:04,511 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045762726 with entries=71, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045764443
2014-07-22 09:16:04,543 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=21041, memsize=193.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/8521e62083ff48dd9bbaab48a25a77e0
2014-07-22 09:16:04,558 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/8521e62083ff48dd9bbaab48a25a77e0 as hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/8521e62083ff48dd9bbaab48a25a77e0
2014-07-22 09:16:05,031 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/8521e62083ff48dd9bbaab48a25a77e0, entries=703590, sequenceid=21041, filesize=50.2m
2014-07-22 09:16:05,031 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~267.5m/280462000, currentsize=36.6m/38389600 for region usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. in 11389ms, sequenceid=21041, compaction requested=true
2014-07-22 09:16:05,032 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:16:05,032 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 2000 blocking
2014-07-22 09:16:05,032 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 851.8m
2014-07-22 09:16:05,032 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-22 09:16:05,032 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:16:05,032 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:16:05,032 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. because compaction request was cancelled
2014-07-22 09:16:05,213 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:16:05,826 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:16:05,971 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:16:06,002 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 93211 synced till here 93203
2014-07-22 09:16:06,078 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045764443 with entries=102, filesize=70.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045765972
2014-07-22 09:16:06,079 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045645370
2014-07-22 09:16:06,079 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045647061
2014-07-22 09:16:06,079 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045648085
2014-07-22 09:16:06,079 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045650222
2014-07-22 09:16:06,079 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045651608
2014-07-22 09:16:06,079 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045653443
2014-07-22 09:16:06,079 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045654928
2014-07-22 09:16:06,079 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045657645
2014-07-22 09:16:06,079 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045659984
2014-07-22 09:16:06,079 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045662006
2014-07-22 09:16:06,079 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045664347
2014-07-22 09:16:06,079 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045666517
2014-07-22 09:16:06,079 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045668404
2014-07-22 09:16:06,079 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045670099
2014-07-22 09:16:06,079 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045671784
2014-07-22 09:16:06,079 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045673590
2014-07-22 09:16:06,079 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045675228
2014-07-22 09:16:06,080 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045677024
2014-07-22 09:16:06,080 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045678840
2014-07-22 09:16:06,080 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045680560
2014-07-22 09:16:06,080 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045682363
2014-07-22 09:16:06,080 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045683467
2014-07-22 09:16:06,080 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045685303
2014-07-22 09:16:06,080 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045687227
2014-07-22 09:16:06,080 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045688984
2014-07-22 09:16:06,080 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045691223
2014-07-22 09:16:06,080 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045693379
2014-07-22 09:16:06,080 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045695306
2014-07-22 09:16:07,882 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:16:07,918 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 93325 synced till here 93319
2014-07-22 09:16:07,931 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045765972 with entries=114, filesize=71.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045767882
2014-07-22 09:16:09,255 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:16:09,402 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 93414 synced till here 93410
2014-07-22 09:16:09,464 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045767882 with entries=89, filesize=76.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045769255
2014-07-22 09:16:10,973 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:16:10,996 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 93497 synced till here 93492
2014-07-22 09:16:11,057 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045769255 with entries=83, filesize=66.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045770974
2014-07-22 09:16:12,607 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:16:12,636 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 93586 synced till here 93584
2014-07-22 09:16:12,705 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045770974 with entries=89, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045772607
2014-07-22 09:16:14,738 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:16:14,757 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 93663 synced till here 93658
2014-07-22 09:16:14,799 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045772607 with entries=77, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045774738
2014-07-22 09:16:16,388 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:16:16,429 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 93744 synced till here 93743
2014-07-22 09:16:16,451 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045774738 with entries=81, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045776389
2014-07-22 09:16:17,411 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:16:17,436 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 93823 synced till here 93821
2014-07-22 09:16:17,488 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045776389 with entries=79, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045777411
2014-07-22 09:16:23,647 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:16:23,796 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045777411 with entries=91, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045783649
2014-07-22 09:16:24,816 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:24,818 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:24,895 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:24,963 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:24,979 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,035 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,100 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,124 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,191 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,197 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,250 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,251 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,252 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,482 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,483 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,484 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,485 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,579 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,580 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,581 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,581 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,582 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,649 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,649 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,650 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,656 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,698 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,698 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,698 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,699 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,733 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,770 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,812 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,849 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,887 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,887 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,900 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,900 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,903 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,903 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,905 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,906 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:25,907 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:27,004 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:27,004 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:27,006 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:27,006 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:27,007 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:27,012 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:27,032 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:29,349 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=18032, memsize=436.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/3df18850d68146ab80147c3393b6e013
2014-07-22 09:16:29,369 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/3df18850d68146ab80147c3393b6e013 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/3df18850d68146ab80147c3393b6e013
2014-07-22 09:16:29,386 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/3df18850d68146ab80147c3393b6e013, entries=1589920, sequenceid=18032, filesize=113.2m
2014-07-22 09:16:29,387 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~853.6m/895100480, currentsize=265.6m/278539920 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 24355ms, sequenceid=18032, compaction requested=true
2014-07-22 09:16:29,388 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:16:29,388 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 45 store files, 0 compacting, 45 eligible, 2000 blocking
2014-07-22 09:16:29,388 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2356ms
2014-07-22 09:16:29,388 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 45 files from compaction candidates
2014-07-22 09:16:29,388 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,388 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:16:29,388 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2376ms
2014-07-22 09:16:29,389 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,388 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 912.1m
2014-07-22 09:16:29,389 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2382ms
2014-07-22 09:16:29,389 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,388 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:16:29,389 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2383ms
2014-07-22 09:16:29,389 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,389 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:16:29,389 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2384ms
2014-07-22 09:16:29,389 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,389 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2385ms
2014-07-22 09:16:29,389 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,390 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2386ms
2014-07-22 09:16:29,390 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,390 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3483ms
2014-07-22 09:16:29,390 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,390 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3484ms
2014-07-22 09:16:29,390 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,390 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3485ms
2014-07-22 09:16:29,390 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,390 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3487ms
2014-07-22 09:16:29,391 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,391 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3488ms
2014-07-22 09:16:29,391 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,391 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3491ms
2014-07-22 09:16:29,391 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,392 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3492ms
2014-07-22 09:16:29,392 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,396 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3508ms
2014-07-22 09:16:29,396 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,396 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3509ms
2014-07-22 09:16:29,397 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,397 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3549ms
2014-07-22 09:16:29,397 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,397 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3585ms
2014-07-22 09:16:29,397 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,397 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3627ms
2014-07-22 09:16:29,397 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,398 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3664ms
2014-07-22 09:16:29,398 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,398 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3699ms
2014-07-22 09:16:29,398 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,409 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3712ms
2014-07-22 09:16:29,409 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,409 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3711ms
2014-07-22 09:16:29,410 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,410 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3713ms
2014-07-22 09:16:29,410 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,410 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3754ms
2014-07-22 09:16:29,411 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,411 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3761ms
2014-07-22 09:16:29,411 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,411 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3762ms
2014-07-22 09:16:29,411 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,411 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3762ms
2014-07-22 09:16:29,411 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,411 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3829ms
2014-07-22 09:16:29,411 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,414 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3832ms
2014-07-22 09:16:29,414 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,414 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3833ms
2014-07-22 09:16:29,414 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,417 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3837ms
2014-07-22 09:16:29,417 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,417 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3839ms
2014-07-22 09:16:29,417 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,418 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3933ms
2014-07-22 09:16:29,418 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,419 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3935ms
2014-07-22 09:16:29,419 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,419 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3936ms
2014-07-22 09:16:29,419 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,419 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3937ms
2014-07-22 09:16:29,420 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,422 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4170ms
2014-07-22 09:16:29,422 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,426 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4175ms
2014-07-22 09:16:29,426 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,427 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4177ms
2014-07-22 09:16:29,427 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,429 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4232ms
2014-07-22 09:16:29,429 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,431 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4240ms
2014-07-22 09:16:29,431 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,433 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4325ms
2014-07-22 09:16:29,433 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,433 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4333ms
2014-07-22 09:16:29,433 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,433 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4398ms
2014-07-22 09:16:29,433 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,434 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4455ms
2014-07-22 09:16:29,434 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,435 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4472ms
2014-07-22 09:16:29,435 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,436 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4542ms
2014-07-22 09:16:29,436 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,437 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4618ms
2014-07-22 09:16:29,437 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,437 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4621ms
2014-07-22 09:16:29,437 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:16:29,923 DEBUG [RpcServer.handler=6,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:16:30,220 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:16:30,310 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 94068 synced till here 94063
2014-07-22 09:16:30,398 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045783649 with entries=154, filesize=74.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045790220
2014-07-22 09:16:31,741 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:16:32,692 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:16:32,781 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 94183 synced till here 94148
2014-07-22 09:16:34,193 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045790220 with entries=115, filesize=97.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045792692
2014-07-22 09:16:34,383 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=18025, memsize=489.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/e5ecd775365b4d51abcb7ac4c886b4c8
2014-07-22 09:16:34,404 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/e5ecd775365b4d51abcb7ac4c886b4c8 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/e5ecd775365b4d51abcb7ac4c886b4c8
2014-07-22 09:16:34,418 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/e5ecd775365b4d51abcb7ac4c886b4c8, entries=1780560, sequenceid=18025, filesize=126.9m
2014-07-22 09:16:34,419 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~984.7m/1032570400, currentsize=365.4m/383180800 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 30638ms, sequenceid=18025, compaction requested=true
2014-07-22 09:16:34,419 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:16:34,419 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 45 store files, 0 compacting, 45 eligible, 2000 blocking
2014-07-22 09:16:34,420 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 45 files from compaction candidates
2014-07-22 09:16:34,420 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 753.2m
2014-07-22 09:16:34,420 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:16:34,420 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:16:34,420 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:16:34,478 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:16:34,983 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:16:35,003 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10050,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045784948,"queuetimems":0,"class":"HRegionServer","responsesize":19177,"method":"Multi"}
2014-07-22 09:16:35,028 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 94285 synced till here 94258
2014-07-22 09:16:35,110 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:16:35,931 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045792692 with entries=102, filesize=84.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045794984
2014-07-22 09:16:35,931 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045697506
2014-07-22 09:16:35,932 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045705976
2014-07-22 09:16:35,932 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045707974
2014-07-22 09:16:35,932 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045709548
2014-07-22 09:16:35,932 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045712110
2014-07-22 09:16:35,932 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045713786
2014-07-22 09:16:35,932 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045715510
2014-07-22 09:16:35,932 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045717546
2014-07-22 09:16:35,932 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045719276
2014-07-22 09:16:35,932 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045721060
2014-07-22 09:16:35,932 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045722608
2014-07-22 09:16:35,932 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045724709
2014-07-22 09:16:36,917 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:16:37,922 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 94396 synced till here 94390
2014-07-22 09:16:37,990 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045794984 with entries=111, filesize=90.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045796917
2014-07-22 09:16:39,941 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:16:40,006 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 94511 synced till here 94494
2014-07-22 09:16:40,201 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045796917 with entries=115, filesize=97.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045799942
2014-07-22 09:16:42,337 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:16:42,407 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 94621 synced till here 94587
2014-07-22 09:16:42,791 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045799942 with entries=110, filesize=91.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045802338
2014-07-22 09:16:45,373 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1837ms
GC pool 'ParNew' had collection(s): count=1 time=1986ms
2014-07-22 09:16:45,864 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:16:45,923 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 94724 synced till here 94695
2014-07-22 09:16:46,162 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045802338 with entries=103, filesize=84.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045805865
2014-07-22 09:16:48,086 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:16:48,175 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 94840 synced till here 94812
2014-07-22 09:16:48,495 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045805865 with entries=116, filesize=96.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045808087
2014-07-22 09:16:50,444 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:16:50,460 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 94945 synced till here 94921
2014-07-22 09:16:50,746 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045808087 with entries=105, filesize=86.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045810445
2014-07-22 09:16:52,504 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:16:52,538 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 95021 synced till here 95020
2014-07-22 09:16:52,591 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045810445 with entries=76, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045812505
2014-07-22 09:16:54,936 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:16:54,960 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045812505 with entries=78, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045814936
2014-07-22 09:16:56,588 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:16:56,635 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 95213 synced till here 95192
2014-07-22 09:16:56,786 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045814936 with entries=114, filesize=70.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045816589
2014-07-22 09:16:57,243 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:57,244 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:57,295 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:57,296 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:57,297 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:57,297 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:57,297 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:57,321 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:57,321 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:57,321 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:57,329 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:57,407 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:57,448 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:57,450 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:57,474 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:57,488 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:59,500 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:59,545 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:59,587 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:59,627 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:59,667 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:59,704 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:59,723 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:59,746 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:16:59,762 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:00,502 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:00,507 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:00,568 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:01,068 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:01,445 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:01,452 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:01,452 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:01,529 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:01,592 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:02,243 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:17:02,244 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:17:02,296 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:17:02,297 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:17:02,297 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:17:02,297 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:17:02,298 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:17:02,321 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:17:02,321 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:17:02,322 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:17:02,330 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:17:02,407 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:17:02,449 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:17:02,450 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:17:02,475 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:17:02,488 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:17:02,534 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:03,599 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:03,641 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:03,683 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:03,727 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:03,776 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:03,827 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:03,842 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:03,867 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:03,894 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:03,916 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:03,931 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:04,501 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:17:04,545 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:17:04,587 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:17:04,627 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:17:04,669 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 09:17:04,705 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:17:04,724 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:17:04,746 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:17:04,763 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:17:05,502 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:17:05,508 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:17:05,539 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:05,544 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:05,545 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:05,569 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:17:05,620 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:06,069 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:17:06,076 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=18193, memsize=510.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/f4a1cb38248b42299fd042c93d1ed10a
2014-07-22 09:17:06,086 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/f4a1cb38248b42299fd042c93d1ed10a as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/f4a1cb38248b42299fd042c93d1ed10a
2014-07-22 09:17:06,094 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/f4a1cb38248b42299fd042c93d1ed10a, entries=1858140, sequenceid=18193, filesize=132.3m
2014-07-22 09:17:06,095 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~912.1m/956418560, currentsize=437.0m/458227200 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 36707ms, sequenceid=18193, compaction requested=true
2014-07-22 09:17:06,095 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:17:06,096 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5028ms
2014-07-22 09:17:06,096 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,096 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 47 store files, 0 compacting, 47 eligible, 2000 blocking
2014-07-22 09:17:06,096 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 688.5m
2014-07-22 09:17:06,096 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 47 files from compaction candidates
2014-07-22 09:17:06,096 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:17:06,096 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:17:06,096 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:17:06,097 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 477ms
2014-07-22 09:17:06,097 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,097 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5529ms
2014-07-22 09:17:06,098 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,098 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 553ms
2014-07-22 09:17:06,098 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,113 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 569ms
2014-07-22 09:17:06,113 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,113 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 574ms
2014-07-22 09:17:06,114 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,114 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5607ms
2014-07-22 09:17:06,114 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,114 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5612ms
2014-07-22 09:17:06,114 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,114 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6352ms
2014-07-22 09:17:06,114 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,115 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6369ms
2014-07-22 09:17:06,115 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,123 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6400ms
2014-07-22 09:17:06,123 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,123 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6419ms
2014-07-22 09:17:06,123 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,123 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6456ms
2014-07-22 09:17:06,123 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,123 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6497ms
2014-07-22 09:17:06,123 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,124 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6537ms
2014-07-22 09:17:06,124 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,124 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6579ms
2014-07-22 09:17:06,124 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,124 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6624ms
2014-07-22 09:17:06,124 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,124 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2193ms
2014-07-22 09:17:06,124 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,125 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2209ms
2014-07-22 09:17:06,125 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,125 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2231ms
2014-07-22 09:17:06,125 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,125 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2258ms
2014-07-22 09:17:06,125 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,126 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2284ms
2014-07-22 09:17:06,126 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,134 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2306ms
2014-07-22 09:17:06,134 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,134 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2358ms
2014-07-22 09:17:06,134 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,135 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2408ms
2014-07-22 09:17:06,135 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,135 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2452ms
2014-07-22 09:17:06,135 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,136 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2494ms
2014-07-22 09:17:06,136 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,136 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2537ms
2014-07-22 09:17:06,136 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,136 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3602ms
2014-07-22 09:17:06,136 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,136 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8648ms
2014-07-22 09:17:06,136 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,137 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8663ms
2014-07-22 09:17:06,137 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,137 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8687ms
2014-07-22 09:17:06,138 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,139 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8690ms
2014-07-22 09:17:06,139 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,141 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8734ms
2014-07-22 09:17:06,141 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,141 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8812ms
2014-07-22 09:17:06,142 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,142 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8821ms
2014-07-22 09:17:06,143 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,143 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8822ms
2014-07-22 09:17:06,143 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,143 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8822ms
2014-07-22 09:17:06,143 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,143 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8846ms
2014-07-22 09:17:06,143 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,144 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8847ms
2014-07-22 09:17:06,144 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,145 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8848ms
2014-07-22 09:17:06,145 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,146 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8850ms
2014-07-22 09:17:06,146 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,146 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8851ms
2014-07-22 09:17:06,146 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,146 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8902ms
2014-07-22 09:17:06,146 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,148 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8905ms
2014-07-22 09:17:06,148 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,149 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4556ms
2014-07-22 09:17:06,149 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,149 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4620ms
2014-07-22 09:17:06,149 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,150 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4698ms
2014-07-22 09:17:06,150 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,150 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4699ms
2014-07-22 09:17:06,150 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,151 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4706ms
2014-07-22 09:17:06,151 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:06,678 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:17:06,680 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:17:07,572 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 95354 synced till here 95329
2014-07-22 09:17:07,713 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10638,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045817075,"queuetimems":0,"class":"HRegionServer","responsesize":6742,"method":"Multi"}
2014-07-22 09:17:07,713 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10741,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045816972,"queuetimems":1,"class":"HRegionServer","responsesize":10138,"method":"Multi"}
2014-07-22 09:17:07,715 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11011,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045816703,"queuetimems":0,"class":"HRegionServer","responsesize":18295,"method":"Multi"}
2014-07-22 09:17:07,715 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10835,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045816879,"queuetimems":0,"class":"HRegionServer","responsesize":18799,"method":"Multi"}
2014-07-22 09:17:07,715 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10899,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045816815,"queuetimems":0,"class":"HRegionServer","responsesize":19211,"method":"Multi"}
2014-07-22 09:17:07,736 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045816589 with entries=141, filesize=85.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045826679
2014-07-22 09:17:07,736 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045725661
2014-07-22 09:17:07,736 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045727835
2014-07-22 09:17:07,736 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045731021
2014-07-22 09:17:07,736 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045733814
2014-07-22 09:17:07,736 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045736190
2014-07-22 09:17:07,739 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045738293
2014-07-22 09:17:07,811 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:17:07,963 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10750,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045817213,"queuetimems":0,"class":"HRegionServer","responsesize":7127,"method":"Multi"}
2014-07-22 09:17:08,291 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11185,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045817105,"queuetimems":1,"class":"HRegionServer","responsesize":9567,"method":"Multi"}
2014-07-22 09:17:08,291 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11091,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045817199,"queuetimems":0,"class":"HRegionServer","responsesize":9232,"method":"Multi"}
2014-07-22 09:17:09,556 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:17:09,695 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 95477 synced till here 95449
2014-07-22 09:17:09,947 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045826679 with entries=123, filesize=95.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045829557
2014-07-22 09:17:10,179 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10515,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045819663,"queuetimems":1,"class":"HRegionServer","responsesize":18799,"method":"Multi"}
2014-07-22 09:17:10,179 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10554,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045819624,"queuetimems":0,"class":"HRegionServer","responsesize":18388,"method":"Multi"}
2014-07-22 09:17:10,181 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12777,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045817403,"queuetimems":1,"class":"HRegionServer","responsesize":19140,"method":"Multi"}
2014-07-22 09:17:10,181 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12734,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045817447,"queuetimems":0,"class":"HRegionServer","responsesize":19655,"method":"Multi"}
2014-07-22 09:17:10,181 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10683,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045819497,"queuetimems":1,"class":"HRegionServer","responsesize":18858,"method":"Multi"}
2014-07-22 09:17:10,206 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10664,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045819542,"queuetimems":0,"class":"HRegionServer","responsesize":18810,"method":"Multi"}
2014-07-22 09:17:10,208 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10624,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045819583,"queuetimems":1,"class":"HRegionServer","responsesize":19211,"method":"Multi"}
2014-07-22 09:17:10,914 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:17:10,933 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 95556 synced till here 95546
2014-07-22 09:17:11,104 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045829557 with entries=79, filesize=69.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045830914
2014-07-22 09:17:12,087 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=18252, memsize=560.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/1aee5425c2c9406a82f9d77168c15208
2014-07-22 09:17:12,103 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/1aee5425c2c9406a82f9d77168c15208 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/1aee5425c2c9406a82f9d77168c15208
2014-07-22 09:17:12,113 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/1aee5425c2c9406a82f9d77168c15208, entries=2042400, sequenceid=18252, filesize=145.5m
2014-07-22 09:17:12,113 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~755.1m/791747120, currentsize=448.6m/470355680 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 37693ms, sequenceid=18252, compaction requested=true
2014-07-22 09:17:12,114 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:17:12,114 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 47 store files, 0 compacting, 47 eligible, 2000 blocking
2014-07-22 09:17:12,114 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 820.7m
2014-07-22 09:17:12,114 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 47 files from compaction candidates
2014-07-22 09:17:12,114 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:17:12,114 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:17:12,114 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:17:12,129 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:17:12,742 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:17:12,767 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 95639 synced till here 95637
2014-07-22 09:17:12,824 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045830914 with entries=83, filesize=65.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045832743
2014-07-22 09:17:12,824 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045740926
2014-07-22 09:17:12,824 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045742532
2014-07-22 09:17:12,824 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045744228
2014-07-22 09:17:12,825 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045745797
2014-07-22 09:17:12,825 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045747485
2014-07-22 09:17:12,825 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045748513
2014-07-22 09:17:12,825 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045750399
2014-07-22 09:17:12,900 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:17:13,916 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:17:13,935 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 95727 synced till here 95717
2014-07-22 09:17:14,006 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045832743 with entries=88, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045833917
2014-07-22 09:17:14,008 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:17:15,330 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:17:15,414 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 95840 synced till here 95826
2014-07-22 09:17:15,518 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045833917 with entries=113, filesize=71.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045835331
2014-07-22 09:17:15,519 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:17:16,941 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:17:17,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 95951 synced till here 95950
2014-07-22 09:17:17,172 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045835331 with entries=111, filesize=74.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045836942
2014-07-22 09:17:17,173 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:17:18,367 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:17:18,413 DEBUG [RpcServer.handler=5,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b.
2014-07-22 09:17:18,640 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 96069 synced till here 96067
2014-07-22 09:17:18,684 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045836942 with entries=118, filesize=78.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045838368
2014-07-22 09:17:18,685 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:17:20,543 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:17:20,562 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 96177 synced till here 96172
2014-07-22 09:17:20,641 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045838368 with entries=108, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045840543
2014-07-22 09:17:20,642 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:17:22,363 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:17:22,389 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 96273 synced till here 96263
2014-07-22 09:17:22,451 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045840543 with entries=96, filesize=69.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045842363
2014-07-22 09:17:22,452 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:17:23,276 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:17:23,959 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 96365 synced till here 96357
2014-07-22 09:17:24,012 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045842363 with entries=92, filesize=65.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045843276
2014-07-22 09:17:24,013 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:17:24,916 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:17:24,954 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 96470 synced till here 96462
2014-07-22 09:17:26,129 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045843276 with entries=105, filesize=69.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045844916
2014-07-22 09:17:26,130 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:17:26,889 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:17:28,209 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1091ms
GC pool 'ParNew' had collection(s): count=1 time=1125ms
2014-07-22 09:17:28,227 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 96578 synced till here 96575
2014-07-22 09:17:28,227 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,228 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,228 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,228 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,229 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,229 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,229 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,239 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,242 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,242 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,248 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,254 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,258 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,258 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,258 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,259 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045844916 with entries=108, filesize=82.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045846890
2014-07-22 09:17:28,259 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:17:28,260 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,260 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,282 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,299 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,300 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,301 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,301 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,301 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,301 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,301 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,302 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,316 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,316 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,333 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,334 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,344 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,346 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,347 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,384 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,420 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,455 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,456 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,457 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,457 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,495 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,496 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,497 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,534 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,573 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,604 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,634 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,670 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,701 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,735 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:28,769 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:29,105 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=18446, memsize=307.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/ebf081e7da35437e9cd32366fcacf168
2014-07-22 09:17:29,124 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/ebf081e7da35437e9cd32366fcacf168 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/ebf081e7da35437e9cd32366fcacf168
2014-07-22 09:17:29,137 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/ebf081e7da35437e9cd32366fcacf168, entries=1119460, sequenceid=18446, filesize=79.8m
2014-07-22 09:17:29,137 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~688.5m/721948240, currentsize=396.4m/415672400 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 23041ms, sequenceid=18446, compaction requested=true
2014-07-22 09:17:29,138 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:17:29,138 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 46 store files, 0 compacting, 46 eligible, 2000 blocking
2014-07-22 09:17:29,138 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 369ms
2014-07-22 09:17:29,138 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 46 files from compaction candidates
2014-07-22 09:17:29,138 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,138 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:17:29,138 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 828.6m
2014-07-22 09:17:29,138 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 403ms
2014-07-22 09:17:29,139 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,138 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:17:29,139 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 438ms
2014-07-22 09:17:29,139 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:17:29,139 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,140 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 471ms
2014-07-22 09:17:29,140 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,140 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 506ms
2014-07-22 09:17:29,140 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,140 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 536ms
2014-07-22 09:17:29,141 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,141 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 569ms
2014-07-22 09:17:29,141 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,145 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 611ms
2014-07-22 09:17:29,145 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,147 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 649ms
2014-07-22 09:17:29,147 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,147 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 651ms
2014-07-22 09:17:29,147 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,148 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 652ms
2014-07-22 09:17:29,148 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,149 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 691ms
2014-07-22 09:17:29,149 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,152 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 695ms
2014-07-22 09:17:29,153 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,159 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 703ms
2014-07-22 09:17:29,159 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,159 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 704ms
2014-07-22 09:17:29,159 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,159 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 739ms
2014-07-22 09:17:29,159 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,165 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 781ms
2014-07-22 09:17:29,166 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,167 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 819ms
2014-07-22 09:17:29,167 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,167 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 821ms
2014-07-22 09:17:29,167 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,167 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 823ms
2014-07-22 09:17:29,167 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,170 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 834ms
2014-07-22 09:17:29,170 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,174 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 840ms
2014-07-22 09:17:29,174 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,174 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 858ms
2014-07-22 09:17:29,174 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,175 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 859ms
2014-07-22 09:17:29,175 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,176 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 874ms
2014-07-22 09:17:29,176 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,178 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 876ms
2014-07-22 09:17:29,178 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,179 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 877ms
2014-07-22 09:17:29,179 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,179 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 878ms
2014-07-22 09:17:29,179 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,185 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 884ms
2014-07-22 09:17:29,185 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,185 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 884ms
2014-07-22 09:17:29,185 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,185 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 885ms
2014-07-22 09:17:29,185 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,186 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 886ms
2014-07-22 09:17:29,186 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,186 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 905ms
2014-07-22 09:17:29,186 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,186 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 926ms
2014-07-22 09:17:29,187 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,187 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 927ms
2014-07-22 09:17:29,187 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,201 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 943ms
2014-07-22 09:17:29,202 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,202 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 944ms
2014-07-22 09:17:29,202 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,202 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 944ms
2014-07-22 09:17:29,203 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,203 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 949ms
2014-07-22 09:17:29,204 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,213 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 965ms
2014-07-22 09:17:29,213 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,213 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 974ms
2014-07-22 09:17:29,214 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,226 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 984ms
2014-07-22 09:17:29,226 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,226 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 988ms
2014-07-22 09:17:29,226 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,226 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 997ms
2014-07-22 09:17:29,227 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,227 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 999ms
2014-07-22 09:17:29,227 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,230 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1002ms
2014-07-22 09:17:29,230 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,230 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1002ms
2014-07-22 09:17:29,230 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,231 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1003ms
2014-07-22 09:17:29,231 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,231 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1003ms
2014-07-22 09:17:29,231 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,242 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1014ms
2014-07-22 09:17:29,242 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:29,474 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:17:31,189 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:17:31,278 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 96694 synced till here 96663
2014-07-22 09:17:31,305 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:17:31,845 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045846890 with entries=116, filesize=92.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045851190
2014-07-22 09:17:31,848 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:17:34,235 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:17:36,075 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1156ms
GC pool 'ParNew' had collection(s): count=1 time=1390ms
2014-07-22 09:17:36,144 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 96830 synced till here 96803
2014-07-22 09:17:36,485 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045851190 with entries=136, filesize=124.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045854235
2014-07-22 09:17:36,506 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:17:39,385 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:17:39,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 96932 synced till here 96907
2014-07-22 09:17:39,783 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045854235 with entries=102, filesize=96.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045859386
2014-07-22 09:17:39,784 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=44, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:17:41,804 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:41,806 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:41,806 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,001 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:17:42,008 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 97042 synced till here 97010
2014-07-22 09:17:42,344 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,345 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,348 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,349 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,349 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,349 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,350 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,351 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,351 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,353 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,354 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,354 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,356 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,357 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,358 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,358 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,359 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,359 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,359 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,360 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,361 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,362 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,362 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,363 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,363 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,364 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,364 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,366 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,367 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,367 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,368 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,369 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,370 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,370 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,371 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,371 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,372 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,373 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,374 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,375 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,375 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,375 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,376 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,377 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,377 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,381 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:17:42,384 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045859386 with entries=110, filesize=105.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045862002
2014-07-22 09:17:42,385 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=45, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:17:42,653 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=18526, memsize=334.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/3fcbb4286ecf4c938b4e4c380bab3ddf
2014-07-22 09:17:42,673 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/3fcbb4286ecf4c938b4e4c380bab3ddf as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/3fcbb4286ecf4c938b4e4c380bab3ddf
2014-07-22 09:17:42,696 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/3fcbb4286ecf4c938b4e4c380bab3ddf, entries=1216480, sequenceid=18526, filesize=86.7m
2014-07-22 09:17:42,696 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~822.4m/862347680, currentsize=477.9m/501153520 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 30582ms, sequenceid=18526, compaction requested=true
2014-07-22 09:17:42,697 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:17:42,697 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 46 store files, 0 compacting, 46 eligible, 2000 blocking
2014-07-22 09:17:42,697 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 316ms
2014-07-22 09:17:42,697 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 46 files from compaction candidates
2014-07-22 09:17:42,697 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 952.1m
2014-07-22 09:17:42,697 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,697 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:17:42,697 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 320ms
2014-07-22 09:17:42,697 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,697 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:17:42,698 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:17:42,701 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 324ms
2014-07-22 09:17:42,701 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,701 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 325ms
2014-07-22 09:17:42,702 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,702 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 327ms
2014-07-22 09:17:42,702 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,702 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 327ms
2014-07-22 09:17:42,702 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,702 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 327ms
2014-07-22 09:17:42,702 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,705 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 331ms
2014-07-22 09:17:42,705 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,705 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 332ms
2014-07-22 09:17:42,705 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,707 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 335ms
2014-07-22 09:17:42,707 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,707 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 336ms
2014-07-22 09:17:42,707 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,707 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 336ms
2014-07-22 09:17:42,707 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,708 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 338ms
2014-07-22 09:17:42,708 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,708 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 338ms
2014-07-22 09:17:42,708 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,708 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 339ms
2014-07-22 09:17:42,708 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,712 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 344ms
2014-07-22 09:17:42,712 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,712 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 345ms
2014-07-22 09:17:42,712 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,712 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 346ms
2014-07-22 09:17:42,713 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,713 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 347ms
2014-07-22 09:17:42,713 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,721 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 357ms
2014-07-22 09:17:42,721 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,721 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 357ms
2014-07-22 09:17:42,721 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,722 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 359ms
2014-07-22 09:17:42,722 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,722 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 359ms
2014-07-22 09:17:42,722 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,722 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 360ms
2014-07-22 09:17:42,722 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,729 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 367ms
2014-07-22 09:17:42,729 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,737 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 376ms
2014-07-22 09:17:42,737 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,757 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 397ms
2014-07-22 09:17:42,758 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,758 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 399ms
2014-07-22 09:17:42,758 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,758 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 400ms
2014-07-22 09:17:42,758 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,758 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 400ms
2014-07-22 09:17:42,758 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,758 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 400ms
2014-07-22 09:17:42,758 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,759 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 401ms
2014-07-22 09:17:42,759 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,759 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 402ms
2014-07-22 09:17:42,759 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,765 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 409ms
2014-07-22 09:17:42,765 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,773 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 419ms
2014-07-22 09:17:42,773 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,773 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 419ms
2014-07-22 09:17:42,774 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,781 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 428ms
2014-07-22 09:17:42,781 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,783 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 432ms
2014-07-22 09:17:42,783 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,783 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 432ms
2014-07-22 09:17:42,783 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,789 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 440ms
2014-07-22 09:17:42,789 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,789 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 440ms
2014-07-22 09:17:42,790 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,790 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 441ms
2014-07-22 09:17:42,790 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,790 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 441ms
2014-07-22 09:17:42,790 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,797 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 449ms
2014-07-22 09:17:42,797 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,805 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 460ms
2014-07-22 09:17:42,805 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,806 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 462ms
2014-07-22 09:17:42,806 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,813 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 806ms
2014-07-22 09:17:42,813 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,821 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1015ms
2014-07-22 09:17:42,821 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,821 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1015ms
2014-07-22 09:17:42,821 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:42,829 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1025ms
2014-07-22 09:17:42,829 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:17:44,078 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1025ms
GC pool 'ParNew' had collection(s): count=1 time=1130ms
2014-07-22 09:17:44,472 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:17:44,795 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:17:45,100 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:17:45,163 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 97159 synced till here 97131
2014-07-22 09:17:46,253 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045862002 with entries=117, filesize=109.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045865100
2014-07-22 09:17:46,254 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=46, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:17:47,105 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:17:48,130 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 97284 synced till here 97281
2014-07-22 09:17:48,151 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045865100 with entries=125, filesize=111.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045867105
2014-07-22 09:17:48,151 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=47, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:17:49,330 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:17:49,353 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 97369 synced till here 97365
2014-07-22 09:17:49,409 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045867105 with entries=85, filesize=65.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045869331
2014-07-22 09:17:49,409 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=48, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:17:50,627 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:17:50,655 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045869331 with entries=82, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045870627
2014-07-22 09:17:50,656 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=49, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:18:11,564 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=6603, hits=3, hitRatio=0.04%, , cachingAccesses=5, cachingHits=3, cachingHitsRatio=60.00%, evictions=0, evicted=0, evictedPerRun=NaN
2014-07-22 09:18:11,564 WARN  [regionserver60020.periodicFlusher] util.Sleeper: We slept 22444ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-22 09:18:11,571 WARN  [regionserver60020.compactionChecker] util.Sleeper: We slept 30019ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-22 09:18:11,571 WARN  [regionserver60020] util.Sleeper: We slept 21456ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-22 09:18:11,572 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 20171ms
GC pool 'ParNew' had collection(s): count=1 time=1656ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=18846ms
2014-07-22 09:18:11,638 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21006,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045870609,"queuetimems":0,"class":"HRegionServer","responsesize":18614,"method":"Multi"}
2014-07-22 09:18:11,672 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 68922 service: ClientService methodName: Multi size: 5.1k connection: 9.1.143.53:54554: output error
2014-07-22 09:18:11,674 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:18:11,674 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 69059 service: ClientService methodName: Multi size: 5.1k connection: 9.1.143.53:54555: output error
2014-07-22 09:18:11,675 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:18:11,676 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 69061 service: ClientService methodName: Multi size: 28.8k connection: 9.1.143.53:54555: output error
2014-07-22 09:18:11,676 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:18:11,676 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 69060 service: ClientService methodName: Multi size: 36.3k connection: 9.1.143.53:54555: output error
2014-07-22 09:18:11,676 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:18:11,685 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 69062 service: ClientService methodName: Multi size: 231.1k connection: 9.1.143.53:54555: output error
2014-07-22 09:18:11,685 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:18:11,686 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 68572 service: ClientService methodName: Multi size: 36.3k connection: 9.1.143.53:54553: output error
2014-07-22 09:18:11,698 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 68923 service: ClientService methodName: Multi size: 193.6k connection: 9.1.143.53:54554: output error
2014-07-22 09:18:11,698 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:18:11,703 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 68578 service: ClientService methodName: Multi size: 7.6k connection: 9.1.143.53:54553: output error
2014-07-22 09:18:11,705 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:18:11,705 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 68581 service: ClientService methodName: Multi size: 5.1k connection: 9.1.143.53:54553: output error
2014-07-22 09:18:11,705 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught: java.io.IOException: Broken pipe
	at sun.nio.ch.FileDispatcher.writev0(Native Method)
	at sun.nio.ch.SocketDispatcher.writev(SocketDispatcher.java:51)
	at sun.nio.ch.IOUtil.write(IOUtil.java:182)
	at sun.nio.ch.SocketChannelImpl.write0(SocketChannelImpl.java:383)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:406)
	at org.apache.hadoop.hbase.ipc.BufferChain.write(BufferChain.java:106)
	at org.apache.hadoop.hbase.ipc.RpcServer.channelWrite(RpcServer.java:2209)
	at org.apache.hadoop.hbase.ipc.RpcServer$Responder.processResponse(RpcServer.java:1004)
	at org.apache.hadoop.hbase.ipc.RpcServer$Responder.doRespond(RpcServer.java:1081)
	at org.apache.hadoop.hbase.ipc.RpcServer$Call.sendResponseIfReady(RpcServer.java:496)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:121)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.consumerLoop(SimpleRpcScheduler.java:168)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.access$000(SimpleRpcScheduler.java:39)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$1.run(SimpleRpcScheduler.java:111)
	at java.lang.Thread.run(Thread.java:701)

2014-07-22 09:18:11,705 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:18:11,705 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 68577 service: ClientService methodName: Multi size: 23.8k connection: 9.1.143.53:54553: output error
2014-07-22 09:18:11,707 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:18:11,707 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 68582 service: ClientService methodName: Multi size: 193.6k connection: 9.1.143.53:54553: output error
2014-07-22 09:18:11,707 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:18:11,709 WARN  [RpcServer.reader=1,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: count of bytes read: 0
java.nio.channels.AsynchronousCloseException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:281)
	at org.apache.hadoop.hbase.ipc.RpcServer.channelIO(RpcServer.java:2263)
	at org.apache.hadoop.hbase.ipc.RpcServer.channelRead(RpcServer.java:2229)
	at org.apache.hadoop.hbase.ipc.RpcServer$Connection.readAndProcess(RpcServer.java:1488)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener.doRead(RpcServer.java:790)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.doRunLoop(RpcServer.java:581)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.run(RpcServer.java:556)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 09:18:11,717 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 69058 service: ClientService methodName: Multi size: 193.6k connection: 9.1.143.53:54555: output error
2014-07-22 09:18:11,718 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:18:11,761 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21037,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045870724,"queuetimems":0,"class":"HRegionServer","responsesize":16525,"method":"Multi"}
2014-07-22 09:18:11,762 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 68592 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54553: output error
2014-07-22 09:18:11,762 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:18:11,850 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21015,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045870835,"queuetimems":0,"class":"HRegionServer","responsesize":18605,"method":"Multi"}
2014-07-22 09:18:11,850 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21075,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045870775,"queuetimems":0,"class":"HRegionServer","responsesize":16106,"method":"Multi"}
2014-07-22 09:18:11,851 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 68590 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54553: output error
2014-07-22 09:18:11,851 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:18:11,851 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 68591 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:54553: output error
2014-07-22 09:18:11,851 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:18:11,940 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21005,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045870935,"queuetimems":0,"class":"HRegionServer","responsesize":15303,"method":"Multi"}
2014-07-22 09:18:11,941 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 68587 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:54553: output error
2014-07-22 09:18:11,941 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:18:11,956 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21065,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045870890,"queuetimems":0,"class":"HRegionServer","responsesize":18188,"method":"Multi"}
2014-07-22 09:18:11,956 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 68589 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54553: output error
2014-07-22 09:18:11,956 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:18:11,991 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20934,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045871057,"queuetimems":0,"class":"HRegionServer","responsesize":18333,"method":"Multi"}
2014-07-22 09:18:11,991 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 68585 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54553: output error
2014-07-22 09:18:11,991 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:18:12,021 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21034,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54553","starttimems":1406045870986,"queuetimems":1,"class":"HRegionServer","responsesize":18837,"method":"Multi"}
2014-07-22 09:18:12,021 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 68575 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54553: output error
2014-07-22 09:18:12,021 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:18:12,021 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 68586 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.53:54553: output error
2014-07-22 09:18:12,021 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 09:18:13,767 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:13,805 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:13,808 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:18:13,814 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:13,874 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 97529 synced till here 97528
2014-07-22 09:18:13,888 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045870627 with entries=78, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045893808
2014-07-22 09:18:13,888 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=50, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:18:13,891 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:13,951 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:13,951 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:13,979 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:13,991 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:14,188 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:14,486 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:14,545 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:14,570 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=18690, memsize=337.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/30a8eb7475fa42669112d477ea930fe4
2014-07-22 09:18:14,593 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/30a8eb7475fa42669112d477ea930fe4 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/30a8eb7475fa42669112d477ea930fe4
2014-07-22 09:18:14,611 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/30a8eb7475fa42669112d477ea930fe4, entries=1229040, sequenceid=18690, filesize=87.5m
2014-07-22 09:18:14,612 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~828.6m/868807280, currentsize=373.6m/391747280 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 45474ms, sequenceid=18690, compaction requested=true
2014-07-22 09:18:14,612 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:18:14,612 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 48 store files, 0 compacting, 48 eligible, 2000 blocking
2014-07-22 09:18:14,612 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 67ms
2014-07-22 09:18:14,612 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:14,612 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 48 files from compaction candidates
2014-07-22 09:18:14,612 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b., current region memstore size 320.3m
2014-07-22 09:18:14,612 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:18:14,612 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 126ms
2014-07-22 09:18:14,613 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:14,613 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:18:14,613 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 425ms
2014-07-22 09:18:14,613 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:14,613 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:18:14,613 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 622ms
2014-07-22 09:18:14,613 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:14,613 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 634ms
2014-07-22 09:18:14,613 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:14,614 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 663ms
2014-07-22 09:18:14,614 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:14,614 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 664ms
2014-07-22 09:18:14,614 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:14,614 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 723ms
2014-07-22 09:18:14,614 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:14,615 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 801ms
2014-07-22 09:18:14,615 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:14,615 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 810ms
2014-07-22 09:18:14,615 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:14,616 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 848ms
2014-07-22 09:18:14,616 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:14,731 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:18:14,865 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:18:16,406 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:18:16,429 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 97626 synced till here 97625
2014-07-22 09:18:16,456 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045893808 with entries=97, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045896406
2014-07-22 09:18:17,894 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:18:17,923 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 97722 synced till here 97720
2014-07-22 09:18:17,968 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045896406 with entries=96, filesize=65.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045897895
2014-07-22 09:18:19,451 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:18:19,485 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 97814 synced till here 97811
2014-07-22 09:18:19,570 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045897895 with entries=92, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045899452
2014-07-22 09:18:20,769 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:18:20,787 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 97897 synced till here 97896
2014-07-22 09:18:20,824 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045899452 with entries=83, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045900769
2014-07-22 09:18:22,006 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:18:22,446 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 98001 synced till here 98000
2014-07-22 09:18:22,459 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045900769 with entries=104, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045902008
2014-07-22 09:18:23,164 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:18:23,188 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 98091 synced till here 98090
2014-07-22 09:18:23,240 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045902008 with entries=90, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045903165
2014-07-22 09:18:24,497 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=18814, memsize=347.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/c35159527fd0491bb0880b57ee59dc58
2014-07-22 09:18:24,519 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/c35159527fd0491bb0880b57ee59dc58 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/c35159527fd0491bb0880b57ee59dc58
2014-07-22 09:18:24,633 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/c35159527fd0491bb0880b57ee59dc58, entries=1265980, sequenceid=18814, filesize=90.2m
2014-07-22 09:18:24,634 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~952.1m/998398000, currentsize=348.0m/364869600 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 41937ms, sequenceid=18814, compaction requested=true
2014-07-22 09:18:24,636 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:18:24,636 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 48 store files, 0 compacting, 48 eligible, 2000 blocking
2014-07-22 09:18:24,636 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 964.5m
2014-07-22 09:18:24,637 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 48 files from compaction candidates
2014-07-22 09:18:24,637 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:18:24,637 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:18:24,637 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:18:24,652 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:18:24,652 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:18:24,671 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 98190 synced till here 98187
2014-07-22 09:18:24,734 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045903165 with entries=99, filesize=64.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045904654
2014-07-22 09:18:25,397 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=22162, memsize=216.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/b83b5c9ca6094cc78e9eb206ce2bc9a6
2014-07-22 09:18:25,411 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/b83b5c9ca6094cc78e9eb206ce2bc9a6 as hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/b83b5c9ca6094cc78e9eb206ce2bc9a6
2014-07-22 09:18:25,421 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/b83b5c9ca6094cc78e9eb206ce2bc9a6, entries=788990, sequenceid=22162, filesize=56.2m
2014-07-22 09:18:25,422 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~320.3m/335900400, currentsize=47.5m/49779840 for region usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. in 10810ms, sequenceid=22162, compaction requested=true
2014-07-22 09:18:25,422 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:18:25,422 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 2000 blocking
2014-07-22 09:18:25,422 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 861.9m
2014-07-22 09:18:25,422 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-22 09:18:25,422 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:18:25,422 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:18:25,423 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. because compaction request was cancelled
2014-07-22 09:18:26,159 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:18:26,317 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:18:26,318 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 98298 synced till here 98294
2014-07-22 09:18:26,390 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045904654 with entries=108, filesize=74.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045906159
2014-07-22 09:18:26,390 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045752276
2014-07-22 09:18:26,390 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045754153
2014-07-22 09:18:26,391 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045755664
2014-07-22 09:18:26,391 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045757222
2014-07-22 09:18:26,391 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045759010
2014-07-22 09:18:26,391 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045760733
2014-07-22 09:18:26,391 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045762726
2014-07-22 09:18:26,391 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045764443
2014-07-22 09:18:26,391 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045765972
2014-07-22 09:18:26,391 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045767882
2014-07-22 09:18:26,391 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045769255
2014-07-22 09:18:26,391 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045770974
2014-07-22 09:18:26,391 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045772607
2014-07-22 09:18:26,391 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045774738
2014-07-22 09:18:26,391 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045776389
2014-07-22 09:18:26,391 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045777411
2014-07-22 09:18:26,391 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045783649
2014-07-22 09:18:26,392 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045790220
2014-07-22 09:18:26,392 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045792692
2014-07-22 09:18:26,392 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045794984
2014-07-22 09:18:26,392 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045796917
2014-07-22 09:18:26,392 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045799942
2014-07-22 09:18:26,392 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045802338
2014-07-22 09:18:26,392 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045805865
2014-07-22 09:18:26,392 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045808087
2014-07-22 09:18:26,392 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045810445
2014-07-22 09:18:26,392 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045812505
2014-07-22 09:18:26,392 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045814936
2014-07-22 09:18:26,617 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:18:27,571 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:18:27,602 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045906159 with entries=95, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045907572
2014-07-22 09:18:29,030 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:18:29,053 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 98485 synced till here 98483
2014-07-22 09:18:29,074 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045907572 with entries=92, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045909030
2014-07-22 09:18:30,022 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:18:30,736 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 98605 synced till here 98604
2014-07-22 09:18:30,758 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045909030 with entries=120, filesize=81.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045910022
2014-07-22 09:18:32,407 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:18:32,424 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 98694 synced till here 98689
2014-07-22 09:18:32,480 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045910022 with entries=89, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045912407
2014-07-22 09:18:33,124 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:18:33,713 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 98822 synced till here 98809
2014-07-22 09:18:33,771 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045912407 with entries=128, filesize=74.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045913124
2014-07-22 09:18:34,687 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:18:34,711 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 98914 synced till here 98913
2014-07-22 09:18:34,733 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045913124 with entries=92, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045914687
2014-07-22 09:18:36,137 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:18:36,165 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045914687 with entries=101, filesize=61.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045916138
2014-07-22 09:18:37,832 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:18:37,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 99108 synced till here 99100
2014-07-22 09:18:37,934 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045916138 with entries=93, filesize=67.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045917832
2014-07-22 09:18:39,163 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:18:39,483 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:39,485 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:39,638 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045917832 with entries=124, filesize=91.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045919163
2014-07-22 09:18:40,070 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:40,071 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:40,071 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:40,072 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:40,138 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:40,204 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:40,250 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:40,319 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:40,383 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:40,445 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:40,446 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:40,448 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:40,491 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:40,492 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:40,492 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:40,531 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:40,533 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:40,568 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:40,571 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:40,615 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:40,658 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:40,659 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:40,660 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:40,661 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:40,710 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:40,747 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:41,485 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:41,496 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:41,514 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:41,514 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:41,515 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:41,515 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:41,549 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:41,550 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:41,550 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:41,551 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:41,603 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:41,636 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:41,669 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:41,672 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:41,705 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:41,706 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:41,708 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:41,709 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:41,711 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:41,711 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:41,733 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:41,759 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:18:44,483 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:18:44,485 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:18:45,070 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:18:45,071 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:18:45,072 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:18:45,072 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:18:45,139 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:18:45,204 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:18:46,376 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5629ms
2014-07-22 09:18:46,377 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5806ms
2014-07-22 09:18:46,377 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5846ms
2014-07-22 09:18:46,377 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5719ms
2014-07-22 09:18:46,378 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5994ms
2014-07-22 09:18:46,378 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5763ms
2014-07-22 09:18:46,379 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5720ms
2014-07-22 09:18:46,379 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5718ms
2014-07-22 09:18:46,379 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5719ms
2014-07-22 09:18:46,379 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5669ms
2014-07-22 09:18:46,379 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6129ms
2014-07-22 09:18:46,380 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6061ms
2014-07-22 09:18:46,380 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5935ms
2014-07-22 09:18:46,380 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5934ms
2014-07-22 09:18:46,380 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5933ms
2014-07-22 09:18:46,380 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5888ms
2014-07-22 09:18:46,381 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5890ms
2014-07-22 09:18:46,381 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5889ms
2014-07-22 09:18:46,381 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5848ms
2014-07-22 09:18:46,381 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5813ms
2014-07-22 09:18:46,486 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:18:46,497 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:18:46,514 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:18:46,515 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:18:46,515 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:18:46,515 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:18:46,549 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:18:46,550 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:18:46,550 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:18:46,551 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:18:46,604 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:18:46,637 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:18:46,669 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:18:46,672 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:18:46,706 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:18:46,706 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:18:46,708 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:18:46,709 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:18:46,711 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:18:46,711 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:18:46,734 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:18:46,759 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:18:49,310 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=19020, memsize=470.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/434aa109e8314bf0bc6431dd2951652f
2014-07-22 09:18:49,334 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/434aa109e8314bf0bc6431dd2951652f as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/434aa109e8314bf0bc6431dd2951652f
2014-07-22 09:18:49,348 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/434aa109e8314bf0bc6431dd2951652f, entries=1712750, sequenceid=19020, filesize=122.1m
2014-07-22 09:18:49,348 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~863.8m/905731760, currentsize=289.0m/303034960 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 23926ms, sequenceid=19020, compaction requested=true
2014-07-22 09:18:49,349 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:18:49,349 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 47 store files, 0 compacting, 47 eligible, 2000 blocking
2014-07-22 09:18:49,349 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7590ms
2014-07-22 09:18:49,349 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 47 files from compaction candidates
2014-07-22 09:18:49,349 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 867.0m
2014-07-22 09:18:49,349 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,349 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:18:49,350 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:18:49,350 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:18:49,353 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7620ms
2014-07-22 09:18:49,353 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,354 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7642ms
2014-07-22 09:18:49,354 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,354 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7644ms
2014-07-22 09:18:49,354 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,354 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7645ms
2014-07-22 09:18:49,354 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,354 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7646ms
2014-07-22 09:18:49,354 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,360 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7654ms
2014-07-22 09:18:49,360 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,360 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7655ms
2014-07-22 09:18:49,360 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,360 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7689ms
2014-07-22 09:18:49,361 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,361 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7692ms
2014-07-22 09:18:49,361 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,361 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7725ms
2014-07-22 09:18:49,361 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,362 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7759ms
2014-07-22 09:18:49,363 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,373 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7822ms
2014-07-22 09:18:49,373 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,374 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7823ms
2014-07-22 09:18:49,374 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,374 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7824ms
2014-07-22 09:18:49,374 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,381 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7832ms
2014-07-22 09:18:49,381 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,382 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7866ms
2014-07-22 09:18:49,382 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,382 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7868ms
2014-07-22 09:18:49,382 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,384 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7869ms
2014-07-22 09:18:49,384 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,386 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7871ms
2014-07-22 09:18:49,386 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,386 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7890ms
2014-07-22 09:18:49,387 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,387 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7902ms
2014-07-22 09:18:49,387 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,387 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8819ms
2014-07-22 09:18:49,387 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,387 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8854ms
2014-07-22 09:18:49,387 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,388 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8896ms
2014-07-22 09:18:49,388 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,389 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8897ms
2014-07-22 09:18:49,389 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,389 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8897ms
2014-07-22 09:18:49,389 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,391 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8943ms
2014-07-22 09:18:49,391 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,393 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8947ms
2014-07-22 09:18:49,393 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,394 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8948ms
2014-07-22 09:18:49,394 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,394 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9075ms
2014-07-22 09:18:49,394 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,396 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9146ms
2014-07-22 09:18:49,396 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,397 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8686ms
2014-07-22 09:18:49,397 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,400 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8740ms
2014-07-22 09:18:49,400 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,400 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8739ms
2014-07-22 09:18:49,401 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,402 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8742ms
2014-07-22 09:18:49,402 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,403 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8787ms
2014-07-22 09:18:49,403 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,403 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9020ms
2014-07-22 09:18:49,403 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,404 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8745ms
2014-07-22 09:18:49,404 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,405 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8874ms
2014-07-22 09:18:49,405 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,406 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8834ms
2014-07-22 09:18:49,406 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,413 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8666ms
2014-07-22 09:18:49,413 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,414 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9209ms
2014-07-22 09:18:49,414 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,415 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9277ms
2014-07-22 09:18:49,415 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,416 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9344ms
2014-07-22 09:18:49,416 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,421 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9350ms
2014-07-22 09:18:49,421 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,422 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9350ms
2014-07-22 09:18:49,422 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,422 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9352ms
2014-07-22 09:18:49,422 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,424 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9938ms
2014-07-22 09:18:49,424 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,424 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9941ms
2014-07-22 09:18:49,424 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:18:49,559 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10244,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406045919314,"queuetimems":1,"class":"HRegionServer","responsesize":9185,"method":"Multi"}
2014-07-22 09:18:49,584 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:18:49,706 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10427,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406045919279,"queuetimems":0,"class":"HRegionServer","responsesize":19297,"method":"Multi"}
2014-07-22 09:18:51,096 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=19007, memsize=474.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/90a4ac8526f44acbac8e6e19d6fb8200
2014-07-22 09:18:51,118 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/90a4ac8526f44acbac8e6e19d6fb8200 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/90a4ac8526f44acbac8e6e19d6fb8200
2014-07-22 09:18:51,133 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/90a4ac8526f44acbac8e6e19d6fb8200, entries=1725950, sequenceid=19007, filesize=123.0m
2014-07-22 09:18:51,134 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~967.9m/1014880000, currentsize=305.3m/320161920 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 26498ms, sequenceid=19007, compaction requested=true
2014-07-22 09:18:51,135 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:18:51,135 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 47 store files, 0 compacting, 47 eligible, 2000 blocking
2014-07-22 09:18:51,135 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 702.6m
2014-07-22 09:18:51,135 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 47 files from compaction candidates
2014-07-22 09:18:51,136 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:18:51,136 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:18:51,136 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:18:51,136 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:18:51,341 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:18:51,343 DEBUG [RpcServer.handler=46,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:18:51,391 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 99373 synced till here 99347
2014-07-22 09:18:51,594 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045919163 with entries=141, filesize=89.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045931341
2014-07-22 09:18:51,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045816589
2014-07-22 09:18:51,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045826679
2014-07-22 09:18:51,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045829557
2014-07-22 09:18:51,595 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045830914
2014-07-22 09:18:51,595 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045832743
2014-07-22 09:18:51,595 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045833917
2014-07-22 09:18:51,595 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045835331
2014-07-22 09:18:51,595 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045836942
2014-07-22 09:18:51,595 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045838368
2014-07-22 09:18:51,595 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045840543
2014-07-22 09:18:51,595 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045842363
2014-07-22 09:18:51,595 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045843276
2014-07-22 09:18:51,595 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045844916
2014-07-22 09:18:51,854 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10148,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406045921705,"queuetimems":1,"class":"HRegionServer","responsesize":19150,"method":"Multi"}
2014-07-22 09:18:52,845 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:18:53,329 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:18:53,335 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11852,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406045921482,"queuetimems":0,"class":"HRegionServer","responsesize":18592,"method":"Multi"}
2014-07-22 09:18:53,335 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12678,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406045920656,"queuetimems":0,"class":"HRegionServer","responsesize":18902,"method":"Multi"}
2014-07-22 09:18:53,337 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12848,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406045920488,"queuetimems":0,"class":"HRegionServer","responsesize":18836,"method":"Multi"}
2014-07-22 09:18:53,337 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13020,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406045920316,"queuetimems":0,"class":"HRegionServer","responsesize":18758,"method":"Multi"}
2014-07-22 09:18:53,337 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12629,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406045920707,"queuetimems":0,"class":"HRegionServer","responsesize":18847,"method":"Multi"}
2014-07-22 09:18:53,338 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12590,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406045920744,"queuetimems":1,"class":"HRegionServer","responsesize":15775,"method":"Multi"}
2014-07-22 09:18:53,392 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 99478 synced till here 99461
2014-07-22 09:18:53,441 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11947,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406045921494,"queuetimems":0,"class":"HRegionServer","responsesize":18557,"method":"Multi"}
2014-07-22 09:18:53,441 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11892,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406045921549,"queuetimems":0,"class":"HRegionServer","responsesize":18326,"method":"Multi"}
2014-07-22 09:18:53,446 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11845,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406045921601,"queuetimems":0,"class":"HRegionServer","responsesize":18878,"method":"Multi"}
2014-07-22 09:18:53,476 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13339,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406045920136,"queuetimems":1,"class":"HRegionServer","responsesize":18651,"method":"Multi"}
2014-07-22 09:18:53,482 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11969,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406045921513,"queuetimems":0,"class":"HRegionServer","responsesize":17049,"method":"Multi"}
2014-07-22 09:18:53,493 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045931341 with entries=105, filesize=88.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045933330
2014-07-22 09:18:54,882 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14438,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406045920444,"queuetimems":0,"class":"HRegionServer","responsesize":18282,"method":"Multi"}
2014-07-22 09:18:54,891 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14278,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406045920612,"queuetimems":0,"class":"HRegionServer","responsesize":18189,"method":"Multi"}
2014-07-22 09:18:54,882 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14814,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406045920068,"queuetimems":0,"class":"HRegionServer","responsesize":18711,"method":"Multi"}
2014-07-22 09:18:54,898 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14651,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406045920247,"queuetimems":0,"class":"HRegionServer","responsesize":18489,"method":"Multi"}
2014-07-22 09:18:54,924 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14355,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406045920568,"queuetimems":1,"class":"HRegionServer","responsesize":18889,"method":"Multi"}
2014-07-22 09:18:54,926 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14545,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406045920380,"queuetimems":1,"class":"HRegionServer","responsesize":18419,"method":"Multi"}
2014-07-22 09:18:54,927 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13292,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406045921634,"queuetimems":0,"class":"HRegionServer","responsesize":18307,"method":"Multi"}
2014-07-22 09:18:54,928 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14726,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406045920201,"queuetimems":0,"class":"HRegionServer","responsesize":18728,"method":"Multi"}
2014-07-22 09:18:55,078 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14548,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406045920529,"queuetimems":0,"class":"HRegionServer","responsesize":18686,"method":"Multi"}
2014-07-22 09:18:55,439 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:18:55,489 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 99609 synced till here 99574
2014-07-22 09:18:56,702 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045933330 with entries=131, filesize=94.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045935439
2014-07-22 09:18:57,534 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:18:58,481 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 99723 synced till here 99700
2014-07-22 09:18:58,732 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045935439 with entries=114, filesize=98.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045937534
2014-07-22 09:19:00,510 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:00,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 99849 synced till here 99810
2014-07-22 09:19:00,848 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045937534 with entries=126, filesize=97.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045940510
2014-07-22 09:19:02,457 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:02,543 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 99977 synced till here 99944
2014-07-22 09:19:02,858 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045940510 with entries=128, filesize=93.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045942458
2014-07-22 09:19:04,779 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:04,830 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 100083 synced till here 100072
2014-07-22 09:19:04,909 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045942458 with entries=106, filesize=74.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045944779
2014-07-22 09:19:05,906 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:05,958 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 100189 synced till here 100162
2014-07-22 09:19:06,497 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045944779 with entries=106, filesize=88.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045945906
2014-07-22 09:19:07,514 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:07,549 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 100318 synced till here 100283
2014-07-22 09:19:08,660 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045945906 with entries=129, filesize=100.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045947514
2014-07-22 09:19:09,522 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:09,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 100427 synced till here 100413
2014-07-22 09:19:10,466 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045947514 with entries=109, filesize=74.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045949523
2014-07-22 09:19:13,537 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:13,562 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045949523 with entries=79, filesize=60.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045953538
2014-07-22 09:19:16,914 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:19:16,938 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:19:16,982 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:19:16,996 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:19:16,996 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:19:17,025 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:19:17,037 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:19:17,202 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:19:17,240 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=19201, memsize=432.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/f50fd58f14da49b48af82bc3fe1872c5
2014-07-22 09:19:17,265 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/f50fd58f14da49b48af82bc3fe1872c5 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/f50fd58f14da49b48af82bc3fe1872c5
2014-07-22 09:19:17,276 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/f50fd58f14da49b48af82bc3fe1872c5, entries=1575610, sequenceid=19201, filesize=112.2m
2014-07-22 09:19:17,276 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~867.0m/909092560, currentsize=437.8m/459042000 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 27927ms, sequenceid=19201, compaction requested=true
2014-07-22 09:19:17,277 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:19:17,277 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 49 store files, 0 compacting, 49 eligible, 2000 blocking
2014-07-22 09:19:17,277 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 75ms
2014-07-22 09:19:17,277 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:19:17,277 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 49 files from compaction candidates
2014-07-22 09:19:17,277 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 241ms
2014-07-22 09:19:17,277 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:19:17,277 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 728.4m
2014-07-22 09:19:17,277 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 252ms
2014-07-22 09:19:17,278 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:19:17,277 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:19:17,281 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:19:17,281 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 285ms
2014-07-22 09:19:17,281 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:19:17,281 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:19:17,281 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 285ms
2014-07-22 09:19:17,281 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:19:17,282 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 300ms
2014-07-22 09:19:17,282 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:19:17,282 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 344ms
2014-07-22 09:19:17,282 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:19:17,282 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 369ms
2014-07-22 09:19:17,282 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:19:17,401 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:19:17,550 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:17,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 100609 synced till here 100607
2014-07-22 09:19:17,582 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045953538 with entries=103, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045957550
2014-07-22 09:19:17,582 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045846890
2014-07-22 09:19:17,582 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045851190
2014-07-22 09:19:17,582 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045854235
2014-07-22 09:19:17,582 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045859386
2014-07-22 09:19:17,932 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:19:19,649 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=19254, memsize=476.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/e85becce8f3c4adcb0db3f2b9a8797c7
2014-07-22 09:19:19,659 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/e85becce8f3c4adcb0db3f2b9a8797c7 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/e85becce8f3c4adcb0db3f2b9a8797c7
2014-07-22 09:19:19,668 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/e85becce8f3c4adcb0db3f2b9a8797c7, entries=1733730, sequenceid=19254, filesize=123.5m
2014-07-22 09:19:19,669 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~731.3m/766850400, currentsize=389.9m/408848160 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 28534ms, sequenceid=19254, compaction requested=true
2014-07-22 09:19:19,670 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:19:19,670 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 49 store files, 0 compacting, 49 eligible, 2000 blocking
2014-07-22 09:19:19,670 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 766.1m
2014-07-22 09:19:19,670 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 49 files from compaction candidates
2014-07-22 09:19:19,670 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:19:19,671 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:19:19,671 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:19:19,689 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:19:19,717 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:19,733 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 100684 synced till here 100683
2014-07-22 09:19:19,766 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045957550 with entries=75, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045959717
2014-07-22 09:19:19,766 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045862002
2014-07-22 09:19:19,766 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045865100
2014-07-22 09:19:19,766 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045867105
2014-07-22 09:19:19,766 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045869331
2014-07-22 09:19:19,766 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045870627
2014-07-22 09:19:20,972 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:19:21,039 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:21,055 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 100762 synced till here 100761
2014-07-22 09:19:21,065 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045959717 with entries=78, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045961039
2014-07-22 09:19:22,420 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:22,636 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045961039 with entries=104, filesize=70.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045962420
2014-07-22 09:19:24,140 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:24,171 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045962420 with entries=76, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045964141
2014-07-22 09:19:24,171 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:19:25,034 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:25,062 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 101025 synced till here 101023
2014-07-22 09:19:25,386 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045964141 with entries=83, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045965034
2014-07-22 09:19:25,387 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:19:26,126 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:26,141 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 101117 synced till here 101109
2014-07-22 09:19:26,207 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045965034 with entries=92, filesize=68.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045966127
2014-07-22 09:19:26,210 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:19:27,486 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:27,515 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 101212 synced till here 101210
2014-07-22 09:19:27,548 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045966127 with entries=95, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045967487
2014-07-22 09:19:27,548 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:19:28,946 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:29,391 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 101355 synced till here 101353
2014-07-22 09:19:29,410 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045967487 with entries=143, filesize=93.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045968946
2014-07-22 09:19:29,410 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:19:30,405 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b.
2014-07-22 09:19:30,845 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:30,864 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 101436 synced till here 101435
2014-07-22 09:19:30,889 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045968946 with entries=81, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045970846
2014-07-22 09:19:30,890 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:19:32,996 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:33,128 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 101554 synced till here 101545
2014-07-22 09:19:33,209 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045970846 with entries=118, filesize=83.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045972997
2014-07-22 09:19:33,209 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:19:34,518 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:35,447 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 101680 synced till here 101677
2014-07-22 09:19:35,497 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045972997 with entries=126, filesize=90.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045974519
2014-07-22 09:19:35,501 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:19:36,227 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=19451, memsize=277.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/58dc907842bb48b5b675f45951103231
2014-07-22 09:19:36,250 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/58dc907842bb48b5b675f45951103231 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/58dc907842bb48b5b675f45951103231
2014-07-22 09:19:36,268 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/58dc907842bb48b5b675f45951103231, entries=1011670, sequenceid=19451, filesize=72.1m
2014-07-22 09:19:36,268 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~728.4m/763784240, currentsize=362.4m/379980560 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 18991ms, sequenceid=19451, compaction requested=true
2014-07-22 09:19:36,269 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:19:36,269 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 48 store files, 0 compacting, 48 eligible, 2000 blocking
2014-07-22 09:19:36,269 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 791.4m
2014-07-22 09:19:36,270 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 48 files from compaction candidates
2014-07-22 09:19:36,270 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:19:36,270 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:19:36,270 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:19:36,397 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:19:36,980 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:37,806 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 101799 synced till here 101784
2014-07-22 09:19:37,962 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045974519 with entries=119, filesize=83.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045976981
2014-07-22 09:19:37,963 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:19:38,188 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:19:39,846 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1065ms
GC pool 'ParNew' had collection(s): count=1 time=1075ms
2014-07-22 09:19:39,896 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:39,951 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 101912 synced till here 101893
2014-07-22 09:19:40,149 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045976981 with entries=113, filesize=79.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045979896
2014-07-22 09:19:40,149 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:19:42,018 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:42,051 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 102022 synced till here 101990
2014-07-22 09:19:42,225 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045979896 with entries=110, filesize=84.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045982019
2014-07-22 09:19:42,226 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:19:42,878 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=19463, memsize=307.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/369ab046de8849ce8c0a30db52d38327
2014-07-22 09:19:42,897 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/369ab046de8849ce8c0a30db52d38327 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/369ab046de8849ce8c0a30db52d38327
2014-07-22 09:19:42,912 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/369ab046de8849ce8c0a30db52d38327, entries=1119990, sequenceid=19463, filesize=79.8m
2014-07-22 09:19:42,913 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~768.0m/805305840, currentsize=409.5m/429382480 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 23242ms, sequenceid=19463, compaction requested=true
2014-07-22 09:19:42,913 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:19:42,913 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 48 store files, 0 compacting, 48 eligible, 2000 blocking
2014-07-22 09:19:42,914 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 844.8m
2014-07-22 09:19:42,914 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 48 files from compaction candidates
2014-07-22 09:19:42,914 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:19:42,914 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:19:42,914 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:19:42,981 DEBUG [RpcServer.handler=5,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:19:44,239 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:44,300 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 102132 synced till here 102104
2014-07-22 09:19:44,568 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045982019 with entries=110, filesize=87.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045984240
2014-07-22 09:19:44,571 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=44, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:19:46,053 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1044ms
GC pool 'ParNew' had collection(s): count=1 time=1050ms
2014-07-22 09:19:46,346 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:19:46,471 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:46,485 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 102245 synced till here 102213
2014-07-22 09:19:46,786 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045984240 with entries=113, filesize=96.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045986471
2014-07-22 09:19:46,787 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=45, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:19:48,773 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:48,848 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 102368 synced till here 102334
2014-07-22 09:19:49,414 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045986471 with entries=123, filesize=97.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045988773
2014-07-22 09:19:49,415 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=46, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:19:50,986 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:51,238 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 102499 synced till here 102457
2014-07-22 09:19:52,653 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045988773 with entries=131, filesize=94.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045990987
2014-07-22 09:19:52,674 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=47, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:19:53,443 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:53,601 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 102605 synced till here 102600
2014-07-22 09:19:53,693 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045990987 with entries=106, filesize=84.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045993444
2014-07-22 09:19:53,695 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=48, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:19:55,138 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:56,496 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 102712 synced till here 102709
2014-07-22 09:19:56,537 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045993444 with entries=107, filesize=90.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045995139
2014-07-22 09:19:56,538 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=49, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:19:57,746 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:19:57,915 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 102808 synced till here 102801
2014-07-22 09:19:58,376 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045995139 with entries=96, filesize=73.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045997746
2014-07-22 09:19:58,382 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=50, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:19:59,512 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:19:59,513 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:19:59,513 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:19:59,514 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:19:59,516 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:19:59,518 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:19:59,526 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:19:59,566 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:19:59,666 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:19:59,669 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:19:59,687 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:19:59,729 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:19:59,778 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:19:59,779 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:00,700 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:00,740 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:00,774 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:00,801 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:00,840 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:00,880 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:00,912 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:01,047 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:01,089 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:01,392 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=19672, memsize=280.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/cec7e45c4aef4a7091b315b67e5727c8
2014-07-22 09:20:01,408 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/cec7e45c4aef4a7091b315b67e5727c8 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/cec7e45c4aef4a7091b315b67e5727c8
2014-07-22 09:20:01,426 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/cec7e45c4aef4a7091b315b67e5727c8, entries=1021240, sequenceid=19672, filesize=72.7m
2014-07-22 09:20:01,426 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~806.4m/845609760, currentsize=377.0m/395286320 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 25157ms, sequenceid=19672, compaction requested=true
2014-07-22 09:20:01,427 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:20:01,427 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 50 store files, 0 compacting, 50 eligible, 2000 blocking
2014-07-22 09:20:01,427 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 338ms
2014-07-22 09:20:01,427 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 50 files from compaction candidates
2014-07-22 09:20:01,427 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b., current region memstore size 313.2m
2014-07-22 09:20:01,427 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:20:01,427 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:01,428 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:20:01,428 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 381ms
2014-07-22 09:20:01,428 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:01,428 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:20:01,429 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 518ms
2014-07-22 09:20:01,429 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:01,437 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 557ms
2014-07-22 09:20:01,438 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:01,438 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 598ms
2014-07-22 09:20:01,438 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:01,438 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 638ms
2014-07-22 09:20:01,438 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:01,439 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 665ms
2014-07-22 09:20:01,439 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:01,440 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 699ms
2014-07-22 09:20:01,440 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:01,440 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 740ms
2014-07-22 09:20:01,440 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:01,441 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1662ms
2014-07-22 09:20:01,441 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:01,442 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1663ms
2014-07-22 09:20:01,442 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:01,457 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1728ms
2014-07-22 09:20:01,457 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:01,458 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1771ms
2014-07-22 09:20:01,458 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:01,458 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1789ms
2014-07-22 09:20:01,458 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:01,458 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1792ms
2014-07-22 09:20:01,458 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:01,459 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1893ms
2014-07-22 09:20:01,459 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:01,460 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1933ms
2014-07-22 09:20:01,460 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:01,463 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1945ms
2014-07-22 09:20:01,463 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:01,463 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1947ms
2014-07-22 09:20:01,464 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:01,464 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1950ms
2014-07-22 09:20:01,464 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:01,464 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1951ms
2014-07-22 09:20:01,464 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:01,464 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1952ms
2014-07-22 09:20:01,465 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:01,465 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1953ms
2014-07-22 09:20:01,465 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:02,810 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:20:02,860 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:02,860 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:20:02,914 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 102900 synced till here 102893
2014-07-22 09:20:02,986 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045997746 with entries=92, filesize=68.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046002860
2014-07-22 09:20:03,870 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:03,894 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046002860 with entries=72, filesize=61.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046003870
2014-07-22 09:20:05,471 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:05,488 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 103061 synced till here 103060
2014-07-22 09:20:05,505 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046003870 with entries=89, filesize=60.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046005472
2014-07-22 09:20:07,583 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:07,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 103149 synced till here 103147
2014-07-22 09:20:07,633 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046005472 with entries=88, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046007584
2014-07-22 09:20:08,102 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=19756, memsize=306.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/0e55a7819e7642048789658d74ebfa18
2014-07-22 09:20:08,117 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/0e55a7819e7642048789658d74ebfa18 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/0e55a7819e7642048789658d74ebfa18
2014-07-22 09:20:08,126 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/0e55a7819e7642048789658d74ebfa18, entries=1115900, sequenceid=19756, filesize=79.5m
2014-07-22 09:20:08,126 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~860.2m/901968080, currentsize=360.1m/377576720 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 25212ms, sequenceid=19756, compaction requested=true
2014-07-22 09:20:08,127 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 50 store files, 0 compacting, 50 eligible, 2000 blocking
2014-07-22 09:20:08,127 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:20:08,127 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 50 files from compaction candidates
2014-07-22 09:20:08,127 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:20:08,127 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 856.0m
2014-07-22 09:20:08,127 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:20:08,128 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:20:08,171 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:20:09,207 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:09,514 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 103277 synced till here 103275
2014-07-22 09:20:09,550 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046007584 with entries=128, filesize=85.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046009208
2014-07-22 09:20:09,931 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:20:11,171 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:11,189 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 103378 synced till here 103376
2014-07-22 09:20:11,223 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046009208 with entries=101, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046011172
2014-07-22 09:20:12,581 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:13,001 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 103509 synced till here 103489
2014-07-22 09:20:13,114 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046011172 with entries=131, filesize=85.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046012581
2014-07-22 09:20:13,157 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=23496, memsize=195.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/8521b2226e844853b9134938f5882149
2014-07-22 09:20:13,198 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/8521b2226e844853b9134938f5882149 as hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/8521b2226e844853b9134938f5882149
2014-07-22 09:20:13,211 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/8521b2226e844853b9134938f5882149, entries=711010, sequenceid=23496, filesize=50.7m
2014-07-22 09:20:13,212 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~313.2m/328393200, currentsize=42.8m/44881600 for region usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. in 11785ms, sequenceid=23496, compaction requested=true
2014-07-22 09:20:13,212 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:20:13,212 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 2000 blocking
2014-07-22 09:20:13,212 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-22 09:20:13,212 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 906.6m
2014-07-22 09:20:13,212 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:20:13,212 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:20:13,213 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. because compaction request was cancelled
2014-07-22 09:20:14,518 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:14,772 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:20:14,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 103624 synced till here 103620
2014-07-22 09:20:15,015 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046012581 with entries=115, filesize=98.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046014518
2014-07-22 09:20:15,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045893808
2014-07-22 09:20:15,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045896406
2014-07-22 09:20:15,016 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045897895
2014-07-22 09:20:15,016 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045899452
2014-07-22 09:20:15,016 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045900769
2014-07-22 09:20:15,016 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045902008
2014-07-22 09:20:15,016 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045903165
2014-07-22 09:20:15,016 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045904654
2014-07-22 09:20:15,016 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045906159
2014-07-22 09:20:15,016 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045907572
2014-07-22 09:20:15,016 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045909030
2014-07-22 09:20:15,016 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045910022
2014-07-22 09:20:15,016 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045912407
2014-07-22 09:20:15,016 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045913124
2014-07-22 09:20:15,016 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045914687
2014-07-22 09:20:15,016 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045916138
2014-07-22 09:20:15,016 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045917832
2014-07-22 09:20:15,016 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045919163
2014-07-22 09:20:15,016 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045931341
2014-07-22 09:20:15,016 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045933330
2014-07-22 09:20:15,016 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045935439
2014-07-22 09:20:15,016 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045937534
2014-07-22 09:20:15,019 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045940510
2014-07-22 09:20:15,019 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045942458
2014-07-22 09:20:15,019 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045944779
2014-07-22 09:20:15,019 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045945906
2014-07-22 09:20:15,019 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045947514
2014-07-22 09:20:15,019 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045949523
2014-07-22 09:20:16,606 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:16,625 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 103722 synced till here 103720
2014-07-22 09:20:16,655 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046014518 with entries=98, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046016606
2014-07-22 09:20:17,910 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:17,976 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 103818 synced till here 103811
2014-07-22 09:20:18,047 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046016606 with entries=96, filesize=69.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046017911
2014-07-22 09:20:19,398 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:19,449 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 103908 synced till here 103905
2014-07-22 09:20:19,530 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046017911 with entries=90, filesize=65.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046019399
2014-07-22 09:20:20,420 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:20,803 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046019399 with entries=81, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046020420
2014-07-22 09:20:21,838 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:22,102 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 104078 synced till here 104077
2014-07-22 09:20:22,610 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046020420 with entries=89, filesize=70.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046021838
2014-07-22 09:20:23,572 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:24,718 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1046ms
GC pool 'ParNew' had collection(s): count=1 time=1073ms
2014-07-22 09:20:24,771 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 104211 synced till here 104186
2014-07-22 09:20:24,848 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046021838 with entries=133, filesize=77.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046023573
2014-07-22 09:20:25,819 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:27,604 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1384ms
GC pool 'ParNew' had collection(s): count=1 time=1706ms
2014-07-22 09:20:27,640 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 104308 synced till here 104288
2014-07-22 09:20:27,952 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046023573 with entries=97, filesize=75.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046025820
2014-07-22 09:20:28,684 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:28,685 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:28,685 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:28,686 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:28,686 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:28,688 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:28,761 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:28,761 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:28,762 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:28,764 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:28,765 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:28,767 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:28,802 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:28,816 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:28,822 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:28,822 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:28,829 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 104394 synced till here 104385
2014-07-22 09:20:30,439 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1334ms
GC pool 'ParNew' had collection(s): count=1 time=1603ms
2014-07-22 09:20:30,442 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:30,454 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:30,478 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:30,491 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:30,491 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:30,492 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:30,492 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:30,492 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:30,492 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:30,492 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:30,500 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:30,500 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:30,503 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:30,503 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:30,503 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:30,503 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:30,504 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:30,731 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:30,741 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046025820 with entries=86, filesize=72.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046028816
2014-07-22 09:20:30,762 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:30,800 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:30,816 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:30,864 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:30,911 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:30,960 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:30,991 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:31,020 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:31,062 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:31,105 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:31,144 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:31,189 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:31,251 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:31,274 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:31,318 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:31,364 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:31,405 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:33,146 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=19963, memsize=356.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/80f572fd228c44b7a4dfd8f616b20e4d
2014-07-22 09:20:33,162 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/80f572fd228c44b7a4dfd8f616b20e4d as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/80f572fd228c44b7a4dfd8f616b20e4d
2014-07-22 09:20:33,176 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/80f572fd228c44b7a4dfd8f616b20e4d, entries=1299350, sequenceid=19963, filesize=92.6m
2014-07-22 09:20:33,177 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~859.9m/901674720, currentsize=368.2m/386113040 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 25050ms, sequenceid=19963, compaction requested=true
2014-07-22 09:20:33,177 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:20:33,177 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 49 store files, 0 compacting, 49 eligible, 2000 blocking
2014-07-22 09:20:33,177 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1772ms
2014-07-22 09:20:33,177 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 49 files from compaction candidates
2014-07-22 09:20:33,177 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 854.0m
2014-07-22 09:20:33,177 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,178 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:20:33,178 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1814ms
2014-07-22 09:20:33,178 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:20:33,178 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,178 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:20:33,178 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1861ms
2014-07-22 09:20:33,178 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,178 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1904ms
2014-07-22 09:20:33,179 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,179 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1928ms
2014-07-22 09:20:33,179 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,179 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1990ms
2014-07-22 09:20:33,179 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,179 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2035ms
2014-07-22 09:20:33,179 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,193 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2088ms
2014-07-22 09:20:33,193 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,194 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2132ms
2014-07-22 09:20:33,194 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,208 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2188ms
2014-07-22 09:20:33,208 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,209 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2218ms
2014-07-22 09:20:33,209 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,210 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2250ms
2014-07-22 09:20:33,210 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,210 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2299ms
2014-07-22 09:20:33,210 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,210 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2347ms
2014-07-22 09:20:33,210 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,211 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2395ms
2014-07-22 09:20:33,211 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,211 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2411ms
2014-07-22 09:20:33,211 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,211 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2449ms
2014-07-22 09:20:33,211 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,211 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2480ms
2014-07-22 09:20:33,211 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,211 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2707ms
2014-07-22 09:20:33,212 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,214 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2711ms
2014-07-22 09:20:33,214 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,215 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2712ms
2014-07-22 09:20:33,215 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,215 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2712ms
2014-07-22 09:20:33,215 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,216 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2713ms
2014-07-22 09:20:33,216 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,216 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2716ms
2014-07-22 09:20:33,216 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,216 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2716ms
2014-07-22 09:20:33,216 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,216 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2725ms
2014-07-22 09:20:33,216 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,231 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2739ms
2014-07-22 09:20:33,232 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,232 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2741ms
2014-07-22 09:20:33,232 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,232 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2741ms
2014-07-22 09:20:33,232 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,241 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2750ms
2014-07-22 09:20:33,241 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,241 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2750ms
2014-07-22 09:20:33,241 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,242 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2752ms
2014-07-22 09:20:33,242 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,242 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2765ms
2014-07-22 09:20:33,242 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,242 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2788ms
2014-07-22 09:20:33,243 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,243 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2801ms
2014-07-22 09:20:33,243 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,243 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4421ms
2014-07-22 09:20:33,243 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,252 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4430ms
2014-07-22 09:20:33,252 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,252 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4450ms
2014-07-22 09:20:33,253 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,253 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4486ms
2014-07-22 09:20:33,253 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,253 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4488ms
2014-07-22 09:20:33,253 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,262 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4498ms
2014-07-22 09:20:33,262 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,262 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4500ms
2014-07-22 09:20:33,262 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,262 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4501ms
2014-07-22 09:20:33,263 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,271 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4511ms
2014-07-22 09:20:33,271 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,271 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4583ms
2014-07-22 09:20:33,271 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,271 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4586ms
2014-07-22 09:20:33,271 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,281 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4595ms
2014-07-22 09:20:33,281 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,282 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4597ms
2014-07-22 09:20:33,282 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,282 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4598ms
2014-07-22 09:20:33,282 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:33,282 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4598ms
2014-07-22 09:20:33,282 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:20:35,224 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1233ms
GC pool 'ParNew' had collection(s): count=1 time=1340ms
2014-07-22 09:20:35,250 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:20:35,622 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:35,653 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:20:35,686 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 104497 synced till here 104470
2014-07-22 09:20:36,045 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046028816 with entries=103, filesize=86.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046035623
2014-07-22 09:20:36,045 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045953538
2014-07-22 09:20:36,045 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045957550
2014-07-22 09:20:38,550 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:39,898 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 104623 synced till here 104620
2014-07-22 09:20:39,961 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046035623 with entries=126, filesize=105.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046038551
2014-07-22 09:20:40,481 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10005,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406046030475,"queuetimems":0,"class":"HRegionServer","responsesize":18785,"method":"Multi"}
2014-07-22 09:20:40,499 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10059,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406046030439,"queuetimems":1606,"class":"HRegionServer","responsesize":16183,"method":"Multi"}
2014-07-22 09:20:40,616 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11816,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406046028799,"queuetimems":0,"class":"HRegionServer","responsesize":17533,"method":"Multi"}
2014-07-22 09:20:40,861 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:40,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 104710 synced till here 104696
2014-07-22 09:20:41,064 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046038551 with entries=87, filesize=74.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046040861
2014-07-22 09:20:42,647 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:42,700 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046040861 with entries=75, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046042648
2014-07-22 09:20:43,427 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=20003, memsize=389.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/07279b95856d4074a51e8112a9800e92
2014-07-22 09:20:43,447 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/07279b95856d4074a51e8112a9800e92 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/07279b95856d4074a51e8112a9800e92
2014-07-22 09:20:43,465 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/07279b95856d4074a51e8112a9800e92, entries=1418260, sequenceid=20003, filesize=101.1m
2014-07-22 09:20:43,466 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~910.3m/954533520, currentsize=439.9m/461216480 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 30254ms, sequenceid=20003, compaction requested=true
2014-07-22 09:20:43,467 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:20:43,467 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 49 store files, 0 compacting, 49 eligible, 2000 blocking
2014-07-22 09:20:43,467 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 886.6m
2014-07-22 09:20:43,467 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 49 files from compaction candidates
2014-07-22 09:20:43,468 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:20:43,468 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:20:43,468 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:20:43,780 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:20:44,945 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:44,966 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 104871 synced till here 104867
2014-07-22 09:20:45,015 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046042648 with entries=86, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046044945
2014-07-22 09:20:45,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045959717
2014-07-22 09:20:45,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045961039
2014-07-22 09:20:45,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045962420
2014-07-22 09:20:45,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045964141
2014-07-22 09:20:45,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045965034
2014-07-22 09:20:45,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045966127
2014-07-22 09:20:45,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045967487
2014-07-22 09:20:45,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045968946
2014-07-22 09:20:45,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045970846
2014-07-22 09:20:45,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045972997
2014-07-22 09:20:45,290 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:20:45,810 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:45,854 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 104953 synced till here 104952
2014-07-22 09:20:45,905 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046044945 with entries=82, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046045810
2014-07-22 09:20:47,567 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:47,596 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 105037 synced till here 105031
2014-07-22 09:20:47,703 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046045810 with entries=84, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046047568
2014-07-22 09:20:49,424 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:49,449 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 105137 synced till here 105127
2014-07-22 09:20:49,514 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046047568 with entries=100, filesize=71.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046049424
2014-07-22 09:20:51,102 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:51,118 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 105221 synced till here 105217
2014-07-22 09:20:51,212 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046049424 with entries=84, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046051103
2014-07-22 09:20:52,478 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:52,505 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 105308 synced till here 105307
2014-07-22 09:20:52,533 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046051103 with entries=87, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046052478
2014-07-22 09:20:53,861 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:53,883 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 105392 synced till here 105388
2014-07-22 09:20:53,948 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046052478 with entries=84, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046053861
2014-07-22 09:20:55,459 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:55,483 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 105484 synced till here 105468
2014-07-22 09:20:55,571 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046053861 with entries=92, filesize=68.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046055459
2014-07-22 09:20:56,292 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:20:56,347 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 105567 synced till here 105559
2014-07-22 09:20:57,191 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046055459 with entries=83, filesize=70.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046056292
2014-07-22 09:20:57,596 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,597 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,624 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,627 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,663 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,708 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,734 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,735 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,735 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,736 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,747 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,748 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,749 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,749 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,750 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,755 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,757 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,769 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,771 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,774 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,812 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,816 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,817 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,854 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,855 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,856 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,860 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,861 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,874 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,926 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,965 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,965 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,967 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,968 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,970 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:57,973 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:58,024 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:58,096 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:58,150 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:58,153 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:58,210 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:58,256 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:58,302 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:58,306 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:58,306 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:58,345 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:58,345 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:58,345 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:58,345 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:58,350 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:20:59,772 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1148ms
GC pool 'ParNew' had collection(s): count=1 time=1216ms
2014-07-22 09:21:01,514 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=20181, memsize=377.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/63e6fd357f9143e7ac501065532e7362
2014-07-22 09:21:01,539 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/63e6fd357f9143e7ac501065532e7362 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/63e6fd357f9143e7ac501065532e7362
2014-07-22 09:21:01,556 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/63e6fd357f9143e7ac501065532e7362, entries=1372680, sequenceid=20181, filesize=97.8m
2014-07-22 09:21:01,556 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~854.0m/895468080, currentsize=422.7m/443230080 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 28379ms, sequenceid=20181, compaction requested=true
2014-07-22 09:21:01,557 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:21:01,557 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 51 store files, 0 compacting, 51 eligible, 2000 blocking
2014-07-22 09:21:01,557 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 51 files from compaction candidates
2014-07-22 09:21:01,557 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:21:01,558 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 793.3m
2014-07-22 09:21:01,558 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:21:01,558 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:21:01,558 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3207ms
2014-07-22 09:21:01,558 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,558 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3213ms
2014-07-22 09:21:01,558 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,559 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3213ms
2014-07-22 09:21:01,559 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,560 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3214ms
2014-07-22 09:21:01,560 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,560 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3215ms
2014-07-22 09:21:01,560 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,562 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3255ms
2014-07-22 09:21:01,562 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,562 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3256ms
2014-07-22 09:21:01,562 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,563 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3260ms
2014-07-22 09:21:01,563 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,563 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3307ms
2014-07-22 09:21:01,563 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,565 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3354ms
2014-07-22 09:21:01,565 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,565 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3412ms
2014-07-22 09:21:01,565 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,566 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3416ms
2014-07-22 09:21:01,566 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,567 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3471ms
2014-07-22 09:21:01,567 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,578 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3554ms
2014-07-22 09:21:01,578 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,578 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3606ms
2014-07-22 09:21:01,578 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,579 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3609ms
2014-07-22 09:21:01,579 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,581 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3613ms
2014-07-22 09:21:01,581 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,582 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3615ms
2014-07-22 09:21:01,582 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,583 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3618ms
2014-07-22 09:21:01,583 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,583 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3618ms
2014-07-22 09:21:01,583 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,584 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3658ms
2014-07-22 09:21:01,584 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,584 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3710ms
2014-07-22 09:21:01,584 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,584 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3723ms
2014-07-22 09:21:01,584 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,584 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3724ms
2014-07-22 09:21:01,584 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,584 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3729ms
2014-07-22 09:21:01,585 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,589 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3734ms
2014-07-22 09:21:01,589 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,591 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3736ms
2014-07-22 09:21:01,591 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,591 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3774ms
2014-07-22 09:21:01,591 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,591 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3775ms
2014-07-22 09:21:01,592 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,593 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3781ms
2014-07-22 09:21:01,613 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,613 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3839ms
2014-07-22 09:21:01,614 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,615 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3843ms
2014-07-22 09:21:01,616 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,617 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3847ms
2014-07-22 09:21:01,617 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,617 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3860ms
2014-07-22 09:21:01,617 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,621 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3866ms
2014-07-22 09:21:01,621 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,622 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3872ms
2014-07-22 09:21:01,622 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,622 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3873ms
2014-07-22 09:21:01,622 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,622 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3873ms
2014-07-22 09:21:01,622 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,623 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3875ms
2014-07-22 09:21:01,624 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,624 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3877ms
2014-07-22 09:21:01,624 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,633 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3897ms
2014-07-22 09:21:01,634 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,634 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3899ms
2014-07-22 09:21:01,634 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,634 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3899ms
2014-07-22 09:21:01,634 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,634 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3900ms
2014-07-22 09:21:01,634 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,641 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3933ms
2014-07-22 09:21:01,641 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,642 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3979ms
2014-07-22 09:21:01,643 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,652 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4025ms
2014-07-22 09:21:01,652 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,652 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4028ms
2014-07-22 09:21:01,652 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,666 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4068ms
2014-07-22 09:21:01,666 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,673 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4077ms
2014-07-22 09:21:01,673 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:01,886 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:21:02,120 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:21:02,293 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:21:02,335 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 105723 synced till here 105691
2014-07-22 09:21:03,682 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046056292 with entries=156, filesize=106.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046062121
2014-07-22 09:21:03,683 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045974519
2014-07-22 09:21:03,683 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045976981
2014-07-22 09:21:03,683 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045979896
2014-07-22 09:21:04,496 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:21:05,341 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 105851 synced till here 105812
2014-07-22 09:21:05,800 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046062121 with entries=128, filesize=99.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046064496
2014-07-22 09:21:06,465 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:21:06,540 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 105958 synced till here 105922
2014-07-22 09:21:07,275 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046064496 with entries=107, filesize=77.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046066465
2014-07-22 09:21:08,153 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:21:08,204 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 106060 synced till here 106044
2014-07-22 09:21:09,247 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046066465 with entries=102, filesize=80.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046068154
2014-07-22 09:21:09,904 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:21:10,002 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 106156 synced till here 106146
2014-07-22 09:21:10,521 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=20290, memsize=379.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/9da66af12b8c4a90b4b0071972c35f33
2014-07-22 09:21:10,522 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046068154 with entries=96, filesize=72.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046069904
2014-07-22 09:21:10,573 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/9da66af12b8c4a90b4b0071972c35f33 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/9da66af12b8c4a90b4b0071972c35f33
2014-07-22 09:21:10,584 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/9da66af12b8c4a90b4b0071972c35f33, entries=1381240, sequenceid=20290, filesize=98.4m
2014-07-22 09:21:10,584 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~889.6m/932865040, currentsize=430.0m/450927520 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 27117ms, sequenceid=20290, compaction requested=true
2014-07-22 09:21:10,585 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:21:10,585 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 51 store files, 0 compacting, 51 eligible, 2000 blocking
2014-07-22 09:21:10,585 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 51 files from compaction candidates
2014-07-22 09:21:10,585 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 876.0m
2014-07-22 09:21:10,585 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:21:10,585 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:21:10,585 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:21:10,593 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:21:11,786 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:21:11,988 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:21:12,244 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 106256 synced till here 106255
2014-07-22 09:21:12,274 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046069904 with entries=100, filesize=76.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046071988
2014-07-22 09:21:12,274 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045982019
2014-07-22 09:21:12,274 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045984240
2014-07-22 09:21:12,275 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045986471
2014-07-22 09:21:12,275 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045988773
2014-07-22 09:21:12,275 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045990987
2014-07-22 09:21:12,275 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045993444
2014-07-22 09:21:12,275 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045995139
2014-07-22 09:21:12,383 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:21:13,966 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:21:13,984 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 106350 synced till here 106343
2014-07-22 09:21:14,088 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046071988 with entries=94, filesize=69.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046073966
2014-07-22 09:21:14,089 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:21:15,356 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:21:15,376 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 106430 synced till here 106426
2014-07-22 09:21:15,420 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046073966 with entries=80, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046075356
2014-07-22 09:21:15,420 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:21:16,813 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:21:16,839 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 106512 synced till here 106510
2014-07-22 09:21:16,890 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046075356 with entries=82, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046076814
2014-07-22 09:21:16,890 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:21:19,209 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:21:19,224 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 106588 synced till here 106587
2014-07-22 09:21:19,235 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046076814 with entries=76, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046079210
2014-07-22 09:21:19,236 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:21:20,238 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:21:20,359 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046079210 with entries=97, filesize=69.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046080239
2014-07-22 09:21:20,360 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:21:22,293 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:21:22,313 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 106776 synced till here 106773
2014-07-22 09:21:22,342 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046080239 with entries=91, filesize=64.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046082294
2014-07-22 09:21:22,343 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:21:23,486 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:21:24,704 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 106888 synced till here 106875
2014-07-22 09:21:24,811 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:24,817 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:24,819 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:24,819 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:24,820 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:24,829 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046082294 with entries=112, filesize=87.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046083486
2014-07-22 09:21:24,829 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:21:24,882 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:24,926 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:24,927 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:24,929 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:24,970 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:24,971 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:24,971 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:24,971 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:24,972 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:24,973 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:24,974 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:24,974 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:24,976 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:24,977 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:24,979 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:24,979 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,007 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,009 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,010 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,012 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,060 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,097 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,134 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,134 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,138 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,179 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,220 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,221 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,222 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,226 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,261 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,262 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,262 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,314 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,355 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,389 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,427 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,467 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,503 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,549 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,609 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,663 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,745 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,794 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:25,848 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:26,813 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=20430, memsize=435.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/fcab20a1bbd74fda892cb6ffa6fab707
2014-07-22 09:21:26,833 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/fcab20a1bbd74fda892cb6ffa6fab707 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/fcab20a1bbd74fda892cb6ffa6fab707
2014-07-22 09:21:26,852 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/fcab20a1bbd74fda892cb6ffa6fab707, entries=1586090, sequenceid=20430, filesize=113.0m
2014-07-22 09:21:26,852 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~793.3m/831862480, currentsize=418.2m/438542320 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 25294ms, sequenceid=20430, compaction requested=true
2014-07-22 09:21:26,853 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:21:26,853 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 50 store files, 0 compacting, 50 eligible, 2000 blocking
2014-07-22 09:21:26,853 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1005ms
2014-07-22 09:21:26,853 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 50 files from compaction candidates
2014-07-22 09:21:26,853 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 840.4m
2014-07-22 09:21:26,853 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,853 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:21:26,853 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:21:26,853 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:21:26,853 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1059ms
2014-07-22 09:21:26,854 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,854 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1109ms
2014-07-22 09:21:26,854 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,854 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1191ms
2014-07-22 09:21:26,854 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,854 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1245ms
2014-07-22 09:21:26,854 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,857 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1308ms
2014-07-22 09:21:26,857 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,858 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1355ms
2014-07-22 09:21:26,858 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,858 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1391ms
2014-07-22 09:21:26,858 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,858 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1431ms
2014-07-22 09:21:26,858 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,858 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1469ms
2014-07-22 09:21:26,858 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,858 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1503ms
2014-07-22 09:21:26,858 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,860 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1546ms
2014-07-22 09:21:26,860 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,860 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1598ms
2014-07-22 09:21:26,860 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,861 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1599ms
2014-07-22 09:21:26,861 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,861 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1600ms
2014-07-22 09:21:26,861 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,861 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1635ms
2014-07-22 09:21:26,861 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,861 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1639ms
2014-07-22 09:21:26,861 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,861 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1640ms
2014-07-22 09:21:26,862 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,862 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1642ms
2014-07-22 09:21:26,862 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,862 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1683ms
2014-07-22 09:21:26,862 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,862 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1724ms
2014-07-22 09:21:26,862 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,863 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1729ms
2014-07-22 09:21:26,863 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,863 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1729ms
2014-07-22 09:21:26,863 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,869 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1772ms
2014-07-22 09:21:26,869 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,869 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1809ms
2014-07-22 09:21:26,869 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,870 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1858ms
2014-07-22 09:21:26,870 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,870 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1860ms
2014-07-22 09:21:26,870 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,877 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1868ms
2014-07-22 09:21:26,877 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,877 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1870ms
2014-07-22 09:21:26,877 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,878 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1899ms
2014-07-22 09:21:26,878 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,880 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1901ms
2014-07-22 09:21:26,880 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,884 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1907ms
2014-07-22 09:21:26,884 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,884 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1908ms
2014-07-22 09:21:26,884 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,884 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1910ms
2014-07-22 09:21:26,884 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,884 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1911ms
2014-07-22 09:21:26,884 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,884 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1911ms
2014-07-22 09:21:26,884 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,885 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1913ms
2014-07-22 09:21:26,885 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,885 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1914ms
2014-07-22 09:21:26,885 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,885 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1914ms
2014-07-22 09:21:26,885 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,885 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1914ms
2014-07-22 09:21:26,885 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,885 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1915ms
2014-07-22 09:21:26,886 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,889 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1960ms
2014-07-22 09:21:26,889 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,890 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1963ms
2014-07-22 09:21:26,890 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,893 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1967ms
2014-07-22 09:21:26,893 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,893 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2011ms
2014-07-22 09:21:26,893 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,897 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2078ms
2014-07-22 09:21:26,897 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,897 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2086ms
2014-07-22 09:21:26,898 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,928 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2109ms
2014-07-22 09:21:26,929 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,933 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2117ms
2014-07-22 09:21:26,933 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:26,933 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2122ms
2014-07-22 09:21:26,934 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:28,595 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:21:28,916 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:21:28,990 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:21:29,335 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 107028 synced till here 107003
2014-07-22 09:21:29,529 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046083486 with entries=140, filesize=99.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046088990
2014-07-22 09:21:29,530 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:21:31,443 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:21:31,491 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 107137 synced till here 107102
2014-07-22 09:21:31,568 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046088990 with entries=109, filesize=90.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046091443
2014-07-22 09:21:31,568 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=44, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:21:33,091 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:21:33,138 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 107249 synced till here 107235
2014-07-22 09:21:33,275 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046091443 with entries=112, filesize=74.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046093092
2014-07-22 09:21:33,276 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=45, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:21:35,166 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:21:35,183 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 107331 synced till here 107323
2014-07-22 09:21:35,255 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046093092 with entries=82, filesize=66.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046095167
2014-07-22 09:21:35,256 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=46, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:21:36,456 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:21:36,470 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=20528, memsize=358.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/34830922d81d4dcfa19336adcc646550
2014-07-22 09:21:36,482 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 107428 synced till here 107424
2014-07-22 09:21:36,504 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/34830922d81d4dcfa19336adcc646550 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/34830922d81d4dcfa19336adcc646550
2014-07-22 09:21:36,554 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/34830922d81d4dcfa19336adcc646550, entries=1304440, sequenceid=20528, filesize=92.9m
2014-07-22 09:21:36,555 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~884.9m/927900480, currentsize=408.7m/428554000 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 25970ms, sequenceid=20528, compaction requested=true
2014-07-22 09:21:36,556 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:21:36,556 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 50 store files, 0 compacting, 50 eligible, 2000 blocking
2014-07-22 09:21:36,556 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 846.0m
2014-07-22 09:21:36,556 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 50 files from compaction candidates
2014-07-22 09:21:36,556 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:21:36,556 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:21:36,556 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:21:36,879 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046095167 with entries=97, filesize=65.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046096457
2014-07-22 09:21:36,880 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=47, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:21:37,166 DEBUG [RpcServer.handler=37,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:21:37,551 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b.
2014-07-22 09:21:37,748 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:21:37,822 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:21:38,417 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 107511 synced till here 107509
2014-07-22 09:21:38,451 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046096457 with entries=83, filesize=71.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046097748
2014-07-22 09:21:38,453 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=48, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:21:39,123 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:21:39,148 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 107598 synced till here 107596
2014-07-22 09:21:39,180 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046097748 with entries=87, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046099124
2014-07-22 09:21:39,180 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=49, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:21:40,553 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:21:40,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 107698 synced till here 107690
2014-07-22 09:21:40,673 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046099124 with entries=100, filesize=68.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046100553
2014-07-22 09:21:40,674 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=50, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:21:42,067 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:21:42,313 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 107807 synced till here 107792
2014-07-22 09:21:42,364 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046100553 with entries=109, filesize=74.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046102068
2014-07-22 09:21:42,365 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=51, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:21:44,019 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:21:44,136 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 107899 synced till here 107881
2014-07-22 09:21:44,299 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046102068 with entries=92, filesize=76.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046104019
2014-07-22 09:21:44,299 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=52, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:21:45,787 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:21:45,807 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 107980 synced till here 107979
2014-07-22 09:21:45,833 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046104019 with entries=81, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046105788
2014-07-22 09:21:45,833 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=53, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:21:47,740 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:21:47,763 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 108098 synced till here 108091
2014-07-22 09:21:47,903 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046105788 with entries=118, filesize=79.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046107741
2014-07-22 09:21:47,904 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=54, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:21:50,089 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1346ms
GC pool 'ParNew' had collection(s): count=1 time=1824ms
2014-07-22 09:21:50,195 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,197 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,206 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,208 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,209 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,240 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,244 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,283 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,287 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,324 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,327 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,328 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,328 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,332 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,334 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,336 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,337 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,396 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,398 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,398 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,398 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,399 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,399 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,400 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,460 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,487 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,535 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,577 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,603 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,652 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,684 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,698 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,755 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,795 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,823 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,856 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,889 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,891 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,891 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,893 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,894 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,895 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,941 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:50,988 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:51,054 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:51,103 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:51,153 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:51,159 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:51,202 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:51,204 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:21:52,290 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=20677, memsize=328.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/9a875d706cfc4e6ea75e9c34900cb74d
2014-07-22 09:21:52,311 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/9a875d706cfc4e6ea75e9c34900cb74d as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/9a875d706cfc4e6ea75e9c34900cb74d
2014-07-22 09:21:52,328 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/9a875d706cfc4e6ea75e9c34900cb74d, entries=1195900, sequenceid=20677, filesize=85.2m
2014-07-22 09:21:52,329 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~840.4m/881247680, currentsize=401.9m/421463680 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 25476ms, sequenceid=20677, compaction requested=true
2014-07-22 09:21:52,329 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:21:52,329 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 52 store files, 0 compacting, 52 eligible, 2000 blocking
2014-07-22 09:21:52,330 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1125ms
2014-07-22 09:21:52,330 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,330 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 52 files from compaction candidates
2014-07-22 09:21:52,330 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b., current region memstore size 300.9m
2014-07-22 09:21:52,330 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:21:52,330 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:21:52,330 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:21:52,334 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1132ms
2014-07-22 09:21:52,334 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,334 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1175ms
2014-07-22 09:21:52,334 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,334 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1182ms
2014-07-22 09:21:52,334 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,335 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1232ms
2014-07-22 09:21:52,335 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,335 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1281ms
2014-07-22 09:21:52,335 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,335 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1347ms
2014-07-22 09:21:52,335 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,335 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1394ms
2014-07-22 09:21:52,335 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,336 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1441ms
2014-07-22 09:21:52,336 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,336 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1442ms
2014-07-22 09:21:52,336 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,337 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1444ms
2014-07-22 09:21:52,337 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,337 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1446ms
2014-07-22 09:21:52,338 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,338 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1447ms
2014-07-22 09:21:52,338 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,338 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1449ms
2014-07-22 09:21:52,338 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,338 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1483ms
2014-07-22 09:21:52,338 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,345 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1523ms
2014-07-22 09:21:52,346 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,349 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1555ms
2014-07-22 09:21:52,349 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,349 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1594ms
2014-07-22 09:21:52,349 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,349 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1651ms
2014-07-22 09:21:52,349 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,349 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1665ms
2014-07-22 09:21:52,349 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,352 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1700ms
2014-07-22 09:21:52,352 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,353 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1750ms
2014-07-22 09:21:52,353 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,355 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1778ms
2014-07-22 09:21:52,355 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,358 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1824ms
2014-07-22 09:21:52,358 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,358 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1871ms
2014-07-22 09:21:52,358 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,358 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1899ms
2014-07-22 09:21:52,358 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,359 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1959ms
2014-07-22 09:21:52,359 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,362 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1964ms
2014-07-22 09:21:52,362 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,362 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1964ms
2014-07-22 09:21:52,363 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,363 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1965ms
2014-07-22 09:21:52,363 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,363 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1965ms
2014-07-22 09:21:52,363 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,363 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1965ms
2014-07-22 09:21:52,363 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,363 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1967ms
2014-07-22 09:21:52,363 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,369 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2032ms
2014-07-22 09:21:52,369 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,370 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2034ms
2014-07-22 09:21:52,370 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,370 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2036ms
2014-07-22 09:21:52,370 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,370 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2038ms
2014-07-22 09:21:52,370 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,370 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2042ms
2014-07-22 09:21:52,370 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,373 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2045ms
2014-07-22 09:21:52,373 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,377 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2050ms
2014-07-22 09:21:52,377 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,377 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2053ms
2014-07-22 09:21:52,377 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,381 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2095ms
2014-07-22 09:21:52,381 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,381 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2099ms
2014-07-22 09:21:52,382 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,382 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2138ms
2014-07-22 09:21:52,382 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,382 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2143ms
2014-07-22 09:21:52,382 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,382 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2173ms
2014-07-22 09:21:52,382 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,382 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2174ms
2014-07-22 09:21:52,382 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,383 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2177ms
2014-07-22 09:21:52,383 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,386 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2189ms
2014-07-22 09:21:52,386 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:52,386 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2191ms
2014-07-22 09:21:52,386 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:21:53,919 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1328ms
GC pool 'ParNew' had collection(s): count=1 time=1470ms
2014-07-22 09:21:54,151 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:21:54,152 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:21:54,275 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:21:54,529 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 108248 synced till here 108232
2014-07-22 09:21:54,718 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046107741 with entries=150, filesize=103.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046114151
2014-07-22 09:21:56,800 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:21:56,829 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 108360 synced till here 108325
2014-07-22 09:21:57,240 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046114151 with entries=112, filesize=94.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046116800
2014-07-22 09:21:59,182 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:21:59,203 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 108488 synced till here 108454
2014-07-22 09:22:00,554 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046116800 with entries=128, filesize=92.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046119183
2014-07-22 09:22:01,385 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:22:02,638 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046119183 with entries=129, filesize=90.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046121386
2014-07-22 09:22:02,974 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=20787, memsize=327.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/fbf03665771643d59776ba9a56afa055
2014-07-22 09:22:02,984 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/fbf03665771643d59776ba9a56afa055 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/fbf03665771643d59776ba9a56afa055
2014-07-22 09:22:02,993 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/fbf03665771643d59776ba9a56afa055, entries=1191410, sequenceid=20787, filesize=84.8m
2014-07-22 09:22:02,994 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~847.8m/889017280, currentsize=414.7m/434817120 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 26438ms, sequenceid=20787, compaction requested=true
2014-07-22 09:22:02,994 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:22:02,994 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 52 store files, 0 compacting, 52 eligible, 2000 blocking
2014-07-22 09:22:02,994 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 52 files from compaction candidates
2014-07-22 09:22:02,994 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 970.9m
2014-07-22 09:22:02,995 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:22:02,995 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:22:02,995 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:22:03,141 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:22:03,352 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:22:03,703 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 108742 synced till here 108737
2014-07-22 09:22:04,522 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046121386 with entries=125, filesize=103.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046123352
2014-07-22 09:22:05,136 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:22:05,405 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:22:05,486 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 108855 synced till here 108840
2014-07-22 09:22:05,619 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046123352 with entries=113, filesize=83.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046125406
2014-07-22 09:22:07,352 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:22:07,566 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 108973 synced till here 108958
2014-07-22 09:22:07,711 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046125406 with entries=118, filesize=82.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046127352
2014-07-22 09:22:09,586 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:22:09,625 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 109072 synced till here 109069
2014-07-22 09:22:09,689 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046127352 with entries=99, filesize=66.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046129587
2014-07-22 09:22:10,004 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=24749, memsize=225.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/c3de90eebf974fadaa649f31eeb1b43b
2014-07-22 09:22:10,019 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/c3de90eebf974fadaa649f31eeb1b43b as hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/c3de90eebf974fadaa649f31eeb1b43b
2014-07-22 09:22:10,033 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/c3de90eebf974fadaa649f31eeb1b43b, entries=819460, sequenceid=24749, filesize=58.4m
2014-07-22 09:22:10,033 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~300.9m/315498080, currentsize=47.3m/49646880 for region usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. in 17703ms, sequenceid=24749, compaction requested=true
2014-07-22 09:22:10,034 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:22:10,034 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 2000 blocking
2014-07-22 09:22:10,035 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 941.2m
2014-07-22 09:22:10,035 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-22 09:22:10,035 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:22:10,035 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:22:10,035 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. because compaction request was cancelled
2014-07-22 09:22:11,172 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:22:11,226 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 109161 synced till here 109155
2014-07-22 09:22:11,240 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:22:11,332 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046129587 with entries=89, filesize=69.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046131172
2014-07-22 09:22:11,332 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406045997746
2014-07-22 09:22:11,332 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046002860
2014-07-22 09:22:11,332 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046003870
2014-07-22 09:22:11,332 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046005472
2014-07-22 09:22:11,332 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046007584
2014-07-22 09:22:11,332 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046009208
2014-07-22 09:22:11,332 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046011172
2014-07-22 09:22:11,332 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046012581
2014-07-22 09:22:11,332 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046014518
2014-07-22 09:22:11,333 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046016606
2014-07-22 09:22:11,333 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046017911
2014-07-22 09:22:11,333 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046019399
2014-07-22 09:22:11,333 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046020420
2014-07-22 09:22:11,333 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046021838
2014-07-22 09:22:11,333 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046023573
2014-07-22 09:22:11,333 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046025820
2014-07-22 09:22:11,333 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046028816
2014-07-22 09:22:11,333 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046035623
2014-07-22 09:22:11,333 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046038551
2014-07-22 09:22:11,333 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046040861
2014-07-22 09:22:11,333 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046042648
2014-07-22 09:22:11,333 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046044945
2014-07-22 09:22:11,333 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046045810
2014-07-22 09:22:11,333 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046047568
2014-07-22 09:22:11,333 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046049424
2014-07-22 09:22:11,333 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046051103
2014-07-22 09:22:11,333 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046052478
2014-07-22 09:22:11,334 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046053861
2014-07-22 09:22:11,334 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046055459
2014-07-22 09:22:12,792 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:22:12,826 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 109237 synced till here 109233
2014-07-22 09:22:12,855 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046131172 with entries=76, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046132793
2014-07-22 09:22:14,160 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:22:14,237 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 109336 synced till here 109317
2014-07-22 09:22:14,338 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046132793 with entries=99, filesize=71.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046134161
2014-07-22 09:22:15,765 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:22:15,789 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 109425 synced till here 109419
2014-07-22 09:22:15,863 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046134161 with entries=89, filesize=67.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046135766
2014-07-22 09:22:16,754 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:22:17,474 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 109508 synced till here 109506
2014-07-22 09:22:17,498 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046135766 with entries=83, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046136754
2014-07-22 09:22:18,632 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:22:18,659 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046136754 with entries=75, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046138632
2014-07-22 09:22:19,288 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,288 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,312 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,341 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,341 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,346 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,375 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,386 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,386 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,430 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,444 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,502 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,548 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,554 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,595 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,631 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,667 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,668 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,668 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,670 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,708 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,710 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,711 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,713 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,733 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,770 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,807 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,808 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,812 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,852 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,891 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,892 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,893 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,937 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,937 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,938 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:19,943 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:20,753 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:20,753 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:20,894 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:21,106 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:21,169 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:21,232 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:21,233 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:21,238 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:21,238 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:21,238 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:21,243 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:21,248 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:21,249 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:23,936 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1115ms
GC pool 'ParNew' had collection(s): count=1 time=1225ms
2014-07-22 09:22:24,288 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:22:24,289 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:22:24,313 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:22:24,341 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:22:24,342 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:22:24,347 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:22:24,375 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:22:24,386 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:22:24,387 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:22:24,431 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:22:24,446 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:22:24,504 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:22:24,549 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:22:24,554 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:22:24,595 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:22:24,631 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:22:24,668 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:22:24,668 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:22:24,669 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:22:24,670 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:22:24,708 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:22:24,710 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:22:24,711 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:22:24,714 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:22:24,734 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:22:24,770 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:22:24,808 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:22:24,809 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:22:24,812 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:22:24,853 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:22:24,891 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:22:24,892 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:22:24,893 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:22:24,937 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:22:24,937 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:22:24,939 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:22:24,944 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:22:25,754 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:22:25,755 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 09:22:25,895 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:22:26,106 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:22:26,125 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=21024, memsize=357.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/803bd25c5fac4c1898f343188ceaa2bf
2014-07-22 09:22:26,149 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/803bd25c5fac4c1898f343188ceaa2bf as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/803bd25c5fac4c1898f343188ceaa2bf
2014-07-22 09:22:26,161 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/803bd25c5fac4c1898f343188ceaa2bf, entries=1300140, sequenceid=21024, filesize=92.6m
2014-07-22 09:22:26,162 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1001.8m/1050479280, currentsize=283.4m/297165040 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 23167ms, sequenceid=21024, compaction requested=true
2014-07-22 09:22:26,162 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:22:26,162 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 51 store files, 0 compacting, 51 eligible, 2000 blocking
2014-07-22 09:22:26,162 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 51 files from compaction candidates
2014-07-22 09:22:26,162 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5056ms
2014-07-22 09:22:26,163 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:22:26,163 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 876.0m
2014-07-22 09:22:26,163 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,163 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:22:26,163 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:22:26,165 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5271ms
2014-07-22 09:22:26,165 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,165 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5412ms
2014-07-22 09:22:26,166 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,166 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5413ms
2014-07-22 09:22:26,166 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,166 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6224ms
2014-07-22 09:22:26,166 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,166 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6228ms
2014-07-22 09:22:26,166 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,166 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6230ms
2014-07-22 09:22:26,167 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,169 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:22:26,169 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,177 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6241ms
2014-07-22 09:22:26,177 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,178 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6285ms
2014-07-22 09:22:26,178 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,178 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6286ms
2014-07-22 09:22:26,178 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,179 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6287ms
2014-07-22 09:22:26,179 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,181 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6328ms
2014-07-22 09:22:26,181 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,185 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6373ms
2014-07-22 09:22:26,185 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,185 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6378ms
2014-07-22 09:22:26,185 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,186 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6379ms
2014-07-22 09:22:26,186 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,193 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6423ms
2014-07-22 09:22:26,193 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,194 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6460ms
2014-07-22 09:22:26,194 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,195 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6481ms
2014-07-22 09:22:26,195 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,196 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6484ms
2014-07-22 09:22:26,196 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,196 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6487ms
2014-07-22 09:22:26,196 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,199 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6491ms
2014-07-22 09:22:26,199 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,200 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6530ms
2014-07-22 09:22:26,200 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,200 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6532ms
2014-07-22 09:22:26,200 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,201 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6532ms
2014-07-22 09:22:26,201 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,202 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6535ms
2014-07-22 09:22:26,202 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,202 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6571ms
2014-07-22 09:22:26,202 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,203 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6608ms
2014-07-22 09:22:26,203 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,203 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6649ms
2014-07-22 09:22:26,203 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,204 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6655ms
2014-07-22 09:22:26,204 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,204 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6702ms
2014-07-22 09:22:26,205 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,205 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6761ms
2014-07-22 09:22:26,206 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,213 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6782ms
2014-07-22 09:22:26,213 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,218 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6831ms
2014-07-22 09:22:26,219 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,219 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6833ms
2014-07-22 09:22:26,219 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,225 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6849ms
2014-07-22 09:22:26,225 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,230 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6884ms
2014-07-22 09:22:26,230 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,231 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6889ms
2014-07-22 09:22:26,231 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,233 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:22:26,233 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,234 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:22:26,234 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,236 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6895ms
2014-07-22 09:22:26,236 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,239 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:22:26,239 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,243 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6929ms
2014-07-22 09:22:26,243 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,245 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:22:26,245 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,248 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:22:26,248 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,249 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6960ms
2014-07-22 09:22:26,249 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,249 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:22:26,249 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,255 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6965ms
2014-07-22 09:22:26,255 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,255 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5017ms
2014-07-22 09:22:26,255 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,255 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5017ms
2014-07-22 09:22:26,255 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:26,574 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:22:27,966 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1026ms
GC pool 'ParNew' had collection(s): count=1 time=1200ms
2014-07-22 09:22:28,184 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:22:28,256 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 109714 synced till here 109694
2014-07-22 09:22:28,398 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:22:28,442 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046138632 with entries=131, filesize=81.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046148185
2014-07-22 09:22:28,442 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046056292
2014-07-22 09:22:28,442 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046062121
2014-07-22 09:22:28,442 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046064496
2014-07-22 09:22:28,442 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046066465
2014-07-22 09:22:28,442 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046068154
2014-07-22 09:22:30,334 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:22:30,390 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 109816 synced till here 109790
2014-07-22 09:22:30,628 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10694,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406046139934,"queuetimems":0,"class":"HRegionServer","responsesize":18495,"method":"Multi"}
2014-07-22 09:22:30,630 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10738,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406046139889,"queuetimems":0,"class":"HRegionServer","responsesize":18009,"method":"Multi"}
2014-07-22 09:22:30,632 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10781,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406046139850,"queuetimems":0,"class":"HRegionServer","responsesize":19053,"method":"Multi"}
2014-07-22 09:22:30,654 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10921,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406046139732,"queuetimems":0,"class":"HRegionServer","responsesize":9770,"method":"Multi"}
2014-07-22 09:22:30,683 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046148185 with entries=102, filesize=92.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046150334
2014-07-22 09:22:31,016 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11388,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406046139628,"queuetimems":0,"class":"HRegionServer","responsesize":18592,"method":"Multi"}
2014-07-22 09:22:31,016 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11211,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406046139805,"queuetimems":0,"class":"HRegionServer","responsesize":18552,"method":"Multi"}
2014-07-22 09:22:31,020 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11579,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406046139441,"queuetimems":0,"class":"HRegionServer","responsesize":18590,"method":"Multi"}
2014-07-22 09:22:31,016 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11632,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406046139384,"queuetimems":0,"class":"HRegionServer","responsesize":15347,"method":"Multi"}
2014-07-22 09:22:31,026 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11435,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406046139591,"queuetimems":0,"class":"HRegionServer","responsesize":18243,"method":"Multi"}
2014-07-22 09:22:31,034 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11535,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406046139498,"queuetimems":0,"class":"HRegionServer","responsesize":17844,"method":"Multi"}
2014-07-22 09:22:31,042 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11336,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406046139705,"queuetimems":0,"class":"HRegionServer","responsesize":18118,"method":"Multi"}
2014-07-22 09:22:31,042 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11495,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406046139546,"queuetimems":0,"class":"HRegionServer","responsesize":18608,"method":"Multi"}
2014-07-22 09:22:32,022 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12354,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406046139667,"queuetimems":0,"class":"HRegionServer","responsesize":18452,"method":"Multi"}
2014-07-22 09:22:32,022 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12253,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406046139768,"queuetimems":0,"class":"HRegionServer","responsesize":17181,"method":"Multi"}
2014-07-22 09:22:32,045 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12706,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54557","starttimems":1406046139338,"queuetimems":1,"class":"HRegionServer","responsesize":18691,"method":"Multi"}
2014-07-22 09:22:32,309 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:22:32,367 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 109948 synced till here 109913
2014-07-22 09:22:32,602 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046150334 with entries=132, filesize=94.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046152309
2014-07-22 09:22:33,986 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1086ms
GC pool 'ParNew' had collection(s): count=1 time=1115ms
2014-07-22 09:22:34,978 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:22:35,998 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=21088, memsize=382.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/ac377ff59e6a4da395ce9842ee8e3499
2014-07-22 09:22:36,027 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/ac377ff59e6a4da395ce9842ee8e3499 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/ac377ff59e6a4da395ce9842ee8e3499
2014-07-22 09:22:36,035 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 110076 synced till here 110075
2014-07-22 09:22:36,071 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/ac377ff59e6a4da395ce9842ee8e3499, entries=1393020, sequenceid=21088, filesize=99.3m
2014-07-22 09:22:36,072 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~944.7m/990540400, currentsize=286.9m/300811120 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 26038ms, sequenceid=21088, compaction requested=true
2014-07-22 09:22:36,073 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:22:36,073 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 51 store files, 0 compacting, 51 eligible, 2000 blocking
2014-07-22 09:22:36,074 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 881.8m
2014-07-22 09:22:36,074 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 51 files from compaction candidates
2014-07-22 09:22:36,074 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:22:36,074 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:22:36,074 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:22:36,085 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046152309 with entries=128, filesize=95.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046154978
2014-07-22 09:22:36,085 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046069904
2014-07-22 09:22:36,085 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046071988
2014-07-22 09:22:36,085 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046073966
2014-07-22 09:22:36,085 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046075356
2014-07-22 09:22:36,085 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046076814
2014-07-22 09:22:36,085 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046079210
2014-07-22 09:22:36,085 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046080239
2014-07-22 09:22:36,085 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046082294
2014-07-22 09:22:36,894 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:22:36,895 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:22:38,008 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 110190 synced till here 110153
2014-07-22 09:22:38,376 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046154978 with entries=114, filesize=98.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046156894
2014-07-22 09:22:38,611 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:22:39,108 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:22:39,966 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 110301 synced till here 110273
2014-07-22 09:22:40,252 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046156894 with entries=111, filesize=81.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046159108
2014-07-22 09:22:42,311 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:22:42,383 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 110422 synced till here 110404
2014-07-22 09:22:42,515 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046159108 with entries=121, filesize=87.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046162311
2014-07-22 09:22:44,296 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:22:44,351 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 110551 synced till here 110527
2014-07-22 09:22:44,791 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046162311 with entries=129, filesize=87.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046164296
2014-07-22 09:22:46,163 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:22:46,202 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 110684 synced till here 110655
2014-07-22 09:22:46,446 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046164296 with entries=133, filesize=100.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046166164
2014-07-22 09:22:47,162 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:22:47,223 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 110768 synced till here 110757
2014-07-22 09:22:48,251 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046166164 with entries=84, filesize=68.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046167163
2014-07-22 09:22:49,092 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:22:49,117 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 110896 synced till here 110867
2014-07-22 09:22:51,138 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1491ms
GC pool 'ParNew' had collection(s): count=1 time=1912ms
2014-07-22 09:22:51,264 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046167163 with entries=128, filesize=95.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046169092
2014-07-22 09:22:52,332 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:22:52,394 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 110994 synced till here 110976
2014-07-22 09:22:52,581 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046169092 with entries=98, filesize=82.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046172333
2014-07-22 09:22:53,497 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:53,497 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:53,498 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:53,498 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:53,498 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:53,499 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:53,499 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:53,501 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:53,502 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:53,503 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:53,503 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:22:57,772 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=21195, memsize=383.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/1b617e30f18945398bc9cad8b1cadf99
2014-07-22 09:22:57,785 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/1b617e30f18945398bc9cad8b1cadf99 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/1b617e30f18945398bc9cad8b1cadf99
2014-07-22 09:22:57,805 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/1b617e30f18945398bc9cad8b1cadf99, entries=1395850, sequenceid=21195, filesize=99.5m
2014-07-22 09:22:57,806 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~876.0m/918578880, currentsize=463.6m/486085520 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 31643ms, sequenceid=21195, compaction requested=true
2014-07-22 09:22:57,807 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:22:57,807 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 53 store files, 0 compacting, 53 eligible, 2000 blocking
2014-07-22 09:22:57,807 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4304ms
2014-07-22 09:22:57,807 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:57,807 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 53 files from compaction candidates
2014-07-22 09:22:57,807 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:22:57,807 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:22:57,807 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:22:57,808 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4305ms
2014-07-22 09:22:57,808 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:57,808 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4306ms
2014-07-22 09:22:57,808 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:57,808 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 746.8m
2014-07-22 09:22:57,808 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4307ms
2014-07-22 09:22:57,808 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:57,809 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4310ms
2014-07-22 09:22:57,809 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:57,809 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4311ms
2014-07-22 09:22:57,809 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:57,814 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4316ms
2014-07-22 09:22:57,814 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:57,815 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4316ms
2014-07-22 09:22:57,815 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:57,815 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4317ms
2014-07-22 09:22:57,815 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:57,816 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4318ms
2014-07-22 09:22:57,816 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:57,816 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4320ms
2014-07-22 09:22:57,816 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:22:58,409 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:22:58,795 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:22:58,836 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046172333 with entries=95, filesize=61.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046178796
2014-07-22 09:22:58,836 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046083486
2014-07-22 09:22:58,836 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046088990
2014-07-22 09:22:58,836 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046091443
2014-07-22 09:22:58,836 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046093092
2014-07-22 09:22:58,837 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046095167
2014-07-22 09:22:58,975 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:23:00,147 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:00,198 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 111171 synced till here 111165
2014-07-22 09:23:00,246 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046178796 with entries=82, filesize=68.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046180148
2014-07-22 09:23:01,562 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:01,612 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 111271 synced till here 111263
2014-07-22 09:23:01,682 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046180148 with entries=100, filesize=68.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046181562
2014-07-22 09:23:03,230 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:03,250 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 111354 synced till here 111353
2014-07-22 09:23:03,265 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046181562 with entries=83, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046183230
2014-07-22 09:23:05,062 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:05,086 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 111445 synced till here 111444
2014-07-22 09:23:05,116 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046183230 with entries=91, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046185063
2014-07-22 09:23:05,341 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=21314, memsize=408.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/8dd4af79a0fc4e199ea5f8b23c4b9beb
2014-07-22 09:23:05,353 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/8dd4af79a0fc4e199ea5f8b23c4b9beb as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/8dd4af79a0fc4e199ea5f8b23c4b9beb
2014-07-22 09:23:05,363 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/8dd4af79a0fc4e199ea5f8b23c4b9beb, entries=1487120, sequenceid=21314, filesize=106.0m
2014-07-22 09:23:05,364 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~896.7m/940279600, currentsize=395.9m/415123440 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 29290ms, sequenceid=21314, compaction requested=true
2014-07-22 09:23:05,365 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:23:05,365 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 53 store files, 0 compacting, 53 eligible, 2000 blocking
2014-07-22 09:23:05,365 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 749.1m
2014-07-22 09:23:05,365 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 53 files from compaction candidates
2014-07-22 09:23:05,366 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:23:05,366 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:23:05,366 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:23:05,367 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:23:06,103 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:23:07,868 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:07,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 111530 synced till here 111528
2014-07-22 09:23:07,900 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046185063 with entries=85, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046187868
2014-07-22 09:23:07,901 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046096457
2014-07-22 09:23:07,901 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046097748
2014-07-22 09:23:07,901 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046099124
2014-07-22 09:23:07,901 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046100553
2014-07-22 09:23:07,901 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046102068
2014-07-22 09:23:07,901 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046104019
2014-07-22 09:23:07,901 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046105788
2014-07-22 09:23:08,318 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b.
2014-07-22 09:23:08,949 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:08,971 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 111621 synced till here 111619
2014-07-22 09:23:09,002 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046187868 with entries=91, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046188950
2014-07-22 09:23:09,003 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:23:10,475 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:10,494 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 111710 synced till here 111708
2014-07-22 09:23:10,499 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=6603, hits=3, hitRatio=0.04%, , cachingAccesses=5, cachingHits=3, cachingHitsRatio=60.00%, evictions=0, evicted=0, evictedPerRun=NaN
2014-07-22 09:23:10,516 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046188950 with entries=89, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046190475
2014-07-22 09:23:10,517 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:23:11,772 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:11,797 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 111800 synced till here 111797
2014-07-22 09:23:11,822 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046190475 with entries=90, filesize=65.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046191773
2014-07-22 09:23:11,823 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:23:13,119 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:13,145 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 111870 synced till here 111868
2014-07-22 09:23:13,395 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046191773 with entries=70, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046193119
2014-07-22 09:23:13,396 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:23:14,153 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=21457, memsize=266.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/8cb8f3cf4b9543b9a7d5f0db48299af6
2014-07-22 09:23:14,167 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/8cb8f3cf4b9543b9a7d5f0db48299af6 as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/8cb8f3cf4b9543b9a7d5f0db48299af6
2014-07-22 09:23:14,178 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/8cb8f3cf4b9543b9a7d5f0db48299af6, entries=969340, sequenceid=21457, filesize=69.0m
2014-07-22 09:23:14,179 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~746.8m/783126560, currentsize=267.4m/280336960 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 16370ms, sequenceid=21457, compaction requested=true
2014-07-22 09:23:14,179 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:23:14,179 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 52 store files, 0 compacting, 52 eligible, 2000 blocking
2014-07-22 09:23:14,179 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 730.2m
2014-07-22 09:23:14,179 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 52 files from compaction candidates
2014-07-22 09:23:14,180 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:23:14,180 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:23:14,180 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:23:14,244 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:23:15,052 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:15,068 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 111957 synced till here 111953
2014-07-22 09:23:15,080 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:23:15,134 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046193119 with entries=87, filesize=64.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046195052
2014-07-22 09:23:15,134 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:23:17,377 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:17,404 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046195052 with entries=70, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046197378
2014-07-22 09:23:17,405 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:23:18,282 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:18,311 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 112114 synced till here 112113
2014-07-22 09:23:18,332 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046197378 with entries=87, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046198282
2014-07-22 09:23:18,332 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:23:20,006 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=21523, memsize=275.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/b3b4e7a7088f43fda64fb249a3e4c4d7
2014-07-22 09:23:20,022 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/b3b4e7a7088f43fda64fb249a3e4c4d7 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/b3b4e7a7088f43fda64fb249a3e4c4d7
2014-07-22 09:23:20,033 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/b3b4e7a7088f43fda64fb249a3e4c4d7, entries=1001430, sequenceid=21523, filesize=71.4m
2014-07-22 09:23:20,034 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~756.3m/793029760, currentsize=234.8m/246241120 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 14669ms, sequenceid=21523, compaction requested=true
2014-07-22 09:23:20,035 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 52 store files, 0 compacting, 52 eligible, 2000 blocking
2014-07-22 09:23:20,035 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 52 files from compaction candidates
2014-07-22 09:23:20,035 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:23:20,035 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:23:20,036 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:23:20,036 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:23:20,036 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 632.1m
2014-07-22 09:23:20,066 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:20,080 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 112184 synced till here 112183
2014-07-22 09:23:20,098 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046198282 with entries=70, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046200067
2014-07-22 09:23:20,098 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:23:20,858 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:23:21,676 DEBUG [RpcServer.handler=19,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:23:22,159 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:22,243 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 112280 synced till here 112274
2014-07-22 09:23:22,390 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046200067 with entries=96, filesize=69.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046202160
2014-07-22 09:23:22,390 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:23:24,383 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:24,399 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 112358 synced till here 112356
2014-07-22 09:23:24,423 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046202160 with entries=78, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046204383
2014-07-22 09:23:24,424 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:23:25,561 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:25,840 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046204383 with entries=96, filesize=75.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046205561
2014-07-22 09:23:25,843 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:23:26,493 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=21615, memsize=236.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/01f64f776799477a89c6313f9b035ce5
2014-07-22 09:23:26,510 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/01f64f776799477a89c6313f9b035ce5 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/01f64f776799477a89c6313f9b035ce5
2014-07-22 09:23:26,522 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/01f64f776799477a89c6313f9b035ce5, entries=861240, sequenceid=21615, filesize=61.3m
2014-07-22 09:23:26,523 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~731.5m/767033440, currentsize=187.8m/196888960 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 12344ms, sequenceid=21615, compaction requested=true
2014-07-22 09:23:26,524 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:23:26,524 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 54 store files, 0 compacting, 54 eligible, 2000 blocking
2014-07-22 09:23:26,524 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 54 files from compaction candidates
2014-07-22 09:23:26,524 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b., current region memstore size 335.8m
2014-07-22 09:23:26,524 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:23:26,524 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:23:26,524 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:23:26,822 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:23:30,641 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:30,672 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046205561 with entries=97, filesize=61.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046210642
2014-07-22 09:23:31,829 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:31,856 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 112630 synced till here 112625
2014-07-22 09:23:31,955 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046210642 with entries=79, filesize=66.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046211829
2014-07-22 09:23:33,156 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:23:33,224 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:33,347 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046211829 with entries=77, filesize=65.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046213224
2014-07-22 09:23:33,566 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=25824, memsize=179.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/a51ddad442564b75af2c341d8087d47a
2014-07-22 09:23:33,593 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/a51ddad442564b75af2c341d8087d47a as hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/a51ddad442564b75af2c341d8087d47a
2014-07-22 09:23:33,613 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/a51ddad442564b75af2c341d8087d47a, entries=653740, sequenceid=25824, filesize=46.6m
2014-07-22 09:23:33,613 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~335.8m/352121600, currentsize=23.7m/24878320 for region usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. in 7089ms, sequenceid=25824, compaction requested=true
2014-07-22 09:23:33,614 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:23:33,614 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 2000 blocking
2014-07-22 09:23:33,614 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-22 09:23:33,614 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 530.5m
2014-07-22 09:23:33,614 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:23:33,614 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:23:33,614 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. because compaction request was cancelled
2014-07-22 09:23:33,838 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=21675, memsize=332.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/487a5c99f1b045cc9ea3f7f28e69cd82
2014-07-22 09:23:33,858 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/487a5c99f1b045cc9ea3f7f28e69cd82 as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/487a5c99f1b045cc9ea3f7f28e69cd82
2014-07-22 09:23:33,887 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/487a5c99f1b045cc9ea3f7f28e69cd82, entries=1211910, sequenceid=21675, filesize=86.2m
2014-07-22 09:23:33,888 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~632.1m/662753280, currentsize=166.9m/174962960 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 13852ms, sequenceid=21675, compaction requested=true
2014-07-22 09:23:33,888 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:23:33,889 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 54 store files, 0 compacting, 54 eligible, 2000 blocking
2014-07-22 09:23:33,889 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 54 files from compaction candidates
2014-07-22 09:23:33,889 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 409.6m
2014-07-22 09:23:33,889 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:23:33,889 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:23:33,889 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:23:33,950 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:23:34,130 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:23:37,302 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:37,321 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 112788 synced till here 112787
2014-07-22 09:23:37,336 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046213224 with entries=81, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046217302
2014-07-22 09:23:37,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046107741
2014-07-22 09:23:37,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046114151
2014-07-22 09:23:37,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046116800
2014-07-22 09:23:37,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046119183
2014-07-22 09:23:37,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046121386
2014-07-22 09:23:37,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046123352
2014-07-22 09:23:37,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046125406
2014-07-22 09:23:37,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046127352
2014-07-22 09:23:37,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046129587
2014-07-22 09:23:37,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046131172
2014-07-22 09:23:37,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046132793
2014-07-22 09:23:37,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046134161
2014-07-22 09:23:37,338 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046135766
2014-07-22 09:23:37,338 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046136754
2014-07-22 09:23:37,338 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046138632
2014-07-22 09:23:37,339 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046148185
2014-07-22 09:23:37,339 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046150334
2014-07-22 09:23:37,339 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046152309
2014-07-22 09:23:37,339 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046154978
2014-07-22 09:23:37,339 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046156894
2014-07-22 09:23:37,339 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046159108
2014-07-22 09:23:37,339 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046162311
2014-07-22 09:23:37,339 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046164296
2014-07-22 09:23:37,339 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046166164
2014-07-22 09:23:37,339 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046167163
2014-07-22 09:23:37,339 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046169092
2014-07-22 09:23:39,058 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:39,075 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 112869 synced till here 112868
2014-07-22 09:23:39,091 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046217302 with entries=81, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046219059
2014-07-22 09:23:40,147 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:40,167 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 112955 synced till here 112953
2014-07-22 09:23:40,196 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046219059 with entries=86, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046220147
2014-07-22 09:23:40,625 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:23:41,836 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:41,956 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 113041 synced till here 113034
2014-07-22 09:23:41,991 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046220147 with entries=86, filesize=68.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046221932
2014-07-22 09:23:42,807 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:42,839 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046221932 with entries=79, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046222807
2014-07-22 09:23:44,730 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:44,749 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 113212 synced till here 113210
2014-07-22 09:23:44,792 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046222807 with entries=92, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046224731
2014-07-22 09:23:46,930 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:47,009 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 113299 synced till here 113293
2014-07-22 09:23:47,419 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046224731 with entries=87, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046226930
2014-07-22 09:23:48,633 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=21765, memsize=392.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/28440d5c5a504fcba2c0450d383b28d5
2014-07-22 09:23:48,654 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/28440d5c5a504fcba2c0450d383b28d5 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/28440d5c5a504fcba2c0450d383b28d5
2014-07-22 09:23:48,667 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/28440d5c5a504fcba2c0450d383b28d5, entries=1428240, sequenceid=21765, filesize=101.7m
2014-07-22 09:23:48,668 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~409.6m/429509920, currentsize=204.8m/214793520 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 14779ms, sequenceid=21765, compaction requested=true
2014-07-22 09:23:48,669 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:23:48,669 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 53 store files, 0 compacting, 53 eligible, 2000 blocking
2014-07-22 09:23:48,669 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 467.9m
2014-07-22 09:23:48,669 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 53 files from compaction candidates
2014-07-22 09:23:48,669 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:23:48,669 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:23:48,669 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:23:48,982 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:23:50,598 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:50,622 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 113383 synced till here 113381
2014-07-22 09:23:50,648 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046226930 with entries=84, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046230598
2014-07-22 09:23:52,305 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=21767, memsize=497.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/5937eef8255b4879b425a0f37025045a
2014-07-22 09:23:52,658 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:52,992 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 113475 synced till here 113474
2014-07-22 09:23:53,009 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/5937eef8255b4879b425a0f37025045a as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/5937eef8255b4879b425a0f37025045a
2014-07-22 09:23:53,011 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046230598 with entries=92, filesize=73.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046232658
2014-07-22 09:23:53,060 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/5937eef8255b4879b425a0f37025045a, entries=1809780, sequenceid=21767, filesize=128.8m
2014-07-22 09:23:53,061 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~530.5m/556231600, currentsize=250.7m/262872640 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 19447ms, sequenceid=21767, compaction requested=true
2014-07-22 09:23:53,061 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:23:53,062 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 53 store files, 0 compacting, 53 eligible, 2000 blocking
2014-07-22 09:23:53,062 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 53 files from compaction candidates
2014-07-22 09:23:53,062 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 418.3m
2014-07-22 09:23:53,062 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:23:53,062 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:23:53,062 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:23:53,533 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:23:53,757 DEBUG [RpcServer.handler=15,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:23:53,768 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:23:54,431 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:54,454 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 113553 synced till here 113549
2014-07-22 09:23:54,494 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046232658 with entries=78, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046234432
2014-07-22 09:23:54,495 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046172333
2014-07-22 09:23:54,495 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046178796
2014-07-22 09:23:54,495 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046180148
2014-07-22 09:23:54,495 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046181562
2014-07-22 09:23:54,495 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046183230
2014-07-22 09:23:54,495 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046185063
2014-07-22 09:23:54,495 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046187868
2014-07-22 09:23:54,495 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046188950
2014-07-22 09:23:54,495 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046190475
2014-07-22 09:23:54,496 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046191773
2014-07-22 09:23:56,995 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:57,034 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046234432 with entries=83, filesize=61.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046236995
2014-07-22 09:23:58,559 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:23:58,578 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 113712 synced till here 113711
2014-07-22 09:23:58,605 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046236995 with entries=76, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046238559
2014-07-22 09:24:00,561 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:00,605 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 113798 synced till here 113794
2014-07-22 09:24:00,659 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046238559 with entries=86, filesize=67.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046240561
2014-07-22 09:24:03,275 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:03,793 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 113912 synced till here 113910
2014-07-22 09:24:03,821 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046240561 with entries=114, filesize=90.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046243276
2014-07-22 09:24:05,507 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:05,525 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 114001 synced till here 113998
2014-07-22 09:24:05,604 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046243276 with entries=89, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046245508
2014-07-22 09:24:07,614 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:07,708 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=21906, memsize=460.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/a8e91d593d5f4a75867cf9b192bc4b8a
2014-07-22 09:24:07,746 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/a8e91d593d5f4a75867cf9b192bc4b8a as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/a8e91d593d5f4a75867cf9b192bc4b8a
2014-07-22 09:24:08,018 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 114103 synced till here 114102
2014-07-22 09:24:08,037 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/a8e91d593d5f4a75867cf9b192bc4b8a, entries=1677240, sequenceid=21906, filesize=119.5m
2014-07-22 09:24:08,038 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~469.2m/491959360, currentsize=246.4m/258407600 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 19369ms, sequenceid=21906, compaction requested=true
2014-07-22 09:24:08,038 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:24:08,039 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 55 store files, 0 compacting, 55 eligible, 2000 blocking
2014-07-22 09:24:08,039 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 455.0m
2014-07-22 09:24:08,039 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 55 files from compaction candidates
2014-07-22 09:24:08,039 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:24:08,039 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:24:08,039 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:24:08,078 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046245508 with entries=102, filesize=79.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046247614
2014-07-22 09:24:08,078 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046193119
2014-07-22 09:24:08,078 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046195052
2014-07-22 09:24:08,078 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046197378
2014-07-22 09:24:08,078 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046198282
2014-07-22 09:24:08,307 DEBUG [RpcServer.handler=19,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:24:08,463 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:24:09,772 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:09,800 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 114187 synced till here 114185
2014-07-22 09:24:09,824 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046247614 with entries=84, filesize=66.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046249772
2014-07-22 09:24:11,555 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:11,845 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 114288 synced till here 114284
2014-07-22 09:24:11,846 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=21942, memsize=413.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/a2c59439b8814026a0b980fea56941ea
2014-07-22 09:24:11,859 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/a2c59439b8814026a0b980fea56941ea as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/a2c59439b8814026a0b980fea56941ea
2014-07-22 09:24:11,871 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/a2c59439b8814026a0b980fea56941ea, entries=1505840, sequenceid=21942, filesize=107.2m
2014-07-22 09:24:11,872 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~421.9m/442377680, currentsize=261.7m/274369680 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 18810ms, sequenceid=21942, compaction requested=true
2014-07-22 09:24:11,873 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:24:11,873 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 55 store files, 0 compacting, 55 eligible, 2000 blocking
2014-07-22 09:24:11,873 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 510.7m
2014-07-22 09:24:11,873 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 55 files from compaction candidates
2014-07-22 09:24:11,873 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:24:11,873 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:24:11,873 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:24:11,878 DEBUG [RpcServer.handler=41,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:24:11,894 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046249772 with entries=101, filesize=78.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046251556
2014-07-22 09:24:11,895 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046200067
2014-07-22 09:24:11,895 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046202160
2014-07-22 09:24:11,895 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046204383
2014-07-22 09:24:12,341 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:24:13,575 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:13,607 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 114371 synced till here 114369
2014-07-22 09:24:13,662 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046251556 with entries=83, filesize=64.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046253575
2014-07-22 09:24:15,054 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:15,246 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 114485 synced till here 114480
2014-07-22 09:24:15,326 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046253575 with entries=114, filesize=78.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046255055
2014-07-22 09:24:17,284 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:17,314 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 114571 synced till here 114568
2014-07-22 09:24:17,349 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046255055 with entries=86, filesize=66.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046257285
2014-07-22 09:24:18,905 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:18,924 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 114660 synced till here 114657
2014-07-22 09:24:18,974 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046257285 with entries=89, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046258905
2014-07-22 09:24:19,770 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:20,808 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 114751 synced till here 114739
2014-07-22 09:24:20,855 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046258905 with entries=91, filesize=67.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046259770
2014-07-22 09:24:21,837 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:21,882 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 114841 synced till here 114840
2014-07-22 09:24:21,927 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046259770 with entries=90, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046261863
2014-07-22 09:24:23,554 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:23,589 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 114942 synced till here 114932
2014-07-22 09:24:23,681 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046261863 with entries=101, filesize=67.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046263554
2014-07-22 09:24:25,488 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:25,507 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 115029 synced till here 115025
2014-07-22 09:24:25,561 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046263554 with entries=87, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046265488
2014-07-22 09:24:27,457 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:27,489 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 115116 synced till here 115109
2014-07-22 09:24:27,591 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046265488 with entries=87, filesize=68.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046267458
2014-07-22 09:24:29,330 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:29,363 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 115210 synced till here 115196
2014-07-22 09:24:29,516 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046267458 with entries=94, filesize=73.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046269330
2014-07-22 09:24:31,643 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:31,659 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 115307 synced till here 115292
2014-07-22 09:24:31,803 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046269330 with entries=97, filesize=75.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046271643
2014-07-22 09:24:33,407 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:33,442 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 115385 synced till here 115378
2014-07-22 09:24:33,551 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046271643 with entries=78, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046273407
2014-07-22 09:24:33,552 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:24:34,170 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b.
2014-07-22 09:24:34,920 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:35,351 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 115518 synced till here 115517
2014-07-22 09:24:35,395 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046273407 with entries=133, filesize=95.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046274920
2014-07-22 09:24:35,395 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:24:35,664 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=22053, memsize=451.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/ce2c0e68a574430fa50a6d7bc57a906c
2014-07-22 09:24:35,681 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/ce2c0e68a574430fa50a6d7bc57a906c as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/ce2c0e68a574430fa50a6d7bc57a906c
2014-07-22 09:24:35,701 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/ce2c0e68a574430fa50a6d7bc57a906c, entries=1645440, sequenceid=22053, filesize=117.3m
2014-07-22 09:24:35,702 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~458.7m/480954560, currentsize=464.1m/486639520 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 27663ms, sequenceid=22053, compaction requested=true
2014-07-22 09:24:35,702 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:24:35,702 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 54 store files, 0 compacting, 54 eligible, 2000 blocking
2014-07-22 09:24:35,703 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 709.6m
2014-07-22 09:24:35,703 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 54 files from compaction candidates
2014-07-22 09:24:35,703 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:24:35,703 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:24:35,703 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:24:35,776 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708.
2014-07-22 09:24:37,004 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:24:37,021 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:37,373 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 115635 synced till here 115632
2014-07-22 09:24:37,401 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046274920 with entries=117, filesize=78.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046277021
2014-07-22 09:24:37,403 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:24:38,827 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:39,086 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 115758 synced till here 115745
2014-07-22 09:24:39,191 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046277021 with entries=123, filesize=78.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046278827
2014-07-22 09:24:39,192 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:24:41,098 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:41,496 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 115874 synced till here 115873
2014-07-22 09:24:41,510 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046278827 with entries=116, filesize=87.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046281100
2014-07-22 09:24:41,511 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:24:43,109 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=22083, memsize=507.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/64c7d93cd5cf4838af7c0da90df71924
2014-07-22 09:24:43,109 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:43,197 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/64c7d93cd5cf4838af7c0da90df71924 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/64c7d93cd5cf4838af7c0da90df71924
2014-07-22 09:24:43,377 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 115989 synced till here 115983
2014-07-22 09:24:43,420 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046281100 with entries=115, filesize=82.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046283109
2014-07-22 09:24:43,420 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:24:43,485 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/64c7d93cd5cf4838af7c0da90df71924, entries=1848390, sequenceid=22083, filesize=131.7m
2014-07-22 09:24:43,486 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~514.3m/539272240, currentsize=527.0m/552638560 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 31613ms, sequenceid=22083, compaction requested=true
2014-07-22 09:24:43,487 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:24:43,487 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 54 store files, 0 compacting, 54 eligible, 2000 blocking
2014-07-22 09:24:43,487 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 54 files from compaction candidates
2014-07-22 09:24:43,487 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:24:43,487 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 800.7m
2014-07-22 09:24:43,487 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:24:43,487 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:24:43,499 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e.
2014-07-22 09:24:44,860 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:44,870 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:24:45,072 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 116091 synced till here 116088
2014-07-22 09:24:45,819 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046283109 with entries=102, filesize=70.3m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046284861
2014-07-22 09:24:45,819 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:24:47,338 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:47,354 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 116191 synced till here 116190
2014-07-22 09:24:47,380 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046284861 with entries=100, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046287338
2014-07-22 09:24:47,381 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:24:49,164 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:49,209 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 116291 synced till here 116288
2014-07-22 09:24:49,262 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046287338 with entries=100, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046289164
2014-07-22 09:24:49,262 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:24:50,893 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:50,911 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 116379 synced till here 116377
2014-07-22 09:24:50,922 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046289164 with entries=88, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046290894
2014-07-22 09:24:50,922 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:24:51,821 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:52,545 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 116475 synced till here 116473
2014-07-22 09:24:52,591 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046290894 with entries=96, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046291822
2014-07-22 09:24:52,592 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:24:53,354 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:53,406 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046291822 with entries=84, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046293355
2014-07-22 09:24:53,406 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=44, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:24:55,184 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:55,200 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 116660 synced till here 116659
2014-07-22 09:24:55,226 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046293355 with entries=101, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046295185
2014-07-22 09:24:55,227 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=45, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:24:56,824 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:24:57,033 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 116768 synced till here 116767
2014-07-22 09:24:57,056 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046295185 with entries=108, filesize=75.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046296824
2014-07-22 09:24:57,057 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=46, maxlogs=32; forcing flush of 1 regions(s): 285afaafd9634c6707beeeeb8207bf2b
2014-07-22 09:24:57,317 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:57,318 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:57,360 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:57,372 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:57,412 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:57,451 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:57,636 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:57,637 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:57,640 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:57,641 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:57,680 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:57,692 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:57,714 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:58,389 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:58,389 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:58,390 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:58,390 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:58,410 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:58,412 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:58,451 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:58,485 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:58,485 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:58,485 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:58,485 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:58,485 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:58,487 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:58,519 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:58,558 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:58,561 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:58,575 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:58,623 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:59,561 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:59,602 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:59,648 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:59,691 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:24:59,731 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:25:01,911 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:25:01,912 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:25:01,912 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:25:01,920 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:25:01,921 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:25:01,926 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:25:01,928 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:25:01,928 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:25:01,928 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:25:01,962 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:25:01,963 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:25:01,963 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:25:02,030 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:25:02,076 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406043190412: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 09:25:02,318 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:25:02,318 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:25:02,360 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:25:02,373 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:25:02,413 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 09:25:02,451 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 09:25:02,554 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=22331, memsize=524.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/52b00e0b569741fd9abb94f4a0b87ae3
2014-07-22 09:25:02,572 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/.tmp/52b00e0b569741fd9abb94f4a0b87ae3 as hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/52b00e0b569741fd9abb94f4a0b87ae3
2014-07-22 09:25:02,583 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/7c5c8675017da2a4119014a8dd35278d/family/52b00e0b569741fd9abb94f4a0b87ae3, entries=1910680, sequenceid=22331, filesize=136.1m
2014-07-22 09:25:02,584 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~716.9m/751751360, currentsize=360.4m/377905680 for region usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. in 26881ms, sequenceid=22331, compaction requested=true
2014-07-22 09:25:02,585 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:25:02,585 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 56 store files, 0 compacting, 56 eligible, 2000 blocking
2014-07-22 09:25:02,585 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5134ms
2014-07-22 09:25:02,585 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,585 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b., current region memstore size 361.2m
2014-07-22 09:25:02,585 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 56 files from compaction candidates
2014-07-22 09:25:02,585 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5173ms
2014-07-22 09:25:02,585 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,585 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:25:02,586 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:25:02,586 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d. because compaction request was cancelled
2014-07-22 09:25:02,586 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5214ms
2014-07-22 09:25:02,586 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,587 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5227ms
2014-07-22 09:25:02,587 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,588 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5270ms
2014-07-22 09:25:02,588 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,588 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5271ms
2014-07-22 09:25:02,588 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,589 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 513ms
2014-07-22 09:25:02,589 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,590 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 560ms
2014-07-22 09:25:02,590 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,590 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 627ms
2014-07-22 09:25:02,590 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,590 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 627ms
2014-07-22 09:25:02,591 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,591 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 629ms
2014-07-22 09:25:02,591 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,591 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 663ms
2014-07-22 09:25:02,592 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,595 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 667ms
2014-07-22 09:25:02,600 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,601 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 673ms
2014-07-22 09:25:02,601 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,601 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 675ms
2014-07-22 09:25:02,601 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,601 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 681ms
2014-07-22 09:25:02,601 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,602 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 681ms
2014-07-22 09:25:02,602 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,602 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 691ms
2014-07-22 09:25:02,602 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,604 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 693ms
2014-07-22 09:25:02,604 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,604 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 693ms
2014-07-22 09:25:02,605 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,605 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2874ms
2014-07-22 09:25:02,605 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,605 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2914ms
2014-07-22 09:25:02,605 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,606 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2958ms
2014-07-22 09:25:02,606 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,606 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3004ms
2014-07-22 09:25:02,606 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,607 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3046ms
2014-07-22 09:25:02,607 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,609 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3984ms
2014-07-22 09:25:02,609 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,609 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4034ms
2014-07-22 09:25:02,609 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,609 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4048ms
2014-07-22 09:25:02,610 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,610 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4052ms
2014-07-22 09:25:02,610 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,610 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4091ms
2014-07-22 09:25:02,610 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,614 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4127ms
2014-07-22 09:25:02,614 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,615 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4129ms
2014-07-22 09:25:02,615 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,616 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4131ms
2014-07-22 09:25:02,616 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,616 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4131ms
2014-07-22 09:25:02,616 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,616 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4131ms
2014-07-22 09:25:02,616 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,617 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4132ms
2014-07-22 09:25:02,617 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,617 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4166ms
2014-07-22 09:25:02,617 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,618 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4206ms
2014-07-22 09:25:02,618 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,618 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4208ms
2014-07-22 09:25:02,618 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,618 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4228ms
2014-07-22 09:25:02,618 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,620 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4229ms
2014-07-22 09:25:02,620 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,621 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4231ms
2014-07-22 09:25:02,621 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,621 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4232ms
2014-07-22 09:25:02,621 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,623 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4908ms
2014-07-22 09:25:02,624 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,625 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4933ms
2014-07-22 09:25:02,625 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,626 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4946ms
2014-07-22 09:25:02,627 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,627 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4986ms
2014-07-22 09:25:02,627 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,627 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4988ms
2014-07-22 09:25:02,627 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,628 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4991ms
2014-07-22 09:25:02,628 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,628 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4992ms
2014-07-22 09:25:02,628 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406043190412
2014-07-22 09:25:02,960 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d.
2014-07-22 09:25:03,023 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:25:04,173 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:25:04,205 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 116902 synced till here 116890
2014-07-22 09:25:04,353 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046296824 with entries=134, filesize=84.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046304174
2014-07-22 09:25:05,014 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:25:05,758 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 116988 synced till here 116985
2014-07-22 09:25:05,827 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046304174 with entries=86, filesize=77.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046305014
2014-07-22 09:25:07,118 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:25:07,143 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 117080 synced till here 117078
2014-07-22 09:25:07,598 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046305014 with entries=92, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046307118
2014-07-22 09:25:08,510 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:25:08,552 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046307118 with entries=99, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046308510
2014-07-22 09:25:10,331 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:25:10,804 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 117309 synced till here 117301
2014-07-22 09:25:11,807 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046308510 with entries=130, filesize=81.6m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046310331
2014-07-22 09:25:12,508 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=22414, memsize=593.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/4bf1b5e7fad2455d80f7a487ad32eece
2014-07-22 09:25:12,527 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/.tmp/4bf1b5e7fad2455d80f7a487ad32eece as hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/4bf1b5e7fad2455d80f7a487ad32eece
2014-07-22 09:25:12,542 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/55c4b5c170380616b272a52c96d0e4ef/family/4bf1b5e7fad2455d80f7a487ad32eece, entries=2161340, sequenceid=22414, filesize=153.9m
2014-07-22 09:25:12,543 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~806.3m/845432880, currentsize=390.6m/409592560 for region usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. in 29056ms, sequenceid=22414, compaction requested=true
2014-07-22 09:25:12,544 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 56 store files, 0 compacting, 56 eligible, 2000 blocking
2014-07-22 09:25:12,545 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 56 files from compaction candidates
2014-07-22 09:25:12,545 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:25:12,545 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:25:12,545 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef. because compaction request was cancelled
2014-07-22 09:25:12,545 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:25:12,546 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708., current region memstore size 992.4m
2014-07-22 09:25:12,547 DEBUG [RpcServer.handler=39,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef.
2014-07-22 09:25:13,050 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:25:13,086 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046310331 with entries=107, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046313050
2014-07-22 09:25:13,874 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:25:14,814 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:25:14,854 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046313050 with entries=86, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046314815
2014-07-22 09:25:17,421 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:25:17,453 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046314815 with entries=108, filesize=60.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046317422
2014-07-22 09:25:17,528 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=26933, memsize=341.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/adb88e6b998d431fa8ab80d05ba67669
2014-07-22 09:25:17,546 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/.tmp/adb88e6b998d431fa8ab80d05ba67669 as hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/adb88e6b998d431fa8ab80d05ba67669
2014-07-22 09:25:17,556 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/285afaafd9634c6707beeeeb8207bf2b/family/adb88e6b998d431fa8ab80d05ba67669, entries=1242450, sequenceid=26933, filesize=88.4m
2014-07-22 09:25:17,557 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~361.2m/378764480, currentsize=53.2m/55819600 for region usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. in 14972ms, sequenceid=26933, compaction requested=true
2014-07-22 09:25:17,557 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:25:17,557 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 2000 blocking
2014-07-22 09:25:17,558 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e., current region memstore size 995.5m
2014-07-22 09:25:17,558 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-22 09:25:17,558 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:25:17,558 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:25:17,558 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user9,1406043570128.285afaafd9634c6707beeeeb8207bf2b. because compaction request was cancelled
2014-07-22 09:25:18,389 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:25:19,796 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:25:19,814 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 117717 synced till here 117716
2014-07-22 09:25:19,831 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046317422 with entries=107, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046319796
2014-07-22 09:25:19,831 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046205561
2014-07-22 09:25:19,831 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046210642
2014-07-22 09:25:19,831 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046211829
2014-07-22 09:25:19,831 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046213224
2014-07-22 09:25:19,831 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046217302
2014-07-22 09:25:19,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046219059
2014-07-22 09:25:19,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046220147
2014-07-22 09:25:19,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046221932
2014-07-22 09:25:19,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046222807
2014-07-22 09:25:19,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046224731
2014-07-22 09:25:19,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046226930
2014-07-22 09:25:19,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046230598
2014-07-22 09:25:19,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046232658
2014-07-22 09:25:19,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046234432
2014-07-22 09:25:19,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046236995
2014-07-22 09:25:19,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046238559
2014-07-22 09:25:19,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046240561
2014-07-22 09:25:19,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046243276
2014-07-22 09:25:19,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046245508
2014-07-22 09:25:22,380 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:25:22,411 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046319796 with entries=116, filesize=61.1m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046322381
2014-07-22 09:25:25,312 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:25:25,347 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046322381 with entries=104, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046325313
2014-07-22 09:25:29,228 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 09:25:29,649 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046325313 with entries=105, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1406043190412/slave1%2C60020%2C1406043190412.1406046329229
2014-07-22 09:25:39,553 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=22622, memsize=711.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/235fa0453b8948a585629299657ddeab
2014-07-22 09:25:39,569 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/.tmp/235fa0453b8948a585629299657ddeab as hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/235fa0453b8948a585629299657ddeab
2014-07-22 09:25:39,584 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dc45bba255d14c6f2d7cb1b56ba708/family/235fa0453b8948a585629299657ddeab, entries=2590010, sequenceid=22622, filesize=184.4m
2014-07-22 09:25:39,585 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~994.3m/1042609520, currentsize=164.8m/172807120 for region usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. in 27039ms, sequenceid=22622, compaction requested=true
2014-07-22 09:25:39,586 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:25:39,586 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 55 store files, 0 compacting, 55 eligible, 2000 blocking
2014-07-22 09:25:39,586 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406043570127.7c5c8675017da2a4119014a8dd35278d., current region memstore size 694.7m
2014-07-22 09:25:39,586 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 55 files from compaction candidates
2014-07-22 09:25:39,586 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:25:39,586 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:25:39,586 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user3,1406043570127.92dc45bba255d14c6f2d7cb1b56ba708. because compaction request was cancelled
2014-07-22 09:25:40,029 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 09:25:43,343 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=22654, memsize=714.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/dbdca088eec94e918a6b937721bf0384
2014-07-22 09:25:43,361 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/.tmp/dbdca088eec94e918a6b937721bf0384 as hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/dbdca088eec94e918a6b937721bf0384
2014-07-22 09:25:43,392 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/97ad17fd41c306f4acbbcbfe821c8a7e/family/dbdca088eec94e918a6b937721bf0384, entries=2602130, sequenceid=22654, filesize=185.3m
2014-07-22 09:25:43,392 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~995.5m/1043827040, currentsize=105.6m/110770080 for region usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. in 25835ms, sequenceid=22654, compaction requested=true
2014-07-22 09:25:43,393 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 09:25:43,393 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Selecting compaction from 55 store files, 0 compacting, 55 eligible, 2000 blocking
2014-07-22 09:25:43,393 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 55 files from compaction candidates
2014-07-22 09:25:43,393 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1406043570127.55c4b5c170380616b272a52c96d0e4ef., current region memstore size 555.7m
2014-07-22 09:25:43,393 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 09:25:43,393 DEBUG [regionserver60020-smallCompactions-1406043231365] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 09:25:43,393 DEBUG [regionserver60020-smallCompactions-1406043231365] regionserver.CompactSplitThread: Not compacting usertable,user5,1406043570127.97ad17fd41c306f4acbbcbfe821c8a7e. because compaction request was cancelled
2014-07-22 09:25:43,728 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
