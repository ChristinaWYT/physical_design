Mon Jul 14 00:56:21 PDT 2014 Starting regionserver on sceplus-vm49
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 128203
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 32768
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 128203
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
2014-07-14 00:56:22,126 INFO  [main] util.VersionInfo: HBase 0.98.3-hadoop1
2014-07-14 00:56:22,127 INFO  [main] util.VersionInfo: Subversion git://acer/usr/src/Hadoop/hbase -r d5e65a9144e315bb0a964e7730871af32f5018d5
2014-07-14 00:56:22,127 INFO  [main] util.VersionInfo: Compiled by apurtell on Sat May 31 19:34:57 PDT 2014
2014-07-14 00:56:22,354 INFO  [main] util.ServerCommandLine: env:JAVA_HOME=/usr/lib/jvm/java-1.6.0-openjdk-amd64/
2014-07-14 00:56:22,354 INFO  [main] util.ServerCommandLine: env:SHLVL=3
2014-07-14 00:56:22,354 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_DIR=/home/hadoop/hbase/bin/../logs
2014-07-14 00:56:22,354 INFO  [main] util.ServerCommandLine: env:HBASE_HOME=/home/hadoop/hbase/bin/..
2014-07-14 00:56:22,355 INFO  [main] util.ServerCommandLine: env:HBASE_OPTS=-XX:+UseConcMarkSweepGC   -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm49.log -Dhbase.home.dir=/home/hadoop/hbase/bin/.. -Dhbase.id.str=hadoop -Dhbase.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64 -Dhbase.security.logger=INFO,RFAS
2014-07-14 00:56:22,355 INFO  [main] util.ServerCommandLine: env:HBASE_ENV_INIT=true
2014-07-14 00:56:22,355 INFO  [main] util.ServerCommandLine: env:SSH_CLIENT=9.1.143.58 53187 22
2014-07-14 00:56:22,355 INFO  [main] util.ServerCommandLine: env:HBASE_HEAPSIZE=10240
2014-07-14 00:56:22,355 INFO  [main] util.ServerCommandLine: env:MAIL=/var/mail/hadoop
2014-07-14 00:56:22,355 INFO  [main] util.ServerCommandLine: env:HBASE_ZNODE_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.znode
2014-07-14 00:56:22,355 INFO  [main] util.ServerCommandLine: env:PWD=/home/hadoop/hbase
2014-07-14 00:56:22,355 INFO  [main] util.ServerCommandLine: env:LOGNAME=hadoop
2014-07-14 00:56:22,355 INFO  [main] util.ServerCommandLine: env:HBASE_MANAGES_ZK=true
2014-07-14 00:56:22,356 INFO  [main] util.ServerCommandLine: env:HBASE_NICENESS=0
2014-07-14 00:56:22,356 INFO  [main] util.ServerCommandLine: env:HBASE_REST_OPTS=
2014-07-14 00:56:22,356 INFO  [main] util.ServerCommandLine: env:LD_LIBRARY_PATH=/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64/server:/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64:/usr/lib/jvm/java-6-openjdk-amd64/jre/../lib/amd64::/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-07-14 00:56:22,356 INFO  [main] util.ServerCommandLine: env:MALLOC_ARENA_MAX=4
2014-07-14 00:56:22,356 INFO  [main] util.ServerCommandLine: env:SSH_CONNECTION=9.1.143.58 53187 9.1.143.59 22
2014-07-14 00:56:22,356 INFO  [main] util.ServerCommandLine: env:SHELL=/bin/bash
2014-07-14 00:56:22,356 INFO  [main] util.ServerCommandLine: env:HBASE_PID_DIR=/var/hadoop/pids
2014-07-14 00:56:22,356 INFO  [main] util.ServerCommandLine: env:HBASE_ROOT_LOGGER=INFO,RFA
2014-07-14 00:56:22,358 INFO  [main] util.ServerCommandLine: env:CLASSPATH=/home/hadoop/hbase/bin/../conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-07-14 00:56:22,358 INFO  [main] util.ServerCommandLine: env:HBASE_THRIFT_OPTS=
2014-07-14 00:56:22,359 INFO  [main] util.ServerCommandLine: env:PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games
2014-07-14 00:56:22,359 INFO  [main] util.ServerCommandLine: env:USER=hadoop
2014-07-14 00:56:22,359 INFO  [main] util.ServerCommandLine: env:HBASE_SECURITY_LOGGER=INFO,RFAS
2014-07-14 00:56:22,359 INFO  [main] util.ServerCommandLine: env:HOME=/home/hadoop
2014-07-14 00:56:22,359 INFO  [main] util.ServerCommandLine: env:HBASE_LIBRARY_PATH=/home/hadoop/hbase/lib/native/Linux-amd64-64
2014-07-14 00:56:22,359 INFO  [main] util.ServerCommandLine: env:HBASE_START_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.autorestart
2014-07-14 00:56:22,359 INFO  [main] util.ServerCommandLine: env:XDG_SESSION_ID=943
2014-07-14 00:56:22,359 INFO  [main] util.ServerCommandLine: env:HBASE_LOGFILE=hbase-hadoop-regionserver-sceplus-vm49.log
2014-07-14 00:56:22,359 INFO  [main] util.ServerCommandLine: env:XDG_RUNTIME_DIR=/run/user/1001
2014-07-14 00:56:22,359 INFO  [main] util.ServerCommandLine: env:LANG=en_US.UTF-8
2014-07-14 00:56:22,359 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_PREFIX=hbase-hadoop-regionserver-sceplus-vm49
2014-07-14 00:56:22,360 INFO  [main] util.ServerCommandLine: env:HBASE_IDENT_STRING=hadoop
2014-07-14 00:56:22,362 INFO  [main] util.ServerCommandLine: vmName=OpenJDK 64-Bit Server VM, vmVendor=Sun Microsystems Inc., vmVersion=23.25-b01
2014-07-14 00:56:22,362 INFO  [main] util.ServerCommandLine: vmInputArguments=[-Dproc_regionserver, -XX:OnOutOfMemoryError=kill -9 %p, -Xmx10240m, -XX:+UseConcMarkSweepGC, -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs, -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm49.log, -Dhbase.home.dir=/home/hadoop/hbase/bin/.., -Dhbase.id.str=hadoop, -Dhbase.root.logger=INFO,RFA, -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64, -Dhbase.security.logger=INFO,RFAS]
2014-07-14 00:56:22,593 DEBUG [main] regionserver.HRegionServer: regionserver/sceplus-vm49.almaden.ibm.com/9.1.143.59:60020 HConnection server-to-server retries=350
2014-07-14 00:56:23,051 INFO  [main] ipc.RpcServer: regionserver/sceplus-vm49.almaden.ibm.com/9.1.143.59:60020: started 10 reader(s).
2014-07-14 00:56:23,153 INFO  [main] impl.MetricsConfig: loaded properties from hadoop-metrics2-hbase.properties
2014-07-14 00:56:23,167 INFO  [main] impl.MetricsSinkAdapter: Sink file-all started
2014-07-14 00:56:23,232 INFO  [main] impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.
2014-07-14 00:56:23,233 INFO  [main] impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2014-07-14 00:56:23,233 INFO  [main] impl.MetricsSystemImpl: HBase metrics system started
2014-07-14 00:56:23,239 INFO  [main] impl.MetricsSourceAdapter: MBean for source jvm registered.
2014-07-14 00:56:23,244 INFO  [main] impl.MetricsSourceAdapter: MBean for source IPC,sub=IPC registered.
2014-07-14 00:56:23,337 INFO  [main] impl.MetricsSourceAdapter: MBean for source ugi registered.
2014-07-14 00:56:23,337 WARN  [main] impl.MetricsSystemImpl: Source name ugi already exists!
2014-07-14 00:56:23,341 DEBUG [main] util.DirectMemoryUtils: Failed to retrieve nio.BufferPool direct MemoryUsed attribute.
javax.management.InstanceNotFoundException: java.nio:type=BufferPool,name=direct
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1117)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:678)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:682)
	at org.apache.hadoop.hbase.util.DirectMemoryUtils.<clinit>(DirectMemoryUtils.java:72)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.instantiateBlockCache(CacheConfig.java:396)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.<init>(CacheConfig.java:179)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:621)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:534)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2393)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:61)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:85)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2410)
2014-07-14 00:56:23,344 INFO  [main] hfile.CacheConfig: Allocating LruBlockCache with maximum size 4.0g
2014-07-14 00:56:23,422 INFO  [main] mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2014-07-14 00:56:23,481 INFO  [main] http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
2014-07-14 00:56:23,491 INFO  [main] http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 60030
2014-07-14 00:56:23,493 INFO  [main] http.HttpServer: listener.getLocalPort() returned 60030 webServer.getConnectors()[0].getLocalPort() returned 60030
2014-07-14 00:56:23,493 INFO  [main] http.HttpServer: Jetty bound to port 60030
2014-07-14 00:56:23,493 INFO  [main] mortbay.log: jetty-6.1.26
2014-07-14 00:56:23,803 INFO  [main] mortbay.log: Started SelectChannelConnector@0.0.0.0:60030
2014-07-14 00:56:23,852 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2014-07-14 00:56:23,852 INFO  [main] regionserver.ShutdownHook: Installed shutdown hook thread: Shutdownhook:regionserver60020
2014-07-14 00:56:23,852 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:host.name=sceplus-vm49.almaden.ibm.com
2014-07-14 00:56:23,852 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.version=1.6.0_31
2014-07-14 00:56:23,852 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.vendor=Sun Microsystems Inc.
2014-07-14 00:56:23,852 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-6-openjdk-amd64/jre
2014-07-14 00:56:23,852 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.class.path=/home/hadoop/hbase/bin/../conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-07-14 00:56:23,852 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-07-14 00:56:23,852 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
2014-07-14 00:56:23,852 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
2014-07-14 00:56:23,852 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.name=Linux
2014-07-14 00:56:23,852 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.arch=amd64
2014-07-14 00:56:23,852 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.version=3.13.0-24-generic
2014-07-14 00:56:23,852 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.name=hadoop
2014-07-14 00:56:23,852 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.home=/home/hadoop
2014-07-14 00:56:23,852 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.dir=/home/hadoop/hbase-0.98.3-hadoop1
2014-07-14 00:56:23,853 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=regionserver:60020, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-14 00:56:23,877 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=regionserver:60020 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-14 00:56:23,881 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-14 00:56:23,885 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-14 00:56:23,893 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, sessionid = 0x14733df62590001, negotiated timeout = 90000
2014-07-14 00:56:55,958 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x67e59e9b, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-14 00:56:55,959 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x67e59e9b connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-14 00:56:55,960 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-14 00:56:55,960 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-14 00:56:55,964 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, sessionid = 0x14733df62590004, negotiated timeout = 90000
2014-07-14 00:56:56,248 DEBUG [regionserver60020] catalog.CatalogTracker: Starting catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@2dd06f21
2014-07-14 00:56:56,251 INFO  [regionserver60020] regionserver.HRegionServer: ClusterId : 3b61b992-e8ee-43f8-b0c6-14cd23a8afbe
2014-07-14 00:56:56,257 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initializing
2014-07-14 00:56:56,272 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Node /hbase/online-snapshot/acquired already exists and this is not a retry
2014-07-14 00:56:56,307 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initialized
2014-07-14 00:56:56,314 INFO  [regionserver60020] regionserver.MemStoreFlusher: globalMemStoreLimit=4.0g, globalMemStoreLimitLowMark=3.8g, maxHeap=9.9g
2014-07-14 00:56:56,318 INFO  [regionserver60020] regionserver.HRegionServer: CompactionChecker runs every 10sec
2014-07-14 00:56:56,344 INFO  [regionserver60020] regionserver.HRegionServer: reportForDuty to master=sceplus-vm48.almaden.ibm.com,60000,1405324581486 with port=60020, startcode=1405324583256
2014-07-14 00:56:56,703 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: hbase.rootdir=hdfs://master:54310/hbase
2014-07-14 00:56:56,704 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: fs.default.name=hdfs://master:54310
2014-07-14 00:56:56,704 INFO  [regionserver60020] regionserver.HRegionServer: Master passed us a different hostname to use; was=sceplus-vm49.almaden.ibm.com, but now=slave1
2014-07-14 00:56:56,729 INFO  [regionserver60020] fs.HFileSystem: Added intercepting call to namenode#getBlockLocations so can do block reordering using class class org.apache.hadoop.hbase.fs.HFileSystem$ReorderWALBlocks
2014-07-14 00:56:56,738 DEBUG [regionserver60020] regionserver.HRegionServer: logdir=hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256
2014-07-14 00:56:56,774 DEBUG [regionserver60020] regionserver.Replication: ReplicationStatisticsThread 300
2014-07-14 00:56:56,785 INFO  [regionserver60020] wal.FSHLog: WAL/HLog configuration: blocksize=64 MB, rollsize=60.8 MB, enabled=true
2014-07-14 00:56:56,881 INFO  [regionserver60020] wal.FSHLog: New WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405324616792
2014-07-14 00:56:56,898 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=WAL registered.
2014-07-14 00:56:56,903 INFO  [regionserver60020] regionserver.MetricsRegionServerWrapperImpl: Computing regionserver metrics every 5000 milliseconds
2014-07-14 00:56:56,906 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Server registered.
2014-07-14 00:56:56,911 INFO  [regionserver60020] trace.SpanReceiverHost: SpanReceiver org.cloudera.htrace.impl.LocalFileSpanReceiver was loaded successfully.
2014-07-14 00:56:56,914 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_REGION-slave1:60020, corePoolSize=3, maxPoolSize=3
2014-07-14 00:56:56,914 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_META-slave1:60020, corePoolSize=1, maxPoolSize=1
2014-07-14 00:56:56,914 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_REGION-slave1:60020, corePoolSize=3, maxPoolSize=3
2014-07-14 00:56:56,914 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_META-slave1:60020, corePoolSize=1, maxPoolSize=1
2014-07-14 00:56:56,914 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_LOG_REPLAY_OPS-slave1:60020, corePoolSize=2, maxPoolSize=2
2014-07-14 00:56:56,925 INFO  [regionserver60020] regionserver.ReplicationSourceManager: Current list of replicators: [slave1,60020,1405324583256, sceplus-vm48.almaden.ibm.com,60020,1405324583388] other RSs: [slave1,60020,1405324583256, sceplus-vm48.almaden.ibm.com,60020,1405324583388]
2014-07-14 00:56:56,951 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Replication registered.
2014-07-14 00:56:56,954 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x507ca72d, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-14 00:56:56,955 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x507ca72d connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-14 00:56:56,955 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-14 00:56:56,956 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-14 00:56:56,961 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, sessionid = 0x14733df62590006, negotiated timeout = 90000
2014-07-14 00:56:56,970 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: starting
2014-07-14 00:56:56,970 INFO  [RpcServer.listener,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: starting
2014-07-14 00:56:57,016 INFO  [regionserver60020] regionserver.HRegionServer: Serving as slave1,60020,1405324583256, RpcServer on sceplus-vm49.almaden.ibm.com/9.1.143.59:60020, sessionid=0x14733df62590001
2014-07-14 00:56:57,016 INFO  [SplitLogWorker-slave1,60020,1405324583256] regionserver.SplitLogWorker: SplitLogWorker slave1,60020,1405324583256 starting
2014-07-14 00:56:57,017 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is starting
2014-07-14 00:56:57,017 DEBUG [regionserver60020] snapshot.RegionServerSnapshotManager: Start Snapshot Manager slave1,60020,1405324583256
2014-07-14 00:56:57,017 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Starting procedure member 'slave1,60020,1405324583256'
2014-07-14 00:56:57,017 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Checking for aborted procedures on node: '/hbase/online-snapshot/abort'
2014-07-14 00:56:57,018 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Looking for new procedures under znode:'/hbase/online-snapshot/acquired'
2014-07-14 00:56:57,019 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is started
2014-07-14 00:57:01,683 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user4,1405321189856.ba3a3ea148e3630e391fc92ec8fce47b.
2014-07-14 00:57:01,823 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user6,1405321189856.0098687d245db35b3c80045d6eefb3ee.
2014-07-14 00:57:01,823 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning ba3a3ea148e3630e391fc92ec8fce47b from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 00:57:01,824 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-14 00:57:01,825 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 0098687d245db35b3c80045d6eefb3ee from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 00:57:01,840 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user1,1405321189855.e790d3c63a11d58c49d2bbca0b49c391.
2014-07-14 00:57:01,841 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e5ee55a21ff19d69490518939b0887e0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 00:57:01,841 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user2,1405321189855.98633400eb20e4fe217a3f5e9d4c29f0.
2014-07-14 00:57:01,841 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user7,1405321189856.0a3fceff1c074904aae38a3f8b5e5cac.
2014-07-14 00:57:01,843 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 0098687d245db35b3c80045d6eefb3ee from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 00:57:01,843 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node ba3a3ea148e3630e391fc92ec8fce47b from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 00:57:01,846 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e5ee55a21ff19d69490518939b0887e0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 00:57:01,866 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => 0098687d245db35b3c80045d6eefb3ee, NAME => 'usertable,user6,1405321189856.0098687d245db35b3c80045d6eefb3ee.', STARTKEY => 'user6', ENDKEY => 'user7'}
2014-07-14 00:57:01,866 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => ba3a3ea148e3630e391fc92ec8fce47b, NAME => 'usertable,user4,1405321189856.ba3a3ea148e3630e391fc92ec8fce47b.', STARTKEY => 'user4', ENDKEY => 'user5'}
2014-07-14 00:57:01,866 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => e5ee55a21ff19d69490518939b0887e0, NAME => 'hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.', STARTKEY => '', ENDKEY => ''}
2014-07-14 00:57:01,895 INFO  [RS_OPEN_REGION-slave1:60020-0] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Regions registered.
2014-07-14 00:57:01,896 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable ba3a3ea148e3630e391fc92ec8fce47b
2014-07-14 00:57:01,896 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 0098687d245db35b3c80045d6eefb3ee
2014-07-14 00:57:01,896 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table namespace e5ee55a21ff19d69490518939b0887e0
2014-07-14 00:57:01,896 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,user4,1405321189856.ba3a3ea148e3630e391fc92ec8fce47b.
2014-07-14 00:57:01,896 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-14 00:57:01,896 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user6,1405321189856.0098687d245db35b3c80045d6eefb3ee.
2014-07-14 00:57:01,910 INFO  [RS_OPEN_REGION-slave1:60020-1] util.NativeCodeLoader: Loaded the native-hadoop library
2014-07-14 00:57:01,912 INFO  [RS_OPEN_REGION-slave1:60020-1] zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
2014-07-14 00:57:01,915 INFO  [RS_OPEN_REGION-slave1:60020-1] compress.CodecPool: Got brand-new compressor
2014-07-14 00:57:01,915 INFO  [RS_OPEN_REGION-slave1:60020-0] compress.CodecPool: Got brand-new compressor
2014-07-14 00:57:01,984 INFO  [StoreOpener-0098687d245db35b3c80045d6eefb3ee-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-14 00:57:01,984 INFO  [StoreOpener-e5ee55a21ff19d69490518939b0887e0-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 604800000, major jitter 0.500000
2014-07-14 00:57:01,990 INFO  [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-14 00:57:02,018 INFO  [StoreFileOpenerThread-info-1] util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc32
2014-07-14 00:57:02,056 INFO  [StoreFileOpenerThread-family-1] compress.CodecPool: Got brand-new decompressor
2014-07-14 00:57:02,056 INFO  [StoreFileOpenerThread-family-1] compress.CodecPool: Got brand-new decompressor
2014-07-14 00:57:02,067 DEBUG [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b/family/1476e2f1bfdd437cba15a3333355d01c, isReference=false, isBulkLoadResult=false, seqid=12402, majorCompaction=false
2014-07-14 00:57:02,067 DEBUG [StoreOpener-0098687d245db35b3c80045d6eefb3ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/family/00fe55592cc2470bbf948998c9a63465, isReference=false, isBulkLoadResult=false, seqid=12590, majorCompaction=false
2014-07-14 00:57:02,067 DEBUG [StoreOpener-e5ee55a21ff19d69490518939b0887e0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/hbase/namespace/e5ee55a21ff19d69490518939b0887e0/info/5b0102065d284f308d4c0a8d64d9fab5, isReference=false, isBulkLoadResult=false, seqid=4, majorCompaction=false
2014-07-14 00:57:02,082 DEBUG [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b/family/16f77cc3541d436bb971343b71b74be4, isReference=false, isBulkLoadResult=false, seqid=25218, majorCompaction=false
2014-07-14 00:57:02,099 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/hbase/namespace/e5ee55a21ff19d69490518939b0887e0
2014-07-14 00:57:02,103 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined e5ee55a21ff19d69490518939b0887e0; next sequenceid=5
2014-07-14 00:57:02,103 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node e5ee55a21ff19d69490518939b0887e0
2014-07-14 00:57:02,107 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] regionserver.HRegionServer: Post open deploy tasks for region=hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-14 00:57:02,117 DEBUG [StoreOpener-0098687d245db35b3c80045d6eefb3ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/family/1fd91ee167794f8398bdbaadec35c479, isReference=false, isBulkLoadResult=false, seqid=4792, majorCompaction=false
2014-07-14 00:57:02,124 DEBUG [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b/family/1869fa364d1f4bef898cbca0d6df8b5a, isReference=false, isBulkLoadResult=false, seqid=22252, majorCompaction=false
2014-07-14 00:57:02,148 DEBUG [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b/family/23fc4bcea2de4ef5ba446a5797961654, isReference=false, isBulkLoadResult=false, seqid=16747, majorCompaction=false
2014-07-14 00:57:02,168 DEBUG [StoreOpener-0098687d245db35b3c80045d6eefb3ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/family/28d83914d7fe4119aacd514d265421e6, isReference=false, isBulkLoadResult=false, seqid=12111, majorCompaction=false
2014-07-14 00:57:02,195 DEBUG [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b/family/25b36170b49a448483fd9ddc952ceb02, isReference=false, isBulkLoadResult=false, seqid=10101, majorCompaction=false
2014-07-14 00:57:02,197 DEBUG [StoreOpener-0098687d245db35b3c80045d6eefb3ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/family/2f6e55810ba54a3193075bb538acf5eb, isReference=false, isBulkLoadResult=false, seqid=25237, majorCompaction=false
2014-07-14 00:57:02,232 DEBUG [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b/family/314f25a2272447c491b6b6815416b080, isReference=false, isBulkLoadResult=false, seqid=21436, majorCompaction=false
2014-07-14 00:57:02,243 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] catalog.MetaEditor: Updated row hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0. with server=slave1,60020,1405324583256
2014-07-14 00:57:02,243 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] regionserver.HRegionServer: Finished post open deploy task for hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-14 00:57:02,243 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e5ee55a21ff19d69490518939b0887e0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 00:57:02,244 DEBUG [StoreOpener-0098687d245db35b3c80045d6eefb3ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/family/40a0586c150d4f04ba8b32f11018418e, isReference=false, isBulkLoadResult=false, seqid=22423, majorCompaction=false
2014-07-14 00:57:02,248 DEBUG [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b/family/325969c95c144e68bbe6ceca7acab0da, isReference=false, isBulkLoadResult=false, seqid=25223, majorCompaction=false
2014-07-14 00:57:02,248 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e5ee55a21ff19d69490518939b0887e0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 00:57:02,248 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned e5ee55a21ff19d69490518939b0887e0 to OPENED in zk on slave1,60020,1405324583256
2014-07-14 00:57:02,248 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0. on slave1,60020,1405324583256
2014-07-14 00:57:02,249 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e790d3c63a11d58c49d2bbca0b49c391 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 00:57:02,253 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e790d3c63a11d58c49d2bbca0b49c391 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 00:57:02,254 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => e790d3c63a11d58c49d2bbca0b49c391, NAME => 'usertable,user1,1405321189855.e790d3c63a11d58c49d2bbca0b49c391.', STARTKEY => 'user1', ENDKEY => 'user2'}
2014-07-14 00:57:02,254 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable e790d3c63a11d58c49d2bbca0b49c391
2014-07-14 00:57:02,254 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user1,1405321189855.e790d3c63a11d58c49d2bbca0b49c391.
2014-07-14 00:57:02,261 INFO  [StoreOpener-e790d3c63a11d58c49d2bbca0b49c391-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-14 00:57:02,264 DEBUG [StoreOpener-0098687d245db35b3c80045d6eefb3ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/family/40b7704123024ae684bd8430dcc0837b, isReference=false, isBulkLoadResult=false, seqid=24561, majorCompaction=false
2014-07-14 00:57:02,290 DEBUG [StoreOpener-0098687d245db35b3c80045d6eefb3ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/family/41446e81d76e4b14993f787a7b6f8b49, isReference=false, isBulkLoadResult=false, seqid=16657, majorCompaction=false
2014-07-14 00:57:02,300 DEBUG [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b/family/37be836763a4427694d7798b41106354, isReference=false, isBulkLoadResult=false, seqid=13007, majorCompaction=false
2014-07-14 00:57:02,305 DEBUG [StoreOpener-e790d3c63a11d58c49d2bbca0b49c391-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/e790d3c63a11d58c49d2bbca0b49c391/family/03f2c25d75f84ba0b25bd328647aedf7, isReference=false, isBulkLoadResult=false, seqid=18116, majorCompaction=false
2014-07-14 00:57:02,321 DEBUG [StoreOpener-0098687d245db35b3c80045d6eefb3ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/family/49953f9db9954cccb0a5df0b1516a19b, isReference=false, isBulkLoadResult=false, seqid=21812, majorCompaction=false
2014-07-14 00:57:02,321 DEBUG [StoreOpener-e790d3c63a11d58c49d2bbca0b49c391-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/e790d3c63a11d58c49d2bbca0b49c391/family/13a69a497d924f7e8bd4803245c665a1, isReference=false, isBulkLoadResult=false, seqid=26513, majorCompaction=false
2014-07-14 00:57:02,334 DEBUG [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b/family/4000c91d340e4fecadb88f8dc9059982, isReference=false, isBulkLoadResult=false, seqid=19950, majorCompaction=false
2014-07-14 00:57:02,355 DEBUG [StoreOpener-e790d3c63a11d58c49d2bbca0b49c391-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/e790d3c63a11d58c49d2bbca0b49c391/family/320cd3a62cb54ec1a7b735bfc71331eb, isReference=false, isBulkLoadResult=false, seqid=20018, majorCompaction=false
2014-07-14 00:57:02,359 DEBUG [StoreOpener-0098687d245db35b3c80045d6eefb3ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/family/5d20d23dc3aa463985e7d3b34794d275, isReference=false, isBulkLoadResult=false, seqid=2767, majorCompaction=true
2014-07-14 00:57:02,365 DEBUG [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b/family/463aeb2863de409da5359e56ebeb7eec, isReference=false, isBulkLoadResult=false, seqid=1987, majorCompaction=true
2014-07-14 00:57:02,407 DEBUG [StoreOpener-e790d3c63a11d58c49d2bbca0b49c391-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/e790d3c63a11d58c49d2bbca0b49c391/family/3324aedfa4144d0390a9b8feb7cf06f9, isReference=false, isBulkLoadResult=false, seqid=6245, majorCompaction=false
2014-07-14 00:57:02,407 DEBUG [StoreOpener-0098687d245db35b3c80045d6eefb3ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/family/7d90c739e66442228865d25dc44a682a, isReference=false, isBulkLoadResult=false, seqid=10282, majorCompaction=false
2014-07-14 00:57:02,412 DEBUG [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b/family/4cd19454142f410e99ba16c7780d3e0b, isReference=false, isBulkLoadResult=false, seqid=10701, majorCompaction=false
2014-07-14 00:57:02,430 DEBUG [StoreOpener-e790d3c63a11d58c49d2bbca0b49c391-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/e790d3c63a11d58c49d2bbca0b49c391/family/38c9dcc993aa4d538edef699d265ef68, isReference=false, isBulkLoadResult=false, seqid=19446, majorCompaction=false
2014-07-14 00:57:02,436 DEBUG [StoreOpener-0098687d245db35b3c80045d6eefb3ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/family/821c999e02b64595bd1856bb1a3a6b2f, isReference=false, isBulkLoadResult=false, seqid=13456, majorCompaction=false
2014-07-14 00:57:02,440 DEBUG [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b/family/5c6de880bb4849d49dcfff1080415a5c, isReference=false, isBulkLoadResult=false, seqid=11318, majorCompaction=false
2014-07-14 00:57:02,457 DEBUG [StoreOpener-e790d3c63a11d58c49d2bbca0b49c391-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/e790d3c63a11d58c49d2bbca0b49c391/family/3fc9092f72fc4f2e9ddce3dab28e5d84, isReference=false, isBulkLoadResult=false, seqid=20840, majorCompaction=false
2014-07-14 00:57:02,460 DEBUG [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b/family/6666036973a0423db304a260823f2b03, isReference=false, isBulkLoadResult=false, seqid=18864, majorCompaction=false
2014-07-14 00:57:02,461 DEBUG [StoreOpener-0098687d245db35b3c80045d6eefb3ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/family/96de6d7012554a879273d1f03ae645c9, isReference=false, isBulkLoadResult=false, seqid=17414, majorCompaction=false
2014-07-14 00:57:02,484 DEBUG [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b/family/6f2cd15517a34653893ba77d3d1412f9, isReference=false, isBulkLoadResult=false, seqid=24954, majorCompaction=false
2014-07-14 00:57:02,491 DEBUG [StoreOpener-e790d3c63a11d58c49d2bbca0b49c391-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/e790d3c63a11d58c49d2bbca0b49c391/family/44b86d0a36a34661ae5d156b4c3c28b3, isReference=false, isBulkLoadResult=false, seqid=14488, majorCompaction=false
2014-07-14 00:57:02,497 DEBUG [StoreOpener-0098687d245db35b3c80045d6eefb3ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/family/99a49e860b494773ba54edb6b51297bd, isReference=false, isBulkLoadResult=false, seqid=3818, majorCompaction=false
2014-07-14 00:57:02,507 DEBUG [StoreOpener-0098687d245db35b3c80045d6eefb3ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/family/9f0aedd3cdcd4221aa529210525cce1c, isReference=false, isBulkLoadResult=false, seqid=25276, majorCompaction=false
2014-07-14 00:57:02,517 DEBUG [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b/family/7ee19a0760394001bbea7355df67eb70, isReference=false, isBulkLoadResult=false, seqid=16398, majorCompaction=false
2014-07-14 00:57:02,519 DEBUG [StoreOpener-e790d3c63a11d58c49d2bbca0b49c391-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/e790d3c63a11d58c49d2bbca0b49c391/family/4c23fb4d1dd64e2cbe65360fd8f30da8, isReference=false, isBulkLoadResult=false, seqid=12953, majorCompaction=false
2014-07-14 00:57:02,528 DEBUG [StoreOpener-0098687d245db35b3c80045d6eefb3ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/family/ae2628c3cfab4d32920ba2a40db5fd36, isReference=false, isBulkLoadResult=false, seqid=14556, majorCompaction=false
2014-07-14 00:57:02,538 DEBUG [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b/family/880f4abf2cb942469fc5ea215d18aa9d, isReference=false, isBulkLoadResult=false, seqid=24250, majorCompaction=false
2014-07-14 00:57:02,545 DEBUG [StoreOpener-e790d3c63a11d58c49d2bbca0b49c391-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/e790d3c63a11d58c49d2bbca0b49c391/family/51a916d03be74bfcb6fba51db7d461ce, isReference=false, isBulkLoadResult=false, seqid=25803, majorCompaction=false
2014-07-14 00:57:02,558 DEBUG [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b/family/97e60aa6a0de40368ed8d96bbc0fd6d1, isReference=false, isBulkLoadResult=false, seqid=22055, majorCompaction=false
2014-07-14 00:57:02,564 DEBUG [StoreOpener-e790d3c63a11d58c49d2bbca0b49c391-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/e790d3c63a11d58c49d2bbca0b49c391/family/5645f32606ab4cc490ea8bec04f67e02, isReference=false, isBulkLoadResult=false, seqid=27549, majorCompaction=false
2014-07-14 00:57:02,565 DEBUG [StoreOpener-0098687d245db35b3c80045d6eefb3ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/family/b8ae055ff6f94f749d9ea74f7b6b8434, isReference=false, isBulkLoadResult=false, seqid=4302, majorCompaction=false
2014-07-14 00:57:02,582 DEBUG [StoreOpener-0098687d245db35b3c80045d6eefb3ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/family/bc5f55c34f8845379c0680d297faf449, isReference=false, isBulkLoadResult=false, seqid=23476, majorCompaction=false
2014-07-14 00:57:02,584 DEBUG [StoreOpener-e790d3c63a11d58c49d2bbca0b49c391-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/e790d3c63a11d58c49d2bbca0b49c391/family/667d82cb5d6749d898272e2df467c273, isReference=false, isBulkLoadResult=false, seqid=27433, majorCompaction=false
2014-07-14 00:57:02,586 DEBUG [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b/family/9c61a05fad944f9e981b0c61f275a797, isReference=false, isBulkLoadResult=false, seqid=13596, majorCompaction=false
2014-07-14 00:57:02,602 DEBUG [StoreOpener-0098687d245db35b3c80045d6eefb3ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/family/c5cd048b69f14346896660547169fd54, isReference=false, isBulkLoadResult=false, seqid=13009, majorCompaction=false
2014-07-14 00:57:02,605 DEBUG [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b/family/9d1046cb6aa347f5a9a6bf9e1b6459ce, isReference=false, isBulkLoadResult=false, seqid=22918, majorCompaction=false
2014-07-14 00:57:02,610 DEBUG [StoreOpener-e790d3c63a11d58c49d2bbca0b49c391-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/e790d3c63a11d58c49d2bbca0b49c391/family/6d054a5551b64cc4ac92ae96ef6bcfc1, isReference=false, isBulkLoadResult=false, seqid=12393, majorCompaction=false
2014-07-14 00:57:02,631 DEBUG [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b/family/a408472320aa4e049d35b309c5c8de83, isReference=false, isBulkLoadResult=false, seqid=24641, majorCompaction=false
2014-07-14 00:57:02,633 DEBUG [StoreOpener-e790d3c63a11d58c49d2bbca0b49c391-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/e790d3c63a11d58c49d2bbca0b49c391/family/6dc978949d344ae48952bc3001f7ddea, isReference=false, isBulkLoadResult=false, seqid=16811, majorCompaction=false
2014-07-14 00:57:02,648 DEBUG [StoreOpener-0098687d245db35b3c80045d6eefb3ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/family/c669d669ced14e029d6349f60bdddd3e, isReference=false, isBulkLoadResult=false, seqid=16240, majorCompaction=false
2014-07-14 00:57:02,655 DEBUG [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b/family/af49b06b495e415cbc5f43ceb3f29240, isReference=false, isBulkLoadResult=false, seqid=8717, majorCompaction=false
2014-07-14 00:57:02,669 DEBUG [StoreOpener-e790d3c63a11d58c49d2bbca0b49c391-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/e790d3c63a11d58c49d2bbca0b49c391/family/706e970de4e24a8b9d1f2d89020186b8, isReference=false, isBulkLoadResult=false, seqid=11960, majorCompaction=false
2014-07-14 00:57:02,674 DEBUG [StoreOpener-0098687d245db35b3c80045d6eefb3ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/family/cb9486df2dd340b1a2f8934e748e2278, isReference=false, isBulkLoadResult=false, seqid=18442, majorCompaction=false
2014-07-14 00:57:02,680 DEBUG [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b/family/bbdfd1543df04c56bdbd7061334f58a9, isReference=false, isBulkLoadResult=false, seqid=11865, majorCompaction=false
2014-07-14 00:57:02,693 DEBUG [StoreOpener-e790d3c63a11d58c49d2bbca0b49c391-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/e790d3c63a11d58c49d2bbca0b49c391/family/7cdebffed9214219b99776613d4a3e9a, isReference=false, isBulkLoadResult=false, seqid=5666, majorCompaction=true
2014-07-14 00:57:02,705 DEBUG [StoreOpener-0098687d245db35b3c80045d6eefb3ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/family/cc0f2c799c1c40feb470077e5410fcfe, isReference=false, isBulkLoadResult=false, seqid=19272, majorCompaction=false
2014-07-14 00:57:02,709 DEBUG [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b/family/c680548b4ff449d2ab888350256b69ac, isReference=false, isBulkLoadResult=false, seqid=7115, majorCompaction=false
2014-07-14 00:57:02,715 DEBUG [StoreOpener-e790d3c63a11d58c49d2bbca0b49c391-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/e790d3c63a11d58c49d2bbca0b49c391/family/8b933a9576894e01a3698d846d9684a9, isReference=false, isBulkLoadResult=false, seqid=13426, majorCompaction=false
2014-07-14 00:57:02,722 DEBUG [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b/family/e050558e81f049a3bf6c4efdf60bb0be, isReference=false, isBulkLoadResult=false, seqid=20473, majorCompaction=false
2014-07-14 00:57:02,738 DEBUG [StoreOpener-e790d3c63a11d58c49d2bbca0b49c391-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/e790d3c63a11d58c49d2bbca0b49c391/family/8ca253c9d53b44ac8348dab611bed11f, isReference=false, isBulkLoadResult=false, seqid=21850, majorCompaction=false
2014-07-14 00:57:02,745 DEBUG [StoreOpener-0098687d245db35b3c80045d6eefb3ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/family/d88e5a040de04016bf8612a65927a6a7, isReference=false, isBulkLoadResult=false, seqid=9639, majorCompaction=false
2014-07-14 00:57:02,759 DEBUG [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b/family/e6c354b8b87c48788f33ff33a7479c97, isReference=false, isBulkLoadResult=false, seqid=17564, majorCompaction=false
2014-07-14 00:57:02,760 DEBUG [StoreOpener-e790d3c63a11d58c49d2bbca0b49c391-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/e790d3c63a11d58c49d2bbca0b49c391/family/8dce38c786074f8981efa8ebeae5d7d7, isReference=false, isBulkLoadResult=false, seqid=26679, majorCompaction=false
2014-07-14 00:57:02,769 DEBUG [StoreOpener-0098687d245db35b3c80045d6eefb3ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/family/dd3dca07692c4d75af391dce723e6778, isReference=false, isBulkLoadResult=false, seqid=23962, majorCompaction=false
2014-07-14 00:57:02,782 DEBUG [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b/family/e822a0dd37534f46868f41eadd2e104b, isReference=false, isBulkLoadResult=false, seqid=19537, majorCompaction=false
2014-07-14 00:57:02,793 DEBUG [StoreOpener-e790d3c63a11d58c49d2bbca0b49c391-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/e790d3c63a11d58c49d2bbca0b49c391/family/91bbef6cf3004610915bb6771d4cf418, isReference=false, isBulkLoadResult=false, seqid=24951, majorCompaction=false
2014-07-14 00:57:02,805 DEBUG [StoreOpener-0098687d245db35b3c80045d6eefb3ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/family/e7868743a70242b28f51479b77af01cf, isReference=false, isBulkLoadResult=false, seqid=10826, majorCompaction=false
2014-07-14 00:57:02,813 DEBUG [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b/family/ee563746052e4f8d80eeff255026308b, isReference=false, isBulkLoadResult=false, seqid=23740, majorCompaction=false
2014-07-14 00:57:02,818 DEBUG [StoreOpener-e790d3c63a11d58c49d2bbca0b49c391-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/e790d3c63a11d58c49d2bbca0b49c391/family/b4ec5a58eb844eb7bfbdea86e0a7dda1, isReference=false, isBulkLoadResult=false, seqid=6896, majorCompaction=false
2014-07-14 00:57:02,838 DEBUG [StoreOpener-0098687d245db35b3c80045d6eefb3ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/family/ecd98c0dff5942da96148b7647c1f752, isReference=false, isBulkLoadResult=false, seqid=3294, majorCompaction=false
2014-07-14 00:57:02,840 DEBUG [StoreOpener-e790d3c63a11d58c49d2bbca0b49c391-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/e790d3c63a11d58c49d2bbca0b49c391/family/bd0df99da0e544878034d6c3c6669be6, isReference=false, isBulkLoadResult=false, seqid=16054, majorCompaction=false
2014-07-14 00:57:02,841 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee
2014-07-14 00:57:02,845 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined 0098687d245db35b3c80045d6eefb3ee; next sequenceid=25277
2014-07-14 00:57:02,845 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 0098687d245db35b3c80045d6eefb3ee
2014-07-14 00:57:02,847 INFO  [PostOpenDeployTasks:0098687d245db35b3c80045d6eefb3ee] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user6,1405321189856.0098687d245db35b3c80045d6eefb3ee.
2014-07-14 00:57:02,849 DEBUG [PostOpenDeployTasks:0098687d245db35b3c80045d6eefb3ee] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 00:57:02,850 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 20 blocking
2014-07-14 00:57:02,858 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 310625972 starting at candidate #1 after considering 156 permutations with 136 in ratio
2014-07-14 00:57:02,859 DEBUG [StoreOpener-ba3a3ea148e3630e391fc92ec8fce47b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b/family/fc07fa95b91c48e68ac9035c7eab1b54, isReference=false, isBulkLoadResult=false, seqid=15032, majorCompaction=false
2014-07-14 00:57:02,860 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.HStore: 0098687d245db35b3c80045d6eefb3ee - family: Initiating minor compaction
2014-07-14 00:57:02,861 INFO  [regionserver60020-smallCompactions-1405324622848] regionserver.HRegion: Starting compaction on family in region usertable,user6,1405321189856.0098687d245db35b3c80045d6eefb3ee.
2014-07-14 00:57:02,861 INFO  [regionserver60020-smallCompactions-1405324622848] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user6,1405321189856.0098687d245db35b3c80045d6eefb3ee. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/.tmp, totalSize=296.2m
2014-07-14 00:57:02,862 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/family/ecd98c0dff5942da96148b7647c1f752, keycount=103787, bloomtype=ROW, size=73.9m, encoding=NONE, seqNum=3294
2014-07-14 00:57:02,862 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/ba3a3ea148e3630e391fc92ec8fce47b
2014-07-14 00:57:02,862 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/family/99a49e860b494773ba54edb6b51297bd, keycount=164277, bloomtype=ROW, size=117.1m, encoding=NONE, seqNum=3818
2014-07-14 00:57:02,863 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/0098687d245db35b3c80045d6eefb3ee/family/b8ae055ff6f94f749d9ea74f7b6b8434, keycount=147802, bloomtype=ROW, size=105.2m, encoding=NONE, seqNum=4302
2014-07-14 00:57:02,865 INFO  [PostOpenDeployTasks:0098687d245db35b3c80045d6eefb3ee] catalog.MetaEditor: Updated row usertable,user6,1405321189856.0098687d245db35b3c80045d6eefb3ee. with server=slave1,60020,1405324583256
2014-07-14 00:57:02,865 INFO  [PostOpenDeployTasks:0098687d245db35b3c80045d6eefb3ee] regionserver.HRegionServer: Finished post open deploy task for usertable,user6,1405321189856.0098687d245db35b3c80045d6eefb3ee.
2014-07-14 00:57:02,866 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 0098687d245db35b3c80045d6eefb3ee from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 00:57:02,867 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined ba3a3ea148e3630e391fc92ec8fce47b; next sequenceid=25224
2014-07-14 00:57:02,867 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node ba3a3ea148e3630e391fc92ec8fce47b
2014-07-14 00:57:02,868 DEBUG [StoreOpener-e790d3c63a11d58c49d2bbca0b49c391-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/e790d3c63a11d58c49d2bbca0b49c391/family/ca62650e81a143fda36d5f157ddbd40a, isReference=false, isBulkLoadResult=false, seqid=23498, majorCompaction=false
2014-07-14 00:57:02,869 INFO  [PostOpenDeployTasks:ba3a3ea148e3630e391fc92ec8fce47b] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user4,1405321189856.ba3a3ea148e3630e391fc92ec8fce47b.
2014-07-14 00:57:02,870 DEBUG [PostOpenDeployTasks:ba3a3ea148e3630e391fc92ec8fce47b] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-14 00:57:02,872 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 0098687d245db35b3c80045d6eefb3ee from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 00:57:02,872 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned 0098687d245db35b3c80045d6eefb3ee to OPENED in zk on slave1,60020,1405324583256
2014-07-14 00:57:02,872 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user6,1405321189856.0098687d245db35b3c80045d6eefb3ee. on slave1,60020,1405324583256
2014-07-14 00:57:02,873 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 98633400eb20e4fe217a3f5e9d4c29f0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 00:57:02,877 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 98633400eb20e4fe217a3f5e9d4c29f0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 00:57:02,877 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => 98633400eb20e4fe217a3f5e9d4c29f0, NAME => 'usertable,user2,1405321189855.98633400eb20e4fe217a3f5e9d4c29f0.', STARTKEY => 'user2', ENDKEY => 'user3'}
2014-07-14 00:57:02,878 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 98633400eb20e4fe217a3f5e9d4c29f0
2014-07-14 00:57:02,878 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user2,1405321189855.98633400eb20e4fe217a3f5e9d4c29f0.
2014-07-14 00:57:02,880 INFO  [PostOpenDeployTasks:ba3a3ea148e3630e391fc92ec8fce47b] catalog.MetaEditor: Updated row usertable,user4,1405321189856.ba3a3ea148e3630e391fc92ec8fce47b. with server=slave1,60020,1405324583256
2014-07-14 00:57:02,881 INFO  [PostOpenDeployTasks:ba3a3ea148e3630e391fc92ec8fce47b] regionserver.HRegionServer: Finished post open deploy task for usertable,user4,1405321189856.ba3a3ea148e3630e391fc92ec8fce47b.
2014-07-14 00:57:02,882 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning ba3a3ea148e3630e391fc92ec8fce47b from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 00:57:02,886 DEBUG [StoreOpener-e790d3c63a11d58c49d2bbca0b49c391-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/e790d3c63a11d58c49d2bbca0b49c391/family/cb7cf651db9c495a9ff35feadbea1d24, isReference=false, isBulkLoadResult=false, seqid=24164, majorCompaction=false
2014-07-14 00:57:02,886 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node ba3a3ea148e3630e391fc92ec8fce47b from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 00:57:02,886 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned ba3a3ea148e3630e391fc92ec8fce47b to OPENED in zk on slave1,60020,1405324583256
2014-07-14 00:57:02,886 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,user4,1405321189856.ba3a3ea148e3630e391fc92ec8fce47b. on slave1,60020,1405324583256
2014-07-14 00:57:02,887 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 0a3fceff1c074904aae38a3f8b5e5cac from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 00:57:02,888 INFO  [StoreOpener-98633400eb20e4fe217a3f5e9d4c29f0-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-14 00:57:02,891 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 0a3fceff1c074904aae38a3f8b5e5cac from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 00:57:02,892 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => 0a3fceff1c074904aae38a3f8b5e5cac, NAME => 'usertable,user7,1405321189856.0a3fceff1c074904aae38a3f8b5e5cac.', STARTKEY => 'user7', ENDKEY => 'user8'}
2014-07-14 00:57:02,892 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 0a3fceff1c074904aae38a3f8b5e5cac
2014-07-14 00:57:02,892 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,user7,1405321189856.0a3fceff1c074904aae38a3f8b5e5cac.
2014-07-14 00:57:02,900 INFO  [StoreOpener-0a3fceff1c074904aae38a3f8b5e5cac-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-14 00:57:02,925 DEBUG [StoreOpener-98633400eb20e4fe217a3f5e9d4c29f0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/98633400eb20e4fe217a3f5e9d4c29f0/family/0700c87e5b244144bcb2bc2e0a4fb7f8, isReference=false, isBulkLoadResult=false, seqid=17547, majorCompaction=false
2014-07-14 00:57:02,927 DEBUG [regionserver60020-smallCompactions-1405324622848] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 00:57:02,933 DEBUG [StoreOpener-0a3fceff1c074904aae38a3f8b5e5cac-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0a3fceff1c074904aae38a3f8b5e5cac/family/02d3355701c04821b46c8df9044e103d, isReference=false, isBulkLoadResult=false, seqid=23583, majorCompaction=false
2014-07-14 00:57:02,941 DEBUG [StoreOpener-e790d3c63a11d58c49d2bbca0b49c391-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/e790d3c63a11d58c49d2bbca0b49c391/family/d0b5041d63e44e4caae3e8b4ff67ab4e, isReference=false, isBulkLoadResult=false, seqid=22692, majorCompaction=false
2014-07-14 00:57:02,949 DEBUG [StoreOpener-98633400eb20e4fe217a3f5e9d4c29f0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/98633400eb20e4fe217a3f5e9d4c29f0/family/07c1daaf3a604e9385ef2595b25909e5, isReference=false, isBulkLoadResult=false, seqid=18575, majorCompaction=false
2014-07-14 00:57:02,956 DEBUG [StoreOpener-0a3fceff1c074904aae38a3f8b5e5cac-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0a3fceff1c074904aae38a3f8b5e5cac/family/039ddac2ea494ad6ba3210ccf94e7b8a, isReference=false, isBulkLoadResult=false, seqid=24953, majorCompaction=false
2014-07-14 00:57:02,992 DEBUG [StoreOpener-e790d3c63a11d58c49d2bbca0b49c391-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/e790d3c63a11d58c49d2bbca0b49c391/family/d5306f5232ca4530ab0869346a073a92, isReference=false, isBulkLoadResult=false, seqid=13897, majorCompaction=false
2014-07-14 00:57:02,999 DEBUG [StoreOpener-98633400eb20e4fe217a3f5e9d4c29f0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/98633400eb20e4fe217a3f5e9d4c29f0/family/16645e8b4f6247ce84167126fcb2560f, isReference=false, isBulkLoadResult=false, seqid=19662, majorCompaction=false
2014-07-14 00:57:03,015 DEBUG [StoreOpener-0a3fceff1c074904aae38a3f8b5e5cac-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0a3fceff1c074904aae38a3f8b5e5cac/family/070603c6fecc491fa5319c203b263c4f, isReference=false, isBulkLoadResult=false, seqid=16792, majorCompaction=false
2014-07-14 00:57:03,016 DEBUG [StoreOpener-e790d3c63a11d58c49d2bbca0b49c391-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/e790d3c63a11d58c49d2bbca0b49c391/family/da5bf7c26c4c4e2fac7c68babc77584c, isReference=false, isBulkLoadResult=false, seqid=25780, majorCompaction=false
2014-07-14 00:57:03,020 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/e790d3c63a11d58c49d2bbca0b49c391
2014-07-14 00:57:03,024 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined e790d3c63a11d58c49d2bbca0b49c391; next sequenceid=27550
2014-07-14 00:57:03,024 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node e790d3c63a11d58c49d2bbca0b49c391
2014-07-14 00:57:03,025 DEBUG [StoreOpener-0a3fceff1c074904aae38a3f8b5e5cac-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0a3fceff1c074904aae38a3f8b5e5cac/family/1ceeb83d6090401cb718ed1e042d33e4, isReference=false, isBulkLoadResult=false, seqid=25277, majorCompaction=false
2014-07-14 00:57:03,026 INFO  [PostOpenDeployTasks:e790d3c63a11d58c49d2bbca0b49c391] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user1,1405321189855.e790d3c63a11d58c49d2bbca0b49c391.
2014-07-14 00:57:03,026 DEBUG [PostOpenDeployTasks:e790d3c63a11d58c49d2bbca0b49c391] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:2), split_queue=0, merge_queue=0
2014-07-14 00:57:03,034 INFO  [PostOpenDeployTasks:e790d3c63a11d58c49d2bbca0b49c391] catalog.MetaEditor: Updated row usertable,user1,1405321189855.e790d3c63a11d58c49d2bbca0b49c391. with server=slave1,60020,1405324583256
2014-07-14 00:57:03,035 INFO  [PostOpenDeployTasks:e790d3c63a11d58c49d2bbca0b49c391] regionserver.HRegionServer: Finished post open deploy task for usertable,user1,1405321189855.e790d3c63a11d58c49d2bbca0b49c391.
2014-07-14 00:57:03,036 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e790d3c63a11d58c49d2bbca0b49c391 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 00:57:03,038 DEBUG [StoreOpener-98633400eb20e4fe217a3f5e9d4c29f0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/98633400eb20e4fe217a3f5e9d4c29f0/family/1b7385cb2fe64801a94a2048c707b9eb, isReference=false, isBulkLoadResult=false, seqid=10745, majorCompaction=false
2014-07-14 00:57:03,039 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e790d3c63a11d58c49d2bbca0b49c391 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 00:57:03,039 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned e790d3c63a11d58c49d2bbca0b49c391 to OPENED in zk on slave1,60020,1405324583256
2014-07-14 00:57:03,039 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user1,1405321189855.e790d3c63a11d58c49d2bbca0b49c391. on slave1,60020,1405324583256
2014-07-14 00:57:03,046 DEBUG [StoreOpener-0a3fceff1c074904aae38a3f8b5e5cac-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0a3fceff1c074904aae38a3f8b5e5cac/family/21d6ce48d8a14d6f814d1c373f6b1917, isReference=false, isBulkLoadResult=false, seqid=22584, majorCompaction=false
2014-07-14 00:57:03,075 DEBUG [StoreOpener-98633400eb20e4fe217a3f5e9d4c29f0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/98633400eb20e4fe217a3f5e9d4c29f0/family/3895341a9c044482acec3ed08852b4a2, isReference=false, isBulkLoadResult=false, seqid=4867, majorCompaction=false
2014-07-14 00:57:03,083 DEBUG [StoreOpener-0a3fceff1c074904aae38a3f8b5e5cac-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0a3fceff1c074904aae38a3f8b5e5cac/family/2333fd1e6e174362bf29a043acc46aa0, isReference=false, isBulkLoadResult=false, seqid=6853, majorCompaction=false
2014-07-14 00:57:03,094 DEBUG [StoreOpener-98633400eb20e4fe217a3f5e9d4c29f0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/98633400eb20e4fe217a3f5e9d4c29f0/family/3eca02ceb9404da69b691aee26d3114e, isReference=false, isBulkLoadResult=false, seqid=25409, majorCompaction=false
2014-07-14 00:57:03,111 DEBUG [StoreOpener-0a3fceff1c074904aae38a3f8b5e5cac-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0a3fceff1c074904aae38a3f8b5e5cac/family/2dd6028f2bf24642bf41a6d33f204d44, isReference=false, isBulkLoadResult=false, seqid=13773, majorCompaction=false
2014-07-14 00:57:03,126 DEBUG [StoreOpener-98633400eb20e4fe217a3f5e9d4c29f0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/98633400eb20e4fe217a3f5e9d4c29f0/family/464e77ff50a1443d85ba744387cf22f1, isReference=false, isBulkLoadResult=false, seqid=4405, majorCompaction=true
2014-07-14 00:57:03,141 DEBUG [StoreOpener-0a3fceff1c074904aae38a3f8b5e5cac-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0a3fceff1c074904aae38a3f8b5e5cac/family/2ea5ace1528c43b68dc43a4459664aaa, isReference=false, isBulkLoadResult=false, seqid=13147, majorCompaction=false
2014-07-14 00:57:03,161 DEBUG [StoreOpener-98633400eb20e4fe217a3f5e9d4c29f0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/98633400eb20e4fe217a3f5e9d4c29f0/family/4a8b6a3733264e6eb3ff92db7d8bcfb6, isReference=false, isBulkLoadResult=false, seqid=5495, majorCompaction=false
2014-07-14 00:57:03,166 DEBUG [StoreOpener-0a3fceff1c074904aae38a3f8b5e5cac-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0a3fceff1c074904aae38a3f8b5e5cac/family/2feae22578984d4ab3fc6af16c4210b0, isReference=false, isBulkLoadResult=false, seqid=15869, majorCompaction=false
2014-07-14 00:57:03,195 DEBUG [StoreOpener-98633400eb20e4fe217a3f5e9d4c29f0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/98633400eb20e4fe217a3f5e9d4c29f0/family/5d4362d90d95402ea64294e4d0abbba0, isReference=false, isBulkLoadResult=false, seqid=23862, majorCompaction=false
2014-07-14 00:57:03,195 DEBUG [StoreOpener-0a3fceff1c074904aae38a3f8b5e5cac-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0a3fceff1c074904aae38a3f8b5e5cac/family/3ecf651f8f1340778de8f29fbd3f7feb, isReference=false, isBulkLoadResult=false, seqid=17695, majorCompaction=false
2014-07-14 00:57:03,225 DEBUG [StoreOpener-0a3fceff1c074904aae38a3f8b5e5cac-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0a3fceff1c074904aae38a3f8b5e5cac/family/440af11156ae493493919aec2bf1e10a, isReference=false, isBulkLoadResult=false, seqid=15406, majorCompaction=false
2014-07-14 00:57:03,235 DEBUG [StoreOpener-98633400eb20e4fe217a3f5e9d4c29f0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/98633400eb20e4fe217a3f5e9d4c29f0/family/6100685b7f144986bc616e31aaf58143, isReference=false, isBulkLoadResult=false, seqid=12741, majorCompaction=false
2014-07-14 00:57:03,245 DEBUG [StoreOpener-98633400eb20e4fe217a3f5e9d4c29f0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/98633400eb20e4fe217a3f5e9d4c29f0/family/67e692ddc48542a4a25bd24293c7e4da, isReference=false, isBulkLoadResult=false, seqid=27584, majorCompaction=false
2014-07-14 00:57:03,251 DEBUG [StoreOpener-0a3fceff1c074904aae38a3f8b5e5cac-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0a3fceff1c074904aae38a3f8b5e5cac/family/4bed8106fb024fc587052421b6c11634, isReference=false, isBulkLoadResult=false, seqid=1718, majorCompaction=true
2014-07-14 00:57:03,269 DEBUG [StoreOpener-98633400eb20e4fe217a3f5e9d4c29f0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/98633400eb20e4fe217a3f5e9d4c29f0/family/6df00d2afa174fb8b18f2b0f9a8d1fa0, isReference=false, isBulkLoadResult=false, seqid=22327, majorCompaction=false
2014-07-14 00:57:03,281 DEBUG [StoreOpener-0a3fceff1c074904aae38a3f8b5e5cac-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0a3fceff1c074904aae38a3f8b5e5cac/family/7a4df5ff7a79446bb9ec1bb611b1e188, isReference=false, isBulkLoadResult=false, seqid=20504, majorCompaction=false
2014-07-14 00:57:03,289 DEBUG [StoreOpener-98633400eb20e4fe217a3f5e9d4c29f0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/98633400eb20e4fe217a3f5e9d4c29f0/family/72dbe01c07a44b3caa5a8708520f5271, isReference=false, isBulkLoadResult=false, seqid=11791, majorCompaction=false
2014-07-14 00:57:03,308 DEBUG [StoreOpener-0a3fceff1c074904aae38a3f8b5e5cac-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0a3fceff1c074904aae38a3f8b5e5cac/family/83c4adcd156444849d2d167d40bcbc35, isReference=false, isBulkLoadResult=false, seqid=2350, majorCompaction=false
2014-07-14 00:57:03,313 DEBUG [StoreOpener-98633400eb20e4fe217a3f5e9d4c29f0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/98633400eb20e4fe217a3f5e9d4c29f0/family/7b7c165fb428434f98d6fca355865016, isReference=false, isBulkLoadResult=false, seqid=12309, majorCompaction=false
2014-07-14 00:57:03,338 DEBUG [StoreOpener-0a3fceff1c074904aae38a3f8b5e5cac-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0a3fceff1c074904aae38a3f8b5e5cac/family/978b1c05d63548478756af9803bdfac8, isReference=false, isBulkLoadResult=false, seqid=11683, majorCompaction=false
2014-07-14 00:57:03,340 DEBUG [StoreOpener-98633400eb20e4fe217a3f5e9d4c29f0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/98633400eb20e4fe217a3f5e9d4c29f0/family/7ef95c154a2046c880c738d397ba7daa, isReference=false, isBulkLoadResult=false, seqid=17085, majorCompaction=false
2014-07-14 00:57:03,358 DEBUG [StoreOpener-0a3fceff1c074904aae38a3f8b5e5cac-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0a3fceff1c074904aae38a3f8b5e5cac/family/a15ddd6478424dc197489e11a4e7ae0b, isReference=false, isBulkLoadResult=false, seqid=24111, majorCompaction=false
2014-07-14 00:57:03,367 DEBUG [StoreOpener-98633400eb20e4fe217a3f5e9d4c29f0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/98633400eb20e4fe217a3f5e9d4c29f0/family/801c57195ce6444f93714446717fe49f, isReference=false, isBulkLoadResult=false, seqid=24058, majorCompaction=false
2014-07-14 00:57:03,379 DEBUG [StoreOpener-0a3fceff1c074904aae38a3f8b5e5cac-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0a3fceff1c074904aae38a3f8b5e5cac/family/b4e2f0f49488420c910ef4560649677d, isReference=false, isBulkLoadResult=false, seqid=12644, majorCompaction=false
2014-07-14 00:57:03,392 DEBUG [StoreOpener-98633400eb20e4fe217a3f5e9d4c29f0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/98633400eb20e4fe217a3f5e9d4c29f0/family/8102cd743cfc4fa68bdb49f8f29ebe09, isReference=false, isBulkLoadResult=false, seqid=15300, majorCompaction=false
2014-07-14 00:57:03,404 DEBUG [StoreOpener-0a3fceff1c074904aae38a3f8b5e5cac-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0a3fceff1c074904aae38a3f8b5e5cac/family/b5e4a3e166394dbe96fe85c5445cdac8, isReference=false, isBulkLoadResult=false, seqid=21297, majorCompaction=false
2014-07-14 00:57:03,419 DEBUG [StoreOpener-98633400eb20e4fe217a3f5e9d4c29f0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/98633400eb20e4fe217a3f5e9d4c29f0/family/8964476ae49d44e4a230e69e3254c56c, isReference=false, isBulkLoadResult=false, seqid=13248, majorCompaction=false
2014-07-14 00:57:03,431 DEBUG [StoreOpener-0a3fceff1c074904aae38a3f8b5e5cac-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0a3fceff1c074904aae38a3f8b5e5cac/family/d673f10c612d482897332d352cb1ea88, isReference=false, isBulkLoadResult=false, seqid=12227, majorCompaction=false
2014-07-14 00:57:03,443 DEBUG [StoreOpener-98633400eb20e4fe217a3f5e9d4c29f0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/98633400eb20e4fe217a3f5e9d4c29f0/family/8b12ae225d434234886728a727bd1a04, isReference=false, isBulkLoadResult=false, seqid=26104, majorCompaction=false
2014-07-14 00:57:03,451 DEBUG [StoreOpener-0a3fceff1c074904aae38a3f8b5e5cac-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0a3fceff1c074904aae38a3f8b5e5cac/family/e83d538aa96c4ee2947ba93a7d0c2d55, isReference=false, isBulkLoadResult=false, seqid=19924, majorCompaction=false
2014-07-14 00:57:03,468 DEBUG [StoreOpener-98633400eb20e4fe217a3f5e9d4c29f0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/98633400eb20e4fe217a3f5e9d4c29f0/family/9d99f6e89024451aa0e29af93bab4f3b, isReference=false, isBulkLoadResult=false, seqid=15873, majorCompaction=false
2014-07-14 00:57:03,478 DEBUG [StoreOpener-0a3fceff1c074904aae38a3f8b5e5cac-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0a3fceff1c074904aae38a3f8b5e5cac/family/e8d748327ba14ed897060d9845ffb522, isReference=false, isBulkLoadResult=false, seqid=24323, majorCompaction=false
2014-07-14 00:57:03,495 DEBUG [StoreOpener-0a3fceff1c074904aae38a3f8b5e5cac-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0a3fceff1c074904aae38a3f8b5e5cac/family/ee7727e8a0f348eb93b8c88ed65b8ab6, isReference=false, isBulkLoadResult=false, seqid=21926, majorCompaction=false
2014-07-14 00:57:03,495 DEBUG [StoreOpener-98633400eb20e4fe217a3f5e9d4c29f0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/98633400eb20e4fe217a3f5e9d4c29f0/family/a02bd3a079644fbfa8819fee77270317, isReference=false, isBulkLoadResult=false, seqid=11303, majorCompaction=false
2014-07-14 00:57:03,518 DEBUG [StoreOpener-0a3fceff1c074904aae38a3f8b5e5cac-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0a3fceff1c074904aae38a3f8b5e5cac/family/fda241b0b9154ba9a5342b4610cfa6fc, isReference=false, isBulkLoadResult=false, seqid=18221, majorCompaction=false
2014-07-14 00:57:03,522 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/0a3fceff1c074904aae38a3f8b5e5cac
2014-07-14 00:57:03,523 DEBUG [StoreOpener-98633400eb20e4fe217a3f5e9d4c29f0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/98633400eb20e4fe217a3f5e9d4c29f0/family/acd7502f182e44a0b44c72bd367775d8, isReference=false, isBulkLoadResult=false, seqid=13801, majorCompaction=false
2014-07-14 00:57:03,526 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined 0a3fceff1c074904aae38a3f8b5e5cac; next sequenceid=25278
2014-07-14 00:57:03,526 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 0a3fceff1c074904aae38a3f8b5e5cac
2014-07-14 00:57:03,528 INFO  [PostOpenDeployTasks:0a3fceff1c074904aae38a3f8b5e5cac] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user7,1405321189856.0a3fceff1c074904aae38a3f8b5e5cac.
2014-07-14 00:57:03,528 DEBUG [PostOpenDeployTasks:0a3fceff1c074904aae38a3f8b5e5cac] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:3), split_queue=0, merge_queue=0
2014-07-14 00:57:03,537 INFO  [PostOpenDeployTasks:0a3fceff1c074904aae38a3f8b5e5cac] catalog.MetaEditor: Updated row usertable,user7,1405321189856.0a3fceff1c074904aae38a3f8b5e5cac. with server=slave1,60020,1405324583256
2014-07-14 00:57:03,537 INFO  [PostOpenDeployTasks:0a3fceff1c074904aae38a3f8b5e5cac] regionserver.HRegionServer: Finished post open deploy task for usertable,user7,1405321189856.0a3fceff1c074904aae38a3f8b5e5cac.
2014-07-14 00:57:03,538 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 0a3fceff1c074904aae38a3f8b5e5cac from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 00:57:03,542 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 0a3fceff1c074904aae38a3f8b5e5cac from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 00:57:03,542 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned 0a3fceff1c074904aae38a3f8b5e5cac to OPENED in zk on slave1,60020,1405324583256
2014-07-14 00:57:03,542 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,user7,1405321189856.0a3fceff1c074904aae38a3f8b5e5cac. on slave1,60020,1405324583256
2014-07-14 00:57:03,548 DEBUG [StoreOpener-98633400eb20e4fe217a3f5e9d4c29f0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/98633400eb20e4fe217a3f5e9d4c29f0/family/b270b0af6fd8454eacd4aa71deeecf73, isReference=false, isBulkLoadResult=false, seqid=24636, majorCompaction=false
2014-07-14 00:57:03,574 DEBUG [StoreOpener-98633400eb20e4fe217a3f5e9d4c29f0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/98633400eb20e4fe217a3f5e9d4c29f0/family/dbff4954c5b04186905c2d03a439366f, isReference=false, isBulkLoadResult=false, seqid=22778, majorCompaction=false
2014-07-14 00:57:03,613 DEBUG [StoreOpener-98633400eb20e4fe217a3f5e9d4c29f0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/98633400eb20e4fe217a3f5e9d4c29f0/family/f2e1a12a06b04744bd8af1dcdd9bdb1a, isReference=false, isBulkLoadResult=false, seqid=23613, majorCompaction=false
2014-07-14 00:57:03,633 DEBUG [StoreOpener-98633400eb20e4fe217a3f5e9d4c29f0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/98633400eb20e4fe217a3f5e9d4c29f0/family/f9aedd52e1dc4061a382a27d5afd7e08, isReference=false, isBulkLoadResult=false, seqid=26980, majorCompaction=false
2014-07-14 00:57:03,637 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/98633400eb20e4fe217a3f5e9d4c29f0
2014-07-14 00:57:03,640 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined 98633400eb20e4fe217a3f5e9d4c29f0; next sequenceid=27585
2014-07-14 00:57:03,640 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 98633400eb20e4fe217a3f5e9d4c29f0
2014-07-14 00:57:03,642 INFO  [PostOpenDeployTasks:98633400eb20e4fe217a3f5e9d4c29f0] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user2,1405321189855.98633400eb20e4fe217a3f5e9d4c29f0.
2014-07-14 00:57:03,642 DEBUG [PostOpenDeployTasks:98633400eb20e4fe217a3f5e9d4c29f0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:4), split_queue=0, merge_queue=0
2014-07-14 00:57:03,653 INFO  [PostOpenDeployTasks:98633400eb20e4fe217a3f5e9d4c29f0] catalog.MetaEditor: Updated row usertable,user2,1405321189855.98633400eb20e4fe217a3f5e9d4c29f0. with server=slave1,60020,1405324583256
2014-07-14 00:57:03,653 INFO  [PostOpenDeployTasks:98633400eb20e4fe217a3f5e9d4c29f0] regionserver.HRegionServer: Finished post open deploy task for usertable,user2,1405321189855.98633400eb20e4fe217a3f5e9d4c29f0.
2014-07-14 00:57:03,654 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 98633400eb20e4fe217a3f5e9d4c29f0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 00:57:03,659 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 98633400eb20e4fe217a3f5e9d4c29f0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 00:57:03,659 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned 98633400eb20e4fe217a3f5e9d4c29f0 to OPENED in zk on slave1,60020,1405324583256
2014-07-14 00:57:03,660 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user2,1405321189855.98633400eb20e4fe217a3f5e9d4c29f0. on slave1,60020,1405324583256
2014-07-14 00:57:06,918 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:5), split_queue=0, merge_queue=0
2014-07-14 00:57:06,919 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:6), split_queue=0, merge_queue=0
2014-07-14 00:57:06,919 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:7), split_queue=0, merge_queue=0
2014-07-14 00:57:06,919 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:8), split_queue=0, merge_queue=0
2014-07-14 00:57:06,920 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:9), split_queue=0, merge_queue=0
2014-07-14 00:57:29,800 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Close ba3a3ea148e3630e391fc92ec8fce47b, via zk=yes, znode version=0, on null
2014-07-14 00:57:29,802 INFO  [Priority.RpcServer.handler=2,port=60020] regionserver.HRegionServer: Close e790d3c63a11d58c49d2bbca0b49c391, via zk=yes, znode version=0, on null
2014-07-14 00:57:29,803 INFO  [Priority.RpcServer.handler=3,port=60020] regionserver.HRegionServer: Close 0098687d245db35b3c80045d6eefb3ee, via zk=yes, znode version=0, on null
2014-07-14 00:57:29,803 INFO  [Priority.RpcServer.handler=4,port=60020] regionserver.HRegionServer: Close 0a3fceff1c074904aae38a3f8b5e5cac, via zk=yes, znode version=0, on null
2014-07-14 00:57:29,804 INFO  [Priority.RpcServer.handler=5,port=60020] regionserver.HRegionServer: Close 98633400eb20e4fe217a3f5e9d4c29f0, via zk=yes, znode version=0, on null
2014-07-14 00:57:29,804 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Processing close of usertable,user1,1405321189855.e790d3c63a11d58c49d2bbca0b49c391.
2014-07-14 00:57:29,805 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Processing close of usertable,user6,1405321189856.0098687d245db35b3c80045d6eefb3ee.
2014-07-14 00:57:29,806 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Processing close of usertable,user4,1405321189856.ba3a3ea148e3630e391fc92ec8fce47b.
2014-07-14 00:57:29,808 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closing usertable,user1,1405321189855.e790d3c63a11d58c49d2bbca0b49c391.: disabling compactions & flushes
2014-07-14 00:57:29,808 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Updates disabled for region usertable,user1,1405321189855.e790d3c63a11d58c49d2bbca0b49c391.
2014-07-14 00:57:29,809 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closing usertable,user6,1405321189856.0098687d245db35b3c80045d6eefb3ee.: disabling compactions & flushes
2014-07-14 00:57:29,809 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: waiting for 1 compactions to complete for region usertable,user6,1405321189856.0098687d245db35b3c80045d6eefb3ee.
2014-07-14 00:57:29,810 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closing usertable,user4,1405321189856.ba3a3ea148e3630e391fc92ec8fce47b.: disabling compactions & flushes
2014-07-14 00:57:29,810 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Updates disabled for region usertable,user4,1405321189856.ba3a3ea148e3630e391fc92ec8fce47b.
2014-07-14 00:57:29,862 INFO  [StoreCloserThread-usertable,user1,1405321189855.e790d3c63a11d58c49d2bbca0b49c391.-1] regionserver.HStore: Closed family
2014-07-14 00:57:29,863 INFO  [StoreCloserThread-usertable,user4,1405321189856.ba3a3ea148e3630e391fc92ec8fce47b.-1] regionserver.HStore: Closed family
2014-07-14 00:57:29,865 INFO  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closed usertable,user1,1405321189855.e790d3c63a11d58c49d2bbca0b49c391.
2014-07-14 00:57:29,865 INFO  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closed usertable,user4,1405321189856.ba3a3ea148e3630e391fc92ec8fce47b.
2014-07-14 00:57:29,866 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e790d3c63a11d58c49d2bbca0b49c391 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-14 00:57:29,866 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning ba3a3ea148e3630e391fc92ec8fce47b from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-14 00:57:29,871 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e790d3c63a11d58c49d2bbca0b49c391 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-14 00:57:29,871 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Set closed state in zk for usertable,user1,1405321189855.e790d3c63a11d58c49d2bbca0b49c391. on slave1,60020,1405324583256
2014-07-14 00:57:29,871 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Closed usertable,user1,1405321189855.e790d3c63a11d58c49d2bbca0b49c391.
2014-07-14 00:57:29,871 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node ba3a3ea148e3630e391fc92ec8fce47b from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-14 00:57:29,871 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Set closed state in zk for usertable,user4,1405321189856.ba3a3ea148e3630e391fc92ec8fce47b. on slave1,60020,1405324583256
2014-07-14 00:57:29,871 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Processing close of usertable,user7,1405321189856.0a3fceff1c074904aae38a3f8b5e5cac.
2014-07-14 00:57:29,872 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Closed usertable,user4,1405321189856.ba3a3ea148e3630e391fc92ec8fce47b.
2014-07-14 00:57:29,872 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Processing close of usertable,user2,1405321189855.98633400eb20e4fe217a3f5e9d4c29f0.
2014-07-14 00:57:29,877 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closing usertable,user7,1405321189856.0a3fceff1c074904aae38a3f8b5e5cac.: disabling compactions & flushes
2014-07-14 00:57:29,877 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Updates disabled for region usertable,user7,1405321189856.0a3fceff1c074904aae38a3f8b5e5cac.
2014-07-14 00:57:29,878 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closing usertable,user2,1405321189855.98633400eb20e4fe217a3f5e9d4c29f0.: disabling compactions & flushes
2014-07-14 00:57:29,878 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Updates disabled for region usertable,user2,1405321189855.98633400eb20e4fe217a3f5e9d4c29f0.
2014-07-14 00:57:29,895 INFO  [StoreCloserThread-usertable,user7,1405321189856.0a3fceff1c074904aae38a3f8b5e5cac.-1] regionserver.HStore: Closed family
2014-07-14 00:57:29,896 INFO  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closed usertable,user7,1405321189856.0a3fceff1c074904aae38a3f8b5e5cac.
2014-07-14 00:57:29,896 INFO  [StoreCloserThread-usertable,user2,1405321189855.98633400eb20e4fe217a3f5e9d4c29f0.-1] regionserver.HStore: Closed family
2014-07-14 00:57:29,896 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 0a3fceff1c074904aae38a3f8b5e5cac from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-14 00:57:29,897 INFO  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closed usertable,user2,1405321189855.98633400eb20e4fe217a3f5e9d4c29f0.
2014-07-14 00:57:29,897 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 98633400eb20e4fe217a3f5e9d4c29f0 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-14 00:57:29,903 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 0a3fceff1c074904aae38a3f8b5e5cac from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-14 00:57:29,903 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Set closed state in zk for usertable,user7,1405321189856.0a3fceff1c074904aae38a3f8b5e5cac. on slave1,60020,1405324583256
2014-07-14 00:57:29,904 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Closed usertable,user7,1405321189856.0a3fceff1c074904aae38a3f8b5e5cac.
2014-07-14 00:57:29,904 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 98633400eb20e4fe217a3f5e9d4c29f0 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-14 00:57:29,904 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Set closed state in zk for usertable,user2,1405321189855.98633400eb20e4fe217a3f5e9d4c29f0. on slave1,60020,1405324583256
2014-07-14 00:57:29,904 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Closed usertable,user2,1405321189855.98633400eb20e4fe217a3f5e9d4c29f0.
2014-07-14 00:57:30,878 INFO  [regionserver60020-smallCompactions-1405324622848] regionserver.HRegion: compaction interrupted
java.io.InterruptedIOException: Aborting compaction of store family in region usertable,user6,1405321189856.0098687d245db35b3c80045d6eefb3ee. because it was interrupted.
	at org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.compact(DefaultCompactor.java:81)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext.compact(DefaultStoreEngine.java:109)
	at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:1086)
	at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1481)
	at org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.run(CompactSplitThread.java:475)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-14 00:57:30,879 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Updates disabled for region usertable,user6,1405321189856.0098687d245db35b3c80045d6eefb3ee.
2014-07-14 00:57:30,884 INFO  [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Aborted compaction: Request = regionName=usertable,user6,1405321189856.0098687d245db35b3c80045d6eefb3ee., storeName=family, fileCount=3, fileSize=296.2m, priority=-5, time=278530287416057; duration=28sec
2014-07-14 00:57:30,885 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:9), split_queue=0, merge_queue=0
2014-07-14 00:57:30,885 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405321189856.0a3fceff1c074904aae38a3f8b5e5cac. because compaction request was cancelled
2014-07-14 00:57:30,885 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405321189856.0a3fceff1c074904aae38a3f8b5e5cac. because compaction request was cancelled
2014-07-14 00:57:30,885 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user4,1405321189856.ba3a3ea148e3630e391fc92ec8fce47b. because compaction request was cancelled
2014-07-14 00:57:30,885 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405321189855.98633400eb20e4fe217a3f5e9d4c29f0. because compaction request was cancelled
2014-07-14 00:57:30,885 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405321189855.98633400eb20e4fe217a3f5e9d4c29f0. because compaction request was cancelled
2014-07-14 00:57:30,886 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405321189855.e790d3c63a11d58c49d2bbca0b49c391. because compaction request was cancelled
2014-07-14 00:57:30,886 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user4,1405321189856.ba3a3ea148e3630e391fc92ec8fce47b. because compaction request was cancelled
2014-07-14 00:57:30,886 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user6,1405321189856.0098687d245db35b3c80045d6eefb3ee. because compaction request was cancelled
2014-07-14 00:57:30,886 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405321189855.e790d3c63a11d58c49d2bbca0b49c391. because compaction request was cancelled
2014-07-14 00:57:30,888 INFO  [StoreCloserThread-usertable,user6,1405321189856.0098687d245db35b3c80045d6eefb3ee.-1] regionserver.HStore: Closed family
2014-07-14 00:57:30,889 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closed usertable,user6,1405321189856.0098687d245db35b3c80045d6eefb3ee.
2014-07-14 00:57:30,889 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 0098687d245db35b3c80045d6eefb3ee from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-14 00:57:30,894 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 0098687d245db35b3c80045d6eefb3ee from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-14 00:57:30,894 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Set closed state in zk for usertable,user6,1405321189856.0098687d245db35b3c80045d6eefb3ee. on slave1,60020,1405324583256
2014-07-14 00:57:30,894 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Closed usertable,user6,1405321189856.0098687d245db35b3c80045d6eefb3ee.
2014-07-14 01:01:23,353 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=10829, hits=3899, hitRatio=36.00%, , cachingAccesses=3903, cachingHits=3898, cachingHitsRatio=99.87%, evictions=0, evicted=3, evictedPerRun=Infinity
2014-07-14 01:02:41,932 INFO  [Priority.RpcServer.handler=6,port=60020] regionserver.HRegionServer: Open usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:02:41,945 INFO  [Priority.RpcServer.handler=6,port=60020] regionserver.HRegionServer: Open usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:02:41,945 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 6e2bf74487f61672cd8bc06d8b34f003 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 01:02:41,946 INFO  [Priority.RpcServer.handler=6,port=60020] regionserver.HRegionServer: Open usertable,,1405324961957.81e87aad5a4de9db5f0ddb9df4d4ccdb.
2014-07-14 01:02:41,946 INFO  [Priority.RpcServer.handler=6,port=60020] regionserver.HRegionServer: Open usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:02:41,946 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning b5c4e1ff5b6b7753b9fecbe9b697bf45 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 01:02:41,946 INFO  [Priority.RpcServer.handler=6,port=60020] regionserver.HRegionServer: Open usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:02:41,947 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 81e87aad5a4de9db5f0ddb9df4d4ccdb from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 01:02:41,952 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 6e2bf74487f61672cd8bc06d8b34f003 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 01:02:41,952 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => 6e2bf74487f61672cd8bc06d8b34f003, NAME => 'usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.', STARTKEY => 'user2', ENDKEY => 'user3'}
2014-07-14 01:02:41,953 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 6e2bf74487f61672cd8bc06d8b34f003
2014-07-14 01:02:41,953 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:02:41,953 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node b5c4e1ff5b6b7753b9fecbe9b697bf45 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 01:02:41,953 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 81e87aad5a4de9db5f0ddb9df4d4ccdb from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 01:02:41,953 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => b5c4e1ff5b6b7753b9fecbe9b697bf45, NAME => 'usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.', STARTKEY => 'user3', ENDKEY => 'user4'}
2014-07-14 01:02:41,954 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => 81e87aad5a4de9db5f0ddb9df4d4ccdb, NAME => 'usertable,,1405324961957.81e87aad5a4de9db5f0ddb9df4d4ccdb.', STARTKEY => '', ENDKEY => 'user1'}
2014-07-14 01:02:41,955 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:02:41,955 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:02:41,955 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 81e87aad5a4de9db5f0ddb9df4d4ccdb
2014-07-14 01:02:41,955 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,,1405324961957.81e87aad5a4de9db5f0ddb9df4d4ccdb.
2014-07-14 01:02:41,965 INFO  [StoreOpener-81e87aad5a4de9db5f0ddb9df4d4ccdb-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-14 01:02:41,965 INFO  [StoreOpener-6e2bf74487f61672cd8bc06d8b34f003-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-14 01:02:41,967 INFO  [StoreOpener-b5c4e1ff5b6b7753b9fecbe9b697bf45-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-14 01:02:41,968 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/81e87aad5a4de9db5f0ddb9df4d4ccdb
2014-07-14 01:02:41,970 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003
2014-07-14 01:02:41,970 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:02:41,971 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined 81e87aad5a4de9db5f0ddb9df4d4ccdb; next sequenceid=1
2014-07-14 01:02:41,971 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 81e87aad5a4de9db5f0ddb9df4d4ccdb
2014-07-14 01:02:41,972 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined 6e2bf74487f61672cd8bc06d8b34f003; next sequenceid=1
2014-07-14 01:02:41,972 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 6e2bf74487f61672cd8bc06d8b34f003
2014-07-14 01:02:41,973 INFO  [PostOpenDeployTasks:81e87aad5a4de9db5f0ddb9df4d4ccdb] regionserver.HRegionServer: Post open deploy tasks for region=usertable,,1405324961957.81e87aad5a4de9db5f0ddb9df4d4ccdb.
2014-07-14 01:02:41,973 INFO  [PostOpenDeployTasks:6e2bf74487f61672cd8bc06d8b34f003] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:02:41,983 INFO  [PostOpenDeployTasks:81e87aad5a4de9db5f0ddb9df4d4ccdb] catalog.MetaEditor: Updated row usertable,,1405324961957.81e87aad5a4de9db5f0ddb9df4d4ccdb. with server=slave1,60020,1405324583256
2014-07-14 01:02:41,983 INFO  [PostOpenDeployTasks:81e87aad5a4de9db5f0ddb9df4d4ccdb] regionserver.HRegionServer: Finished post open deploy task for usertable,,1405324961957.81e87aad5a4de9db5f0ddb9df4d4ccdb.
2014-07-14 01:02:41,985 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 81e87aad5a4de9db5f0ddb9df4d4ccdb from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 01:02:41,986 INFO  [PostOpenDeployTasks:6e2bf74487f61672cd8bc06d8b34f003] catalog.MetaEditor: Updated row usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. with server=slave1,60020,1405324583256
2014-07-14 01:02:41,986 INFO  [PostOpenDeployTasks:6e2bf74487f61672cd8bc06d8b34f003] regionserver.HRegionServer: Finished post open deploy task for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:02:41,987 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 6e2bf74487f61672cd8bc06d8b34f003 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 01:02:41,991 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 81e87aad5a4de9db5f0ddb9df4d4ccdb from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 01:02:41,991 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned 81e87aad5a4de9db5f0ddb9df4d4ccdb to OPENED in zk on slave1,60020,1405324583256
2014-07-14 01:02:41,991 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,,1405324961957.81e87aad5a4de9db5f0ddb9df4d4ccdb. on slave1,60020,1405324583256
2014-07-14 01:02:41,991 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 6e2bf74487f61672cd8bc06d8b34f003 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 01:02:41,992 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned 6e2bf74487f61672cd8bc06d8b34f003 to OPENED in zk on slave1,60020,1405324583256
2014-07-14 01:02:41,992 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. on slave1,60020,1405324583256
2014-07-14 01:02:41,992 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning d5d5cc6db1bf5bd9142b4d4e667bac72 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 01:02:41,992 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 92dfa1977cf31f19d29822ea57a55422 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 01:02:41,997 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node d5d5cc6db1bf5bd9142b4d4e667bac72 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 01:02:41,997 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 92dfa1977cf31f19d29822ea57a55422 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-14 01:02:41,997 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => d5d5cc6db1bf5bd9142b4d4e667bac72, NAME => 'usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.', STARTKEY => 'user7', ENDKEY => 'user8'}
2014-07-14 01:02:41,997 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => 92dfa1977cf31f19d29822ea57a55422, NAME => 'usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.', STARTKEY => 'user1', ENDKEY => 'user2'}
2014-07-14 01:02:41,998 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:02:41,998 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:02:41,998 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:02:41,998 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:02:42,000 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined b5c4e1ff5b6b7753b9fecbe9b697bf45; next sequenceid=1
2014-07-14 01:02:42,000 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:02:42,002 INFO  [PostOpenDeployTasks:b5c4e1ff5b6b7753b9fecbe9b697bf45] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:02:42,008 INFO  [StoreOpener-d5d5cc6db1bf5bd9142b4d4e667bac72-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-14 01:02:42,010 INFO  [PostOpenDeployTasks:b5c4e1ff5b6b7753b9fecbe9b697bf45] catalog.MetaEditor: Updated row usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. with server=slave1,60020,1405324583256
2014-07-14 01:02:42,010 INFO  [PostOpenDeployTasks:b5c4e1ff5b6b7753b9fecbe9b697bf45] regionserver.HRegionServer: Finished post open deploy task for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:02:42,011 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning b5c4e1ff5b6b7753b9fecbe9b697bf45 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 01:02:42,012 INFO  [StoreOpener-92dfa1977cf31f19d29822ea57a55422-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-14 01:02:42,015 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:02:42,016 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:02:42,016 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node b5c4e1ff5b6b7753b9fecbe9b697bf45 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 01:02:42,016 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned b5c4e1ff5b6b7753b9fecbe9b697bf45 to OPENED in zk on slave1,60020,1405324583256
2014-07-14 01:02:42,017 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. on slave1,60020,1405324583256
2014-07-14 01:02:42,018 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined d5d5cc6db1bf5bd9142b4d4e667bac72; next sequenceid=1
2014-07-14 01:02:42,018 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:02:42,019 INFO  [PostOpenDeployTasks:d5d5cc6db1bf5bd9142b4d4e667bac72] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:02:42,025 INFO  [PostOpenDeployTasks:d5d5cc6db1bf5bd9142b4d4e667bac72] catalog.MetaEditor: Updated row usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. with server=slave1,60020,1405324583256
2014-07-14 01:02:42,026 INFO  [PostOpenDeployTasks:d5d5cc6db1bf5bd9142b4d4e667bac72] regionserver.HRegionServer: Finished post open deploy task for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:02:42,026 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning d5d5cc6db1bf5bd9142b4d4e667bac72 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 01:02:42,031 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node d5d5cc6db1bf5bd9142b4d4e667bac72 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 01:02:42,031 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned d5d5cc6db1bf5bd9142b4d4e667bac72 to OPENED in zk on slave1,60020,1405324583256
2014-07-14 01:02:42,031 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. on slave1,60020,1405324583256
2014-07-14 01:02:42,057 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined 92dfa1977cf31f19d29822ea57a55422; next sequenceid=1
2014-07-14 01:02:42,057 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:02:42,060 INFO  [PostOpenDeployTasks:92dfa1977cf31f19d29822ea57a55422] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:02:42,068 INFO  [PostOpenDeployTasks:92dfa1977cf31f19d29822ea57a55422] catalog.MetaEditor: Updated row usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. with server=slave1,60020,1405324583256
2014-07-14 01:02:42,068 INFO  [PostOpenDeployTasks:92dfa1977cf31f19d29822ea57a55422] regionserver.HRegionServer: Finished post open deploy task for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:02:42,069 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 92dfa1977cf31f19d29822ea57a55422 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 01:02:42,074 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x14733df62590001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 92dfa1977cf31f19d29822ea57a55422 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-14 01:02:42,074 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned 92dfa1977cf31f19d29822ea57a55422 to OPENED in zk on slave1,60020,1405324583256
2014-07-14 01:02:42,075 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. on slave1,60020,1405324583256
2014-07-14 01:03:00,964 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:03:01,264 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 109 synced till here 77
2014-07-14 01:03:01,770 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405324616792 with entries=109, filesize=93.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405324980965
2014-07-14 01:03:03,789 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:03:03,947 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 201 synced till here 190
2014-07-14 01:03:04,320 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405324980965 with entries=92, filesize=78.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405324983790
2014-07-14 01:03:06,413 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:03:06,538 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 306 synced till here 281
2014-07-14 01:03:07,627 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405324983790 with entries=105, filesize=90.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405324986413
2014-07-14 01:03:09,571 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:03:10,047 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405324986413 with entries=102, filesize=87.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405324989571
2014-07-14 01:03:11,875 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:03:11,893 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 522 synced till here 487
2014-07-14 01:03:12,473 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405324989571 with entries=114, filesize=97.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405324991876
2014-07-14 01:03:14,081 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:03:14,084 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 266.7m
2014-07-14 01:03:14,453 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:03:14,695 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 638 synced till here 596
2014-07-14 01:03:15,605 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405324991876 with entries=116, filesize=99.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405324994453
2014-07-14 01:03:15,743 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:03:15,757 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 259.1m
2014-07-14 01:03:16,214 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:03:17,001 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:03:17,018 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:03:17,019 INFO  [MemStoreFlusher.0] compress.CodecPool: Got brand-new compressor
2014-07-14 01:03:17,021 INFO  [MemStoreFlusher.0] compress.CodecPool: Got brand-new compressor
2014-07-14 01:03:17,389 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:03:17,507 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 749 synced till here 716
2014-07-14 01:03:18,010 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:03:18,195 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405324994453 with entries=111, filesize=95.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405324997389
2014-07-14 01:03:20,001 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:03:20,147 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 859 synced till here 833
2014-07-14 01:03:20,808 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405324997389 with entries=110, filesize=94.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325000001
2014-07-14 01:03:23,821 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=177, memsize=73.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/5c33baabb76c44f48dc23399db2effa6
2014-07-14 01:03:23,842 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/5c33baabb76c44f48dc23399db2effa6 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/5c33baabb76c44f48dc23399db2effa6
2014-07-14 01:03:23,858 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/5c33baabb76c44f48dc23399db2effa6, entries=266610, sequenceid=177, filesize=19.0m
2014-07-14 01:03:23,859 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~272.8m/286051440, currentsize=117.1m/122754480 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 9775ms, sequenceid=177, compaction requested=false
2014-07-14 01:03:23,864 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 357.8m
2014-07-14 01:03:24,014 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=177, memsize=73.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/7f8fc72b2896499a95112062ee5a7e91
2014-07-14 01:03:24,029 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/7f8fc72b2896499a95112062ee5a7e91 as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/7f8fc72b2896499a95112062ee5a7e91
2014-07-14 01:03:24,039 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/7f8fc72b2896499a95112062ee5a7e91, entries=265720, sequenceid=177, filesize=18.9m
2014-07-14 01:03:24,040 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~271.4m/284550000, currentsize=85.5m/89632000 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 8283ms, sequenceid=177, compaction requested=false
2014-07-14 01:03:24,040 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 321.7m
2014-07-14 01:03:24,174 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:03:24,508 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325000001 with entries=109, filesize=93.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325004174
2014-07-14 01:03:24,677 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:03:25,498 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:03:27,561 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:03:27,583 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1086 synced till here 1064
2014-07-14 01:03:27,899 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325004174 with entries=118, filesize=101.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325007561
2014-07-14 01:03:29,758 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:03:30,026 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1205 synced till here 1187
2014-07-14 01:03:30,394 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325007561 with entries=119, filesize=97.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325009759
2014-07-14 01:03:32,150 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:03:32,653 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1334 synced till here 1311
2014-07-14 01:03:32,912 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325009759 with entries=129, filesize=110.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325012151
2014-07-14 01:03:33,181 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=232, memsize=72.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/5295e76249184e7baeb659dfb740d646
2014-07-14 01:03:33,201 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/5295e76249184e7baeb659dfb740d646 as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/5295e76249184e7baeb659dfb740d646
2014-07-14 01:03:33,219 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/5295e76249184e7baeb659dfb740d646, entries=265000, sequenceid=232, filesize=18.9m
2014-07-14 01:03:33,219 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~355.8m/373035440, currentsize=134.5m/141013680 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 9179ms, sequenceid=232, compaction requested=false
2014-07-14 01:03:33,270 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=232, memsize=73.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/eac3439bfad6445b96807f114b50f759
2014-07-14 01:03:33,282 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/eac3439bfad6445b96807f114b50f759 as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/eac3439bfad6445b96807f114b50f759
2014-07-14 01:03:33,293 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/eac3439bfad6445b96807f114b50f759, entries=266170, sequenceid=232, filesize=19.0m
2014-07-14 01:03:33,294 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~357.8m/375178640, currentsize=166.4m/174503360 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 9430ms, sequenceid=232, compaction requested=false
2014-07-14 01:03:33,523 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:03:33,524 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 263.6m
2014-07-14 01:03:33,934 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:03:34,030 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:03:34,030 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 259.3m
2014-07-14 01:03:34,133 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:03:34,174 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325012151 with entries=73, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325014133
2014-07-14 01:03:34,174 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405324616792
2014-07-14 01:03:34,174 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405324980965
2014-07-14 01:03:34,174 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405324983790
2014-07-14 01:03:34,174 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405324986413
2014-07-14 01:03:34,174 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405324989571
2014-07-14 01:03:34,174 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405324991876
2014-07-14 01:03:34,611 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:03:35,271 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:03:35,339 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325014133 with entries=72, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325015271
2014-07-14 01:03:38,107 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:03:39,077 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:03:39,280 DEBUG [RpcServer.handler=38,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:03:40,110 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1633 synced till here 1626
2014-07-14 01:03:40,129 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=371, memsize=71.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/658d409ca47c4ccbb814342c43281818
2014-07-14 01:03:40,151 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325015271 with entries=154, filesize=131.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325018108
2014-07-14 01:03:40,163 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/658d409ca47c4ccbb814342c43281818 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/658d409ca47c4ccbb814342c43281818
2014-07-14 01:03:40,290 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/658d409ca47c4ccbb814342c43281818, entries=261110, sequenceid=371, filesize=18.6m
2014-07-14 01:03:40,291 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~294.5m/308824000, currentsize=74.9m/78503840 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 6767ms, sequenceid=371, compaction requested=false
2014-07-14 01:03:40,292 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 271.9m
2014-07-14 01:03:40,497 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=367, memsize=71.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/c9e658467c8b459aa5deac0d3a4ddedf
2014-07-14 01:03:40,604 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/c9e658467c8b459aa5deac0d3a4ddedf as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/c9e658467c8b459aa5deac0d3a4ddedf
2014-07-14 01:03:40,621 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/c9e658467c8b459aa5deac0d3a4ddedf, entries=260020, sequenceid=367, filesize=18.5m
2014-07-14 01:03:40,621 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~293.6m/307844720, currentsize=74.6m/78187280 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 6591ms, sequenceid=367, compaction requested=false
2014-07-14 01:03:40,621 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 270.6m
2014-07-14 01:03:41,327 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:03:41,380 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:03:41,842 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:03:41,963 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1713 synced till here 1707
2014-07-14 01:03:42,049 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325018108 with entries=80, filesize=68.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325021842
2014-07-14 01:03:42,050 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405324994453
2014-07-14 01:03:42,050 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405324997389
2014-07-14 01:03:42,050 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325000001
2014-07-14 01:03:43,699 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:03:43,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1791 synced till here 1786
2014-07-14 01:03:44,056 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325021842 with entries=78, filesize=66.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325023700
2014-07-14 01:03:45,864 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:03:46,047 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1883 synced till here 1864
2014-07-14 01:03:46,407 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325023700 with entries=92, filesize=79.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325025864
2014-07-14 01:03:48,039 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:03:48,059 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1966 synced till here 1960
2014-07-14 01:03:48,245 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325025864 with entries=83, filesize=71.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325028040
2014-07-14 01:03:50,590 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:03:50,796 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=411, memsize=97.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/4afe7d47407c45b49f285910a9a5acf5
2014-07-14 01:03:50,824 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2063 synced till here 2051
2014-07-14 01:03:50,828 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/4afe7d47407c45b49f285910a9a5acf5 as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/4afe7d47407c45b49f285910a9a5acf5
2014-07-14 01:03:51,079 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/4afe7d47407c45b49f285910a9a5acf5, entries=354760, sequenceid=411, filesize=25.3m
2014-07-14 01:03:51,081 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~275.4m/288777920, currentsize=147.1m/154266800 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 10460ms, sequenceid=411, compaction requested=false
2014-07-14 01:03:51,374 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325028040 with entries=97, filesize=82.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325030590
2014-07-14 01:03:51,672 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=412, memsize=97.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/8b6c37943fd9408384e135c204ec1c18
2014-07-14 01:03:51,689 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/8b6c37943fd9408384e135c204ec1c18 as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/8b6c37943fd9408384e135c204ec1c18
2014-07-14 01:03:51,708 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/8b6c37943fd9408384e135c204ec1c18, entries=355720, sequenceid=412, filesize=25.4m
2014-07-14 01:03:51,709 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~276.4m/289841200, currentsize=157.7m/165341200 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 11417ms, sequenceid=412, compaction requested=false
2014-07-14 01:03:51,953 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:03:51,954 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 258.0m
2014-07-14 01:03:52,788 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:03:52,788 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 258.9m
2014-07-14 01:03:52,988 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:03:53,164 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:03:53,327 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2156 synced till here 2137
2014-07-14 01:03:53,878 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325030590 with entries=93, filesize=79.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325033164
2014-07-14 01:03:53,878 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325004174
2014-07-14 01:03:53,878 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325007561
2014-07-14 01:03:53,879 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325009759
2014-07-14 01:03:53,879 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325012151
2014-07-14 01:03:53,900 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:03:55,608 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:03:55,966 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2259 synced till here 2233
2014-07-14 01:03:56,208 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325033164 with entries=103, filesize=88.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325035608
2014-07-14 01:03:57,334 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:03:57,616 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:03:58,117 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:03:59,039 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2393 synced till here 2381
2014-07-14 01:03:59,323 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325035608 with entries=134, filesize=115.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325037616
2014-07-14 01:04:00,627 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:04:00,687 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325037616 with entries=80, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325040628
2014-07-14 01:04:02,580 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:04:02,606 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2548 synced till here 2545
2014-07-14 01:04:02,772 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325040628 with entries=75, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325042581
2014-07-14 01:04:04,395 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:04:04,436 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325042581 with entries=72, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325044396
2014-07-14 01:04:06,159 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:04:06,266 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2694 synced till here 2693
2014-07-14 01:04:06,282 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325044396 with entries=74, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325046160
2014-07-14 01:04:08,152 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:04:08,358 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2775 synced till here 2767
2014-07-14 01:04:08,583 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325046160 with entries=81, filesize=69.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325048152
2014-07-14 01:04:10,146 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=548, memsize=203.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/9fcc50cdc0d140fb96be0b7d95f11ea6
2014-07-14 01:04:10,170 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/9fcc50cdc0d140fb96be0b7d95f11ea6 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/9fcc50cdc0d140fb96be0b7d95f11ea6
2014-07-14 01:04:10,192 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/9fcc50cdc0d140fb96be0b7d95f11ea6, entries=741210, sequenceid=548, filesize=52.8m
2014-07-14 01:04:10,193 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~273.6m/286890080, currentsize=266.0m/278964640 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 18239ms, sequenceid=548, compaction requested=true
2014-07-14 01:04:10,194 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:04:10,195 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:04:10,195 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-14 01:04:10,195 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-14 01:04:10,196 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:04:10,196 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 465.3m
2014-07-14 01:04:10,196 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:04:10,196 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:04:10,388 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:04:10,516 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2851 synced till here 2845
2014-07-14 01:04:10,692 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325048152 with entries=76, filesize=65.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325050388
2014-07-14 01:04:11,634 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:04:11,969 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=544, memsize=203.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/0b7565883a7e49ad86423c26058ea11d
2014-07-14 01:04:12,002 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/0b7565883a7e49ad86423c26058ea11d as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/0b7565883a7e49ad86423c26058ea11d
2014-07-14 01:04:12,137 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/0b7565883a7e49ad86423c26058ea11d, entries=741220, sequenceid=544, filesize=52.8m
2014-07-14 01:04:12,137 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~272.8m/286005440, currentsize=291.3m/305481040 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 19349ms, sequenceid=544, compaction requested=true
2014-07-14 01:04:12,147 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:04:12,147 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-14 01:04:12,147 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-14 01:04:12,147 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:04:12,147 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:04:12,147 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 481.7m
2014-07-14 01:04:12,147 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:04:12,320 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:04:12,321 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:04:12,348 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2934 synced till here 2922
2014-07-14 01:04:12,550 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325050388 with entries=83, filesize=71.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325052321
2014-07-14 01:04:12,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325014133
2014-07-14 01:04:12,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325015271
2014-07-14 01:04:13,637 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:04:14,016 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:04:14,039 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3012 synced till here 3007
2014-07-14 01:04:14,137 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325052321 with entries=78, filesize=66.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325054016
2014-07-14 01:04:15,980 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:04:17,290 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3120 synced till here 3119
2014-07-14 01:04:17,306 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325054016 with entries=108, filesize=92.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325055981
2014-07-14 01:04:19,528 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:04:19,544 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3195 synced till here 3191
2014-07-14 01:04:19,630 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325055981 with entries=75, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325059528
2014-07-14 01:04:22,159 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:04:22,175 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3276 synced till here 3268
2014-07-14 01:04:22,414 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325059528 with entries=81, filesize=69.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325062159
2014-07-14 01:04:24,948 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:04:25,132 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3368 synced till here 3357
2014-07-14 01:04:25,634 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325062159 with entries=92, filesize=78.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325064948
2014-07-14 01:04:27,844 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:04:28,093 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3463 synced till here 3444
2014-07-14 01:04:28,640 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325064948 with entries=95, filesize=81.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325067844
2014-07-14 01:04:30,756 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:04:32,212 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3599 synced till here 3593
2014-07-14 01:04:32,270 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325067844 with entries=136, filesize=116.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325070756
2014-07-14 01:04:34,242 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:04:34,272 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3680 synced till here 3673
2014-07-14 01:04:34,457 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325070756 with entries=81, filesize=69.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325074243
2014-07-14 01:04:51,697 WARN  [regionserver60020.periodicFlusher] util.Sleeper: We slept 24661ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-14 01:04:51,697 WARN  [regionserver60020.compactionChecker] util.Sleeper: We slept 24661ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-14 01:04:51,697 WARN  [regionserver60020] util.Sleeper: We slept 17044ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-14 01:04:51,749 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 16786ms
GC pool 'ParNew' had collection(s): count=2 time=105ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=17015ms
2014-07-14 01:04:51,864 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18721,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47123","starttimems":1405325072996,"queuetimems":1,"class":"HRegionServer","responsesize":15572,"method":"Multi"}
2014-07-14 01:04:51,864 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18491,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47123","starttimems":1405325073349,"queuetimems":1,"class":"HRegionServer","responsesize":15933,"method":"Multi"}
2014-07-14 01:04:51,864 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18454,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47123","starttimems":1405325073386,"queuetimems":0,"class":"HRegionServer","responsesize":15591,"method":"Multi"}
2014-07-14 01:04:51,864 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18794,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47123","starttimems":1405325073046,"queuetimems":0,"class":"HRegionServer","responsesize":15857,"method":"Multi"}
2014-07-14 01:04:51,864 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1464 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47123: output error
2014-07-14 01:04:51,962 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:04:51,962 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1465 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47123: output error
2014-07-14 01:04:51,962 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:04:51,962 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1470 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47123: output error
2014-07-14 01:04:51,963 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:04:51,963 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1469 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47123: output error
2014-07-14 01:04:51,963 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:04:52,036 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18171,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47123","starttimems":1405325073865,"queuetimems":1,"class":"HRegionServer","responsesize":15893,"method":"Multi"}
2014-07-14 01:04:52,036 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18446,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47123","starttimems":1405325073590,"queuetimems":1,"class":"HRegionServer","responsesize":15833,"method":"Multi"}
2014-07-14 01:04:52,037 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1477 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47123: output error
2014-07-14 01:04:52,037 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:04:52,037 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1472 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47123: output error
2014-07-14 01:04:52,037 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:04:52,157 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18512,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47123","starttimems":1405325073645,"queuetimems":1,"class":"HRegionServer","responsesize":15697,"method":"Multi"}
2014-07-14 01:04:52,158 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1474 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47123: output error
2014-07-14 01:04:52,158 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:04:52,218 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18327,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47123","starttimems":1405325073891,"queuetimems":1,"class":"HRegionServer","responsesize":15580,"method":"Multi"}
2014-07-14 01:04:52,219 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1476 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47123: output error
2014-07-14 01:04:52,219 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:04:52,278 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18036,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47123","starttimems":1405325074242,"queuetimems":0,"class":"HRegionServer","responsesize":15610,"method":"Multi"}
2014-07-14 01:04:52,278 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18244,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47123","starttimems":1405325074034,"queuetimems":0,"class":"HRegionServer","responsesize":15796,"method":"Multi"}
2014-07-14 01:04:52,279 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1481 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47123: output error
2014-07-14 01:04:52,279 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:04:52,279 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1475 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47123: output error
2014-07-14 01:04:52,279 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:04:52,305 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18213,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47123","starttimems":1405325074092,"queuetimems":2,"class":"HRegionServer","responsesize":15685,"method":"Multi"}
2014-07-14 01:04:52,306 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1480 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47123: output error
2014-07-14 01:04:52,306 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:04:52,407 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17965,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47123","starttimems":1405325074441,"queuetimems":0,"class":"HRegionServer","responsesize":15839,"method":"Multi"}
2014-07-14 01:04:52,407 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17762,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47123","starttimems":1405325074644,"queuetimems":0,"class":"HRegionServer","responsesize":15865,"method":"Multi"}
2014-07-14 01:04:52,407 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17994,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47123","starttimems":1405325074412,"queuetimems":1,"class":"HRegionServer","responsesize":15651,"method":"Multi"}
2014-07-14 01:04:52,407 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1485 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47123: output error
2014-07-14 01:04:52,408 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:04:52,408 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1483 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47123: output error
2014-07-14 01:04:52,408 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:04:52,408 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1490 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47123: output error
2014-07-14 01:04:52,408 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:04:52,572 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17957,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47123","starttimems":1405325074614,"queuetimems":0,"class":"HRegionServer","responsesize":16131,"method":"Multi"}
2014-07-14 01:04:52,572 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1489 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47123: output error
2014-07-14 01:04:52,572 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:04:52,572 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1484 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47123: output error
2014-07-14 01:04:52,572 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:04:52,573 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1487 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47123: output error
2014-07-14 01:04:52,573 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:04:52,573 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1500 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47123: output error
2014-07-14 01:04:52,573 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:04:53,200 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=715, memsize=250.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/465c3809e5f34467a3ba005345029d22
2014-07-14 01:04:53,213 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/465c3809e5f34467a3ba005345029d22 as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/465c3809e5f34467a3ba005345029d22
2014-07-14 01:04:53,224 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/465c3809e5f34467a3ba005345029d22, entries=913530, sequenceid=715, filesize=65.1m
2014-07-14 01:04:53,225 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~473.2m/496146800, currentsize=346.8m/363613600 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 43029ms, sequenceid=715, compaction requested=true
2014-07-14 01:04:53,225 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:04:53,225 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-14 01:04:53,225 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 610.4m
2014-07-14 01:04:53,225 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-14 01:04:53,226 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:04:53,226 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:04:53,226 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:04:53,647 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:04:54,131 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=730, memsize=262.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/1880eec90d13436aa5eecb11b006f537
2014-07-14 01:04:54,145 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/1880eec90d13436aa5eecb11b006f537 as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/1880eec90d13436aa5eecb11b006f537
2014-07-14 01:04:54,157 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/1880eec90d13436aa5eecb11b006f537, entries=955510, sequenceid=730, filesize=68.0m
2014-07-14 01:04:54,158 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~492.6m/516520720, currentsize=321.5m/337089920 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 42011ms, sequenceid=730, compaction requested=true
2014-07-14 01:04:54,158 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:04:54,158 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-14 01:04:54,159 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-14 01:04:54,159 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:04:54,159 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:04:54,159 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:04:54,159 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 608.7m
2014-07-14 01:04:54,570 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:04:56,804 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:04:56,820 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3756 synced till here 3753
2014-07-14 01:04:56,840 DEBUG [RpcServer.handler=13,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:04:57,340 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325074243 with entries=76, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325096805
2014-07-14 01:04:57,340 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325018108
2014-07-14 01:04:57,340 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325021842
2014-07-14 01:04:57,340 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325023700
2014-07-14 01:04:57,340 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325025864
2014-07-14 01:04:57,340 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325028040
2014-07-14 01:04:57,413 DEBUG [RpcServer.handler=13,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:05:02,189 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:05:02,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3888 synced till here 3884
2014-07-14 01:05:02,910 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325096805 with entries=132, filesize=112.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325102190
2014-07-14 01:05:04,204 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:05:04,227 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3963 synced till here 3960
2014-07-14 01:05:04,266 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325102190 with entries=75, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325104204
2014-07-14 01:05:05,824 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:05:05,856 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4037 synced till here 4035
2014-07-14 01:05:05,886 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325104204 with entries=74, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325105825
2014-07-14 01:05:07,353 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:05:07,389 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4111 synced till here 4110
2014-07-14 01:05:07,404 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325105825 with entries=74, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325107354
2014-07-14 01:05:07,730 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=942, memsize=331.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/9980169cddda44419058659f9ca6b986
2014-07-14 01:05:07,742 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/9980169cddda44419058659f9ca6b986 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/9980169cddda44419058659f9ca6b986
2014-07-14 01:05:07,754 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/9980169cddda44419058659f9ca6b986, entries=1205500, sequenceid=942, filesize=85.9m
2014-07-14 01:05:07,755 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~610.4m/640061600, currentsize=159.9m/167663440 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 14530ms, sequenceid=942, compaction requested=true
2014-07-14 01:05:07,755 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:05:07,755 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-14 01:05:07,755 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 500.2m
2014-07-14 01:05:07,755 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-14 01:05:07,756 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:05:07,756 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:05:07,756 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:05:08,630 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:05:08,681 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:05:08,698 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4186 synced till here 4183
2014-07-14 01:05:08,734 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325107354 with entries=75, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325108682
2014-07-14 01:05:09,806 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:05:09,822 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=938, memsize=330.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/ef9cec03543c49118d3b83b71a10f254
2014-07-14 01:05:09,852 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/ef9cec03543c49118d3b83b71a10f254 as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/ef9cec03543c49118d3b83b71a10f254
2014-07-14 01:05:09,861 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4262 synced till here 4260
2014-07-14 01:05:10,504 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325108682 with entries=76, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325109806
2014-07-14 01:05:10,506 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/ef9cec03543c49118d3b83b71a10f254, entries=1203720, sequenceid=938, filesize=85.7m
2014-07-14 01:05:10,506 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~608.7m/638251520, currentsize=205.3m/215254320 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 16347ms, sequenceid=938, compaction requested=true
2014-07-14 01:05:10,507 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:05:10,507 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-14 01:05:10,507 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-14 01:05:10,508 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:05:10,509 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 522.5m
2014-07-14 01:05:10,509 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:05:10,509 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:05:10,981 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:05:11,834 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:05:11,856 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4336 synced till here 4335
2014-07-14 01:05:11,874 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325109806 with entries=74, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325111835
2014-07-14 01:05:11,875 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325030590
2014-07-14 01:05:11,875 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325033164
2014-07-14 01:05:11,875 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325035608
2014-07-14 01:05:11,875 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325037616
2014-07-14 01:05:11,875 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325040628
2014-07-14 01:05:11,875 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325042581
2014-07-14 01:05:11,875 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325044396
2014-07-14 01:05:11,875 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325046160
2014-07-14 01:05:14,740 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:05:14,798 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:05:15,118 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:05:15,601 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4424 synced till here 4423
2014-07-14 01:05:15,609 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325111835 with entries=88, filesize=75.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325115119
2014-07-14 01:05:17,379 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:05:17,405 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4498 synced till here 4497
2014-07-14 01:05:17,420 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325115119 with entries=74, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325117380
2014-07-14 01:05:20,533 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:05:20,556 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4573 synced till here 4571
2014-07-14 01:05:20,592 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325117380 with entries=75, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325120534
2014-07-14 01:05:21,600 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1038, memsize=301.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/a660850a0f564671a943787936f3853b
2014-07-14 01:05:21,616 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/a660850a0f564671a943787936f3853b as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/a660850a0f564671a943787936f3853b
2014-07-14 01:05:21,627 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/a660850a0f564671a943787936f3853b, entries=1096900, sequenceid=1038, filesize=78.2m
2014-07-14 01:05:21,628 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~500.2m/524491360, currentsize=187.8m/196886160 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 13873ms, sequenceid=1038, compaction requested=true
2014-07-14 01:05:21,629 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:05:21,629 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-14 01:05:21,629 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 345.0m
2014-07-14 01:05:21,629 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-14 01:05:21,629 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:05:21,629 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:05:21,629 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:05:21,888 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:05:21,915 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4648 synced till here 4647
2014-07-14 01:05:21,927 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:05:22,273 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325120534 with entries=75, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325121888
2014-07-14 01:05:22,273 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325048152
2014-07-14 01:05:24,594 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1069, memsize=339.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/ab64ab7a9ef44897a419a9b94274aa20
2014-07-14 01:05:24,613 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/ab64ab7a9ef44897a419a9b94274aa20 as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/ab64ab7a9ef44897a419a9b94274aa20
2014-07-14 01:05:24,629 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:05:24,631 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/ab64ab7a9ef44897a419a9b94274aa20, entries=1235810, sequenceid=1069, filesize=88.1m
2014-07-14 01:05:24,633 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~525.6m/551115520, currentsize=173.7m/182096400 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 14123ms, sequenceid=1069, compaction requested=true
2014-07-14 01:05:24,633 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:05:24,634 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-14 01:05:24,634 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-14 01:05:24,634 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 382.5m
2014-07-14 01:05:24,634 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:05:24,634 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:05:24,634 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:05:24,659 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325121888 with entries=72, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325124630
2014-07-14 01:05:24,659 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325050388
2014-07-14 01:05:24,659 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325052321
2014-07-14 01:05:24,659 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325054016
2014-07-14 01:05:24,659 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325055981
2014-07-14 01:05:24,659 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325059528
2014-07-14 01:05:24,659 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325062159
2014-07-14 01:05:24,659 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325064948
2014-07-14 01:05:24,659 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325067844
2014-07-14 01:05:24,660 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325070756
2014-07-14 01:05:25,002 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:05:25,843 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:05:25,881 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:05:28,306 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325124630 with entries=164, filesize=140.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325125843
2014-07-14 01:05:30,144 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:05:30,417 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:05:30,440 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4958 synced till here 4954
2014-07-14 01:05:30,478 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325125843 with entries=74, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325130418
2014-07-14 01:05:32,216 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:05:32,242 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5033 synced till here 5029
2014-07-14 01:05:32,301 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325130418 with entries=75, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325132217
2014-07-14 01:05:33,457 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:05:33,475 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5107 synced till here 5105
2014-07-14 01:05:33,501 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325132217 with entries=74, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325133457
2014-07-14 01:05:35,009 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1167, memsize=321.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/8ef00030c0854b9fb6be5e5fe5c05bfb
2014-07-14 01:05:35,033 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/8ef00030c0854b9fb6be5e5fe5c05bfb as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/8ef00030c0854b9fb6be5e5fe5c05bfb
2014-07-14 01:05:35,054 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/8ef00030c0854b9fb6be5e5fe5c05bfb, entries=1171720, sequenceid=1167, filesize=83.4m
2014-07-14 01:05:35,054 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~348.1m/365038720, currentsize=192.2m/201492480 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 13425ms, sequenceid=1167, compaction requested=true
2014-07-14 01:05:35,054 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:05:35,054 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-14 01:05:35,055 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-14 01:05:35,055 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:05:35,055 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 387.1m
2014-07-14 01:05:35,055 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:05:35,055 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:05:35,283 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:05:37,949 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:05:37,977 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325133457 with entries=72, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325137950
2014-07-14 01:05:39,473 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1187, memsize=357.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/0753de3abee2415ca8a05b8475d2451b
2014-07-14 01:05:39,494 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/0753de3abee2415ca8a05b8475d2451b as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/0753de3abee2415ca8a05b8475d2451b
2014-07-14 01:05:39,515 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/0753de3abee2415ca8a05b8475d2451b, entries=1301890, sequenceid=1187, filesize=92.7m
2014-07-14 01:05:39,516 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~385.6m/404333440, currentsize=196.5m/206024160 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 14882ms, sequenceid=1187, compaction requested=true
2014-07-14 01:05:39,517 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:05:39,517 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-14 01:05:39,517 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 371.8m
2014-07-14 01:05:39,517 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-14 01:05:39,517 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:05:39,517 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:05:39,517 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:05:40,035 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:05:40,073 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325137950 with entries=72, filesize=61.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325140036
2014-07-14 01:05:40,073 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325074243
2014-07-14 01:05:40,073 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325096805
2014-07-14 01:05:40,073 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325102190
2014-07-14 01:05:40,074 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325104204
2014-07-14 01:05:40,074 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325105825
2014-07-14 01:05:40,155 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:05:40,602 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:05:41,119 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:05:41,145 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325140036 with entries=71, filesize=61.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325141119
2014-07-14 01:05:43,636 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:05:43,741 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:05:43,768 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5396 synced till here 5395
2014-07-14 01:05:43,804 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325141119 with entries=74, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325143741
2014-07-14 01:05:45,518 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:05:45,561 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325143741 with entries=72, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325145519
2014-07-14 01:05:47,005 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1288, memsize=347.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/ca03d9c570c745938be5095dd74cf4ce
2014-07-14 01:05:47,021 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/ca03d9c570c745938be5095dd74cf4ce as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/ca03d9c570c745938be5095dd74cf4ce
2014-07-14 01:05:47,036 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/ca03d9c570c745938be5095dd74cf4ce, entries=1263240, sequenceid=1288, filesize=89.9m
2014-07-14 01:05:47,037 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~387.1m/405955840, currentsize=153.9m/161384960 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 11982ms, sequenceid=1288, compaction requested=true
2014-07-14 01:05:47,038 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:05:47,038 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 347.7m
2014-07-14 01:05:47,039 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-14 01:05:47,040 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-14 01:05:47,041 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:05:47,042 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:05:47,042 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:05:47,286 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:05:47,298 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:05:47,307 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5540 synced till here 5539
2014-07-14 01:05:47,328 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325145519 with entries=72, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325147286
2014-07-14 01:05:47,328 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325107354
2014-07-14 01:05:47,330 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325108682
2014-07-14 01:05:49,870 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:05:50,003 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325147286 with entries=79, filesize=67.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325149871
2014-07-14 01:05:51,478 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1311, memsize=333.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/4accda75959541bead6d6662aa7a82dc
2014-07-14 01:05:51,497 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/4accda75959541bead6d6662aa7a82dc as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/4accda75959541bead6d6662aa7a82dc
2014-07-14 01:05:51,536 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/4accda75959541bead6d6662aa7a82dc, entries=1212290, sequenceid=1311, filesize=86.3m
2014-07-14 01:05:51,536 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~373.3m/391485120, currentsize=164.8m/172779680 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 12019ms, sequenceid=1311, compaction requested=true
2014-07-14 01:05:51,537 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:05:51,537 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-14 01:05:51,537 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-14 01:05:51,537 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:05:51,537 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 357.9m
2014-07-14 01:05:51,537 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:05:51,537 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:05:51,748 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:05:52,784 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:05:52,810 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5693 synced till here 5692
2014-07-14 01:05:52,829 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325149871 with entries=74, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325152785
2014-07-14 01:05:52,829 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325109806
2014-07-14 01:05:52,829 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325111835
2014-07-14 01:05:52,829 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325115119
2014-07-14 01:05:52,829 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325117380
2014-07-14 01:05:54,257 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:05:54,275 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5766 synced till here 5764
2014-07-14 01:05:54,293 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325152785 with entries=73, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325154258
2014-07-14 01:05:54,547 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:05:55,786 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:05:56,038 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5856 synced till here 5854
2014-07-14 01:05:56,067 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325154258 with entries=90, filesize=76.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325155786
2014-07-14 01:05:56,809 DEBUG [RpcServer.handler=19,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:05:58,312 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1393, memsize=324.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/0fa34a5869eb48a5930f6eb8f0974bfe
2014-07-14 01:05:58,334 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/0fa34a5869eb48a5930f6eb8f0974bfe as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/0fa34a5869eb48a5930f6eb8f0974bfe
2014-07-14 01:05:58,348 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/0fa34a5869eb48a5930f6eb8f0974bfe, entries=1180660, sequenceid=1393, filesize=84.1m
2014-07-14 01:05:58,349 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~349.1m/366103200, currentsize=152.4m/159801440 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 11310ms, sequenceid=1393, compaction requested=true
2014-07-14 01:05:58,349 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:05:58,349 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-14 01:05:58,349 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 308.8m
2014-07-14 01:05:58,350 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-14 01:05:58,350 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:05:58,350 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:05:58,350 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:05:58,550 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:05:59,068 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:05:59,121 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325155786 with entries=72, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325159068
2014-07-14 01:05:59,121 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325120534
2014-07-14 01:05:59,122 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325121888
2014-07-14 01:06:00,485 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:06:00,529 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6004 synced till here 6002
2014-07-14 01:06:00,586 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325159068 with entries=76, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325160486
2014-07-14 01:06:02,243 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:06:02,265 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6076 synced till here 6075
2014-07-14 01:06:02,277 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325160486 with entries=72, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325162243
2014-07-14 01:06:03,634 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1418, memsize=342.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/ce69d6af21cd4b55b5ae704c889f27c3
2014-07-14 01:06:03,657 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/ce69d6af21cd4b55b5ae704c889f27c3 as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/ce69d6af21cd4b55b5ae704c889f27c3
2014-07-14 01:06:03,679 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/ce69d6af21cd4b55b5ae704c889f27c3, entries=1245980, sequenceid=1418, filesize=88.8m
2014-07-14 01:06:03,680 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~357.9m/375274160, currentsize=177.0m/185602160 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 12143ms, sequenceid=1418, compaction requested=true
2014-07-14 01:06:03,680 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:06:03,680 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-14 01:06:03,681 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 342.5m
2014-07-14 01:06:03,681 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-14 01:06:03,681 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:06:03,681 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:06:03,681 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:06:03,877 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:06:06,661 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:06:06,913 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6150 synced till here 6149
2014-07-14 01:06:06,925 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325162243 with entries=74, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325166661
2014-07-14 01:06:06,925 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325124630
2014-07-14 01:06:06,925 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325125843
2014-07-14 01:06:06,925 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325130418
2014-07-14 01:06:06,926 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325132217
2014-07-14 01:06:08,841 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1488, memsize=308.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/ee1a0331d51743fbb200dea1ec602e46
2014-07-14 01:06:08,865 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/ee1a0331d51743fbb200dea1ec602e46 as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/ee1a0331d51743fbb200dea1ec602e46
2014-07-14 01:06:08,886 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/ee1a0331d51743fbb200dea1ec602e46, entries=1124380, sequenceid=1488, filesize=80.1m
2014-07-14 01:06:08,886 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~308.8m/323811680, currentsize=100.7m/105626800 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 10537ms, sequenceid=1488, compaction requested=true
2014-07-14 01:06:08,886 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:06:08,887 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-14 01:06:08,887 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-14 01:06:08,887 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:06:08,887 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:06:08,887 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:06:09,534 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:06:09,535 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 256.7m
2014-07-14 01:06:09,719 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:06:09,998 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:06:10,021 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6223 synced till here 6221
2014-07-14 01:06:10,045 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325166661 with entries=73, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325169998
2014-07-14 01:06:10,045 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325133457
2014-07-14 01:06:12,909 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:06:13,018 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6303 synced till here 6302
2014-07-14 01:06:13,040 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325169998 with entries=80, filesize=68.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325172910
2014-07-14 01:06:13,145 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:06:14,315 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:06:14,633 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6402 synced till here 6397
2014-07-14 01:06:14,686 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325172910 with entries=99, filesize=85.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325174315
2014-07-14 01:06:15,423 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1532, memsize=342.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/1f97128584064f68bead373a2675d8a3
2014-07-14 01:06:15,438 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/1f97128584064f68bead373a2675d8a3 as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/1f97128584064f68bead373a2675d8a3
2014-07-14 01:06:15,449 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/1f97128584064f68bead373a2675d8a3, entries=1247000, sequenceid=1532, filesize=88.8m
2014-07-14 01:06:15,450 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~342.5m/359124480, currentsize=126.0m/132172160 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 11770ms, sequenceid=1532, compaction requested=true
2014-07-14 01:06:15,450 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:06:15,450 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-14 01:06:15,451 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-14 01:06:15,451 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 306.8m
2014-07-14 01:06:15,451 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:06:15,451 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:06:15,451 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:06:15,717 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:06:15,800 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:06:15,821 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6477 synced till here 6474
2014-07-14 01:06:15,866 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325174315 with entries=75, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325175801
2014-07-14 01:06:15,866 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325137950
2014-07-14 01:06:15,866 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325140036
2014-07-14 01:06:15,866 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325141119
2014-07-14 01:06:15,866 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325143741
2014-07-14 01:06:19,005 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1560, memsize=258.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/d2367a83cbf24d9e998e998e9cbd6da9
2014-07-14 01:06:19,023 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/d2367a83cbf24d9e998e998e9cbd6da9 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/d2367a83cbf24d9e998e998e9cbd6da9
2014-07-14 01:06:19,046 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/d2367a83cbf24d9e998e998e9cbd6da9, entries=940200, sequenceid=1560, filesize=66.9m
2014-07-14 01:06:19,046 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~258.2m/270771040, currentsize=130.1m/136470560 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 9511ms, sequenceid=1560, compaction requested=true
2014-07-14 01:06:19,047 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:06:19,047 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-14 01:06:19,047 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-14 01:06:19,047 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:06:19,047 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:06:19,047 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:06:23,352 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=10829, hits=3899, hitRatio=36.00%, , cachingAccesses=3903, cachingHits=3898, cachingHitsRatio=99.87%, evictions=0, evicted=3, evictedPerRun=Infinity
2014-07-14 01:06:24,742 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1618, memsize=296.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/cba1f0b59e4544e1bbdc6c6e946994b6
2014-07-14 01:06:24,763 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/cba1f0b59e4544e1bbdc6c6e946994b6 as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/cba1f0b59e4544e1bbdc6c6e946994b6
2014-07-14 01:06:24,780 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/cba1f0b59e4544e1bbdc6c6e946994b6, entries=1078220, sequenceid=1618, filesize=76.7m
2014-07-14 01:06:24,781 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~308.4m/323394560, currentsize=34.3m/35949840 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 9331ms, sequenceid=1618, compaction requested=true
2014-07-14 01:06:24,781 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:06:24,782 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-14 01:06:24,782 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-14 01:06:24,782 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:06:24,782 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:06:24,782 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:06:34,193 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:06:34,209 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6549 synced till here 6548
2014-07-14 01:06:34,234 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325175801 with entries=72, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325194193
2014-07-14 01:06:34,234 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325145519
2014-07-14 01:06:34,235 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325147286
2014-07-14 01:06:34,235 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325149871
2014-07-14 01:06:34,236 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325152785
2014-07-14 01:06:34,237 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325154258
2014-07-14 01:06:34,238 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325155786
2014-07-14 01:06:35,075 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:06:35,075 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 256.6m
2014-07-14 01:06:35,371 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:06:35,528 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:06:35,549 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6622 synced till here 6621
2014-07-14 01:06:35,562 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325194193 with entries=73, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325195529
2014-07-14 01:06:38,003 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:06:38,022 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6696 synced till here 6694
2014-07-14 01:06:38,047 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325195529 with entries=74, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325198003
2014-07-14 01:06:40,694 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:06:40,846 DEBUG [RpcServer.handler=38,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:06:40,847 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 256.3m
2014-07-14 01:06:40,868 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6777 synced till here 6774
2014-07-14 01:06:40,951 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325198003 with entries=81, filesize=69.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325200695
2014-07-14 01:06:41,057 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:06:42,840 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:06:42,870 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6849 synced till here 6848
2014-07-14 01:06:42,898 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325200695 with entries=72, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325202841
2014-07-14 01:06:43,404 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:06:44,681 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1656, memsize=247.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/9a3c9858db75467999f2801b41d5e982
2014-07-14 01:06:44,701 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/9a3c9858db75467999f2801b41d5e982 as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/9a3c9858db75467999f2801b41d5e982
2014-07-14 01:06:44,719 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/9a3c9858db75467999f2801b41d5e982, entries=899300, sequenceid=1656, filesize=64.0m
2014-07-14 01:06:44,720 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~259.6m/272158000, currentsize=126.3m/132440240 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 9645ms, sequenceid=1656, compaction requested=true
2014-07-14 01:06:44,720 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:06:44,721 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-14 01:06:44,721 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-14 01:06:44,721 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 279.8m
2014-07-14 01:06:44,721 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:06:44,721 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:06:44,721 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:06:44,919 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:06:46,054 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:06:46,211 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6931 synced till here 6930
2014-07-14 01:06:46,262 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325202841 with entries=82, filesize=70.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325206055
2014-07-14 01:06:46,263 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325159068
2014-07-14 01:06:46,263 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325160486
2014-07-14 01:06:52,279 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4753ms
GC pool 'ParNew' had collection(s): count=1 time=361ms
2014-07-14 01:06:52,297 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:06:52,348 WARN  [RpcServer.reader=0,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: count of bytes read: 0
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:251)
	at sun.nio.ch.IOUtil.read(IOUtil.java:224)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:254)
	at org.apache.hadoop.hbase.ipc.RpcServer.channelIO(RpcServer.java:2263)
	at org.apache.hadoop.hbase.ipc.RpcServer.channelRead(RpcServer.java:2229)
	at org.apache.hadoop.hbase.ipc.RpcServer$Connection.readAndProcess(RpcServer.java:1488)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener.doRead(RpcServer.java:790)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.doRunLoop(RpcServer.java:581)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.run(RpcServer.java:556)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-14 01:06:52,366 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325206055 with entries=72, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325212298
2014-07-14 01:06:52,443 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3275 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47128: output error
2014-07-14 01:06:52,485 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:06:52,635 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3271 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47128: output error
2014-07-14 01:06:52,635 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:06:52,635 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3274 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47128: output error
2014-07-14 01:06:52,635 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:06:52,805 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3265 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47128: output error
2014-07-14 01:06:52,805 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:06:52,805 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3269 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47128: output error
2014-07-14 01:06:52,806 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:06:52,825 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3267 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47128: output error
2014-07-14 01:06:52,825 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:06:52,862 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3261 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47128: output error
2014-07-14 01:06:52,862 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:06:52,870 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3264 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47128: output error
2014-07-14 01:06:52,870 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:06:53,444 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:06:53,463 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7080 synced till here 7077
2014-07-14 01:06:53,524 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325212298 with entries=77, filesize=66.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325213445
2014-07-14 01:06:54,252 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:06:55,075 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:06:55,145 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7168 synced till here 7156
2014-07-14 01:06:56,838 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325213445 with entries=88, filesize=75.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325215075
2014-07-14 01:06:57,190 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:06:57,226 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7252 synced till here 7241
2014-07-14 01:06:57,446 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325215075 with entries=84, filesize=72.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325217190
2014-07-14 01:06:58,474 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1699, memsize=245.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/6c6d74f14cbe4b7dbbd425c3ae572370
2014-07-14 01:06:58,493 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/6c6d74f14cbe4b7dbbd425c3ae572370 as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/6c6d74f14cbe4b7dbbd425c3ae572370
2014-07-14 01:06:58,505 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/6c6d74f14cbe4b7dbbd425c3ae572370, entries=893690, sequenceid=1699, filesize=63.7m
2014-07-14 01:06:58,505 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~257.9m/270415440, currentsize=172.0m/180354880 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 17658ms, sequenceid=1699, compaction requested=true
2014-07-14 01:06:58,505 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:06:58,506 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-14 01:06:58,506 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 318.7m
2014-07-14 01:06:58,506 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-14 01:06:58,506 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:06:58,506 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:06:58,506 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:06:58,521 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:06:58,897 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:07:00,801 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1006ms
GC pool 'ParNew' had collection(s): count=1 time=1359ms
2014-07-14 01:07:00,802 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:00,924 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7357 synced till here 7327
2014-07-14 01:07:01,307 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325217190 with entries=105, filesize=89.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325220804
2014-07-14 01:07:01,307 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325162243
2014-07-14 01:07:03,425 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1121ms
GC pool 'ParNew' had collection(s): count=1 time=1277ms
2014-07-14 01:07:03,790 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:03,872 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7479 synced till here 7446
2014-07-14 01:07:04,891 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325220804 with entries=122, filesize=104.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325223791
2014-07-14 01:07:06,313 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1386ms
GC pool 'ParNew' had collection(s): count=1 time=1386ms
2014-07-14 01:07:06,940 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:07:09,217 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1402ms
GC pool 'ParNew' had collection(s): count=1 time=1406ms
2014-07-14 01:07:09,323 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:09,347 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7585 synced till here 7561
2014-07-14 01:07:09,597 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325223791 with entries=106, filesize=90.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325229324
2014-07-14 01:07:09,764 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1741, memsize=267.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/05897ce13acb499f9b62d840f685158d
2014-07-14 01:07:09,783 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/05897ce13acb499f9b62d840f685158d as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/05897ce13acb499f9b62d840f685158d
2014-07-14 01:07:09,799 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/05897ce13acb499f9b62d840f685158d, entries=973400, sequenceid=1741, filesize=69.3m
2014-07-14 01:07:09,800 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~279.8m/293441440, currentsize=287.3m/301270800 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 25079ms, sequenceid=1741, compaction requested=true
2014-07-14 01:07:09,800 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:07:09,800 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 20 blocking
2014-07-14 01:07:09,800 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-14 01:07:09,801 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:07:09,801 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 377.6m
2014-07-14 01:07:09,801 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:07:09,801 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:07:10,092 DEBUG [RpcServer.handler=5,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:07:11,466 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:07:11,715 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:11,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7701 synced till here 7686
2014-07-14 01:07:11,966 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325229324 with entries=116, filesize=98.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325231715
2014-07-14 01:07:11,966 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325166661
2014-07-14 01:07:11,966 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325169998
2014-07-14 01:07:11,967 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325172910
2014-07-14 01:07:13,865 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:13,884 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7813 synced till here 7782
2014-07-14 01:07:14,615 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325231715 with entries=112, filesize=95.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325233865
2014-07-14 01:07:16,611 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:16,749 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7920 synced till here 7919
2014-07-14 01:07:16,836 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325233865 with entries=107, filesize=91.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325236611
2014-07-14 01:07:18,461 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:18,488 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8034 synced till here 8001
2014-07-14 01:07:18,678 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325236611 with entries=114, filesize=97.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325238461
2014-07-14 01:07:20,789 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1304ms
GC pool 'ParNew' had collection(s): count=1 time=1693ms
2014-07-14 01:07:21,328 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:21,351 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8126 synced till here 8124
2014-07-14 01:07:21,370 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325238461 with entries=92, filesize=78.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325241328
2014-07-14 01:07:21,648 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1831, memsize=287.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/f192137aff3f4dff859f6a0d95122fd7
2014-07-14 01:07:21,669 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/f192137aff3f4dff859f6a0d95122fd7 as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/f192137aff3f4dff859f6a0d95122fd7
2014-07-14 01:07:21,686 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/f192137aff3f4dff859f6a0d95122fd7, entries=1046960, sequenceid=1831, filesize=74.6m
2014-07-14 01:07:21,686 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~329.6m/345627040, currentsize=330.6m/346632960 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 23180ms, sequenceid=1831, compaction requested=true
2014-07-14 01:07:21,686 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:07:21,687 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 20 blocking
2014-07-14 01:07:21,687 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 526.9m
2014-07-14 01:07:21,687 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-14 01:07:21,687 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:07:21,687 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:07:21,687 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:07:22,168 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:07:22,332 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:07:23,476 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:23,495 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8200 synced till here 8197
2014-07-14 01:07:23,526 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325241328 with entries=74, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325243476
2014-07-14 01:07:23,526 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325174315
2014-07-14 01:07:23,527 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325175801
2014-07-14 01:07:24,788 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:24,808 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8275 synced till here 8272
2014-07-14 01:07:24,886 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325243476 with entries=75, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325244788
2014-07-14 01:07:25,755 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:26,785 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8379 synced till here 8371
2014-07-14 01:07:26,793 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1910, memsize=249.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/5898fcd43b7b44a8bcf3e5e5a000b6cc
2014-07-14 01:07:26,858 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/5898fcd43b7b44a8bcf3e5e5a000b6cc as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/5898fcd43b7b44a8bcf3e5e5a000b6cc
2014-07-14 01:07:26,934 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325244788 with entries=104, filesize=89.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325245755
2014-07-14 01:07:26,985 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/5898fcd43b7b44a8bcf3e5e5a000b6cc, entries=908010, sequenceid=1910, filesize=64.7m
2014-07-14 01:07:26,985 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~394.7m/413845120, currentsize=299.8m/314361520 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 17184ms, sequenceid=1910, compaction requested=true
2014-07-14 01:07:26,986 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:07:26,986 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 20 blocking
2014-07-14 01:07:26,986 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 572.3m
2014-07-14 01:07:26,986 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-14 01:07:26,986 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:07:26,986 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:07:26,987 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:07:27,095 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:07:27,521 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:07:28,071 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:28,095 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8452 synced till here 8450
2014-07-14 01:07:28,114 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325245755 with entries=73, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325248072
2014-07-14 01:07:28,114 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325194193
2014-07-14 01:07:28,115 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325195529
2014-07-14 01:07:28,115 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325198003
2014-07-14 01:07:28,941 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:29,668 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325248072 with entries=84, filesize=71.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325248942
2014-07-14 01:07:31,008 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:31,602 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8628 synced till here 8625
2014-07-14 01:07:31,621 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2044, memsize=179.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/6f74fd98ca7e4603819c6e1fba0c91bc
2014-07-14 01:07:31,641 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/6f74fd98ca7e4603819c6e1fba0c91bc as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/6f74fd98ca7e4603819c6e1fba0c91bc
2014-07-14 01:07:31,657 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/6f74fd98ca7e4603819c6e1fba0c91bc, entries=655020, sequenceid=2044, filesize=46.7m
2014-07-14 01:07:31,657 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~534.6m/560619360, currentsize=185.2m/194215760 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 9970ms, sequenceid=2044, compaction requested=true
2014-07-14 01:07:31,658 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:07:31,658 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 20 blocking
2014-07-14 01:07:31,658 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-14 01:07:31,658 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 518.6m
2014-07-14 01:07:31,658 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:07:31,658 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:07:31,658 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:07:31,659 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325248942 with entries=92, filesize=78.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325251008
2014-07-14 01:07:31,659 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325200695
2014-07-14 01:07:32,053 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:07:33,253 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:33,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8705 synced till here 8702
2014-07-14 01:07:33,415 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325251008 with entries=77, filesize=66.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325253254
2014-07-14 01:07:34,638 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:34,655 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8778 synced till here 8777
2014-07-14 01:07:34,670 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325253254 with entries=73, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325254638
2014-07-14 01:07:35,437 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:07:35,890 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:35,909 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8853 synced till here 8851
2014-07-14 01:07:35,960 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325254638 with entries=75, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325255890
2014-07-14 01:07:36,649 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2119, memsize=178.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/480fb673d3c04d4aab6a1ee832449bae
2014-07-14 01:07:36,670 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/480fb673d3c04d4aab6a1ee832449bae as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/480fb673d3c04d4aab6a1ee832449bae
2014-07-14 01:07:36,683 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/480fb673d3c04d4aab6a1ee832449bae, entries=649240, sequenceid=2119, filesize=46.3m
2014-07-14 01:07:36,683 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~581.6m/609848400, currentsize=186.8m/195903600 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 9697ms, sequenceid=2119, compaction requested=true
2014-07-14 01:07:36,684 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 20 blocking
2014-07-14 01:07:36,685 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-14 01:07:36,685 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:07:36,685 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:07:36,685 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:07:36,688 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:07:36,689 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 499.4m
2014-07-14 01:07:37,021 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:07:37,342 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:37,382 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8926 synced till here 8925
2014-07-14 01:07:37,485 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325255890 with entries=73, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325257343
2014-07-14 01:07:37,485 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325202841
2014-07-14 01:07:37,485 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325206055
2014-07-14 01:07:37,485 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325212298
2014-07-14 01:07:37,485 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325213445
2014-07-14 01:07:37,485 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325215075
2014-07-14 01:07:38,921 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:38,979 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9007 synced till here 8999
2014-07-14 01:07:39,032 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325257343 with entries=81, filesize=69.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325258922
2014-07-14 01:07:40,732 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:07:40,784 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:40,924 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2167, memsize=200.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/7039b4e00e3a47bda3bd2df8cd9a3e23
2014-07-14 01:07:40,984 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/7039b4e00e3a47bda3bd2df8cd9a3e23 as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/7039b4e00e3a47bda3bd2df8cd9a3e23
2014-07-14 01:07:40,993 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325258922 with entries=89, filesize=76.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325260784
2014-07-14 01:07:41,008 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/7039b4e00e3a47bda3bd2df8cd9a3e23, entries=729740, sequenceid=2167, filesize=52.0m
2014-07-14 01:07:41,008 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~520.2m/545454160, currentsize=181.8m/190595680 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 9350ms, sequenceid=2167, compaction requested=true
2014-07-14 01:07:41,009 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:07:41,009 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 20 blocking
2014-07-14 01:07:41,009 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 368.1m
2014-07-14 01:07:41,009 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-14 01:07:41,009 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:07:41,009 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:07:41,009 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:07:41,224 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:07:42,108 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:42,169 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9178 synced till here 9170
2014-07-14 01:07:42,246 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325260784 with entries=82, filesize=70.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325262108
2014-07-14 01:07:42,246 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325217190
2014-07-14 01:07:42,246 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325220804
2014-07-14 01:07:42,247 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325223791
2014-07-14 01:07:43,532 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:43,912 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9272 synced till here 9269
2014-07-14 01:07:43,946 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325262108 with entries=94, filesize=80.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325263532
2014-07-14 01:07:44,205 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:07:45,345 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:45,366 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9347 synced till here 9343
2014-07-14 01:07:45,616 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325263532 with entries=75, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325265345
2014-07-14 01:07:46,637 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:46,929 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9440 synced till here 9434
2014-07-14 01:07:47,015 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325265345 with entries=93, filesize=79.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325266637
2014-07-14 01:07:48,089 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:48,544 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325266637 with entries=108, filesize=92.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325268090
2014-07-14 01:07:48,768 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2235, memsize=288.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/aefd4ce804914b46ba83ef1316d5280d
2014-07-14 01:07:48,784 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/aefd4ce804914b46ba83ef1316d5280d as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/aefd4ce804914b46ba83ef1316d5280d
2014-07-14 01:07:48,799 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/aefd4ce804914b46ba83ef1316d5280d, entries=1050520, sequenceid=2235, filesize=74.8m
2014-07-14 01:07:48,800 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~502.7m/527068560, currentsize=255.9m/268370480 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 12110ms, sequenceid=2235, compaction requested=true
2014-07-14 01:07:48,800 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:07:48,800 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 20 blocking
2014-07-14 01:07:48,800 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-14 01:07:48,801 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 449.5m
2014-07-14 01:07:48,801 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:07:48,801 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:07:48,801 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:07:48,814 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:07:49,482 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:07:50,908 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:50,944 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325268090 with entries=73, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325270909
2014-07-14 01:07:50,944 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325229324
2014-07-14 01:07:50,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325231715
2014-07-14 01:07:50,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325233865
2014-07-14 01:07:50,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325236611
2014-07-14 01:07:50,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325238461
2014-07-14 01:07:52,309 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:52,477 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9700 synced till here 9698
2014-07-14 01:07:52,496 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325270909 with entries=79, filesize=67.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325272310
2014-07-14 01:07:53,769 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2281, memsize=341.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/18c5e06d94114d1aa5eadd9b135d46f3
2014-07-14 01:07:53,807 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/18c5e06d94114d1aa5eadd9b135d46f3 as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/18c5e06d94114d1aa5eadd9b135d46f3
2014-07-14 01:07:53,825 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/18c5e06d94114d1aa5eadd9b135d46f3, entries=1243110, sequenceid=2281, filesize=88.5m
2014-07-14 01:07:53,826 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~368.1m/385971840, currentsize=245.3m/257190560 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 12816ms, sequenceid=2281, compaction requested=true
2014-07-14 01:07:53,826 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:07:53,826 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 20 blocking
2014-07-14 01:07:53,826 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-14 01:07:53,826 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 425.7m
2014-07-14 01:07:53,826 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:07:53,827 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:07:53,827 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:07:54,144 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:07:54,440 DEBUG [RpcServer.handler=13,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:07:54,846 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:55,232 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9796 synced till here 9794
2014-07-14 01:07:55,251 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325272310 with entries=96, filesize=82.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325274846
2014-07-14 01:07:55,251 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325241328
2014-07-14 01:07:55,251 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325243476
2014-07-14 01:07:55,251 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325244788
2014-07-14 01:07:56,387 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:07:56,417 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9871 synced till here 9870
2014-07-14 01:07:56,428 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325274846 with entries=75, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325276388
2014-07-14 01:08:02,025 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2410, memsize=394.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/3ba167876af849c3a5441b332be2aaff
2014-07-14 01:08:02,052 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/3ba167876af849c3a5441b332be2aaff as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/3ba167876af849c3a5441b332be2aaff
2014-07-14 01:08:02,091 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/3ba167876af849c3a5441b332be2aaff, entries=1437640, sequenceid=2410, filesize=102.4m
2014-07-14 01:08:02,093 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~451.0m/472916480, currentsize=133.0m/139428480 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 13293ms, sequenceid=2410, compaction requested=true
2014-07-14 01:08:02,093 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:08:02,093 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 20 blocking
2014-07-14 01:08:02,093 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-14 01:08:02,093 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:08:02,093 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 396.1m
2014-07-14 01:08:02,094 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:08:02,094 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:08:02,334 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:08:02,491 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:02,705 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9950 synced till here 9948
2014-07-14 01:08:02,722 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325276388 with entries=79, filesize=67.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325282492
2014-07-14 01:08:02,723 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325245755
2014-07-14 01:08:02,723 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325248072
2014-07-14 01:08:02,723 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325248942
2014-07-14 01:08:04,939 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:04,959 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10024 synced till here 10022
2014-07-14 01:08:04,983 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325282492 with entries=74, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325284940
2014-07-14 01:08:06,120 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:06,479 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325284940 with entries=91, filesize=78.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325286120
2014-07-14 01:08:07,591 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2444, memsize=374.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/28216b81975f47fd8670a1a57a946d8e
2014-07-14 01:08:07,608 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/28216b81975f47fd8670a1a57a946d8e as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/28216b81975f47fd8670a1a57a946d8e
2014-07-14 01:08:07,621 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/28216b81975f47fd8670a1a57a946d8e, entries=1361900, sequenceid=2444, filesize=97.0m
2014-07-14 01:08:07,622 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~428.7m/449555520, currentsize=171.9m/180242720 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 13796ms, sequenceid=2444, compaction requested=true
2014-07-14 01:08:07,623 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:08:07,623 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 20 blocking
2014-07-14 01:08:07,623 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-14 01:08:07,623 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:08:07,623 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:08:07,623 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 418.3m
2014-07-14 01:08:07,623 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:08:07,723 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:07,768 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10192 synced till here 10189
2014-07-14 01:08:07,809 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325286120 with entries=77, filesize=66.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325287723
2014-07-14 01:08:07,809 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325251008
2014-07-14 01:08:07,809 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325253254
2014-07-14 01:08:07,810 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325254638
2014-07-14 01:08:07,923 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:08:08,890 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:08:09,154 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:09,179 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10270 synced till here 10265
2014-07-14 01:08:09,304 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325287723 with entries=78, filesize=66.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325289154
2014-07-14 01:08:10,946 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:11,415 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10386 synced till here 10385
2014-07-14 01:08:11,444 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325289154 with entries=116, filesize=99.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325290947
2014-07-14 01:08:11,575 DEBUG [RpcServer.handler=30,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:08:12,545 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:12,574 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10461 synced till here 10459
2014-07-14 01:08:12,657 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325290947 with entries=75, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325292546
2014-07-14 01:08:13,991 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:14,023 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10534 synced till here 10533
2014-07-14 01:08:14,072 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325292546 with entries=73, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325293991
2014-07-14 01:08:15,653 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:15,682 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325293991 with entries=73, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325295654
2014-07-14 01:08:16,764 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:16,955 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2491, memsize=360.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/d862e7aad5ea4e3a921d6fa4891e2a45
2014-07-14 01:08:17,066 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/d862e7aad5ea4e3a921d6fa4891e2a45 as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/d862e7aad5ea4e3a921d6fa4891e2a45
2014-07-14 01:08:17,078 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10701 synced till here 10699
2014-07-14 01:08:17,085 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/d862e7aad5ea4e3a921d6fa4891e2a45, entries=1313270, sequenceid=2491, filesize=93.5m
2014-07-14 01:08:17,086 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~396.1m/415356640, currentsize=301.5m/316118400 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 14993ms, sequenceid=2491, compaction requested=true
2014-07-14 01:08:17,086 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:08:17,086 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 20 blocking
2014-07-14 01:08:17,086 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-14 01:08:17,086 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 434.5m
2014-07-14 01:08:17,086 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:08:17,086 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:08:17,087 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:08:17,099 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325295654 with entries=94, filesize=80.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325296764
2014-07-14 01:08:17,099 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325255890
2014-07-14 01:08:17,099 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325257343
2014-07-14 01:08:17,099 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325258922
2014-07-14 01:08:17,150 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:08:17,447 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:08:18,580 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:18,602 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10778 synced till here 10775
2014-07-14 01:08:18,643 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325296764 with entries=77, filesize=66.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325298581
2014-07-14 01:08:20,506 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:20,529 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10854 synced till here 10851
2014-07-14 01:08:20,583 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325298581 with entries=76, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325300506
2014-07-14 01:08:23,322 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:23,345 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10931 synced till here 10927
2014-07-14 01:08:23,463 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325300506 with entries=77, filesize=65.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325303323
2014-07-14 01:08:24,272 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:24,903 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11014 synced till here 11003
2014-07-14 01:08:25,004 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325303323 with entries=83, filesize=71.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325304273
2014-07-14 01:08:25,567 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2553, memsize=378.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/1338e268c9be4ef1ab9df3ab2ce38fcd
2014-07-14 01:08:25,590 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/1338e268c9be4ef1ab9df3ab2ce38fcd as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/1338e268c9be4ef1ab9df3ab2ce38fcd
2014-07-14 01:08:25,615 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/1338e268c9be4ef1ab9df3ab2ce38fcd, entries=1376410, sequenceid=2553, filesize=98.1m
2014-07-14 01:08:25,615 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~419.9m/440247920, currentsize=342.4m/359059440 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 17992ms, sequenceid=2553, compaction requested=true
2014-07-14 01:08:25,616 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:08:25,616 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 20 blocking
2014-07-14 01:08:25,616 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-14 01:08:25,616 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:08:25,617 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:08:25,617 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:08:25,616 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 518.1m
2014-07-14 01:08:25,617 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:08:25,827 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:25,865 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11090 synced till here 11088
2014-07-14 01:08:25,911 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325304273 with entries=76, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325305828
2014-07-14 01:08:25,912 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325260784
2014-07-14 01:08:25,912 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325262108
2014-07-14 01:08:25,912 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325263532
2014-07-14 01:08:25,912 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325265345
2014-07-14 01:08:25,912 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325266637
2014-07-14 01:08:26,745 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:08:27,195 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:27,221 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11167 synced till here 11163
2014-07-14 01:08:27,272 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325305828 with entries=77, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325307195
2014-07-14 01:08:28,473 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:28,934 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11277 synced till here 11274
2014-07-14 01:08:28,959 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325307195 with entries=110, filesize=94.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325308473
2014-07-14 01:08:30,508 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:30,881 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325308473 with entries=75, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325310508
2014-07-14 01:08:34,091 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2693, memsize=394.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/4dd8d1cd486a4702a6499dc884bd8097
2014-07-14 01:08:34,098 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:34,113 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/4dd8d1cd486a4702a6499dc884bd8097 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/4dd8d1cd486a4702a6499dc884bd8097
2014-07-14 01:08:34,125 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/4dd8d1cd486a4702a6499dc884bd8097, entries=1434860, sequenceid=2693, filesize=102.2m
2014-07-14 01:08:34,126 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~437.6m/458817200, currentsize=279.0m/292577440 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 17040ms, sequenceid=2693, compaction requested=true
2014-07-14 01:08:34,127 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:08:34,127 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 20 blocking
2014-07-14 01:08:34,127 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 582.4m
2014-07-14 01:08:34,127 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-14 01:08:34,127 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:08:34,127 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:08:34,127 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:08:34,148 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325310508 with entries=72, filesize=61.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325314098
2014-07-14 01:08:34,148 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325268090
2014-07-14 01:08:34,148 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325270909
2014-07-14 01:08:34,402 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:08:34,598 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:08:35,677 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:35,710 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11499 synced till here 11498
2014-07-14 01:08:35,745 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325314098 with entries=75, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325315677
2014-07-14 01:08:37,547 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:37,565 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11573 synced till here 11571
2014-07-14 01:08:37,596 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325315677 with entries=74, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325317548
2014-07-14 01:08:38,716 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:38,752 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325317548 with entries=72, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325318716
2014-07-14 01:08:40,312 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:40,412 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325318716 with entries=72, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325320312
2014-07-14 01:08:43,998 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2780, memsize=462.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/94d7d40dd7ea424fb3f1b97c739f44aa
2014-07-14 01:08:44,019 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/94d7d40dd7ea424fb3f1b97c739f44aa as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/94d7d40dd7ea424fb3f1b97c739f44aa
2014-07-14 01:08:44,035 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/94d7d40dd7ea424fb3f1b97c739f44aa, entries=1682120, sequenceid=2780, filesize=119.8m
2014-07-14 01:08:44,036 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~521.2m/546522560, currentsize=249.3m/261443200 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 18420ms, sequenceid=2780, compaction requested=true
2014-07-14 01:08:44,036 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:08:44,036 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 20 blocking
2014-07-14 01:08:44,036 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-14 01:08:44,037 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 601.7m
2014-07-14 01:08:44,037 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:08:44,037 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:08:44,037 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:08:44,424 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:08:46,611 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:08:49,633 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:49,648 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11791 synced till here 11790
2014-07-14 01:08:49,679 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325320312 with entries=74, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325329634
2014-07-14 01:08:49,680 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325272310
2014-07-14 01:08:49,680 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325274846
2014-07-14 01:08:50,931 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:50,957 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325329634 with entries=73, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325330931
2014-07-14 01:08:53,027 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:53,075 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2868, memsize=482.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/24da4f3746d24033976b612a0f276673
2014-07-14 01:08:53,117 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/24da4f3746d24033976b612a0f276673 as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/24da4f3746d24033976b612a0f276673
2014-07-14 01:08:53,146 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11959 synced till here 11950
2014-07-14 01:08:53,186 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/24da4f3746d24033976b612a0f276673, entries=1757960, sequenceid=2868, filesize=125.1m
2014-07-14 01:08:53,187 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~584.0m/612419600, currentsize=198.8m/208502080 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 19059ms, sequenceid=2868, compaction requested=true
2014-07-14 01:08:53,187 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:08:53,188 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 20 blocking
2014-07-14 01:08:53,188 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 494.0m
2014-07-14 01:08:53,188 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-14 01:08:53,188 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:08:53,188 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:08:53,188 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:08:53,225 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325330931 with entries=95, filesize=81.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325333028
2014-07-14 01:08:53,225 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325276388
2014-07-14 01:08:53,225 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325282492
2014-07-14 01:08:53,225 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325284940
2014-07-14 01:08:54,543 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:08:54,868 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:55,324 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12066 synced till here 12059
2014-07-14 01:08:55,413 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325333028 with entries=107, filesize=92.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325334869
2014-07-14 01:08:56,544 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:08:56,993 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:57,117 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12144 synced till here 12140
2014-07-14 01:08:57,223 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325334869 with entries=78, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325336994
2014-07-14 01:08:58,412 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:58,436 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12225 synced till here 12215
2014-07-14 01:08:58,526 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325336994 with entries=81, filesize=69.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325338412
2014-07-14 01:08:59,743 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:08:59,797 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12312 synced till here 12298
2014-07-14 01:09:01,412 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1100ms
GC pool 'ParNew' had collection(s): count=1 time=1125ms
2014-07-14 01:09:01,418 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325338412 with entries=87, filesize=74.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325339743
2014-07-14 01:09:02,217 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:09:02,273 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12405 synced till here 12383
2014-07-14 01:09:03,330 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325339743 with entries=93, filesize=79.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325342217
2014-07-14 01:09:04,174 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:09:04,247 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12515 synced till here 12480
2014-07-14 01:09:05,397 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325342217 with entries=110, filesize=93.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325344175
2014-07-14 01:09:06,053 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:09:06,111 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12590 synced till here 12586
2014-07-14 01:09:06,203 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325344175 with entries=75, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325346054
2014-07-14 01:09:07,987 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:09:08,019 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12680 synced till here 12671
2014-07-14 01:09:08,112 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325346054 with entries=90, filesize=77.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325347988
2014-07-14 01:09:09,607 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:09:09,640 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325347988 with entries=74, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325349608
2014-07-14 01:09:10,474 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2941, memsize=511.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/282a4b7152e1468ab69e997f4d9e20e9
2014-07-14 01:09:10,488 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/282a4b7152e1468ab69e997f4d9e20e9 as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/282a4b7152e1468ab69e997f4d9e20e9
2014-07-14 01:09:10,502 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/282a4b7152e1468ab69e997f4d9e20e9, entries=1862870, sequenceid=2941, filesize=132.5m
2014-07-14 01:09:10,502 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~601.7m/630876720, currentsize=412.2m/432231840 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 26465ms, sequenceid=2941, compaction requested=true
2014-07-14 01:09:10,503 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:09:10,503 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 20 blocking
2014-07-14 01:09:10,503 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-14 01:09:10,503 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 667.5m
2014-07-14 01:09:10,503 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:09:10,503 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:09:10,503 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:09:10,994 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:09:11,279 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:09:11,301 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12830 synced till here 12824
2014-07-14 01:09:11,370 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325349608 with entries=76, filesize=65.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325351280
2014-07-14 01:09:11,370 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325286120
2014-07-14 01:09:11,370 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325287723
2014-07-14 01:09:11,370 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325289154
2014-07-14 01:09:11,370 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325290947
2014-07-14 01:09:11,370 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325292546
2014-07-14 01:09:11,370 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325293991
2014-07-14 01:09:11,370 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325295654
2014-07-14 01:09:11,609 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:09:12,590 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:09:12,611 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12904 synced till here 12901
2014-07-14 01:09:12,644 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325351280 with entries=74, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325352590
2014-07-14 01:09:13,691 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:09:14,084 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12980 synced till here 12978
2014-07-14 01:09:14,118 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325352590 with entries=76, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325353691
2014-07-14 01:09:15,133 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:09:15,153 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13053 synced till here 13052
2014-07-14 01:09:15,231 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325353691 with entries=73, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325355134
2014-07-14 01:09:16,518 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:09:17,517 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3013, memsize=416.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/77d8e6b9b947457f9073a153bfc3ea95
2014-07-14 01:09:17,535 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13156 synced till here 13151
2014-07-14 01:09:17,541 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/77d8e6b9b947457f9073a153bfc3ea95 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/77d8e6b9b947457f9073a153bfc3ea95
2014-07-14 01:09:17,557 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325355134 with entries=103, filesize=88.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325356519
2014-07-14 01:09:17,561 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/77d8e6b9b947457f9073a153bfc3ea95, entries=1517540, sequenceid=3013, filesize=108.0m
2014-07-14 01:09:17,562 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~497.1m/521203280, currentsize=461.0m/483382000 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 24374ms, sequenceid=3013, compaction requested=true
2014-07-14 01:09:17,562 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:09:17,562 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 20 blocking
2014-07-14 01:09:17,563 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-14 01:09:17,563 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 671.8m
2014-07-14 01:09:17,563 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:09:17,563 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:09:17,563 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:09:17,653 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:09:18,114 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:09:20,231 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:09:20,268 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325356519 with entries=71, filesize=60.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325360231
2014-07-14 01:09:20,268 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325296764
2014-07-14 01:09:20,268 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325298581
2014-07-14 01:09:20,268 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325300506
2014-07-14 01:09:20,268 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325303323
2014-07-14 01:09:25,793 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3213, memsize=378.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/cc526b84327445c3b48dd82baa1678ee
2014-07-14 01:09:25,819 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/cc526b84327445c3b48dd82baa1678ee as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/cc526b84327445c3b48dd82baa1678ee
2014-07-14 01:09:25,861 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/cc526b84327445c3b48dd82baa1678ee, entries=1379510, sequenceid=3213, filesize=98.3m
2014-07-14 01:09:25,862 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~669.1m/701574480, currentsize=184.7m/193663280 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 15359ms, sequenceid=3213, compaction requested=true
2014-07-14 01:09:25,862 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:09:25,862 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 20 blocking
2014-07-14 01:09:25,862 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-14 01:09:25,863 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 606.3m
2014-07-14 01:09:25,863 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:09:25,863 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:09:25,863 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:09:26,448 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:09:26,756 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:09:27,113 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13314 synced till here 13313
2014-07-14 01:09:27,175 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325360231 with entries=87, filesize=74.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325366756
2014-07-14 01:09:27,175 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325304273
2014-07-14 01:09:27,176 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325305828
2014-07-14 01:09:27,176 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325307195
2014-07-14 01:09:27,176 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325308473
2014-07-14 01:09:27,176 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325310508
2014-07-14 01:09:28,288 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:09:28,327 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13389 synced till here 13387
2014-07-14 01:09:28,417 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325366756 with entries=75, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325368288
2014-07-14 01:09:29,761 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:09:29,987 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13479 synced till here 13476
2014-07-14 01:09:29,993 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:09:30,058 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325368288 with entries=90, filesize=77.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325369762
2014-07-14 01:09:31,356 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:09:31,478 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13563 synced till here 13556
2014-07-14 01:09:31,505 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325369762 with entries=84, filesize=72.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325371356
2014-07-14 01:09:32,121 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3306, memsize=374.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/c1d891f85ed04f94b73970ae10f93905
2014-07-14 01:09:32,175 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/c1d891f85ed04f94b73970ae10f93905 as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/c1d891f85ed04f94b73970ae10f93905
2014-07-14 01:09:32,200 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/c1d891f85ed04f94b73970ae10f93905, entries=1365180, sequenceid=3306, filesize=97.2m
2014-07-14 01:09:32,201 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~681.2m/714309120, currentsize=165.9m/173921600 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 14638ms, sequenceid=3306, compaction requested=true
2014-07-14 01:09:32,201 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:09:32,201 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 20 blocking
2014-07-14 01:09:32,201 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-14 01:09:32,201 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 629.0m
2014-07-14 01:09:32,201 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:09:32,202 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:09:32,202 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:09:32,645 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:09:33,504 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:09:33,825 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13653 synced till here 13652
2014-07-14 01:09:33,844 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325371356 with entries=90, filesize=77.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325373504
2014-07-14 01:09:33,845 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325314098
2014-07-14 01:09:33,845 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325315677
2014-07-14 01:09:33,845 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325317548
2014-07-14 01:09:33,845 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325318716
2014-07-14 01:09:35,563 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:09:35,583 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13725 synced till here 13724
2014-07-14 01:09:35,595 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325373504 with entries=72, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325375564
2014-07-14 01:09:38,784 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3332, memsize=307.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/9d13f5f3aa114391a4def95436479350
2014-07-14 01:09:38,800 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/9d13f5f3aa114391a4def95436479350 as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/9d13f5f3aa114391a4def95436479350
2014-07-14 01:09:38,821 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/9d13f5f3aa114391a4def95436479350, entries=1119130, sequenceid=3332, filesize=79.7m
2014-07-14 01:09:38,821 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~606.3m/635800800, currentsize=185.4m/194368320 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 12958ms, sequenceid=3332, compaction requested=true
2014-07-14 01:09:38,822 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:09:38,823 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 20 blocking
2014-07-14 01:09:38,823 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 369.2m
2014-07-14 01:09:38,823 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-14 01:09:38,823 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:09:38,823 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:09:38,823 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:09:39,394 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:09:39,566 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:09:39,588 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13800 synced till here 13798
2014-07-14 01:09:39,626 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325375564 with entries=75, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325379566
2014-07-14 01:09:39,627 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325320312
2014-07-14 01:09:39,627 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325329634
2014-07-14 01:09:39,627 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325330931
2014-07-14 01:09:40,035 DEBUG [RpcServer.handler=24,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:09:40,961 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:09:40,996 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13877 synced till here 13874
2014-07-14 01:09:41,047 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325379566 with entries=77, filesize=66.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325380961
2014-07-14 01:09:42,918 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:09:42,918 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:09:43,237 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13951 synced till here 13950
2014-07-14 01:09:43,256 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325380961 with entries=74, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325382919
2014-07-14 01:09:44,375 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:09:44,398 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14025 synced till here 14024
2014-07-14 01:09:44,423 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325382919 with entries=74, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325384375
2014-07-14 01:09:45,623 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:09:45,665 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325384375 with entries=71, filesize=60.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325385623
2014-07-14 01:09:47,035 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:09:47,411 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3420, memsize=368.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/06cbfb867aa54399bca6eeab1870bec7
2014-07-14 01:09:47,450 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14194 synced till here 14192
2014-07-14 01:09:47,563 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/06cbfb867aa54399bca6eeab1870bec7 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/06cbfb867aa54399bca6eeab1870bec7
2014-07-14 01:09:47,604 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325385623 with entries=98, filesize=84.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325387035
2014-07-14 01:09:47,690 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/06cbfb867aa54399bca6eeab1870bec7, entries=1342290, sequenceid=3420, filesize=95.6m
2014-07-14 01:09:47,691 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~632.2m/662902560, currentsize=234.7m/246053040 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 15490ms, sequenceid=3420, compaction requested=true
2014-07-14 01:09:47,691 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:09:47,692 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 20 blocking
2014-07-14 01:09:47,692 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 399.1m
2014-07-14 01:09:47,692 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-14 01:09:47,692 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:09:47,692 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:09:47,693 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:09:48,424 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:09:48,780 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:09:48,823 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:09:49,178 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325387035 with entries=94, filesize=80.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325388823
2014-07-14 01:09:49,178 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325333028
2014-07-14 01:09:49,179 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325334869
2014-07-14 01:09:49,179 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325336994
2014-07-14 01:09:49,179 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325338412
2014-07-14 01:09:49,179 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325339743
2014-07-14 01:09:49,179 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325342217
2014-07-14 01:09:49,179 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325344175
2014-07-14 01:09:49,179 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325346054
2014-07-14 01:09:49,179 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325347988
2014-07-14 01:09:51,099 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:09:51,117 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14361 synced till here 14359
2014-07-14 01:09:51,130 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325388823 with entries=73, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325391099
2014-07-14 01:09:52,262 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3454, memsize=360.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/2bc5562d8cfb4e5799b21cae9148a9eb
2014-07-14 01:09:52,277 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/2bc5562d8cfb4e5799b21cae9148a9eb as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/2bc5562d8cfb4e5799b21cae9148a9eb
2014-07-14 01:09:52,303 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/2bc5562d8cfb4e5799b21cae9148a9eb, entries=1311220, sequenceid=3454, filesize=93.3m
2014-07-14 01:09:52,303 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~372.3m/390430720, currentsize=256.6m/269077840 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 13480ms, sequenceid=3454, compaction requested=true
2014-07-14 01:09:52,304 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:09:52,304 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 20 blocking
2014-07-14 01:09:52,304 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 441.8m
2014-07-14 01:09:52,304 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-14 01:09:52,304 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:09:52,305 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:09:52,305 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:09:52,313 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:09:52,382 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:09:52,580 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:09:52,923 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325391099 with entries=82, filesize=70.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325392383
2014-07-14 01:09:52,924 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325349608
2014-07-14 01:09:52,924 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325351280
2014-07-14 01:09:52,924 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325352590
2014-07-14 01:09:52,924 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325353691
2014-07-14 01:09:52,924 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325355134
2014-07-14 01:10:00,750 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3565, memsize=399.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/7703482e5c62432c80aefc91c39923fb
2014-07-14 01:10:00,769 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/7703482e5c62432c80aefc91c39923fb as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/7703482e5c62432c80aefc91c39923fb
2014-07-14 01:10:00,785 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/7703482e5c62432c80aefc91c39923fb, entries=1453000, sequenceid=3565, filesize=103.5m
2014-07-14 01:10:00,786 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~400.6m/420092880, currentsize=104.0m/109104000 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 13093ms, sequenceid=3565, compaction requested=true
2014-07-14 01:10:00,786 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:10:00,786 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 20 blocking
2014-07-14 01:10:00,786 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-14 01:10:00,786 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 333.9m
2014-07-14 01:10:00,786 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:10:00,786 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:10:00,786 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:10:01,000 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:10:02,101 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:10:02,117 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14516 synced till here 14514
2014-07-14 01:10:02,137 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325392383 with entries=73, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325402101
2014-07-14 01:10:02,137 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325356519
2014-07-14 01:10:03,526 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:10:04,042 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14620 synced till here 14618
2014-07-14 01:10:04,213 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325402101 with entries=104, filesize=89.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325403526
2014-07-14 01:10:05,256 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:10:05,273 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14697 synced till here 14692
2014-07-14 01:10:05,351 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325403526 with entries=77, filesize=65.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325405256
2014-07-14 01:10:06,557 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:10:06,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14796 synced till here 14791
2014-07-14 01:10:06,905 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325405256 with entries=99, filesize=84.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325406559
2014-07-14 01:10:07,035 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3618, memsize=433.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/f32a4da695434189ae7c1c66c9f495c9
2014-07-14 01:10:07,057 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/f32a4da695434189ae7c1c66c9f495c9 as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/f32a4da695434189ae7c1c66c9f495c9
2014-07-14 01:10:07,073 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/f32a4da695434189ae7c1c66c9f495c9, entries=1579290, sequenceid=3618, filesize=112.4m
2014-07-14 01:10:07,073 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~443.3m/464842960, currentsize=144.3m/151333280 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 14769ms, sequenceid=3618, compaction requested=true
2014-07-14 01:10:07,074 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:10:07,074 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 20 blocking
2014-07-14 01:10:07,074 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-14 01:10:07,074 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 404.1m
2014-07-14 01:10:07,074 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:10:07,074 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:10:07,074 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:10:07,344 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:10:09,528 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:10:09,674 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:10:09,690 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14868 synced till here 14867
2014-07-14 01:10:09,703 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325406559 with entries=72, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325409674
2014-07-14 01:10:09,703 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325360231
2014-07-14 01:10:09,703 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325366756
2014-07-14 01:10:09,704 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325368288
2014-07-14 01:10:09,704 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325369762
2014-07-14 01:10:12,615 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3636, memsize=324.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/9fec4fc8f21a4909beb85161a4f8378e
2014-07-14 01:10:12,628 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/9fec4fc8f21a4909beb85161a4f8378e as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/9fec4fc8f21a4909beb85161a4f8378e
2014-07-14 01:10:12,641 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/9fec4fc8f21a4909beb85161a4f8378e, entries=1181300, sequenceid=3636, filesize=84.1m
2014-07-14 01:10:12,642 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~333.9m/350113040, currentsize=173.3m/181723440 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 11856ms, sequenceid=3636, compaction requested=true
2014-07-14 01:10:12,642 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:10:12,642 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 20 blocking
2014-07-14 01:10:12,643 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-14 01:10:12,643 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 277.9m
2014-07-14 01:10:12,643 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:10:12,643 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:10:12,643 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:10:12,783 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:10:14,017 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:10:14,428 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14977 synced till here 14975
2014-07-14 01:10:14,456 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325409674 with entries=109, filesize=93.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325414017
2014-07-14 01:10:14,457 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325371356
2014-07-14 01:10:14,457 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325373504
2014-07-14 01:10:15,873 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:10:16,102 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325414017 with entries=79, filesize=67.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325415874
2014-07-14 01:10:16,821 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:10:17,178 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:10:17,231 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:10:17,679 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15153 synced till here 15151
2014-07-14 01:10:18,126 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325415874 with entries=97, filesize=83.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325417231
2014-07-14 01:10:19,343 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:10:19,528 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325417231 with entries=81, filesize=69.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325419344
2014-07-14 01:10:20,806 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:10:21,242 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15326 synced till here 15325
2014-07-14 01:10:21,248 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3715, memsize=394.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/6b3933bcddaf499da0704d99ede9d907
2014-07-14 01:10:21,257 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325419344 with entries=92, filesize=79.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325420807
2014-07-14 01:10:21,581 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/6b3933bcddaf499da0704d99ede9d907 as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/6b3933bcddaf499da0704d99ede9d907
2014-07-14 01:10:21,591 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/6b3933bcddaf499da0704d99ede9d907, entries=1437800, sequenceid=3715, filesize=102.4m
2014-07-14 01:10:21,592 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~404.1m/423712480, currentsize=203.7m/213611920 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 14518ms, sequenceid=3715, compaction requested=true
2014-07-14 01:10:21,592 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:10:21,592 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 347.9m
2014-07-14 01:10:21,593 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 20 blocking
2014-07-14 01:10:21,594 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-14 01:10:21,594 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:10:21,594 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:10:21,594 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:10:21,833 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:10:22,259 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3745, memsize=270.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/7e0db44a74c044cf917e4fbbee3caaa0
2014-07-14 01:10:22,273 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/7e0db44a74c044cf917e4fbbee3caaa0 as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/7e0db44a74c044cf917e4fbbee3caaa0
2014-07-14 01:10:22,283 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/7e0db44a74c044cf917e4fbbee3caaa0, entries=983140, sequenceid=3745, filesize=70.0m
2014-07-14 01:10:22,283 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~277.9m/291409440, currentsize=177.3m/185924320 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 9640ms, sequenceid=3745, compaction requested=true
2014-07-14 01:10:22,284 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:10:22,284 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 20 blocking
2014-07-14 01:10:22,284 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-14 01:10:22,284 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 350.7m
2014-07-14 01:10:22,284 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:10:22,284 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:10:22,284 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:10:22,534 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:10:22,767 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:10:22,791 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325420807 with entries=73, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325422768
2014-07-14 01:10:22,792 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325375564
2014-07-14 01:10:22,792 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325379566
2014-07-14 01:10:22,792 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325380961
2014-07-14 01:10:22,792 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325382919
2014-07-14 01:10:22,792 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325384375
2014-07-14 01:10:22,792 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325385623
2014-07-14 01:10:22,792 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325387035
2014-07-14 01:10:22,792 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325388823
2014-07-14 01:10:32,100 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3845, memsize=351.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/0b9693f360454f25baf1d38133206e9c
2014-07-14 01:10:32,120 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/0b9693f360454f25baf1d38133206e9c as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/0b9693f360454f25baf1d38133206e9c
2014-07-14 01:10:32,135 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/0b9693f360454f25baf1d38133206e9c, entries=1278230, sequenceid=3845, filesize=91.1m
2014-07-14 01:10:32,135 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~351.1m/368118880, currentsize=40.6m/42542320 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 10543ms, sequenceid=3845, compaction requested=true
2014-07-14 01:10:32,136 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:10:32,136 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 20 blocking
2014-07-14 01:10:32,137 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-14 01:10:32,137 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:10:32,137 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:10:32,137 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:10:33,163 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3864, memsize=352.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/d179b2a221a54d36b38ec3ecd1e538e3
2014-07-14 01:10:33,191 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/d179b2a221a54d36b38ec3ecd1e538e3 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/d179b2a221a54d36b38ec3ecd1e538e3
2014-07-14 01:10:33,202 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/d179b2a221a54d36b38ec3ecd1e538e3, entries=1282430, sequenceid=3864, filesize=91.3m
2014-07-14 01:10:33,203 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~352.2m/369330160, currentsize=28.0m/29363520 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 10919ms, sequenceid=3864, compaction requested=true
2014-07-14 01:10:33,203 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:10:33,203 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 20 blocking
2014-07-14 01:10:33,203 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-14 01:10:33,203 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:10:33,203 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:10:33,203 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:10:35,018 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:10:35,018 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 256.3m
2014-07-14 01:10:35,092 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:10:35,113 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15472 synced till here 15471
2014-07-14 01:10:35,127 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325422768 with entries=73, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325435092
2014-07-14 01:10:35,127 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325391099
2014-07-14 01:10:35,127 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325392383
2014-07-14 01:10:35,127 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325402101
2014-07-14 01:10:35,127 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325403526
2014-07-14 01:10:35,127 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325405256
2014-07-14 01:10:35,215 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:10:37,187 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:10:37,214 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15545 synced till here 15543
2014-07-14 01:10:37,244 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325435092 with entries=73, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325437188
2014-07-14 01:10:37,916 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:10:37,917 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 256.5m
2014-07-14 01:10:38,163 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:10:38,776 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:10:38,908 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15631 synced till here 15630
2014-07-14 01:10:38,923 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325437188 with entries=86, filesize=73.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325438776
2014-07-14 01:10:40,099 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:10:40,115 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15709 synced till here 15706
2014-07-14 01:10:40,162 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325438776 with entries=78, filesize=66.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325440100
2014-07-14 01:10:41,059 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:10:41,080 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15782 synced till here 15781
2014-07-14 01:10:41,092 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325440100 with entries=73, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325441060
2014-07-14 01:10:43,013 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:10:43,032 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15854 synced till here 15853
2014-07-14 01:10:43,043 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325441060 with entries=72, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325443013
2014-07-14 01:10:44,215 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3882, memsize=257.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/40e6228e39bb481ba44a3eba6eb70f8e
2014-07-14 01:10:44,235 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/40e6228e39bb481ba44a3eba6eb70f8e as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/40e6228e39bb481ba44a3eba6eb70f8e
2014-07-14 01:10:44,253 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/40e6228e39bb481ba44a3eba6eb70f8e, entries=938540, sequenceid=3882, filesize=66.8m
2014-07-14 01:10:44,253 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~257.8m/270292080, currentsize=151.1m/158444400 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 9235ms, sequenceid=3882, compaction requested=true
2014-07-14 01:10:44,254 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:10:44,254 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 20 blocking
2014-07-14 01:10:44,254 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-14 01:10:44,254 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:10:44,254 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:10:44,254 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:10:46,576 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3912, memsize=258.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/486364407d2e45ea81c727ffb16ca7d6
2014-07-14 01:10:46,588 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/486364407d2e45ea81c727ffb16ca7d6 as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/486364407d2e45ea81c727ffb16ca7d6
2014-07-14 01:10:46,598 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/486364407d2e45ea81c727ffb16ca7d6, entries=939620, sequenceid=3912, filesize=66.9m
2014-07-14 01:10:46,599 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~258.1m/270602240, currentsize=106.9m/112069280 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 8682ms, sequenceid=3912, compaction requested=true
2014-07-14 01:10:46,599 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:10:46,599 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 20 blocking
2014-07-14 01:10:46,599 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-14 01:10:46,600 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:10:46,600 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:10:46,600 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:10:49,664 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:10:50,009 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15944 synced till here 15943
2014-07-14 01:10:50,030 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325443013 with entries=90, filesize=77.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325449671
2014-07-14 01:10:50,031 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325406559
2014-07-14 01:10:50,031 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325409674
2014-07-14 01:10:50,031 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325414017
2014-07-14 01:10:50,031 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325415874
2014-07-14 01:10:50,031 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325417231
2014-07-14 01:10:50,031 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325419344
2014-07-14 01:10:50,840 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:10:50,841 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 256.7m
2014-07-14 01:10:51,009 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:10:51,284 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:10:51,527 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:10:51,528 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 256.4m
2014-07-14 01:10:51,645 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16040 synced till here 16038
2014-07-14 01:10:51,695 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:10:52,055 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325449671 with entries=96, filesize=82.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325451285
2014-07-14 01:10:53,109 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:10:53,264 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:10:53,739 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325451285 with entries=91, filesize=78.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325453109
2014-07-14 01:10:55,165 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:10:55,571 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325453109 with entries=78, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325455165
2014-07-14 01:10:55,997 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:10:56,440 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:10:56,978 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16295 synced till here 16294
2014-07-14 01:10:57,072 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325455165 with entries=86, filesize=73.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325456440
2014-07-14 01:10:57,746 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:10:58,091 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325456440 with entries=72, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325457746
2014-07-14 01:10:59,414 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:11:00,255 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16467 synced till here 16464
2014-07-14 01:11:00,256 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4011, memsize=242.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/d22704ec36824d23b68ef459fe739e3c
2014-07-14 01:11:00,271 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325457746 with entries=100, filesize=85.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325459415
2014-07-14 01:11:00,274 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/d22704ec36824d23b68ef459fe739e3c as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/d22704ec36824d23b68ef459fe739e3c
2014-07-14 01:11:00,290 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/d22704ec36824d23b68ef459fe739e3c, entries=883950, sequenceid=4011, filesize=62.9m
2014-07-14 01:11:00,295 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~256.7m/269188880, currentsize=179.6m/188357840 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 9454ms, sequenceid=4011, compaction requested=true
2014-07-14 01:11:00,296 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:11:00,296 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 20 blocking
2014-07-14 01:11:00,296 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-14 01:11:00,296 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:11:00,296 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 388.6m
2014-07-14 01:11:00,296 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:11:00,296 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:11:00,615 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:11:00,928 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4030, memsize=242.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/cb227f9914fb47e0b5857d617330f0c9
2014-07-14 01:11:00,953 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/cb227f9914fb47e0b5857d617330f0c9 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/cb227f9914fb47e0b5857d617330f0c9
2014-07-14 01:11:00,970 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/cb227f9914fb47e0b5857d617330f0c9, entries=882520, sequenceid=4030, filesize=62.8m
2014-07-14 01:11:00,971 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~256.4m/268805840, currentsize=186.8m/195888000 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 9444ms, sequenceid=4030, compaction requested=true
2014-07-14 01:11:00,971 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:11:00,971 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 20 blocking
2014-07-14 01:11:00,971 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-14 01:11:00,971 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 362.1m
2014-07-14 01:11:00,971 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:11:00,972 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:11:00,972 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:11:01,226 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:11:01,228 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:11:01,243 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16541 synced till here 16539
2014-07-14 01:11:01,272 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325459415 with entries=74, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325461228
2014-07-14 01:11:01,272 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325420807
2014-07-14 01:11:02,526 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:11:02,555 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16617 synced till here 16615
2014-07-14 01:11:02,607 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325461228 with entries=76, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325462526
2014-07-14 01:11:03,508 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:11:03,797 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:11:03,900 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:11:04,347 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325462526 with entries=86, filesize=73.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325463797
2014-07-14 01:11:12,023 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4135, memsize=359.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/03af0c61fd3440e0b5acebda2a31d26a
2014-07-14 01:11:12,045 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/03af0c61fd3440e0b5acebda2a31d26a as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/03af0c61fd3440e0b5acebda2a31d26a
2014-07-14 01:11:12,061 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/03af0c61fd3440e0b5acebda2a31d26a, entries=1308300, sequenceid=4135, filesize=93.2m
2014-07-14 01:11:12,062 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~391.7m/410701440, currentsize=86.7m/90942000 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 11766ms, sequenceid=4135, compaction requested=true
2014-07-14 01:11:12,063 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:11:12,063 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 20 blocking
2014-07-14 01:11:12,063 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-14 01:11:12,063 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 277.5m
2014-07-14 01:11:12,064 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:11:12,064 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:11:12,064 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:11:12,222 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:11:12,379 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4146, memsize=343.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/74ec5eece70647a289d3ce01c5ee5949
2014-07-14 01:11:12,394 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/74ec5eece70647a289d3ce01c5ee5949 as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/74ec5eece70647a289d3ce01c5ee5949
2014-07-14 01:11:12,410 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/74ec5eece70647a289d3ce01c5ee5949, entries=1249800, sequenceid=4146, filesize=89.0m
2014-07-14 01:11:12,410 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~362.1m/379702720, currentsize=73.2m/76737680 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 11439ms, sequenceid=4146, compaction requested=true
2014-07-14 01:11:12,411 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:11:12,411 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 20 blocking
2014-07-14 01:11:12,411 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-14 01:11:12,411 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 258.6m
2014-07-14 01:11:12,411 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:11:12,411 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:11:12,412 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:11:12,545 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:11:19,802 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4191, memsize=257.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/7aff4535d79849d2b33c0d755b2d54b2
2014-07-14 01:11:19,814 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4197, memsize=238.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/6af29fcae578490e822cda928cedacbe
2014-07-14 01:11:19,823 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/7aff4535d79849d2b33c0d755b2d54b2 as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/7aff4535d79849d2b33c0d755b2d54b2
2014-07-14 01:11:19,834 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/6af29fcae578490e822cda928cedacbe as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/6af29fcae578490e822cda928cedacbe
2014-07-14 01:11:19,843 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/7aff4535d79849d2b33c0d755b2d54b2, entries=937770, sequenceid=4191, filesize=66.8m
2014-07-14 01:11:19,843 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~277.5m/290993280, currentsize=0.0/0 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 7780ms, sequenceid=4191, compaction requested=true
2014-07-14 01:11:19,844 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:11:19,844 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 20 blocking
2014-07-14 01:11:19,844 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-14 01:11:19,844 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:11:19,845 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:11:19,845 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:11:19,856 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/6af29fcae578490e822cda928cedacbe, entries=866630, sequenceid=4197, filesize=61.7m
2014-07-14 01:11:19,857 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~258.6m/271212960, currentsize=0.0/0 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 7446ms, sequenceid=4197, compaction requested=true
2014-07-14 01:11:19,857 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:11:19,857 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 17 store files, 0 compacting, 17 eligible, 20 blocking
2014-07-14 01:11:19,858 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 17 files from compaction candidates
2014-07-14 01:11:19,858 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:11:19,858 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:11:19,858 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:11:23,352 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=10829, hits=3899, hitRatio=36.00%, , cachingAccesses=3903, cachingHits=3898, cachingHitsRatio=99.87%, evictions=0, evicted=3, evictedPerRun=Infinity
2014-07-14 01:11:28,348 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:11:28,366 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16785 synced till here 16774
2014-07-14 01:11:28,452 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325463797 with entries=82, filesize=70.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325488348
2014-07-14 01:11:28,453 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325422768
2014-07-14 01:11:28,453 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325435092
2014-07-14 01:11:28,453 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325437188
2014-07-14 01:11:28,453 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325438776
2014-07-14 01:11:28,453 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325440100
2014-07-14 01:11:28,453 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325441060
2014-07-14 01:11:28,453 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325443013
2014-07-14 01:11:28,453 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325449671
2014-07-14 01:11:28,453 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325451285
2014-07-14 01:11:28,453 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325453109
2014-07-14 01:11:28,453 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325455165
2014-07-14 01:11:28,454 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325456440
2014-07-14 01:11:28,454 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325457746
2014-07-14 01:11:29,270 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:11:29,620 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16880 synced till here 16878
2014-07-14 01:11:29,650 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325488348 with entries=95, filesize=81.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325489270
2014-07-14 01:11:30,867 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:11:30,991 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16964 synced till here 16962
2014-07-14 01:11:31,010 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325489270 with entries=84, filesize=72.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325490867
2014-07-14 01:11:32,134 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:11:32,154 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17037 synced till here 17036
2014-07-14 01:11:32,169 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325490867 with entries=73, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325492135
2014-07-14 01:11:34,091 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:11:34,121 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17122 synced till here 17110
2014-07-14 01:11:34,245 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:11:34,247 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 256.1m
2014-07-14 01:11:34,334 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325492135 with entries=85, filesize=72.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325494092
2014-07-14 01:11:34,391 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:11:35,871 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:11:35,872 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 256.4m
2014-07-14 01:11:35,985 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:11:36,005 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17198 synced till here 17195
2014-07-14 01:11:36,039 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325494092 with entries=76, filesize=65.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325495985
2014-07-14 01:11:36,057 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:11:36,583 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:11:36,961 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17272 synced till here 17270
2014-07-14 01:11:37,508 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325495985 with entries=74, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325496584
2014-07-14 01:11:38,338 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:11:38,643 DEBUG [RpcServer.handler=13,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:11:38,911 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17356 synced till here 17355
2014-07-14 01:11:38,990 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325496584 with entries=84, filesize=71.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325498338
2014-07-14 01:11:39,142 DEBUG [RpcServer.handler=13,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:11:40,552 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:11:40,598 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325498338 with entries=71, filesize=61.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325500553
2014-07-14 01:11:41,982 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:11:42,011 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17502 synced till here 17500
2014-07-14 01:11:42,077 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325500553 with entries=75, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325501983
2014-07-14 01:11:42,661 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4301, memsize=237.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/545364d9f6d0472eb6315a00f3d3603e
2014-07-14 01:11:42,682 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/545364d9f6d0472eb6315a00f3d3603e as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/545364d9f6d0472eb6315a00f3d3603e
2014-07-14 01:11:42,695 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/545364d9f6d0472eb6315a00f3d3603e, entries=864610, sequenceid=4301, filesize=61.6m
2014-07-14 01:11:42,695 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~256.1m/268554560, currentsize=148.7m/155874800 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 8448ms, sequenceid=4301, compaction requested=true
2014-07-14 01:11:42,696 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:11:42,696 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 17 store files, 0 compacting, 17 eligible, 20 blocking
2014-07-14 01:11:42,696 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 17 files from compaction candidates
2014-07-14 01:11:42,696 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 319.0m
2014-07-14 01:11:42,696 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:11:42,696 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:11:42,696 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:11:42,843 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:11:43,816 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4314, memsize=230.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/d5c2dd12bf14412ca77ce71d886edc7c
2014-07-14 01:11:43,830 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/d5c2dd12bf14412ca77ce71d886edc7c as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/d5c2dd12bf14412ca77ce71d886edc7c
2014-07-14 01:11:43,860 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/d5c2dd12bf14412ca77ce71d886edc7c, entries=837450, sequenceid=4314, filesize=59.6m
2014-07-14 01:11:43,861 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~259.6m/272206080, currentsize=145.5m/152587200 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 7989ms, sequenceid=4314, compaction requested=true
2014-07-14 01:11:43,861 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:11:43,862 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 17 store files, 0 compacting, 17 eligible, 20 blocking
2014-07-14 01:11:43,862 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 331.2m
2014-07-14 01:11:43,862 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 17 files from compaction candidates
2014-07-14 01:11:43,862 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:11:43,862 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:11:43,862 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:11:44,010 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:11:44,131 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:11:44,225 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17593 synced till here 17591
2014-07-14 01:11:44,252 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325501983 with entries=91, filesize=78.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325504010
2014-07-14 01:11:44,253 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325459415
2014-07-14 01:11:44,256 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325461228
2014-07-14 01:11:44,256 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325462526
2014-07-14 01:11:45,590 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:11:46,117 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17690 synced till here 17688
2014-07-14 01:11:46,135 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325504010 with entries=97, filesize=83.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325505591
2014-07-14 01:11:47,556 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:11:47,601 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17764 synced till here 17762
2014-07-14 01:11:47,632 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325505591 with entries=74, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325507557
2014-07-14 01:11:48,410 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:11:49,031 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:11:49,060 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17839 synced till here 17837
2014-07-14 01:11:49,103 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325507557 with entries=75, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325509032
2014-07-14 01:11:49,177 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:11:50,328 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:11:50,372 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17915 synced till here 17913
2014-07-14 01:11:50,414 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325509032 with entries=76, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325510328
2014-07-14 01:11:53,077 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4403, memsize=285.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/47f8e94e80f140c5874eb5a35f6c48b1
2014-07-14 01:11:53,091 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/47f8e94e80f140c5874eb5a35f6c48b1 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/47f8e94e80f140c5874eb5a35f6c48b1
2014-07-14 01:11:53,104 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/47f8e94e80f140c5874eb5a35f6c48b1, entries=1037540, sequenceid=4403, filesize=73.8m
2014-07-14 01:11:53,104 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~319.0m/334496400, currentsize=175.3m/183810880 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 10408ms, sequenceid=4403, compaction requested=true
2014-07-14 01:11:53,105 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:11:53,105 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 18 store files, 0 compacting, 18 eligible, 20 blocking
2014-07-14 01:11:53,105 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 18 files from compaction candidates
2014-07-14 01:11:53,105 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 324.4m
2014-07-14 01:11:53,105 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:11:53,105 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:11:53,105 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:11:53,273 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:11:54,730 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4406, memsize=298.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/9b719e3e484d4898819594d77493a323
2014-07-14 01:11:54,751 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/9b719e3e484d4898819594d77493a323 as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/9b719e3e484d4898819594d77493a323
2014-07-14 01:11:54,767 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/9b719e3e484d4898819594d77493a323, entries=1086850, sequenceid=4406, filesize=77.3m
2014-07-14 01:11:54,767 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~332.8m/349013840, currentsize=161.4m/169286480 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 10905ms, sequenceid=4406, compaction requested=true
2014-07-14 01:11:54,768 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:11:54,768 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 17 store files, 0 compacting, 17 eligible, 20 blocking
2014-07-14 01:11:54,768 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 17 files from compaction candidates
2014-07-14 01:11:54,768 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 307.1m
2014-07-14 01:11:54,768 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:11:54,768 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:11:54,768 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:11:54,937 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:12:03,182 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4511, memsize=324.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/8aaad0b169834cfd88f2d1bb5322cd2d
2014-07-14 01:12:03,242 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/8aaad0b169834cfd88f2d1bb5322cd2d as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/8aaad0b169834cfd88f2d1bb5322cd2d
2014-07-14 01:12:03,345 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/8aaad0b169834cfd88f2d1bb5322cd2d, entries=1181270, sequenceid=4511, filesize=84.1m
2014-07-14 01:12:03,345 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~324.4m/340196400, currentsize=0.0/0 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 10240ms, sequenceid=4511, compaction requested=true
2014-07-14 01:12:03,345 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:12:03,346 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 18 store files, 0 compacting, 18 eligible, 20 blocking
2014-07-14 01:12:03,346 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 18 files from compaction candidates
2014-07-14 01:12:03,346 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:12:03,346 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:12:03,346 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:12:03,794 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4513, memsize=307.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/9207af4c24b44c5d81256068cc3d44de
2014-07-14 01:12:03,810 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/9207af4c24b44c5d81256068cc3d44de as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/9207af4c24b44c5d81256068cc3d44de
2014-07-14 01:12:03,823 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/9207af4c24b44c5d81256068cc3d44de, entries=1117980, sequenceid=4513, filesize=79.6m
2014-07-14 01:12:03,824 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~307.1m/321970240, currentsize=4.6m/4852720 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 9055ms, sequenceid=4513, compaction requested=true
2014-07-14 01:12:03,824 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:12:03,824 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 18 store files, 0 compacting, 18 eligible, 20 blocking
2014-07-14 01:12:03,824 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 18 files from compaction candidates
2014-07-14 01:12:03,824 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:12:03,824 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:12:03,824 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:12:03,826 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:03,848 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325510328 with entries=73, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325523827
2014-07-14 01:12:03,849 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325463797
2014-07-14 01:12:03,849 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325488348
2014-07-14 01:12:03,849 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325489270
2014-07-14 01:12:03,849 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325490867
2014-07-14 01:12:03,849 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325492135
2014-07-14 01:12:03,849 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325494092
2014-07-14 01:12:03,850 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325495985
2014-07-14 01:12:03,850 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325496584
2014-07-14 01:12:03,850 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325498338
2014-07-14 01:12:03,850 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325500553
2014-07-14 01:12:07,865 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:07,893 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325523827 with entries=71, filesize=60.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325527865
2014-07-14 01:12:09,127 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:09,147 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18131 synced till here 18130
2014-07-14 01:12:09,157 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325527865 with entries=72, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325529127
2014-07-14 01:12:09,970 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:12:09,971 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 256.1m
2014-07-14 01:12:10,156 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:12:10,234 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:10,252 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18203 synced till here 18202
2014-07-14 01:12:10,263 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325529127 with entries=72, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325530234
2014-07-14 01:12:10,608 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:12:10,608 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 256.2m
2014-07-14 01:12:10,973 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:12:11,457 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:11,513 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18282 synced till here 18278
2014-07-14 01:12:11,548 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325530234 with entries=79, filesize=67.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325531458
2014-07-14 01:12:12,810 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:12,938 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18362 synced till here 18356
2014-07-14 01:12:12,984 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325531458 with entries=80, filesize=68.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325532903
2014-07-14 01:12:14,271 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:14,287 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18434 synced till here 18433
2014-07-14 01:12:14,359 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325532903 with entries=72, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325534271
2014-07-14 01:12:15,551 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:15,762 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18526 synced till here 18524
2014-07-14 01:12:15,781 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325534271 with entries=92, filesize=78.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325535552
2014-07-14 01:12:17,025 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:17,040 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18598 synced till here 18597
2014-07-14 01:12:17,052 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325535552 with entries=72, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325537026
2014-07-14 01:12:17,520 DEBUG [RpcServer.handler=24,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:12:17,590 DEBUG [RpcServer.handler=24,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:12:18,314 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:18,507 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18688 synced till here 18686
2014-07-14 01:12:18,530 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325537026 with entries=90, filesize=77.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325538314
2014-07-14 01:12:19,555 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:19,975 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4570, memsize=251.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/9f4f533e751c45e984569533553603db
2014-07-14 01:12:19,977 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18786 synced till here 18784
2014-07-14 01:12:19,988 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/9f4f533e751c45e984569533553603db as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/9f4f533e751c45e984569533553603db
2014-07-14 01:12:19,996 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/9f4f533e751c45e984569533553603db, entries=915780, sequenceid=4570, filesize=65.2m
2014-07-14 01:12:19,996 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~257.7m/270213760, currentsize=234.5m/245940320 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 10025ms, sequenceid=4570, compaction requested=true
2014-07-14 01:12:19,997 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:12:19,997 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 19 store files, 0 compacting, 19 eligible, 20 blocking
2014-07-14 01:12:19,997 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 19 files from compaction candidates
2014-07-14 01:12:19,998 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 315.7m
2014-07-14 01:12:19,998 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:12:19,998 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:12:19,998 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:12:20,013 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325538314 with entries=98, filesize=84.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325539556
2014-07-14 01:12:20,252 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:12:20,263 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4572, memsize=245.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/af012ea9f066452fb5c58f7f6a8c601f
2014-07-14 01:12:20,281 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/af012ea9f066452fb5c58f7f6a8c601f as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/af012ea9f066452fb5c58f7f6a8c601f
2014-07-14 01:12:20,298 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/af012ea9f066452fb5c58f7f6a8c601f, entries=893140, sequenceid=4572, filesize=63.6m
2014-07-14 01:12:20,298 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~256.2m/268656880, currentsize=226.1m/237131440 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 9690ms, sequenceid=4572, compaction requested=true
2014-07-14 01:12:20,298 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:12:20,298 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 18 store files, 0 compacting, 18 eligible, 20 blocking
2014-07-14 01:12:20,299 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 18 files from compaction candidates
2014-07-14 01:12:20,299 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 325.4m
2014-07-14 01:12:20,299 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:12:20,299 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:12:20,299 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:12:20,557 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:12:21,591 DEBUG [RpcServer.handler=39,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:12:21,814 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:21,828 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18861 synced till here 18858
2014-07-14 01:12:21,880 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325539556 with entries=75, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325541814
2014-07-14 01:12:21,880 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325501983
2014-07-14 01:12:21,881 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325504010
2014-07-14 01:12:21,881 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325505591
2014-07-14 01:12:21,881 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325507557
2014-07-14 01:12:21,881 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325509032
2014-07-14 01:12:22,188 DEBUG [RpcServer.handler=23,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:12:23,169 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:23,196 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18936 synced till here 18934
2014-07-14 01:12:23,222 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325541814 with entries=75, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325543170
2014-07-14 01:12:23,971 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:24,021 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325543170 with entries=73, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325543971
2014-07-14 01:12:25,057 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:25,474 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325543971 with entries=94, filesize=80.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325545058
2014-07-14 01:12:28,898 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4716, memsize=255.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/c2f06fc2cbbc46eeb44cef0473df2cd4
2014-07-14 01:12:28,915 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/c2f06fc2cbbc46eeb44cef0473df2cd4 as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/c2f06fc2cbbc46eeb44cef0473df2cd4
2014-07-14 01:12:28,931 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/c2f06fc2cbbc46eeb44cef0473df2cd4, entries=928780, sequenceid=4716, filesize=66.2m
2014-07-14 01:12:28,932 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~317.2m/332638880, currentsize=137.0m/143603760 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 8934ms, sequenceid=4716, compaction requested=true
2014-07-14 01:12:28,932 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:12:28,932 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 19 store files, 0 compacting, 19 eligible, 20 blocking
2014-07-14 01:12:28,932 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 371.2m
2014-07-14 01:12:28,932 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 19 files from compaction candidates
2014-07-14 01:12:28,933 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:12:28,933 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:12:28,933 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:12:29,140 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:12:29,533 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4725, memsize=264.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/add62d373b7b4a6fb391544c3b6a3335
2014-07-14 01:12:29,550 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/add62d373b7b4a6fb391544c3b6a3335 as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/add62d373b7b4a6fb391544c3b6a3335
2014-07-14 01:12:29,565 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/add62d373b7b4a6fb391544c3b6a3335, entries=962020, sequenceid=4725, filesize=68.5m
2014-07-14 01:12:29,565 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~328.5m/344414320, currentsize=126.0m/132152880 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 9266ms, sequenceid=4725, compaction requested=true
2014-07-14 01:12:29,566 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:12:29,566 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 19 store files, 0 compacting, 19 eligible, 20 blocking
2014-07-14 01:12:29,566 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 19 files from compaction candidates
2014-07-14 01:12:29,566 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:12:29,566 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 358.2m
2014-07-14 01:12:29,566 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:12:29,566 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:12:29,757 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:12:31,608 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:31,642 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325545058 with entries=72, filesize=61.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325551609
2014-07-14 01:12:31,642 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325510328
2014-07-14 01:12:31,642 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325523827
2014-07-14 01:12:31,642 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325527865
2014-07-14 01:12:35,002 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:35,029 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19248 synced till here 19247
2014-07-14 01:12:35,051 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325551609 with entries=73, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325555002
2014-07-14 01:12:36,312 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:36,351 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325555002 with entries=74, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325556313
2014-07-14 01:12:37,585 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:37,810 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19411 synced till here 19406
2014-07-14 01:12:37,846 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325556313 with entries=89, filesize=76.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325557585
2014-07-14 01:12:38,645 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:12:39,549 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:39,560 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:12:39,591 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19486 synced till here 19485
2014-07-14 01:12:39,636 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325557585 with entries=75, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325559550
2014-07-14 01:12:40,192 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4810, memsize=304.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/79c69e39de0246259b2ab8a25689e3d9
2014-07-14 01:12:40,210 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/79c69e39de0246259b2ab8a25689e3d9 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/79c69e39de0246259b2ab8a25689e3d9
2014-07-14 01:12:40,223 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/79c69e39de0246259b2ab8a25689e3d9, entries=1108350, sequenceid=4810, filesize=79.0m
2014-07-14 01:12:40,224 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~371.2m/389242640, currentsize=150.8m/158098720 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 11292ms, sequenceid=4810, compaction requested=true
2014-07-14 01:12:40,224 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:12:40,224 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 20 store files, 0 compacting, 20 eligible, 20 blocking
2014-07-14 01:12:40,225 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 20 files from compaction candidates
2014-07-14 01:12:40,225 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:12:40,225 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 286.9m
2014-07-14 01:12:40,225 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:12:40,225 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:12:40,290 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4804, memsize=291.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/bfbfe22cdb434c6e9f66c48b0ddbdb41
2014-07-14 01:12:40,311 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/bfbfe22cdb434c6e9f66c48b0ddbdb41 as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/bfbfe22cdb434c6e9f66c48b0ddbdb41
2014-07-14 01:12:40,327 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/bfbfe22cdb434c6e9f66c48b0ddbdb41, entries=1062010, sequenceid=4804, filesize=75.7m
2014-07-14 01:12:40,328 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~358.2m/375574400, currentsize=149.7m/156959200 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 10762ms, sequenceid=4804, compaction requested=true
2014-07-14 01:12:40,328 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:12:40,328 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 19 store files, 0 compacting, 19 eligible, 20 blocking
2014-07-14 01:12:40,329 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 19 files from compaction candidates
2014-07-14 01:12:40,329 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 277.1m
2014-07-14 01:12:40,329 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-14 01:12:40,329 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:12:40,329 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:12:40,390 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:12:40,484 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:12:42,708 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:42,758 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325559550 with entries=72, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325562709
2014-07-14 01:12:42,758 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325529127
2014-07-14 01:12:42,758 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325530234
2014-07-14 01:12:42,759 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325531458
2014-07-14 01:12:42,759 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325532903
2014-07-14 01:12:42,759 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325534271
2014-07-14 01:12:42,759 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325535552
2014-07-14 01:12:42,759 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325537026
2014-07-14 01:12:42,759 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325538314
2014-07-14 01:12:46,417 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:46,436 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19635 synced till here 19631
2014-07-14 01:12:46,484 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325562709 with entries=77, filesize=66.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325566417
2014-07-14 01:12:47,772 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:47,804 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325566417 with entries=73, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325567772
2014-07-14 01:12:48,739 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4904, memsize=248.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/c75689ec8ad34fe4a7160a9593878a47
2014-07-14 01:12:48,755 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/c75689ec8ad34fe4a7160a9593878a47 as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/c75689ec8ad34fe4a7160a9593878a47
2014-07-14 01:12:48,770 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/c75689ec8ad34fe4a7160a9593878a47, entries=906070, sequenceid=4904, filesize=64.5m
2014-07-14 01:12:48,770 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~277.1m/290606320, currentsize=93.4m/97980080 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 8441ms, sequenceid=4904, compaction requested=true
2014-07-14 01:12:48,770 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:12:48,771 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 20 store files, 0 compacting, 20 eligible, 20 blocking
2014-07-14 01:12:48,771 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 20 files from compaction candidates
2014-07-14 01:12:48,771 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:12:48,771 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:12:48,771 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:12:48,794 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4902, memsize=257.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/be21b8cd79254aabb13b3718f4845f8d
2014-07-14 01:12:48,806 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/be21b8cd79254aabb13b3718f4845f8d as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/be21b8cd79254aabb13b3718f4845f8d
2014-07-14 01:12:48,817 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/be21b8cd79254aabb13b3718f4845f8d, entries=936950, sequenceid=4902, filesize=66.7m
2014-07-14 01:12:48,818 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~286.9m/300847840, currentsize=95.0m/99616480 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 8593ms, sequenceid=4902, compaction requested=true
2014-07-14 01:12:48,818 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:12:48,818 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 20 store files, 0 compacting, 20 eligible, 20 blocking
2014-07-14 01:12:48,818 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 20 files from compaction candidates
2014-07-14 01:12:48,818 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:12:48,818 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:12:48,818 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:12:48,886 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:48,917 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325567772 with entries=71, filesize=60.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325568887
2014-07-14 01:12:48,917 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325539556
2014-07-14 01:12:48,917 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325541814
2014-07-14 01:12:48,917 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325543170
2014-07-14 01:12:48,918 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325543971
2014-07-14 01:12:49,395 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:12:49,395 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 256.0m
2014-07-14 01:12:49,815 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:12:49,844 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:12:49,845 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 256.9m
2014-07-14 01:12:50,030 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:12:50,288 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:50,729 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19881 synced till here 19880
2014-07-14 01:12:50,744 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325568887 with entries=102, filesize=87.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325570288
2014-07-14 01:12:52,105 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:52,141 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325570288 with entries=72, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325572105
2014-07-14 01:12:53,350 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:53,375 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325572105 with entries=72, filesize=61.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325573350
2014-07-14 01:12:54,539 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:54,561 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20100 synced till here 20099
2014-07-14 01:12:54,598 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325573350 with entries=75, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325574540
2014-07-14 01:12:55,977 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:12:56,254 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20186 synced till here 20185
2014-07-14 01:12:56,271 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325574540 with entries=86, filesize=73.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325575978
2014-07-14 01:12:56,312 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:12:56,357 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:12:58,393 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4976, memsize=249.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/79379bc4419b43a29ffb858058c388ea
2014-07-14 01:12:58,414 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/79379bc4419b43a29ffb858058c388ea as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/79379bc4419b43a29ffb858058c388ea
2014-07-14 01:12:58,425 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/79379bc4419b43a29ffb858058c388ea, entries=909690, sequenceid=4976, filesize=64.7m
2014-07-14 01:12:58,425 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~256.0m/268448560, currentsize=174.4m/182848880 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 9030ms, sequenceid=4976, compaction requested=true
2014-07-14 01:12:58,426 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:12:58,426 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 20 blocking
2014-07-14 01:12:58,426 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 278.6m
2014-07-14 01:12:58,426 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-14 01:12:58,426 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:12:58,426 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:12:58,427 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:12:58,567 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4972, memsize=253.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/1942ed93a451437ba93c349e0de62c6b
2014-07-14 01:12:58,583 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/1942ed93a451437ba93c349e0de62c6b as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/1942ed93a451437ba93c349e0de62c6b
2014-07-14 01:12:58,593 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/1942ed93a451437ba93c349e0de62c6b, entries=923300, sequenceid=4972, filesize=65.7m
2014-07-14 01:12:58,594 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~260.1m/272703760, currentsize=170.5m/178738080 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 8750ms, sequenceid=4972, compaction requested=true
2014-07-14 01:12:58,594 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:12:58,594 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 20 store files, 0 compacting, 20 eligible, 20 blocking
2014-07-14 01:12:58,594 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 20 files from compaction candidates
2014-07-14 01:12:58,594 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 281.2m
2014-07-14 01:12:58,594 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:12:58,594 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:12:58,594 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:12:58,597 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:12:58,746 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:13:06,553 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5083, memsize=280.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/d31972339cf34a59987257622e1310c3
2014-07-14 01:13:06,576 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/d31972339cf34a59987257622e1310c3 as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/d31972339cf34a59987257622e1310c3
2014-07-14 01:13:06,596 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/d31972339cf34a59987257622e1310c3, entries=1019540, sequenceid=5083, filesize=72.6m
2014-07-14 01:13:06,597 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~280.0m/293619040, currentsize=1.5m/1621440 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 8171ms, sequenceid=5083, compaction requested=true
2014-07-14 01:13:06,598 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:13:06,598 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 20 blocking
2014-07-14 01:13:06,598 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-14 01:13:06,598 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:13:06,598 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:13:06,598 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:13:06,752 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5086, memsize=281.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/afb97d0516ed4c70ad591bf6fd2c38f4
2014-07-14 01:13:06,769 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/afb97d0516ed4c70ad591bf6fd2c38f4 as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/afb97d0516ed4c70ad591bf6fd2c38f4
2014-07-14 01:13:06,785 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/afb97d0516ed4c70ad591bf6fd2c38f4, entries=1023780, sequenceid=5086, filesize=72.9m
2014-07-14 01:13:06,786 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~281.2m/294839760, currentsize=0.0/0 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 8192ms, sequenceid=5086, compaction requested=true
2014-07-14 01:13:06,786 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:13:06,786 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 20 blocking
2014-07-14 01:13:06,787 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-14 01:13:06,787 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:13:06,787 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:13:06,787 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:13:19,698 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:13:19,725 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325575978 with entries=72, filesize=61.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325599699
2014-07-14 01:13:19,725 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325545058
2014-07-14 01:13:19,725 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325551609
2014-07-14 01:13:19,725 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325555002
2014-07-14 01:13:19,725 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325556313
2014-07-14 01:13:19,725 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325557585
2014-07-14 01:13:19,725 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325559550
2014-07-14 01:13:19,726 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325562709
2014-07-14 01:13:19,726 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325566417
2014-07-14 01:13:19,726 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325567772
2014-07-14 01:13:22,640 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:13:23,105 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20342 synced till here 20340
2014-07-14 01:13:23,132 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325599699 with entries=84, filesize=71.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325602640
2014-07-14 01:13:24,181 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:13:24,196 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20416 synced till here 20414
2014-07-14 01:13:24,216 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325602640 with entries=74, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325604182
2014-07-14 01:13:25,519 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:13:25,519 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files; delaying flush up to 90000ms
2014-07-14 01:13:25,520 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:13:25,520 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 20 blocking
2014-07-14 01:13:25,520 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-14 01:13:25,520 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:13:25,520 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:13:25,520 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:13:25,927 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:13:25,927 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 257.4m
2014-07-14 01:13:26,009 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:13:26,034 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325604182 with entries=73, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325606010
2014-07-14 01:13:26,089 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:13:27,284 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:13:27,301 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20563 synced till here 20562
2014-07-14 01:13:27,318 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325606010 with entries=74, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325607285
2014-07-14 01:13:28,675 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:13:29,192 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325607285 with entries=90, filesize=77.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325608675
2014-07-14 01:13:30,423 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:13:30,438 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20726 synced till here 20725
2014-07-14 01:13:30,447 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325608675 with entries=73, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325610423
2014-07-14 01:13:32,262 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:13:32,289 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20800 synced till here 20799
2014-07-14 01:13:32,315 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325610423 with entries=74, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325612262
2014-07-14 01:13:34,069 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:13:34,787 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20894 synced till here 20887
2014-07-14 01:13:34,853 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325612262 with entries=94, filesize=80.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325614070
2014-07-14 01:13:35,012 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:13:35,012 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files; delaying flush up to 90000ms
2014-07-14 01:13:35,015 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:13:35,015 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 20 blocking
2014-07-14 01:13:35,015 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-14 01:13:35,015 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:13:35,015 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:13:35,015 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:13:35,047 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:13:35,047 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files; delaying flush up to 90000ms
2014-07-14 01:13:35,048 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:13:35,048 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 20 blocking
2014-07-14 01:13:35,048 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-14 01:13:35,048 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:13:35,048 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:13:35,048 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:13:35,254 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5139, memsize=254.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/0fc322baa6824c12b8ad63ef14d2ad05
2014-07-14 01:13:35,266 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/0fc322baa6824c12b8ad63ef14d2ad05 as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/0fc322baa6824c12b8ad63ef14d2ad05
2014-07-14 01:13:35,276 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/0fc322baa6824c12b8ad63ef14d2ad05, entries=925780, sequenceid=5139, filesize=65.9m
2014-07-14 01:13:35,277 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~257.4m/269870800, currentsize=177.1m/185733920 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 9350ms, sequenceid=5139, compaction requested=true
2014-07-14 01:13:35,277 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:13:35,277 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 20 blocking
2014-07-14 01:13:35,277 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-14 01:13:35,277 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:13:35,277 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:13:35,277 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:13:35,792 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:13:35,813 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20967 synced till here 20966
2014-07-14 01:13:35,824 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325614070 with entries=73, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325615792
2014-07-14 01:13:37,133 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:13:37,162 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21040 synced till here 21039
2014-07-14 01:13:37,188 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325615792 with entries=73, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325617134
2014-07-14 01:13:38,580 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:13:38,596 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21114 synced till here 21112
2014-07-14 01:13:38,620 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325617134 with entries=74, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325618580
2014-07-14 01:13:39,035 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:13:39,035 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files; delaying flush up to 90000ms
2014-07-14 01:13:39,036 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:13:39,036 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 20 blocking
2014-07-14 01:13:39,036 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-14 01:13:39,037 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:13:39,037 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:13:39,037 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:13:39,702 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:13:40,140 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21190 synced till here 21186
2014-07-14 01:13:40,196 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325618580 with entries=76, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325619702
2014-07-14 01:13:41,053 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:13:41,184 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21273 synced till here 21271
2014-07-14 01:13:41,204 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325619702 with entries=83, filesize=71.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325621054
2014-07-14 01:13:42,618 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:13:42,643 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325621054 with entries=73, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325622619
2014-07-14 01:13:44,748 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:13:44,778 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325622619 with entries=72, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325624748
2014-07-14 01:13:47,067 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:13:47,088 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21492 synced till here 21491
2014-07-14 01:13:47,121 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325624748 with entries=74, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325627068
2014-07-14 01:13:54,099 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:13:54,114 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21567 synced till here 21564
2014-07-14 01:13:54,161 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325627068 with entries=75, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325634099
2014-07-14 01:13:56,939 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:13:56,965 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21640 synced till here 21639
2014-07-14 01:13:57,235 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325634099 with entries=73, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325636941
2014-07-14 01:13:58,523 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:13:59,775 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21776 synced till here 21775
2014-07-14 01:13:59,794 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325636941 with entries=136, filesize=116.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325638523
2014-07-14 01:14:00,765 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:14:01,098 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21873 synced till here 21870
2014-07-14 01:14:01,161 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325638523 with entries=97, filesize=83.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325640765
2014-07-14 01:14:02,430 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:14:03,228 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21973 synced till here 21971
2014-07-14 01:14:03,262 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325640765 with entries=100, filesize=85.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325642430
2014-07-14 01:14:03,887 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:14:03,925 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325642430 with entries=73, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325643887
2014-07-14 01:14:06,235 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:14:06,263 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22120 synced till here 22118
2014-07-14 01:14:06,288 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325643887 with entries=74, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325646236
2014-07-14 01:14:10,154 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:14:10,515 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325646236 with entries=79, filesize=67.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325650155
2014-07-14 01:14:13,022 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:14:13,790 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22285 synced till here 22283
2014-07-14 01:14:13,998 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325650155 with entries=86, filesize=73.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325653022
2014-07-14 01:14:14,645 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:14:14,659 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22358 synced till here 22357
2014-07-14 01:14:14,682 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325653022 with entries=73, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325654645
2014-07-14 01:14:16,150 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:14:16,169 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22433 synced till here 22432
2014-07-14 01:14:16,181 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325654645 with entries=75, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325656150
2014-07-14 01:14:16,182 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:14:17,573 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:14:17,740 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325656150 with entries=82, filesize=70.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325657573
2014-07-14 01:14:17,741 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:14:19,152 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:14:19,171 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22592 synced till here 22587
2014-07-14 01:14:19,298 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325657573 with entries=77, filesize=66.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325659152
2014-07-14 01:14:19,299 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:14:20,955 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:14:20,989 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22682 synced till here 22669
2014-07-14 01:14:21,120 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:14:21,150 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325659152 with entries=90, filesize=77.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325660956
2014-07-14 01:14:21,150 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:14:21,177 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files, but is 1.1g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:14:21,177 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. due to global heap pressure
2014-07-14 01:14:21,178 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 1.1g
2014-07-14 01:14:21,663 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:14:21,663 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files, but is 968.9m vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:14:21,663 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. due to global heap pressure
2014-07-14 01:14:21,663 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 968.9m
2014-07-14 01:14:23,125 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:14:23,147 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22768 synced till here 22759
2014-07-14 01:14:23,207 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325660956 with entries=86, filesize=73.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325663125
2014-07-14 01:14:23,430 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:14:23,569 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:23,569 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:23,569 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:23,590 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:23,611 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:23,622 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:23,626 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:14:23,634 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:23,673 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:23,721 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:23,751 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:23,804 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:23,865 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:23,972 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:24,021 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:24,056 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:24,102 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:24,132 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:24,178 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:24,214 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:24,268 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:24,315 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:24,351 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:24,404 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:24,436 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:24,480 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:24,520 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:24,565 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:24,594 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:24,648 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:24,804 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:24,835 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:24,866 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:24,896 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:26,331 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:26,351 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:26,360 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:26,389 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:26,422 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:26,473 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:26,535 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:26,585 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:26,636 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:26,694 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:26,747 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:26,792 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:26,837 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:26,896 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:26,950 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:27,000 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:27,049 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:14:28,569 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:28,569 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:28,570 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:28,591 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:14:28,612 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:14:28,623 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:14:28,634 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:28,674 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:14:28,722 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:14:28,751 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:28,804 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:28,865 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:28,972 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:14:29,021 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:29,056 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:29,103 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:14:29,132 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:29,178 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:14:29,215 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:14:29,268 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:29,316 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:29,351 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:29,404 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:29,437 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:29,480 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:29,521 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:29,566 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:14:29,594 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:29,649 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:29,804 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:29,836 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:14:29,866 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:14:29,896 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:31,331 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:31,351 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:31,360 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:31,389 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:31,423 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:14:31,473 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:31,536 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:14:31,585 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:31,636 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:31,694 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:31,747 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:31,792 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:31,837 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:31,896 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:31,950 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:32,000 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:32,050 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:14:33,570 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:14:33,570 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:33,570 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:33,591 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:33,612 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:33,623 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:33,635 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:14:33,674 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:33,722 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:33,751 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:14:33,805 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:33,866 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:33,973 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:34,022 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:34,056 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:14:34,103 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:34,133 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:34,179 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:34,215 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:34,269 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:34,316 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:34,352 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:34,405 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:14:34,437 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:34,480 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:14:34,521 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:35,855 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10959ms
2014-07-14 01:14:35,855 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1205ms
GC pool 'ParNew' had collection(s): count=1 time=1331ms
2014-07-14 01:14:35,857 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11292ms
2014-07-14 01:14:35,857 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11209ms
2014-07-14 01:14:35,857 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11263ms
2014-07-14 01:14:35,857 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11053ms
2014-07-14 01:14:35,858 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11022ms
2014-07-14 01:14:35,858 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10993ms
2014-07-14 01:14:36,332 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:36,351 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:14:36,361 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:36,389 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:14:36,423 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:36,473 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:14:36,537 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:36,586 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:36,636 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:14:36,695 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:36,748 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:36,792 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:14:36,838 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:36,896 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:14:36,950 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:14:37,001 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:37,050 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:14:38,570 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:14:38,571 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:14:38,571 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:14:38,591 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:14:38,613 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:14:38,624 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:14:38,635 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:14:38,674 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:14:38,723 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:14:38,752 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:14:38,805 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:14:38,867 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:14:38,973 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:14:39,022 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:14:39,057 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 01:14:39,104 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:14:39,133 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:14:39,179 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:14:39,215 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:14:39,269 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:14:39,316 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:14:39,353 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:14:39,405 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:14:39,438 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:14:39,481 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:14:39,521 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:14:41,252 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16387ms
2014-07-14 01:14:41,252 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16356ms
2014-07-14 01:14:41,252 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16687ms
2014-07-14 01:14:41,252 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16604ms
2014-07-14 01:14:41,253 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16449ms
2014-07-14 01:14:41,253 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16659ms
2014-07-14 01:14:41,253 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16418ms
2014-07-14 01:14:41,332 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:14:41,352 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:14:41,361 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:14:41,390 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 01:14:41,424 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:14:41,474 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:14:41,537 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:14:41,586 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:14:41,637 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:14:41,695 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:14:41,748 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:14:41,792 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 01:14:41,838 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:14:41,897 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:14:41,951 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:14:42,001 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:14:42,050 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:14:43,571 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:14:43,571 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:14:43,571 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:14:43,591 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:14:43,613 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:14:43,624 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:14:43,635 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:14:43,675 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:14:43,723 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:14:43,752 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:14:43,805 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:14:43,867 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:14:43,973 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:14:44,022 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:14:44,057 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:14:44,104 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:14:44,134 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:14:44,180 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:14:44,216 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:14:44,270 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:14:44,317 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:14:44,353 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:14:44,405 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:14:44,438 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:14:44,481 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:14:44,522 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:14:46,488 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20015ms
2014-07-14 01:14:46,489 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20158ms
2014-07-14 01:14:46,489 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21841ms
2014-07-14 01:14:46,490 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21593ms
2014-07-14 01:14:46,490 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20139ms
2014-07-14 01:14:46,490 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20130ms
2014-07-14 01:14:46,490 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20101ms
2014-07-14 01:14:46,491 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20069ms
2014-07-14 01:14:46,491 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21626ms
2014-07-14 01:14:46,492 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21927ms
2014-07-14 01:14:46,492 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21688ms
2014-07-14 01:14:46,492 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21898ms
2014-07-14 01:14:46,493 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21658ms
2014-07-14 01:14:46,537 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:14:46,587 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:14:46,637 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:14:46,695 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:14:46,748 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:14:46,793 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:14:46,838 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:14:46,897 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:14:46,951 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:14:47,002 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:14:47,050 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:14:48,571 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:14:48,572 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25003ms
2014-07-14 01:14:48,572 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25003ms
2014-07-14 01:14:48,592 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:14:48,614 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:14:48,624 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:14:48,636 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:14:48,675 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:14:48,723 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:14:48,753 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:14:48,805 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:14:48,867 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:14:48,974 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25003ms
2014-07-14 01:14:49,023 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:14:49,057 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:14:49,104 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:14:49,134 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:14:49,180 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25003ms
2014-07-14 01:14:49,216 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:14:49,270 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:14:49,317 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:14:49,354 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25003ms
2014-07-14 01:14:49,405 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:14:49,438 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:14:49,481 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:14:49,522 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:14:51,489 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25016ms
2014-07-14 01:14:51,489 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25158ms
2014-07-14 01:14:51,490 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26841ms
2014-07-14 01:14:51,490 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26594ms
2014-07-14 01:14:51,490 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25139ms
2014-07-14 01:14:51,490 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25130ms
2014-07-14 01:14:51,491 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25102ms
2014-07-14 01:14:51,491 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25069ms
2014-07-14 01:14:51,492 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26627ms
2014-07-14 01:14:51,492 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26927ms
2014-07-14 01:14:51,493 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26689ms
2014-07-14 01:14:51,493 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26899ms
2014-07-14 01:14:51,494 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26658ms
2014-07-14 01:14:51,538 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25003ms
2014-07-14 01:14:52,461 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25412ms
2014-07-14 01:14:52,462 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25715ms
2014-07-14 01:14:52,462 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25670ms
2014-07-14 01:14:52,462 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25625ms
2014-07-14 01:14:52,463 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25566ms
2014-07-14 01:14:52,463 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25513ms
2014-07-14 01:14:52,463 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25463ms
2014-07-14 01:14:52,465 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25771ms
2014-07-14 01:14:52,465 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25880ms
2014-07-14 01:14:52,466 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25829ms
2014-07-14 01:14:53,572 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30003ms
2014-07-14 01:14:53,572 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30003ms
2014-07-14 01:14:53,572 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30003ms
2014-07-14 01:14:53,592 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:14:53,614 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30003ms
2014-07-14 01:14:53,624 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:14:53,636 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:14:53,675 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:14:53,723 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:14:53,753 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:14:53,806 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:14:53,868 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30003ms
2014-07-14 01:14:53,974 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30003ms
2014-07-14 01:14:54,023 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:14:54,057 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30001ms
2014-07-14 01:14:54,104 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:14:54,134 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:14:54,180 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30003ms
2014-07-14 01:14:54,216 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:14:54,270 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:14:54,317 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:14:54,354 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30003ms
2014-07-14 01:14:54,406 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30001ms
2014-07-14 01:14:54,439 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:14:54,482 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:14:54,522 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:14:56,490 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30016ms
2014-07-14 01:14:56,490 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30159ms
2014-07-14 01:14:56,491 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31842ms
2014-07-14 01:14:56,491 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30131ms
2014-07-14 01:14:56,491 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30140ms
2014-07-14 01:14:56,491 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31595ms
2014-07-14 01:14:56,492 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30070ms
2014-07-14 01:14:56,492 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30103ms
2014-07-14 01:14:56,492 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31627ms
2014-07-14 01:14:56,493 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31928ms
2014-07-14 01:14:56,493 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31689ms
2014-07-14 01:14:56,494 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31899ms
2014-07-14 01:14:56,494 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31659ms
2014-07-14 01:14:56,538 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30003ms
2014-07-14 01:14:56,620 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5711, memsize=817.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/4843b47b64a241da818cc9c7061375d6
2014-07-14 01:14:56,644 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/4843b47b64a241da818cc9c7061375d6 as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/4843b47b64a241da818cc9c7061375d6
2014-07-14 01:14:56,662 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/4843b47b64a241da818cc9c7061375d6, entries=2975030, sequenceid=5711, filesize=211.7m
2014-07-14 01:14:56,663 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~973.6m/1020877040, currentsize=21.8m/22889600 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 35000ms, sequenceid=5711, compaction requested=true
2014-07-14 01:14:56,663 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:14:56,663 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 20 blocking
2014-07-14 01:14:56,663 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30128ms
2014-07-14 01:14:56,663 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-14 01:14:56,664 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,664 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 91145ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:14:56,664 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31829ms
2014-07-14 01:14:56,664 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:14:56,664 DEBUG [MemStoreFlusher.0] regionserver.HRegion: NOT flushing memstore for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., flushing=true, writesEnabled=true
2014-07-14 01:14:56,664 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,664 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:14:56,664 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:14:56,665 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32071ms
2014-07-14 01:14:56,665 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,665 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31861ms
2014-07-14 01:14:56,665 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,665 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32100ms
2014-07-14 01:14:56,665 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,665 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31800ms
2014-07-14 01:14:56,665 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,665 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30276ms
2014-07-14 01:14:56,665 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,666 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30244ms
2014-07-14 01:14:56,666 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,673 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31777ms
2014-07-14 01:14:56,673 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,673 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30322ms
2014-07-14 01:14:56,673 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,673 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30313ms
2014-07-14 01:14:56,673 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,673 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32025ms
2014-07-14 01:14:56,673 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,677 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30346ms
2014-07-14 01:14:56,677 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,677 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30204ms
2014-07-14 01:14:56,677 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,677 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32157ms
2014-07-14 01:14:56,678 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,678 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32198ms
2014-07-14 01:14:56,678 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,678 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32242ms
2014-07-14 01:14:56,678 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,678 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32274ms
2014-07-14 01:14:56,678 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,678 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32327ms
2014-07-14 01:14:56,678 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,680 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32365ms
2014-07-14 01:14:56,680 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,680 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32412ms
2014-07-14 01:14:56,680 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,681 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32467ms
2014-07-14 01:14:56,681 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,681 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32504ms
2014-07-14 01:14:56,681 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,686 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32554ms
2014-07-14 01:14:56,686 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,689 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32587ms
2014-07-14 01:14:56,689 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,694 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32638ms
2014-07-14 01:14:56,694 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,694 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32673ms
2014-07-14 01:14:56,694 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,694 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32723ms
2014-07-14 01:14:56,694 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,694 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32829ms
2014-07-14 01:14:56,695 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,696 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32892ms
2014-07-14 01:14:56,697 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,697 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32946ms
2014-07-14 01:14:56,697 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,701 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32980ms
2014-07-14 01:14:56,701 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,701 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33028ms
2014-07-14 01:14:56,701 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,709 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33075ms
2014-07-14 01:14:56,709 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,709 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33087ms
2014-07-14 01:14:56,709 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,711 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33100ms
2014-07-14 01:14:56,711 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,717 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33127ms
2014-07-14 01:14:56,717 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,717 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33148ms
2014-07-14 01:14:56,717 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,719 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33150ms
2014-07-14 01:14:56,719 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,719 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33150ms
2014-07-14 01:14:56,719 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,719 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30083ms
2014-07-14 01:14:56,719 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,719 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30134ms
2014-07-14 01:14:56,720 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,721 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30026ms
2014-07-14 01:14:56,721 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,722 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 29721ms
2014-07-14 01:14:56,722 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,725 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 29775ms
2014-07-14 01:14:56,725 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,727 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 29830ms
2014-07-14 01:14:56,727 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,728 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 29890ms
2014-07-14 01:14:56,728 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,729 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 29937ms
2014-07-14 01:14:56,729 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,731 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 29984ms
2014-07-14 01:14:56,731 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,731 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 29682ms
2014-07-14 01:14:56,731 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:14:56,901 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33501,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325663398,"queuetimems":0,"class":"HRegionServer","responsesize":15749,"method":"Multi"}
2014-07-14 01:14:56,982 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33707,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325663274,"queuetimems":0,"class":"HRegionServer","responsesize":15701,"method":"Multi"}
2014-07-14 01:14:57,100 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:14:57,126 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22873 synced till here 22849
2014-07-14 01:14:57,911 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34465,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325663444,"queuetimems":1,"class":"HRegionServer","responsesize":15494,"method":"Multi"}
2014-07-14 01:14:57,932 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325663125 with entries=105, filesize=89.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325697101
2014-07-14 01:14:57,965 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34434,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325663531,"queuetimems":0,"class":"HRegionServer","responsesize":15763,"method":"Multi"}
2014-07-14 01:14:57,965 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34480,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325663485,"queuetimems":1,"class":"HRegionServer","responsesize":15963,"method":"Multi"}
2014-07-14 01:14:58,127 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31596,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325666530,"queuetimems":0,"class":"HRegionServer","responsesize":15642,"method":"Multi"}
2014-07-14 01:14:58,711 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33877,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325664833,"queuetimems":1,"class":"HRegionServer","responsesize":15732,"method":"Multi"}
2014-07-14 01:14:58,717 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33853,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325664864,"queuetimems":1,"class":"HRegionServer","responsesize":15607,"method":"Multi"}
2014-07-14 01:14:58,930 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:14:58,934 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34340,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325664592,"queuetimems":0,"class":"HRegionServer","responsesize":15397,"method":"Multi"}
2014-07-14 01:14:59,801 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22985 synced till here 22952
2014-07-14 01:14:59,862 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":35059,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325664802,"queuetimems":0,"class":"HRegionServer","responsesize":15532,"method":"Multi"}
2014-07-14 01:15:00,006 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":35443,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325664563,"queuetimems":0,"class":"HRegionServer","responsesize":16193,"method":"Multi"}
2014-07-14 01:15:00,006 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36287,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325663719,"queuetimems":0,"class":"HRegionServer","responsesize":15636,"method":"Multi"}
2014-07-14 01:15:00,007 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":35113,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325664894,"queuetimems":0,"class":"HRegionServer","responsesize":16102,"method":"Multi"}
2014-07-14 01:15:00,014 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":35367,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325664646,"queuetimems":0,"class":"HRegionServer","responsesize":15765,"method":"Multi"}
2014-07-14 01:15:00,014 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33656,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325666358,"queuetimems":0,"class":"HRegionServer","responsesize":16198,"method":"Multi"}
2014-07-14 01:15:00,014 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33592,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325666421,"queuetimems":1,"class":"HRegionServer","responsesize":15548,"method":"Multi"}
2014-07-14 01:15:00,014 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33664,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325666349,"queuetimems":1,"class":"HRegionServer","responsesize":15874,"method":"Multi"}
2014-07-14 01:15:00,014 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33626,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325666387,"queuetimems":0,"class":"HRegionServer","responsesize":15590,"method":"Multi"}
2014-07-14 01:15:00,032 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325697101 with entries=112, filesize=95.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325698931
2014-07-14 01:15:00,324 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33742,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325666581,"queuetimems":1,"class":"HRegionServer","responsesize":15955,"method":"Multi"}
2014-07-14 01:15:00,324 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36691,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325663632,"queuetimems":0,"class":"HRegionServer","responsesize":15731,"method":"Multi"}
2014-07-14 01:15:00,324 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":35132,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325665191,"queuetimems":0,"class":"HRegionServer","responsesize":15700,"method":"Multi"}
2014-07-14 01:15:00,326 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36013,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325664313,"queuetimems":0,"class":"HRegionServer","responsesize":15773,"method":"Multi"}
2014-07-14 01:15:00,328 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33636,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325666692,"queuetimems":0,"class":"HRegionServer","responsesize":15864,"method":"Multi"}
2014-07-14 01:15:00,329 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33857,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325666471,"queuetimems":0,"class":"HRegionServer","responsesize":15715,"method":"Multi"}
2014-07-14 01:15:00,338 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36368,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325663969,"queuetimems":0,"class":"HRegionServer","responsesize":16085,"method":"Multi"}
2014-07-14 01:15:00,349 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":35915,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325664434,"queuetimems":0,"class":"HRegionServer","responsesize":15831,"method":"Multi"}
2014-07-14 01:15:00,357 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33410,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325666947,"queuetimems":0,"class":"HRegionServer","responsesize":15963,"method":"Multi"}
2014-07-14 01:15:00,357 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36008,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325664349,"queuetimems":0,"class":"HRegionServer","responsesize":15786,"method":"Multi"}
2014-07-14 01:15:00,357 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":35955,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325664402,"queuetimems":0,"class":"HRegionServer","responsesize":16050,"method":"Multi"}
2014-07-14 01:15:00,358 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36227,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325664130,"queuetimems":0,"class":"HRegionServer","responsesize":15582,"method":"Multi"}
2014-07-14 01:15:00,361 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36307,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325664054,"queuetimems":1,"class":"HRegionServer","responsesize":15732,"method":"Multi"}
2014-07-14 01:15:00,494 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33708,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325666784,"queuetimems":0,"class":"HRegionServer","responsesize":15879,"method":"Multi"}
2014-07-14 01:15:00,494 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36905,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325663588,"queuetimems":0,"class":"HRegionServer","responsesize":15840,"method":"Multi"}
2014-07-14 01:15:00,494 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33860,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325666633,"queuetimems":1,"class":"HRegionServer","responsesize":15759,"method":"Multi"}
2014-07-14 01:15:00,494 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36690,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325663802,"queuetimems":0,"class":"HRegionServer","responsesize":15856,"method":"Multi"}
2014-07-14 01:15:00,494 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33754,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325666738,"queuetimems":0,"class":"HRegionServer","responsesize":15837,"method":"Multi"}
2014-07-14 01:15:00,494 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33601,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325666893,"queuetimems":1,"class":"HRegionServer","responsesize":15749,"method":"Multi"}
2014-07-14 01:15:00,494 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36475,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325664018,"queuetimems":0,"class":"HRegionServer","responsesize":15911,"method":"Multi"}
2014-07-14 01:15:00,494 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36317,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325664176,"queuetimems":1,"class":"HRegionServer","responsesize":15623,"method":"Multi"}
2014-07-14 01:15:00,498 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36826,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325663672,"queuetimems":0,"class":"HRegionServer","responsesize":15864,"method":"Multi"}
2014-07-14 01:15:00,504 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36640,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325663863,"queuetimems":0,"class":"HRegionServer","responsesize":15721,"method":"Multi"}
2014-07-14 01:15:00,504 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33670,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325666833,"queuetimems":0,"class":"HRegionServer","responsesize":15727,"method":"Multi"}
2014-07-14 01:15:00,505 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33458,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325667047,"queuetimems":1,"class":"HRegionServer","responsesize":15731,"method":"Multi"}
2014-07-14 01:15:00,511 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36298,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325664212,"queuetimems":0,"class":"HRegionServer","responsesize":15647,"method":"Multi"}
2014-07-14 01:15:00,513 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36413,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325664100,"queuetimems":0,"class":"HRegionServer","responsesize":15551,"method":"Multi"}
2014-07-14 01:15:00,698 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:15:00,714 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36447,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325664266,"queuetimems":0,"class":"HRegionServer","responsesize":15868,"method":"Multi"}
2014-07-14 01:15:00,717 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33721,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325666996,"queuetimems":0,"class":"HRegionServer","responsesize":15840,"method":"Multi"}
2014-07-14 01:15:00,733 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36255,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325664478,"queuetimems":0,"class":"HRegionServer","responsesize":15984,"method":"Multi"}
2014-07-14 01:15:00,739 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36220,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325664518,"queuetimems":0,"class":"HRegionServer","responsesize":15447,"method":"Multi"}
2014-07-14 01:15:01,442 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23104 synced till here 23059
2014-07-14 01:15:01,774 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325698931 with entries=119, filesize=100.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325700698
2014-07-14 01:15:02,162 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":38413,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325663749,"queuetimems":0,"class":"HRegionServer","responsesize":15866,"method":"Multi"}
2014-07-14 01:15:02,536 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:15:02,581 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23211 synced till here 23181
2014-07-14 01:15:03,354 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325700698 with entries=107, filesize=91.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325702536
2014-07-14 01:15:04,233 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:15:04,447 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23302 synced till here 23287
2014-07-14 01:15:04,611 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325702536 with entries=91, filesize=77.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325704234
2014-07-14 01:15:04,832 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:15:04,833 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files, but is 1.1g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:15:04,833 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. due to global heap pressure
2014-07-14 01:15:04,833 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 1.1g
2014-07-14 01:15:05,945 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:15:05,973 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:15:06,085 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5712, memsize=991.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/e6b2ad0f4a7f4f0aa15e27e65071cc5c
2014-07-14 01:15:06,143 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/e6b2ad0f4a7f4f0aa15e27e65071cc5c as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/e6b2ad0f4a7f4f0aa15e27e65071cc5c
2014-07-14 01:15:06,173 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23429 synced till here 23415
2014-07-14 01:15:06,191 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/e6b2ad0f4a7f4f0aa15e27e65071cc5c, entries=3610550, sequenceid=5712, filesize=256.9m
2014-07-14 01:15:06,208 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.1g/1197906880, currentsize=275.9m/289278640 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 45030ms, sequenceid=5712, compaction requested=true
2014-07-14 01:15:06,209 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:15:06,209 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 91197ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:15:06,209 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 20 blocking
2014-07-14 01:15:06,209 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 269.2m
2014-07-14 01:15:06,209 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-14 01:15:06,209 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:15:06,209 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:15:06,209 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:15:06,371 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325704234 with entries=127, filesize=108.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325705945
2014-07-14 01:15:06,371 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325568887
2014-07-14 01:15:06,371 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325570288
2014-07-14 01:15:06,372 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325572105
2014-07-14 01:15:06,372 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325573350
2014-07-14 01:15:06,372 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325574540
2014-07-14 01:15:06,593 DEBUG [RpcServer.handler=5,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:15:06,717 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:15:07,273 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:15:07,521 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:15:07,533 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23537 synced till here 23504
2014-07-14 01:15:07,830 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325705945 with entries=108, filesize=92.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325707521
2014-07-14 01:15:09,275 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:15:09,303 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23646 synced till here 23619
2014-07-14 01:15:09,497 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325707521 with entries=109, filesize=92.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325709275
2014-07-14 01:15:10,744 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:15:10,920 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23730 synced till here 23720
2014-07-14 01:15:11,045 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325709275 with entries=84, filesize=72.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325710744
2014-07-14 01:15:12,683 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:15:12,722 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23831 synced till here 23807
2014-07-14 01:15:12,917 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325710744 with entries=101, filesize=86.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325712684
2014-07-14 01:15:13,357 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5886, memsize=85.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/c937fbda2f65498dbead541e9af2ec75
2014-07-14 01:15:13,379 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/c937fbda2f65498dbead541e9af2ec75 as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/c937fbda2f65498dbead541e9af2ec75
2014-07-14 01:15:13,395 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/c937fbda2f65498dbead541e9af2ec75, entries=311750, sequenceid=5886, filesize=22.2m
2014-07-14 01:15:13,395 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~270.7m/283893040, currentsize=175.7m/184207280 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 7186ms, sequenceid=5886, compaction requested=true
2014-07-14 01:15:13,396 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:15:13,396 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 20 blocking
2014-07-14 01:15:13,396 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files; delaying flush up to 90000ms
2014-07-14 01:15:13,396 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-14 01:15:13,396 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-14 01:15:13,396 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:15:13,396 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files; delaying flush up to 90000ms
2014-07-14 01:15:13,396 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:15:13,396 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:2), split_queue=0, merge_queue=0
2014-07-14 01:15:13,396 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:15:13,396 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 20 blocking
2014-07-14 01:15:13,396 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-14 01:15:13,396 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:15:13,397 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:15:13,397 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:15:13,397 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 20 blocking
2014-07-14 01:15:13,397 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-14 01:15:13,397 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:15:13,397 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:15:13,397 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:15:13,622 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:15:14,545 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23932 synced till here 23921
2014-07-14 01:15:14,617 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325712684 with entries=101, filesize=86.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325713623
2014-07-14 01:15:15,270 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:15:15,414 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24037 synced till here 24005
2014-07-14 01:15:15,671 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325713623 with entries=105, filesize=90.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325715270
2014-07-14 01:15:17,304 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:15:17,407 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:15:17,407 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files; delaying flush up to 90000ms
2014-07-14 01:15:17,413 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 20 blocking
2014-07-14 01:15:17,413 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-14 01:15:17,413 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:15:17,413 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:15:17,413 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:15:17,414 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:15:17,726 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24157 synced till here 24153
2014-07-14 01:15:17,856 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325715270 with entries=120, filesize=102.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325717304
2014-07-14 01:15:19,301 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:15:19,302 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files, but is 1.4g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:15:19,302 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. due to global heap pressure
2014-07-14 01:15:19,302 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 1.4g
2014-07-14 01:15:19,614 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:15:19,717 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24270 synced till here 24255
2014-07-14 01:15:19,898 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325717304 with entries=113, filesize=96.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325719614
2014-07-14 01:15:21,512 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:15:21,538 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24369 synced till here 24347
2014-07-14 01:15:21,700 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,701 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,702 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,702 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,711 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,712 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,713 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,713 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,716 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,716 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,717 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,718 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,724 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,732 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,733 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,735 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,737 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,738 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,738 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,739 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,739 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,740 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,740 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,741 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,742 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,746 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,747 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,747 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,750 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,750 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,751 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,758 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,761 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325719614 with entries=99, filesize=84.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325721514
2014-07-14 01:15:21,818 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,819 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,821 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:21,867 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:22,311 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:15:22,976 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:23,015 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:23,056 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:23,088 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:23,122 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:23,164 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:23,211 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:23,240 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:23,282 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:23,319 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:23,365 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:23,410 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:23,444 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:23,485 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:15:26,700 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:15:26,701 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:15:26,702 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:15:26,703 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:15:26,712 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:15:26,713 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:15:26,713 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5011ms
2014-07-14 01:15:26,714 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:15:26,716 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:15:26,716 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5004ms
2014-07-14 01:15:26,718 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:15:26,718 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:15:26,725 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:15:26,733 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:15:26,734 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:15:26,735 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:15:26,738 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:15:26,739 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:15:26,739 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:15:26,740 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5003ms
2014-07-14 01:15:26,740 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5016ms
2014-07-14 01:15:26,740 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-14 01:15:26,740 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5024ms
2014-07-14 01:15:26,742 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:15:26,742 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:15:26,747 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5005ms
2014-07-14 01:15:26,747 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5005ms
2014-07-14 01:15:26,747 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:15:26,750 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:15:26,750 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:15:26,751 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:15:26,758 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:15:26,818 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:15:26,820 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:15:26,822 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:15:26,868 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:15:27,977 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:15:28,016 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:15:28,057 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:15:28,088 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:15:28,122 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:15:28,164 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:15:28,211 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:15:28,240 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:15:28,282 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:15:28,319 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:15:28,365 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:15:28,410 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:15:28,444 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:15:28,485 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:15:31,701 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:31,703 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:15:31,703 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:15:31,703 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:31,713 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:31,714 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:31,714 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10012ms
2014-07-14 01:15:31,714 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:31,717 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:15:31,717 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10005ms
2014-07-14 01:15:31,718 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:31,719 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:31,725 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:31,734 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:15:31,734 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:31,736 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:31,739 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:15:31,740 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:31,740 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:31,740 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-14 01:15:31,741 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-14 01:15:31,741 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10017ms
2014-07-14 01:15:31,742 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10025ms
2014-07-14 01:15:31,742 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:31,743 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:31,747 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10005ms
2014-07-14 01:15:31,747 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10005ms
2014-07-14 01:15:31,748 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:31,750 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:31,751 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:31,751 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:15:31,759 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:15:31,819 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:15:31,820 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:31,823 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:15:31,868 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:32,977 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:33,017 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:33,057 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:33,088 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:15:33,123 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:15:33,165 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:33,212 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:33,241 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:33,283 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:15:33,320 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:33,365 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:15:33,411 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:33,445 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:33,486 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:15:36,703 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15003ms
2014-07-14 01:15:36,703 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:15:36,703 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:15:36,704 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:15:36,713 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:15:36,714 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:15:36,715 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15012ms
2014-07-14 01:15:36,715 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:15:36,717 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:15:36,717 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15005ms
2014-07-14 01:15:36,718 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:15:36,719 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:15:36,725 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:15:36,734 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:15:36,734 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:15:36,736 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:15:36,739 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:15:36,740 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:15:36,740 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:15:36,741 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15004ms
2014-07-14 01:15:36,741 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15003ms
2014-07-14 01:15:36,742 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15018ms
2014-07-14 01:15:36,742 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15026ms
2014-07-14 01:15:36,743 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:15:36,743 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:15:36,747 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15005ms
2014-07-14 01:15:36,748 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15006ms
2014-07-14 01:15:36,748 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:15:36,750 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:15:36,751 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:15:36,752 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:15:36,759 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:15:36,819 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:15:36,821 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:15:36,823 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:15:36,869 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:15:37,978 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:15:38,017 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:15:38,057 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:15:38,089 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:15:38,123 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:15:38,166 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:15:38,212 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:15:38,241 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:15:38,283 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:15:38,320 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:15:38,366 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:15:38,412 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:15:38,445 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:15:38,486 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:15:41,703 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20003ms
2014-07-14 01:15:41,704 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:15:41,704 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20003ms
2014-07-14 01:15:41,705 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20003ms
2014-07-14 01:15:41,713 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:15:41,714 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:15:41,715 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20013ms
2014-07-14 01:15:41,715 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:15:41,717 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:15:41,718 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20006ms
2014-07-14 01:15:41,718 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:15:41,719 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:15:41,726 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:15:41,734 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:15:41,734 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:15:41,737 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:15:41,739 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:15:41,740 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:15:41,741 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:15:41,741 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20004ms
2014-07-14 01:15:41,742 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20004ms
2014-07-14 01:15:41,743 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20019ms
2014-07-14 01:15:41,743 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20027ms
2014-07-14 01:15:41,743 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:15:41,744 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20003ms
2014-07-14 01:15:41,747 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20005ms
2014-07-14 01:15:41,748 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20006ms
2014-07-14 01:15:41,748 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:15:41,751 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:15:41,752 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:15:41,752 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:15:41,759 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:15:41,820 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:15:41,821 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:15:41,823 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:15:41,869 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:15:42,979 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20003ms
2014-07-14 01:15:43,017 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:15:43,057 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:15:43,089 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:15:43,123 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:15:43,166 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:15:43,213 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:15:43,241 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:15:43,284 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:15:43,320 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:15:43,366 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:15:43,413 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20003ms
2014-07-14 01:15:43,446 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:15:43,486 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:15:46,503 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5858, memsize=870.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/bcc76302ccc843139b7758fb33b28946
2014-07-14 01:15:46,516 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/bcc76302ccc843139b7758fb33b28946 as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/bcc76302ccc843139b7758fb33b28946
2014-07-14 01:15:46,527 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/bcc76302ccc843139b7758fb33b28946, entries=3170600, sequenceid=5858, filesize=225.6m
2014-07-14 01:15:46,527 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.2g/1253661200, currentsize=394.7m/413909680 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 41694ms, sequenceid=5858, compaction requested=true
2014-07-14 01:15:46,528 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:15:46,528 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 20 blocking
2014-07-14 01:15:46,528 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-14 01:15:46,528 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:15:46,528 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23044ms
2014-07-14 01:15:46,528 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:15:46,528 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,528 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:15:46,529 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23084ms
2014-07-14 01:15:46,529 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,529 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23119ms
2014-07-14 01:15:46,529 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,529 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23164ms
2014-07-14 01:15:46,529 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,529 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23210ms
2014-07-14 01:15:46,529 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,530 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23248ms
2014-07-14 01:15:46,530 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,532 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23292ms
2014-07-14 01:15:46,532 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,533 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23322ms
2014-07-14 01:15:46,533 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,533 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23369ms
2014-07-14 01:15:46,533 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,543 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23421ms
2014-07-14 01:15:46,543 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,543 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23455ms
2014-07-14 01:15:46,543 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,545 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23489ms
2014-07-14 01:15:46,545 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,545 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23530ms
2014-07-14 01:15:46,546 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,546 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23570ms
2014-07-14 01:15:46,546 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,549 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24682ms
2014-07-14 01:15:46,549 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,549 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24728ms
2014-07-14 01:15:46,549 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,549 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24730ms
2014-07-14 01:15:46,549 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,550 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24732ms
2014-07-14 01:15:46,550 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,550 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24792ms
2014-07-14 01:15:46,550 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,550 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24799ms
2014-07-14 01:15:46,550 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,561 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24811ms
2014-07-14 01:15:46,561 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,561 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24812ms
2014-07-14 01:15:46,561 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,569 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24822ms
2014-07-14 01:15:46,569 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,569 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24827ms
2014-07-14 01:15:46,569 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,569 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24827ms
2014-07-14 01:15:46,569 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,569 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24828ms
2014-07-14 01:15:46,570 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,577 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24836ms
2014-07-14 01:15:46,577 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,585 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24869ms
2014-07-14 01:15:46,585 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,585 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24861ms
2014-07-14 01:15:46,585 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,589 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24851ms
2014-07-14 01:15:46,589 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,593 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24856ms
2014-07-14 01:15:46,593 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,597 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24858ms
2014-07-14 01:15:46,597 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,601 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24863ms
2014-07-14 01:15:46,601 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,605 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24867ms
2014-07-14 01:15:46,605 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,606 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24871ms
2014-07-14 01:15:46,606 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,613 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24880ms
2014-07-14 01:15:46,613 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,613 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24881ms
2014-07-14 01:15:46,613 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,613 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24889ms
2014-07-14 01:15:46,613 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,623 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24905ms
2014-07-14 01:15:46,623 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,623 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24906ms
2014-07-14 01:15:46,623 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,629 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24917ms
2014-07-14 01:15:46,629 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,629 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24913ms
2014-07-14 01:15:46,629 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,636 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24922ms
2014-07-14 01:15:46,636 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,641 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24939ms
2014-07-14 01:15:46,641 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,641 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24929ms
2014-07-14 01:15:46,642 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,642 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24931ms
2014-07-14 01:15:46,642 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,649 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24948ms
2014-07-14 01:15:46,649 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,650 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24948ms
2014-07-14 01:15:46,650 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,651 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24948ms
2014-07-14 01:15:46,651 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,653 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24953ms
2014-07-14 01:15:46,653 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:15:46,656 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27038,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325719617,"queuetimems":7000,"class":"HRegionServer","responsesize":15764,"method":"Multi"}
2014-07-14 01:15:46,656 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27038,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325719617,"queuetimems":6983,"class":"HRegionServer","responsesize":15727,"method":"Multi"}
2014-07-14 01:15:46,736 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27434,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325719302,"queuetimems":7919,"class":"HRegionServer","responsesize":15642,"method":"Multi"}
2014-07-14 01:15:47,123 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28156,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325718966,"queuetimems":7800,"class":"HRegionServer","responsesize":15874,"method":"Multi"}
2014-07-14 01:15:47,125 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28151,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325718974,"queuetimems":7767,"class":"HRegionServer","responsesize":16102,"method":"Multi"}
2014-07-14 01:15:47,125 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28199,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325718925,"queuetimems":7983,"class":"HRegionServer","responsesize":15773,"method":"Multi"}
2014-07-14 01:15:47,124 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27829,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325719295,"queuetimems":7994,"class":"HRegionServer","responsesize":15623,"method":"Multi"}
2014-07-14 01:15:47,124 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27821,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325719303,"queuetimems":7463,"class":"HRegionServer","responsesize":15551,"method":"Multi"}
2014-07-14 01:15:47,124 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27823,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325719301,"queuetimems":7964,"class":"HRegionServer","responsesize":15700,"method":"Multi"}
2014-07-14 01:15:47,123 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28189,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325718933,"queuetimems":7815,"class":"HRegionServer","responsesize":16198,"method":"Multi"}
2014-07-14 01:15:47,123 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28204,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325718918,"queuetimems":8107,"class":"HRegionServer","responsesize":15397,"method":"Multi"}
2014-07-14 01:15:47,123 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28189,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325718933,"queuetimems":7935,"class":"HRegionServer","responsesize":16193,"method":"Multi"}
2014-07-14 01:15:47,123 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28189,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325718933,"queuetimems":7863,"class":"HRegionServer","responsesize":15715,"method":"Multi"}
2014-07-14 01:15:47,349 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:15:47,349 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files; delaying flush up to 90000ms
2014-07-14 01:15:47,360 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 20 blocking
2014-07-14 01:15:47,360 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-14 01:15:47,360 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:15:47,361 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:15:47,361 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:15:47,361 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:15:48,070 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:15:48,125 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24487 synced till here 24457
2014-07-14 01:15:48,297 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27308,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325720989,"queuetimems":0,"class":"HRegionServer","responsesize":15856,"method":"Multi"}
2014-07-14 01:15:48,297 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28427,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325719870,"queuetimems":5399,"class":"HRegionServer","responsesize":15749,"method":"Multi"}
2014-07-14 01:15:48,301 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26926,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325721375,"queuetimems":0,"class":"HRegionServer","responsesize":15763,"method":"Multi"}
2014-07-14 01:15:48,302 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28438,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325719863,"queuetimems":7004,"class":"HRegionServer","responsesize":15662,"method":"Multi"}
2014-07-14 01:15:48,305 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28434,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325719871,"queuetimems":5359,"class":"HRegionServer","responsesize":15963,"method":"Multi"}
2014-07-14 01:15:48,309 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26901,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325721408,"queuetimems":0,"class":"HRegionServer","responsesize":15636,"method":"Multi"}
2014-07-14 01:15:48,309 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28683,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325719626,"queuetimems":6928,"class":"HRegionServer","responsesize":15955,"method":"Multi"}
2014-07-14 01:15:48,311 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28684,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325719626,"queuetimems":6857,"class":"HRegionServer","responsesize":15879,"method":"Multi"}
2014-07-14 01:15:48,301 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27186,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325721115,"queuetimems":0,"class":"HRegionServer","responsesize":15494,"method":"Multi"}
2014-07-14 01:15:48,317 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28684,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325719626,"queuetimems":6887,"class":"HRegionServer","responsesize":15732,"method":"Multi"}
2014-07-14 01:15:48,325 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28447,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325719877,"queuetimems":5297,"class":"HRegionServer","responsesize":15731,"method":"Multi"}
2014-07-14 01:15:48,325 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28448,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325719862,"queuetimems":7032,"class":"HRegionServer","responsesize":15582,"method":"Multi"}
2014-07-14 01:15:48,325 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28433,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325719877,"queuetimems":5331,"class":"HRegionServer","responsesize":15840,"method":"Multi"}
2014-07-14 01:15:48,325 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28708,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325719616,"queuetimems":7009,"class":"HRegionServer","responsesize":15590,"method":"Multi"}
2014-07-14 01:15:48,325 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28691,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325719634,"queuetimems":6835,"class":"HRegionServer","responsesize":15759,"method":"Multi"}
2014-07-14 01:15:48,326 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26992,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325721334,"queuetimems":0,"class":"HRegionServer","responsesize":15786,"method":"Multi"}
2014-07-14 01:15:48,333 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28692,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325719617,"queuetimems":6951,"class":"HRegionServer","responsesize":15837,"method":"Multi"}
2014-07-14 01:15:48,393 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325721514 with entries=118, filesize=101.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325748070
2014-07-14 01:15:48,393 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325575978
2014-07-14 01:15:48,393 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325599699
2014-07-14 01:15:48,393 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325602640
2014-07-14 01:15:48,611 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27443,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325721167,"queuetimems":1,"class":"HRegionServer","responsesize":16085,"method":"Multi"}
2014-07-14 01:15:48,611 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27394,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325721216,"queuetimems":1,"class":"HRegionServer","responsesize":15701,"method":"Multi"}
2014-07-14 01:15:48,846 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27552,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325721293,"queuetimems":0,"class":"HRegionServer","responsesize":15868,"method":"Multi"}
2014-07-14 01:15:48,846 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27824,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325721021,"queuetimems":0,"class":"HRegionServer","responsesize":15647,"method":"Multi"}
2014-07-14 01:15:48,852 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27587,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325721264,"queuetimems":0,"class":"HRegionServer","responsesize":15866,"method":"Multi"}
2014-07-14 01:15:48,852 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27798,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325721053,"queuetimems":0,"class":"HRegionServer","responsesize":15911,"method":"Multi"}
2014-07-14 01:15:48,852 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25446,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325723406,"queuetimems":0,"class":"HRegionServer","responsesize":16050,"method":"Multi"}
2014-07-14 01:15:48,852 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25410,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325723441,"queuetimems":0,"class":"HRegionServer","responsesize":15447,"method":"Multi"}
2014-07-14 01:15:48,852 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25368,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325723483,"queuetimems":1,"class":"HRegionServer","responsesize":15773,"method":"Multi"}
2014-07-14 01:15:49,941 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:15:49,943 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26670,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325723273,"queuetimems":1,"class":"HRegionServer","responsesize":15874,"method":"Multi"}
2014-07-14 01:15:49,985 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24596 synced till here 24557
2014-07-14 01:15:50,242 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27006,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325723236,"queuetimems":0,"class":"HRegionServer","responsesize":15642,"method":"Multi"}
2014-07-14 01:15:50,242 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27158,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325723084,"queuetimems":0,"class":"HRegionServer","responsesize":15864,"method":"Multi"}
2014-07-14 01:15:50,242 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26928,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325723314,"queuetimems":0,"class":"HRegionServer","responsesize":16198,"method":"Multi"}
2014-07-14 01:15:50,243 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27084,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325723159,"queuetimems":0,"class":"HRegionServer","responsesize":15700,"method":"Multi"}
2014-07-14 01:15:50,253 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26891,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325723362,"queuetimems":0,"class":"HRegionServer","responsesize":15732,"method":"Multi"}
2014-07-14 01:15:50,243 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27272,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325722970,"queuetimems":0,"class":"HRegionServer","responsesize":15548,"method":"Multi"}
2014-07-14 01:15:50,254 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27204,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325723050,"queuetimems":0,"class":"HRegionServer","responsesize":16102,"method":"Multi"}
2014-07-14 01:15:50,255 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27243,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325723011,"queuetimems":1,"class":"HRegionServer","responsesize":15532,"method":"Multi"}
2014-07-14 01:15:50,253 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27054,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325723199,"queuetimems":0,"class":"HRegionServer","responsesize":16193,"method":"Multi"}
2014-07-14 01:15:50,261 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27141,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325723120,"queuetimems":1,"class":"HRegionServer","responsesize":15607,"method":"Multi"}
2014-07-14 01:15:50,335 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325748070 with entries=109, filesize=93.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325749941
2014-07-14 01:15:52,056 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:15:52,122 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24698 synced till here 24667
2014-07-14 01:15:52,621 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325749941 with entries=102, filesize=87.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325752056
2014-07-14 01:15:53,404 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:15:53,423 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24795 synced till here 24775
2014-07-14 01:15:53,571 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325752056 with entries=97, filesize=83.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325753404
2014-07-14 01:15:54,852 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:15:55,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24899 synced till here 24869
2014-07-14 01:15:55,257 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325753404 with entries=104, filesize=89.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325754852
2014-07-14 01:15:57,767 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6074, memsize=790.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/82a6d1969b974606be366e0750fc4ec6
2014-07-14 01:15:57,789 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/82a6d1969b974606be366e0750fc4ec6 as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/82a6d1969b974606be366e0750fc4ec6
2014-07-14 01:15:57,804 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/82a6d1969b974606be366e0750fc4ec6, entries=2877510, sequenceid=6074, filesize=204.8m
2014-07-14 01:15:57,804 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.4g/1519038960, currentsize=285.9m/299750560 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 38502ms, sequenceid=6074, compaction requested=true
2014-07-14 01:15:57,805 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:15:57,805 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 20 blocking
2014-07-14 01:15:57,806 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-14 01:15:57,806 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:15:57,806 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:15:57,806 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:15:58,059 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:15:58,060 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files; delaying flush up to 90000ms
2014-07-14 01:15:58,060 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:15:58,060 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 20 blocking
2014-07-14 01:15:58,060 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-14 01:15:58,060 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:15:58,060 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:15:58,060 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:15:58,567 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:15:58,621 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325754852 with entries=72, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325758568
2014-07-14 01:15:58,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325604182
2014-07-14 01:15:58,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325606010
2014-07-14 01:15:58,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325607285
2014-07-14 01:15:58,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325608675
2014-07-14 01:15:58,622 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325610423
2014-07-14 01:15:58,622 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325612262
2014-07-14 01:15:58,622 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325614070
2014-07-14 01:15:58,622 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325615792
2014-07-14 01:15:58,622 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325617134
2014-07-14 01:15:58,622 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325618580
2014-07-14 01:15:58,622 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325619702
2014-07-14 01:15:58,622 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325621054
2014-07-14 01:15:58,622 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325622619
2014-07-14 01:15:58,622 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325624748
2014-07-14 01:15:58,622 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325627068
2014-07-14 01:15:58,622 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325634099
2014-07-14 01:15:58,623 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325636941
2014-07-14 01:15:58,623 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325638523
2014-07-14 01:15:58,623 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325640765
2014-07-14 01:15:58,623 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325642430
2014-07-14 01:15:58,623 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325643887
2014-07-14 01:15:58,624 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325646236
2014-07-14 01:15:58,625 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325650155
2014-07-14 01:15:58,625 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325653022
2014-07-14 01:15:58,625 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325654645
2014-07-14 01:15:58,625 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325656150
2014-07-14 01:15:58,625 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325657573
2014-07-14 01:15:58,625 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325659152
2014-07-14 01:16:00,377 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:16:00,399 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25045 synced till here 25043
2014-07-14 01:16:00,427 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325758568 with entries=74, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325760377
2014-07-14 01:16:01,745 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:16:01,790 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25118 synced till here 25117
2014-07-14 01:16:01,824 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325760377 with entries=73, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325761746
2014-07-14 01:16:03,067 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:16:03,277 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25199 synced till here 25192
2014-07-14 01:16:03,619 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325761746 with entries=81, filesize=69.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325763241
2014-07-14 01:16:04,922 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:16:05,129 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25287 synced till here 25286
2014-07-14 01:16:05,150 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325763241 with entries=88, filesize=75.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325764922
2014-07-14 01:16:06,501 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:16:06,827 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325764922 with entries=91, filesize=78.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325766502
2014-07-14 01:16:08,194 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:16:08,217 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25452 synced till here 25451
2014-07-14 01:16:08,232 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325766502 with entries=74, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325768195
2014-07-14 01:16:09,654 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:16:09,676 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25527 synced till here 25524
2014-07-14 01:16:09,706 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325768195 with entries=75, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325769655
2014-07-14 01:16:10,696 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:16:10,745 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25602 synced till here 25601
2014-07-14 01:16:10,767 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325769655 with entries=75, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325770697
2014-07-14 01:16:12,203 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:16:12,218 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25685 synced till here 25676
2014-07-14 01:16:12,299 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325770697 with entries=83, filesize=71.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325772203
2014-07-14 01:16:13,900 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:16:15,145 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25806 synced till here 25804
2014-07-14 01:16:15,191 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325772203 with entries=121, filesize=103.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325773901
2014-07-14 01:16:16,121 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:16:16,178 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25887 synced till here 25878
2014-07-14 01:16:16,389 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325773901 with entries=81, filesize=69.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325776122
2014-07-14 01:16:16,390 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:16:17,412 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:16:17,412 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files, but is 1.2g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:16:17,412 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. due to global heap pressure
2014-07-14 01:16:17,412 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 1.2g
2014-07-14 01:16:17,926 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:16:17,950 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25961 synced till here 25960
2014-07-14 01:16:17,975 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325776122 with entries=74, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325777927
2014-07-14 01:16:18,131 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:16:18,132 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files, but is 1018.8m vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:16:18,132 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. due to global heap pressure
2014-07-14 01:16:18,132 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 1018.8m
2014-07-14 01:16:18,394 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:16:19,190 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:16:19,216 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26038 synced till here 26035
2014-07-14 01:16:19,284 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325777927 with entries=77, filesize=66.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325779191
2014-07-14 01:16:19,366 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:19,366 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:19,367 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:19,367 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:19,374 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:19,392 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:19,392 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:19,394 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:19,398 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:19,431 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:19,542 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:16:19,553 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:19,598 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:19,642 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:19,685 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:19,723 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:19,790 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:19,829 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:19,876 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:19,936 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:20,102 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:20,319 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:20,358 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:20,400 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:20,448 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:20,746 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:21,096 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:21,184 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:21,364 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:21,670 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:21,701 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:22,578 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:22,588 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:22,610 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:22,642 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:22,671 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:22,704 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:22,743 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:22,770 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:22,815 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:23,352 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=10829, hits=3899, hitRatio=36.00%, , cachingAccesses=3903, cachingHits=3898, cachingHitsRatio=99.87%, evictions=0, evicted=3, evictedPerRun=Infinity
2014-07-14 01:16:23,723 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:23,757 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:23,801 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:23,828 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:23,861 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:23,904 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:23,949 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:23,990 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:24,030 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:24,073 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:24,110 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:16:24,366 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:16:24,367 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:16:24,367 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:16:24,368 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:16:24,374 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:16:24,393 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:16:24,393 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:16:24,394 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:16:24,398 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:16:24,431 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:16:24,553 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:16:24,598 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:16:24,642 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:16:24,686 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:16:24,723 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:16:24,790 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:16:24,829 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:16:24,876 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:16:24,936 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:16:25,102 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:16:25,319 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:16:25,359 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:16:25,401 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:16:25,448 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:16:25,747 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:16:26,097 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:16:26,184 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:16:26,365 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:16:26,671 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:16:26,702 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:16:27,579 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:16:27,589 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:16:27,610 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:16:27,644 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:16:27,672 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:16:27,705 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:16:27,743 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:16:27,771 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:16:27,815 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:16:29,253 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5142ms
2014-07-14 01:16:29,253 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5180ms
2014-07-14 01:16:29,253 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5425ms
2014-07-14 01:16:29,254 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5224ms
2014-07-14 01:16:29,254 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5532ms
2014-07-14 01:16:29,254 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5497ms
2014-07-14 01:16:29,254 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5453ms
2014-07-14 01:16:29,255 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5394ms
2014-07-14 01:16:29,255 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5352ms
2014-07-14 01:16:29,255 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5306ms
2014-07-14 01:16:29,256 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5266ms
2014-07-14 01:16:29,367 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:16:29,368 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:16:29,368 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:16:29,369 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:16:29,374 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:16:29,393 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:16:29,393 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:16:29,395 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:16:29,398 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:16:29,431 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:16:29,553 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:16:29,598 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:16:29,643 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:16:29,686 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:16:29,724 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:16:29,791 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:16:29,830 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:16:29,876 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:16:29,936 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:16:30,103 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:16:30,320 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:16:30,359 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:16:30,401 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:16:30,449 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:16:30,747 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:16:31,097 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:16:31,184 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:16:31,365 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:16:31,671 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:16:31,702 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:16:32,580 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:16:32,589 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:16:32,611 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:16:32,644 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:16:32,672 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:16:32,705 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:16:32,743 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:16:32,771 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:16:32,815 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:16:34,254 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10143ms
2014-07-14 01:16:34,254 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10426ms
2014-07-14 01:16:34,254 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10181ms
2014-07-14 01:16:34,254 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10224ms
2014-07-14 01:16:34,254 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10532ms
2014-07-14 01:16:34,255 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10498ms
2014-07-14 01:16:34,255 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10394ms
2014-07-14 01:16:34,255 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10454ms
2014-07-14 01:16:34,256 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10353ms
2014-07-14 01:16:34,256 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10307ms
2014-07-14 01:16:34,256 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10266ms
2014-07-14 01:16:34,367 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:16:34,368 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:16:34,369 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:16:34,369 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:16:34,375 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:16:34,393 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:16:34,394 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:16:34,395 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:16:34,399 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:16:34,431 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 01:16:34,554 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:16:34,599 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:16:34,643 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:16:34,686 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:16:34,724 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:16:34,792 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:16:34,830 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:16:35,770 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15023ms
2014-07-14 01:16:35,770 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15668ms
2014-07-14 01:16:35,770 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15451ms
2014-07-14 01:16:35,770 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15412ms
2014-07-14 01:16:35,771 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15371ms
2014-07-14 01:16:35,771 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15324ms
2014-07-14 01:16:35,771 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15896ms
2014-07-14 01:16:35,771 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15835ms
2014-07-14 01:16:36,098 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:16:36,185 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:16:36,366 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:16:36,672 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:16:36,702 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:16:37,185 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6519, memsize=384.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/c2589f1b704641b4b944623263e4a7c2
2014-07-14 01:16:37,209 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/c2589f1b704641b4b944623263e4a7c2 as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/c2589f1b704641b4b944623263e4a7c2
2014-07-14 01:16:37,223 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/c2589f1b704641b4b944623263e4a7c2, entries=1398910, sequenceid=6519, filesize=99.6m
2014-07-14 01:16:37,223 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1021.9m/1071589760, currentsize=28.0m/29398320 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 19091ms, sequenceid=6519, compaction requested=true
2014-07-14 01:16:37,224 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:16:37,224 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 20 blocking
2014-07-14 01:16:37,224 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-14 01:16:37,224 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15523ms
2014-07-14 01:16:37,224 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,224 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:16:37,225 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:16:37,224 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90631ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:16:37,225 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:16:37,224 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15554ms
2014-07-14 01:16:37,225 DEBUG [MemStoreFlusher.0] regionserver.HRegion: NOT flushing memstore for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., flushing=true, writesEnabled=true
2014-07-14 01:16:37,225 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,225 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15861ms
2014-07-14 01:16:37,225 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,225 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16042ms
2014-07-14 01:16:37,225 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,233 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16137ms
2014-07-14 01:16:37,233 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,233 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17297ms
2014-07-14 01:16:37,233 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,233 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17358ms
2014-07-14 01:16:37,233 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,233 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16786ms
2014-07-14 01:16:37,233 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,233 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16833ms
2014-07-14 01:16:37,234 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,238 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16880ms
2014-07-14 01:16:37,238 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,238 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16919ms
2014-07-14 01:16:37,238 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,242 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17140ms
2014-07-14 01:16:37,242 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,245 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16499ms
2014-07-14 01:16:37,245 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,253 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17425ms
2014-07-14 01:16:37,253 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,254 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17464ms
2014-07-14 01:16:37,254 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,254 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17531ms
2014-07-14 01:16:37,254 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,254 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17569ms
2014-07-14 01:16:37,254 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,257 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17615ms
2014-07-14 01:16:37,257 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,257 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17659ms
2014-07-14 01:16:37,257 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,257 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17704ms
2014-07-14 01:16:37,257 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,258 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17827ms
2014-07-14 01:16:37,258 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,259 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17861ms
2014-07-14 01:16:37,259 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,261 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17867ms
2014-07-14 01:16:37,261 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,261 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17869ms
2014-07-14 01:16:37,262 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,262 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17870ms
2014-07-14 01:16:37,262 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,273 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17899ms
2014-07-14 01:16:37,273 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,273 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17906ms
2014-07-14 01:16:37,273 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,281 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17915ms
2014-07-14 01:16:37,281 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,282 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17916ms
2014-07-14 01:16:37,282 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,282 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17916ms
2014-07-14 01:16:37,282 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,282 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13292ms
2014-07-14 01:16:37,283 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,293 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13344ms
2014-07-14 01:16:37,293 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,293 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13390ms
2014-07-14 01:16:37,293 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,294 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13492ms
2014-07-14 01:16:37,294 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,294 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13433ms
2014-07-14 01:16:37,294 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,295 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13537ms
2014-07-14 01:16:37,295 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,296 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13573ms
2014-07-14 01:16:37,296 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,296 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13266ms
2014-07-14 01:16:37,296 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,296 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13223ms
2014-07-14 01:16:37,296 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,297 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13468ms
2014-07-14 01:16:37,297 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,297 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13187ms
2014-07-14 01:16:37,297 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,297 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14482ms
2014-07-14 01:16:37,297 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,297 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14527ms
2014-07-14 01:16:37,298 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,298 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14555ms
2014-07-14 01:16:37,298 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,298 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14594ms
2014-07-14 01:16:37,298 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,298 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14627ms
2014-07-14 01:16:37,298 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,298 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14656ms
2014-07-14 01:16:37,298 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,299 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14689ms
2014-07-14 01:16:37,299 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,301 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14712ms
2014-07-14 01:16:37,301 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,301 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14723ms
2014-07-14 01:16:37,302 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:16:37,302 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 91329ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:16:37,302 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 1009.1m
2014-07-14 01:16:37,601 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18491,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325779109,"queuetimems":0,"class":"HRegionServer","responsesize":15993,"method":"Multi"}
2014-07-14 01:16:37,627 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6512, memsize=394.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/a3668c7fb8e043beac8a705730df36e8
2014-07-14 01:16:37,640 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/a3668c7fb8e043beac8a705730df36e8 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/a3668c7fb8e043beac8a705730df36e8
2014-07-14 01:16:37,655 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/a3668c7fb8e043beac8a705730df36e8, entries=1437670, sequenceid=6512, filesize=102.4m
2014-07-14 01:16:37,656 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.2g/1299652800, currentsize=62.1m/65121120 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 20244ms, sequenceid=6512, compaction requested=true
2014-07-14 01:16:37,656 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:16:37,656 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 20 blocking
2014-07-14 01:16:37,657 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-14 01:16:37,657 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:16:37,657 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:16:37,657 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:16:37,807 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18664,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325779142,"queuetimems":0,"class":"HRegionServer","responsesize":15457,"method":"Multi"}
2014-07-14 01:16:37,814 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18642,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325779172,"queuetimems":0,"class":"HRegionServer","responsesize":15504,"method":"Multi"}
2014-07-14 01:16:38,002 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:16:38,918 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26149 synced till here 26117
2014-07-14 01:16:39,154 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325779191 with entries=111, filesize=93.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325798003
2014-07-14 01:16:39,155 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325660956
2014-07-14 01:16:39,155 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325663125
2014-07-14 01:16:39,155 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325697101
2014-07-14 01:16:39,155 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325698931
2014-07-14 01:16:39,155 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325700698
2014-07-14 01:16:39,155 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325702536
2014-07-14 01:16:39,155 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325704234
2014-07-14 01:16:39,285 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20037,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325779248,"queuetimems":1,"class":"HRegionServer","responsesize":15889,"method":"Multi"}
2014-07-14 01:16:39,285 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20000,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325779285,"queuetimems":0,"class":"HRegionServer","responsesize":15911,"method":"Multi"}
2014-07-14 01:16:39,285 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19974,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325779311,"queuetimems":0,"class":"HRegionServer","responsesize":15937,"method":"Multi"}
2014-07-14 01:16:39,286 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20075,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325779210,"queuetimems":0,"class":"HRegionServer","responsesize":15887,"method":"Multi"}
2014-07-14 01:16:39,285 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19945,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325779340,"queuetimems":1,"class":"HRegionServer","responsesize":15730,"method":"Multi"}
2014-07-14 01:16:39,685 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:16:39,893 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:16:39,894 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18800,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325781094,"queuetimems":0,"class":"HRegionServer","responsesize":16010,"method":"Multi"}
2014-07-14 01:16:39,895 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20020,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325779874,"queuetimems":0,"class":"HRegionServer","responsesize":15502,"method":"Multi"}
2014-07-14 01:16:39,909 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20082,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325779827,"queuetimems":0,"class":"HRegionServer","responsesize":15646,"method":"Multi"}
2014-07-14 01:16:39,909 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18727,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325781182,"queuetimems":0,"class":"HRegionServer","responsesize":15657,"method":"Multi"}
2014-07-14 01:16:39,910 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19810,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325780100,"queuetimems":0,"class":"HRegionServer","responsesize":15298,"method":"Multi"}
2014-07-14 01:16:39,909 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20188,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325779721,"queuetimems":0,"class":"HRegionServer","responsesize":15734,"method":"Multi"}
2014-07-14 01:16:39,909 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20227,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325779682,"queuetimems":0,"class":"HRegionServer","responsesize":15629,"method":"Multi"}
2014-07-14 01:16:39,921 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18252,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325781669,"queuetimems":1,"class":"HRegionServer","responsesize":15457,"method":"Multi"}
2014-07-14 01:16:39,921 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19475,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325780446,"queuetimems":1,"class":"HRegionServer","responsesize":15694,"method":"Multi"}
2014-07-14 01:16:39,922 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18223,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325781699,"queuetimems":1,"class":"HRegionServer","responsesize":15914,"method":"Multi"}
2014-07-14 01:16:39,922 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19605,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325780317,"queuetimems":1,"class":"HRegionServer","responsesize":15912,"method":"Multi"}
2014-07-14 01:16:39,921 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18559,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325781362,"queuetimems":0,"class":"HRegionServer","responsesize":15941,"method":"Multi"}
2014-07-14 01:16:39,925 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16136,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325783789,"queuetimems":0,"class":"HRegionServer","responsesize":15694,"method":"Multi"}
2014-07-14 01:16:39,921 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19177,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325780744,"queuetimems":0,"class":"HRegionServer","responsesize":15530,"method":"Multi"}
2014-07-14 01:16:39,921 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19523,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325780398,"queuetimems":0,"class":"HRegionServer","responsesize":15528,"method":"Multi"}
2014-07-14 01:16:39,925 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19569,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325780356,"queuetimems":0,"class":"HRegionServer","responsesize":15888,"method":"Multi"}
2014-07-14 01:16:39,933 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19999,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325779934,"queuetimems":0,"class":"HRegionServer","responsesize":15966,"method":"Multi"}
2014-07-14 01:16:40,077 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26256 synced till here 26241
2014-07-14 01:16:40,778 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325798003 with entries=107, filesize=91.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325799894
2014-07-14 01:16:41,135 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17028,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325784107,"queuetimems":1,"class":"HRegionServer","responsesize":15629,"method":"Multi"}
2014-07-14 01:16:41,135 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18558,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325782577,"queuetimems":1,"class":"HRegionServer","responsesize":15504,"method":"Multi"}
2014-07-14 01:16:41,135 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17108,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325784027,"queuetimems":1,"class":"HRegionServer","responsesize":15502,"method":"Multi"}
2014-07-14 01:16:41,142 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18442,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325782699,"queuetimems":0,"class":"HRegionServer","responsesize":15937,"method":"Multi"}
2014-07-14 01:16:41,142 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21713,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325779429,"queuetimems":0,"class":"HRegionServer","responsesize":15914,"method":"Multi"}
2014-07-14 01:16:41,149 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21554,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325779595,"queuetimems":0,"class":"HRegionServer","responsesize":16143,"method":"Multi"}
2014-07-14 01:16:41,150 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21778,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325779372,"queuetimems":0,"class":"HRegionServer","responsesize":15479,"method":"Multi"}
2014-07-14 01:16:41,149 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21598,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325779551,"queuetimems":0,"class":"HRegionServer","responsesize":15861,"method":"Multi"}
2014-07-14 01:16:41,150 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17398,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325783752,"queuetimems":1,"class":"HRegionServer","responsesize":15528,"method":"Multi"}
2014-07-14 01:16:41,149 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18381,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325782768,"queuetimems":0,"class":"HRegionServer","responsesize":15730,"method":"Multi"}
2014-07-14 01:16:41,153 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17430,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325783711,"queuetimems":0,"class":"HRegionServer","responsesize":15298,"method":"Multi"}
2014-07-14 01:16:41,153 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18545,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325782608,"queuetimems":1,"class":"HRegionServer","responsesize":15993,"method":"Multi"}
2014-07-14 01:16:41,167 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17180,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325783986,"queuetimems":0,"class":"HRegionServer","responsesize":15966,"method":"Multi"}
2014-07-14 01:16:41,169 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17224,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325783945,"queuetimems":1,"class":"HRegionServer","responsesize":16072,"method":"Multi"}
2014-07-14 01:16:41,169 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17311,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325783858,"queuetimems":0,"class":"HRegionServer","responsesize":15941,"method":"Multi"}
2014-07-14 01:16:41,170 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18501,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325782669,"queuetimems":0,"class":"HRegionServer","responsesize":15887,"method":"Multi"}
2014-07-14 01:16:41,169 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17273,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325783896,"queuetimems":0,"class":"HRegionServer","responsesize":15734,"method":"Multi"}
2014-07-14 01:16:41,178 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18367,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325782810,"queuetimems":0,"class":"HRegionServer","responsesize":15911,"method":"Multi"}
2014-07-14 01:16:41,178 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17110,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325784068,"queuetimems":1,"class":"HRegionServer","responsesize":16143,"method":"Multi"}
2014-07-14 01:16:41,178 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21537,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325779640,"queuetimems":0,"class":"HRegionServer","responsesize":15650,"method":"Multi"}
2014-07-14 01:16:41,189 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18603,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325782586,"queuetimems":0,"class":"HRegionServer","responsesize":15479,"method":"Multi"}
2014-07-14 01:16:41,189 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21401,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325779788,"queuetimems":0,"class":"HRegionServer","responsesize":16072,"method":"Multi"}
2014-07-14 01:16:41,190 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18550,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325782640,"queuetimems":0,"class":"HRegionServer","responsesize":15889,"method":"Multi"}
2014-07-14 01:16:41,189 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18456,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325782733,"queuetimems":1,"class":"HRegionServer","responsesize":15861,"method":"Multi"}
2014-07-14 01:16:41,193 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17369,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325783824,"queuetimems":1,"class":"HRegionServer","responsesize":15646,"method":"Multi"}
2014-07-14 01:16:41,792 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:16:42,600 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26371 synced till here 26351
2014-07-14 01:16:42,753 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325799894 with entries=115, filesize=97.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325801792
2014-07-14 01:16:43,469 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:16:43,560 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26482 synced till here 26443
2014-07-14 01:16:44,601 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325801792 with entries=111, filesize=94.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325803469
2014-07-14 01:16:45,454 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:16:45,469 DEBUG [RpcServer.handler=39,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:16:45,469 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files; delaying flush up to 90000ms
2014-07-14 01:16:45,470 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 20 blocking
2014-07-14 01:16:45,470 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-14 01:16:45,470 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:16:45,470 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:16:45,471 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:16:45,471 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:16:46,297 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26593 synced till here 26574
2014-07-14 01:16:46,516 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325803469 with entries=111, filesize=95.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325805455
2014-07-14 01:16:47,349 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:16:47,474 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26703 synced till here 26666
2014-07-14 01:16:47,709 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:16:47,709 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files; delaying flush up to 90000ms
2014-07-14 01:16:47,711 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 20 blocking
2014-07-14 01:16:47,712 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-14 01:16:47,712 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:16:47,712 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:16:47,712 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:16:47,713 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:16:47,849 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325805455 with entries=110, filesize=94.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325807350
2014-07-14 01:16:49,195 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:16:49,260 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26811 synced till here 26793
2014-07-14 01:16:49,348 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325807350 with entries=108, filesize=92.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325809196
2014-07-14 01:16:50,606 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:16:50,868 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26909 synced till here 26908
2014-07-14 01:16:50,886 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325809196 with entries=98, filesize=83.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325810606
2014-07-14 01:16:52,269 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:16:52,307 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26985 synced till here 26981
2014-07-14 01:16:52,371 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325810606 with entries=76, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325812269
2014-07-14 01:16:53,655 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:16:53,677 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325812269 with entries=72, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325813655
2014-07-14 01:16:55,181 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:16:55,211 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325813655 with entries=73, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325815181
2014-07-14 01:16:56,420 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:16:56,530 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27212 synced till here 27206
2014-07-14 01:16:56,589 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325815181 with entries=82, filesize=70.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325816420
2014-07-14 01:16:57,808 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:16:58,069 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27301 synced till here 27299
2014-07-14 01:16:58,092 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325816420 with entries=89, filesize=76.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325817809
2014-07-14 01:16:59,232 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:16:59,293 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:16:59,293 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files, but is 1.2g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:16:59,293 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. due to global heap pressure
2014-07-14 01:16:59,293 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 1.2g
2014-07-14 01:16:59,441 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27393 synced till here 27391
2014-07-14 01:16:59,641 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325817809 with entries=92, filesize=78.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325819232
2014-07-14 01:17:00,805 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:17:00,924 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:17:01,057 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27485 synced till here 27483
2014-07-14 01:17:01,085 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325819232 with entries=92, filesize=78.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325820805
2014-07-14 01:17:01,634 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:01,636 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:01,669 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:01,683 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:01,748 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:01,826 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:01,903 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:01,964 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:02,311 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:02,322 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:02,375 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:02,422 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:02,472 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:02,520 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:02,573 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:02,624 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:02,676 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:02,728 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:02,904 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:02,969 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:03,021 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6541, memsize=414.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/df21b5a0ef214236b718029f49193eaa
2014-07-14 01:17:03,036 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:03,038 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/df21b5a0ef214236b718029f49193eaa as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/df21b5a0ef214236b718029f49193eaa
2014-07-14 01:17:03,054 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/df21b5a0ef214236b718029f49193eaa, entries=1510390, sequenceid=6541, filesize=107.5m
2014-07-14 01:17:03,055 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1016.8m/1066221920, currentsize=555.9m/582873040 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 25753ms, sequenceid=6541, compaction requested=true
2014-07-14 01:17:03,055 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:17:03,055 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 20 blocking
2014-07-14 01:17:03,055 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 19ms
2014-07-14 01:17:03,055 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:03,055 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-14 01:17:03,055 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 87ms
2014-07-14 01:17:03,056 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:17:03,056 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:03,056 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:17:03,056 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 152ms
2014-07-14 01:17:03,056 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:03,056 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:17:03,056 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 328ms
2014-07-14 01:17:03,056 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:03,056 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 380ms
2014-07-14 01:17:03,056 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:03,056 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 432ms
2014-07-14 01:17:03,056 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:03,056 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 483ms
2014-07-14 01:17:03,057 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:03,057 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 537ms
2014-07-14 01:17:03,057 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:03,057 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 585ms
2014-07-14 01:17:03,057 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:03,057 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 636ms
2014-07-14 01:17:03,058 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:03,058 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 683ms
2014-07-14 01:17:03,058 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:03,063 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 741ms
2014-07-14 01:17:03,063 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:03,063 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 752ms
2014-07-14 01:17:03,063 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:03,064 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1100ms
2014-07-14 01:17:03,064 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:03,064 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1161ms
2014-07-14 01:17:03,064 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:03,064 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1238ms
2014-07-14 01:17:03,064 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:03,064 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1316ms
2014-07-14 01:17:03,064 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:03,065 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1381ms
2014-07-14 01:17:03,065 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:03,065 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1396ms
2014-07-14 01:17:03,065 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:03,069 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1434ms
2014-07-14 01:17:03,069 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:03,069 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1435ms
2014-07-14 01:17:03,069 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:03,176 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:17:03,177 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files; delaying flush up to 90000ms
2014-07-14 01:17:03,177 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 20 blocking
2014-07-14 01:17:03,177 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-14 01:17:03,177 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:17:03,177 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:17:03,177 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:17:03,178 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:17:03,655 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:17:03,668 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27573 synced till here 27556
2014-07-14 01:17:04,736 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325820805 with entries=88, filesize=75.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325823655
2014-07-14 01:17:04,737 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325705945
2014-07-14 01:17:04,737 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325707521
2014-07-14 01:17:04,737 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325709275
2014-07-14 01:17:04,737 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325710744
2014-07-14 01:17:04,737 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325712684
2014-07-14 01:17:04,737 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325713623
2014-07-14 01:17:04,737 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325715270
2014-07-14 01:17:05,596 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:17:06,615 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27679 synced till here 27657
2014-07-14 01:17:06,831 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325823655 with entries=106, filesize=91.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325825596
2014-07-14 01:17:08,531 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:17:08,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27787 synced till here 27757
2014-07-14 01:17:08,913 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325825596 with entries=108, filesize=92.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325828531
2014-07-14 01:17:10,694 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:17:10,785 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27869 synced till here 27860
2014-07-14 01:17:10,909 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325828531 with entries=82, filesize=70.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325830695
2014-07-14 01:17:13,034 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:17:13,081 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27980 synced till here 27956
2014-07-14 01:17:13,269 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325830695 with entries=111, filesize=95.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325833034
2014-07-14 01:17:14,936 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:17:14,936 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files, but is 841.3m vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:17:14,936 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. due to global heap pressure
2014-07-14 01:17:14,936 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 841.3m
2014-07-14 01:17:15,079 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:17:15,603 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28114 synced till here 28089
2014-07-14 01:17:15,821 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325833034 with entries=134, filesize=113.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325835079
2014-07-14 01:17:17,153 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:17:17,416 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,416 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,416 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,419 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,419 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,420 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,420 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,420 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,420 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,422 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,422 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,422 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,423 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,423 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,423 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,423 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,423 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,425 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,441 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,471 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,630 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:17:17,633 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,634 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,635 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,635 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,637 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,638 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,638 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,638 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,639 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,639 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,639 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,640 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,640 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,648 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,657 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28211 synced till here 28206
2014-07-14 01:17:17,673 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,674 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,674 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,676 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,680 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,701 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325835079 with entries=97, filesize=82.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325837630
2014-07-14 01:17:17,702 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,731 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,761 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,791 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,821 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,850 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,879 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,910 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:17,939 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:18,127 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:18,157 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:22,417 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:17:22,417 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:17:22,417 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:17:22,420 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:17:22,420 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:17:22,420 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:17:22,420 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:17:22,420 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:17:22,421 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:17:22,423 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:17:22,423 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5003ms
2014-07-14 01:17:22,423 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5003ms
2014-07-14 01:17:22,423 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:17:22,423 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:17:22,423 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:17:22,424 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:17:22,424 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5005ms
2014-07-14 01:17:22,425 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:17:22,441 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:17:22,471 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:17:22,634 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:17:22,635 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-14 01:17:22,635 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-14 01:17:22,636 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:17:22,638 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:17:22,638 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:17:22,638 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:17:22,639 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-14 01:17:22,639 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-14 01:17:22,640 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:17:22,640 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:17:22,640 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:17:22,641 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:17:22,649 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:17:22,674 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:17:22,674 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:17:22,674 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:17:22,676 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:17:22,680 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:17:22,702 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:17:22,731 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:17:22,761 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:17:22,791 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:17:22,821 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:17:22,850 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:17:22,881 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:17:22,910 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:17:22,939 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:17:23,187 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5030ms
2014-07-14 01:17:23,187 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5060ms
2014-07-14 01:17:27,417 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:17:27,418 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:17:27,418 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:17:27,420 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:17:27,421 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:17:27,421 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:17:27,421 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:17:27,421 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:17:27,422 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-14 01:17:27,423 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:17:27,423 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-14 01:17:27,424 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-14 01:17:27,424 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:17:27,424 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:17:27,425 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-14 01:17:27,425 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10006ms
2014-07-14 01:17:27,426 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:17:27,426 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:17:27,441 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:17:27,472 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:17:27,635 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:17:27,636 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:17:27,636 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-14 01:17:27,636 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:17:27,638 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:17:27,639 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:17:27,639 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:17:27,639 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:17:27,640 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:17:27,640 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:17:27,640 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:17:27,641 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:17:27,642 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:17:27,649 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:17:27,674 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:17:27,675 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:17:27,675 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:17:27,677 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:17:27,680 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:17:27,702 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:17:27,732 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:17:27,762 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:17:27,791 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:17:27,821 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:17:27,851 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:17:27,881 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:17:27,910 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:17:27,940 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:17:28,187 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10030ms
2014-07-14 01:17:28,187 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10060ms
2014-07-14 01:17:31,541 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7072, memsize=340.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/b575c602910240afb2d83c071483247a
2014-07-14 01:17:31,556 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/b575c602910240afb2d83c071483247a as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/b575c602910240afb2d83c071483247a
2014-07-14 01:17:31,567 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/b575c602910240afb2d83c071483247a, entries=1238280, sequenceid=7072, filesize=88.1m
2014-07-14 01:17:31,568 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~861.4m/903236320, currentsize=48.2m/50490560 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 16632ms, sequenceid=7072, compaction requested=true
2014-07-14 01:17:31,569 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:17:31,569 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 20 blocking
2014-07-14 01:17:31,569 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13442ms
2014-07-14 01:17:31,569 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-14 01:17:31,569 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,570 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:17:31,570 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:17:31,570 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13413ms
2014-07-14 01:17:31,570 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,570 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:17:31,570 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13632ms
2014-07-14 01:17:31,571 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,571 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13661ms
2014-07-14 01:17:31,571 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,573 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13694ms
2014-07-14 01:17:31,573 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,574 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13724ms
2014-07-14 01:17:31,574 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,574 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13753ms
2014-07-14 01:17:31,574 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,574 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13783ms
2014-07-14 01:17:31,574 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,574 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13813ms
2014-07-14 01:17:31,575 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,575 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13844ms
2014-07-14 01:17:31,575 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,577 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13875ms
2014-07-14 01:17:31,578 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,578 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13898ms
2014-07-14 01:17:31,578 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,579 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13903ms
2014-07-14 01:17:31,579 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,579 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13905ms
2014-07-14 01:17:31,579 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,579 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13906ms
2014-07-14 01:17:31,579 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,579 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13906ms
2014-07-14 01:17:31,579 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,580 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13932ms
2014-07-14 01:17:31,580 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,580 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13940ms
2014-07-14 01:17:31,580 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,580 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13941ms
2014-07-14 01:17:31,580 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,589 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13949ms
2014-07-14 01:17:31,589 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,590 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13950ms
2014-07-14 01:17:31,590 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,590 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13953ms
2014-07-14 01:17:31,590 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,593 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13956ms
2014-07-14 01:17:31,593 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,593 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13955ms
2014-07-14 01:17:31,593 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,593 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13955ms
2014-07-14 01:17:31,593 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,594 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13957ms
2014-07-14 01:17:31,594 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,597 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13962ms
2014-07-14 01:17:31,597 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,598 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13964ms
2014-07-14 01:17:31,598 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,598 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13965ms
2014-07-14 01:17:31,598 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,598 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13965ms
2014-07-14 01:17:31,598 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,609 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14138ms
2014-07-14 01:17:31,609 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,609 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14168ms
2014-07-14 01:17:31,609 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,609 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14184ms
2014-07-14 01:17:31,609 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,621 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14198ms
2014-07-14 01:17:31,621 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,621 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14202ms
2014-07-14 01:17:31,621 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,622 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14199ms
2014-07-14 01:17:31,622 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,622 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14200ms
2014-07-14 01:17:31,622 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,626 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14204ms
2014-07-14 01:17:31,626 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,626 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14206ms
2014-07-14 01:17:31,626 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,627 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14206ms
2014-07-14 01:17:31,627 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,628 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14206ms
2014-07-14 01:17:31,628 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,628 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14209ms
2014-07-14 01:17:31,628 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,629 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14208ms
2014-07-14 01:17:31,629 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,629 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14209ms
2014-07-14 01:17:31,629 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,629 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14209ms
2014-07-14 01:17:31,629 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,629 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14210ms
2014-07-14 01:17:31,630 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,630 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14211ms
2014-07-14 01:17:31,630 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,632 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14216ms
2014-07-14 01:17:31,632 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,632 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14216ms
2014-07-14 01:17:31,632 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:31,641 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14225ms
2014-07-14 01:17:31,641 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:17:32,941 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17175,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325835765,"queuetimems":666,"class":"HRegionServer","responsesize":15996,"method":"Multi"}
2014-07-14 01:17:32,941 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17588,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325835353,"queuetimems":323,"class":"HRegionServer","responsesize":15489,"method":"Multi"}
2014-07-14 01:17:33,109 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16963,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325836146,"queuetimems":854,"class":"HRegionServer","responsesize":16245,"method":"Multi"}
2014-07-14 01:17:33,116 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16969,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325836146,"queuetimems":692,"class":"HRegionServer","responsesize":16017,"method":"Multi"}
2014-07-14 01:17:33,480 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:17:33,511 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28322 synced till here 28297
2014-07-14 01:17:33,671 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16262,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837408,"queuetimems":1,"class":"HRegionServer","responsesize":15612,"method":"Multi"}
2014-07-14 01:17:33,671 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16292,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837378,"queuetimems":0,"class":"HRegionServer","responsesize":16017,"method":"Multi"}
2014-07-14 01:17:33,706 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325837630 with entries=111, filesize=94.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325853480
2014-07-14 01:17:34,038 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16965,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837073,"queuetimems":1291,"class":"HRegionServer","responsesize":15989,"method":"Multi"}
2014-07-14 01:17:34,038 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16729,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837309,"queuetimems":0,"class":"HRegionServer","responsesize":16051,"method":"Multi"}
2014-07-14 01:17:34,170 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17046,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837124,"queuetimems":0,"class":"HRegionServer","responsesize":15515,"method":"Multi"}
2014-07-14 01:17:34,170 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16902,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837268,"queuetimems":0,"class":"HRegionServer","responsesize":15996,"method":"Multi"}
2014-07-14 01:17:34,170 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16351,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837819,"queuetimems":0,"class":"HRegionServer","responsesize":15806,"method":"Multi"}
2014-07-14 01:17:34,170 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17014,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837156,"queuetimems":0,"class":"HRegionServer","responsesize":15806,"method":"Multi"}
2014-07-14 01:17:34,170 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18024,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325836146,"queuetimems":927,"class":"HRegionServer","responsesize":15714,"method":"Multi"}
2014-07-14 01:17:34,170 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18023,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325836147,"queuetimems":798,"class":"HRegionServer","responsesize":15711,"method":"Multi"}
2014-07-14 01:17:34,171 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16982,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837188,"queuetimems":1,"class":"HRegionServer","responsesize":15735,"method":"Multi"}
2014-07-14 01:17:34,170 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17102,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837068,"queuetimems":1459,"class":"HRegionServer","responsesize":15520,"method":"Multi"}
2014-07-14 01:17:34,171 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16236,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837935,"queuetimems":0,"class":"HRegionServer","responsesize":15667,"method":"Multi"}
2014-07-14 01:17:34,172 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17095,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837076,"queuetimems":972,"class":"HRegionServer","responsesize":15992,"method":"Multi"}
2014-07-14 01:17:34,171 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17097,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837074,"queuetimems":1168,"class":"HRegionServer","responsesize":15941,"method":"Multi"}
2014-07-14 01:17:34,171 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16015,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325838155,"queuetimems":0,"class":"HRegionServer","responsesize":15529,"method":"Multi"}
2014-07-14 01:17:34,592 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17355,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837236,"queuetimems":0,"class":"HRegionServer","responsesize":15636,"method":"Multi"}
2014-07-14 01:17:34,592 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17247,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837344,"queuetimems":1,"class":"HRegionServer","responsesize":15667,"method":"Multi"}
2014-07-14 01:17:34,592 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16465,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325838126,"queuetimems":1,"class":"HRegionServer","responsesize":15489,"method":"Multi"}
2014-07-14 01:17:34,592 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18443,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325836149,"queuetimems":606,"class":"HRegionServer","responsesize":15612,"method":"Multi"}
2014-07-14 01:17:34,592 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18447,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325836145,"queuetimems":980,"class":"HRegionServer","responsesize":15671,"method":"Multi"}
2014-07-14 01:17:34,592 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17519,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837073,"queuetimems":1211,"class":"HRegionServer","responsesize":15731,"method":"Multi"}
2014-07-14 01:17:34,593 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17516,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837076,"queuetimems":1093,"class":"HRegionServer","responsesize":15566,"method":"Multi"}
2014-07-14 01:17:34,593 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17487,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837106,"queuetimems":1,"class":"HRegionServer","responsesize":15920,"method":"Multi"}
2014-07-14 01:17:34,593 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17516,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837077,"queuetimems":9,"class":"HRegionServer","responsesize":16062,"method":"Multi"}
2014-07-14 01:17:34,592 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17517,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837075,"queuetimems":1133,"class":"HRegionServer","responsesize":15616,"method":"Multi"}
2014-07-14 01:17:34,599 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17516,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837076,"queuetimems":1016,"class":"HRegionServer","responsesize":15876,"method":"Multi"}
2014-07-14 01:17:34,593 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17520,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837073,"queuetimems":1332,"class":"HRegionServer","responsesize":15821,"method":"Multi"}
2014-07-14 01:17:34,593 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17498,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837095,"queuetimems":3,"class":"HRegionServer","responsesize":15808,"method":"Multi"}
2014-07-14 01:17:34,727 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:17:34,730 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17259,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837469,"queuetimems":0,"class":"HRegionServer","responsesize":15731,"method":"Multi"}
2014-07-14 01:17:34,731 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17031,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837700,"queuetimems":0,"class":"HRegionServer","responsesize":15537,"method":"Multi"}
2014-07-14 01:17:34,731 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17059,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837672,"queuetimems":1,"class":"HRegionServer","responsesize":15900,"method":"Multi"}
2014-07-14 01:17:34,731 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16824,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837907,"queuetimems":1,"class":"HRegionServer","responsesize":15973,"method":"Multi"}
2014-07-14 01:17:34,733 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17294,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837439,"queuetimems":0,"class":"HRegionServer","responsesize":15520,"method":"Multi"}
2014-07-14 01:17:34,733 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17097,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837636,"queuetimems":21,"class":"HRegionServer","responsesize":15989,"method":"Multi"}
2014-07-14 01:17:34,733 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17098,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837635,"queuetimems":78,"class":"HRegionServer","responsesize":15671,"method":"Multi"}
2014-07-14 01:17:34,733 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16944,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837789,"queuetimems":1,"class":"HRegionServer","responsesize":15745,"method":"Multi"}
2014-07-14 01:17:34,734 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17098,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837635,"queuetimems":107,"class":"HRegionServer","responsesize":15714,"method":"Multi"}
2014-07-14 01:17:34,736 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16977,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837759,"queuetimems":0,"class":"HRegionServer","responsesize":16040,"method":"Multi"}
2014-07-14 01:17:34,737 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17006,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837730,"queuetimems":1,"class":"HRegionServer","responsesize":15987,"method":"Multi"}
2014-07-14 01:17:34,741 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17105,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837635,"queuetimems":50,"class":"HRegionServer","responsesize":15711,"method":"Multi"}
2014-07-14 01:17:34,741 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17094,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837647,"queuetimems":0,"class":"HRegionServer","responsesize":15821,"method":"Multi"}
2014-07-14 01:17:34,743 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16895,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837848,"queuetimems":0,"class":"HRegionServer","responsesize":15701,"method":"Multi"}
2014-07-14 01:17:34,749 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17113,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837635,"queuetimems":136,"class":"HRegionServer","responsesize":16245,"method":"Multi"}
2014-07-14 01:17:34,755 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16878,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837877,"queuetimems":0,"class":"HRegionServer","responsesize":15672,"method":"Multi"}
2014-07-14 01:17:34,782 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28419 synced till here 28395
2014-07-14 01:17:34,876 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17803,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325837073,"queuetimems":1386,"class":"HRegionServer","responsesize":15667,"method":"Multi"}
2014-07-14 01:17:35,038 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325853480 with entries=97, filesize=82.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325854727
2014-07-14 01:17:35,545 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6865, memsize=584.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/d71dbd3a466d4f9ca142167cb3a0faa7
2014-07-14 01:17:35,561 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/d71dbd3a466d4f9ca142167cb3a0faa7 as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/d71dbd3a466d4f9ca142167cb3a0faa7
2014-07-14 01:17:35,583 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/d71dbd3a466d4f9ca142167cb3a0faa7, entries=2129490, sequenceid=6865, filesize=151.6m
2014-07-14 01:17:35,584 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.2g/1287984080, currentsize=403.6m/423234000 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 36291ms, sequenceid=6865, compaction requested=true
2014-07-14 01:17:35,585 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 20 blocking
2014-07-14 01:17:35,585 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-14 01:17:35,585 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:17:35,585 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:17:35,585 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:17:35,585 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:17:35,797 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:17:35,798 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:17:35,799 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files; delaying flush up to 90000ms
2014-07-14 01:17:35,799 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:17:35,799 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 20 blocking
2014-07-14 01:17:35,799 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-14 01:17:35,800 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:17:35,800 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:17:35,800 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:17:35,811 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28523 synced till here 28500
2014-07-14 01:17:36,460 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325854727 with entries=104, filesize=86.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325855797
2014-07-14 01:17:36,460 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325717304
2014-07-14 01:17:36,460 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325719614
2014-07-14 01:17:36,460 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325721514
2014-07-14 01:17:36,460 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325748070
2014-07-14 01:17:36,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325749941
2014-07-14 01:17:36,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325752056
2014-07-14 01:17:36,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325753404
2014-07-14 01:17:36,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325754852
2014-07-14 01:17:36,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325758568
2014-07-14 01:17:36,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325760377
2014-07-14 01:17:36,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325761746
2014-07-14 01:17:36,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325763241
2014-07-14 01:17:36,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325764922
2014-07-14 01:17:36,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325766502
2014-07-14 01:17:36,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325768195
2014-07-14 01:17:36,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325769655
2014-07-14 01:17:36,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325770697
2014-07-14 01:17:36,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325772203
2014-07-14 01:17:36,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325773901
2014-07-14 01:17:36,462 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325776122
2014-07-14 01:17:37,334 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:17:37,389 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28602 synced till here 28599
2014-07-14 01:17:37,457 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325855797 with entries=79, filesize=66.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325857334
2014-07-14 01:17:38,478 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:17:38,493 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28676 synced till here 28674
2014-07-14 01:17:38,505 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325857334 with entries=74, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325858478
2014-07-14 01:17:42,200 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:17:42,221 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28756 synced till here 28750
2014-07-14 01:17:42,291 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325858478 with entries=80, filesize=68.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325862202
2014-07-14 01:17:43,591 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:17:43,591 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files; delaying flush up to 90000ms
2014-07-14 01:17:43,592 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:17:43,592 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 20 blocking
2014-07-14 01:17:43,592 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-14 01:17:43,593 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:17:43,593 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:17:43,593 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:17:43,615 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:17:43,635 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28833 synced till here 28830
2014-07-14 01:17:43,683 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325862202 with entries=77, filesize=66.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325863616
2014-07-14 01:17:44,927 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:17:44,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28912 synced till here 28905
2014-07-14 01:17:45,005 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325863616 with entries=79, filesize=67.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325864927
2014-07-14 01:17:46,339 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:17:46,377 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28988 synced till here 28986
2014-07-14 01:17:46,432 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325864927 with entries=76, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325866339
2014-07-14 01:17:47,158 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:17:47,317 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29068 synced till here 29067
2014-07-14 01:17:47,341 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325866339 with entries=80, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325867158
2014-07-14 01:17:47,341 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:17:48,976 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:17:49,665 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29165 synced till here 29164
2014-07-14 01:17:49,684 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325867158 with entries=97, filesize=83.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325868977
2014-07-14 01:17:49,685 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:17:50,668 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:17:51,444 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29261 synced till here 29260
2014-07-14 01:17:51,463 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325868977 with entries=96, filesize=82.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325870669
2014-07-14 01:17:51,463 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:17:52,369 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:17:52,397 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29336 synced till here 29335
2014-07-14 01:17:52,444 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325870669 with entries=75, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325872370
2014-07-14 01:17:52,445 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:17:53,540 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:17:53,540 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files, but is 1.3g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:17:53,540 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. due to global heap pressure
2014-07-14 01:17:53,541 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 1.3g
2014-07-14 01:17:53,777 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:17:54,009 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29423 synced till here 29422
2014-07-14 01:17:54,025 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325872370 with entries=87, filesize=74.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325873778
2014-07-14 01:17:54,096 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:17:54,096 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files, but is 1.3g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:17:54,096 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. due to global heap pressure
2014-07-14 01:17:54,096 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 1.3g
2014-07-14 01:17:55,313 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:17:55,504 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:17:55,637 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325873778 with entries=91, filesize=78.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325875313
2014-07-14 01:17:55,680 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:55,690 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:55,701 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:55,726 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:55,729 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:55,736 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:55,765 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:55,795 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:55,796 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:17:55,828 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:55,862 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:55,896 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:55,926 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:55,962 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:56,015 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:56,070 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:56,128 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:56,189 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:56,253 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:56,310 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:56,359 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:56,422 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:56,479 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:56,530 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:56,696 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:56,755 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:56,808 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:57,653 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:57,661 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:57,673 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:57,702 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:57,732 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:57,769 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:57,827 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:57,884 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:57,918 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:57,950 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:57,990 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:58,015 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:58,053 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:58,088 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:58,126 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:58,182 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:58,218 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:58,260 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:58,295 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:58,331 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:58,368 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:58,407 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:58,449 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:17:58,504 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:00,681 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:18:00,691 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:18:00,701 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:18:00,726 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:18:00,729 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:18:00,736 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:18:00,765 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:18:00,795 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:18:00,829 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:18:00,862 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:18:00,897 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:18:00,926 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:18:00,962 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:18:01,015 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:18:01,071 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:18:01,128 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:18:01,190 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:18:01,254 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:18:01,310 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:18:01,360 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:18:01,423 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:18:01,480 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:18:01,530 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:18:01,697 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:18:01,755 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:18:01,809 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:18:03,027 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5012ms
2014-07-14 01:18:03,028 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5374ms
2014-07-14 01:18:03,028 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5367ms
2014-07-14 01:18:03,028 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5355ms
2014-07-14 01:18:03,028 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5327ms
2014-07-14 01:18:03,029 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5297ms
2014-07-14 01:18:03,029 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5260ms
2014-07-14 01:18:03,029 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5111ms
2014-07-14 01:18:03,030 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5203ms
2014-07-14 01:18:03,030 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5146ms
2014-07-14 01:18:03,031 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5081ms
2014-07-14 01:18:03,031 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5041ms
2014-07-14 01:18:03,054 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:18:03,088 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:18:03,126 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:18:03,182 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:18:03,218 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:18:03,261 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:18:03,296 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:18:03,332 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:18:03,368 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:18:03,407 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:18:03,449 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:18:03,504 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:18:05,682 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:18:05,691 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:18:05,701 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:18:05,727 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:18:05,730 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:18:05,737 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:18:05,766 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:18:05,796 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:18:05,829 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:18:05,863 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:18:05,897 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:18:05,927 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:18:05,963 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:18:06,016 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:18:06,071 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:18:06,129 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:18:06,190 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:18:06,254 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:18:06,310 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:18:06,360 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:18:06,424 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:18:06,481 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:18:06,531 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:18:06,698 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:18:07,992 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11184ms
2014-07-14 01:18:07,993 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11239ms
2014-07-14 01:18:08,028 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10012ms
2014-07-14 01:18:08,028 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10375ms
2014-07-14 01:18:08,029 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10367ms
2014-07-14 01:18:08,029 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10297ms
2014-07-14 01:18:08,029 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10328ms
2014-07-14 01:18:08,029 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10356ms
2014-07-14 01:18:08,030 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10260ms
2014-07-14 01:18:08,030 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10112ms
2014-07-14 01:18:08,030 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10203ms
2014-07-14 01:18:08,031 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10147ms
2014-07-14 01:18:08,031 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10081ms
2014-07-14 01:18:08,032 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10041ms
2014-07-14 01:18:08,054 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:18:08,089 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:18:08,126 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:18:08,182 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:18:08,219 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:18:08,261 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:18:08,296 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:18:08,332 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:18:08,369 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:18:08,408 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:18:08,449 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:18:08,505 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:18:10,682 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:18:10,692 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:18:10,701 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:18:10,727 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:18:10,730 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:18:10,737 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:18:10,766 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:18:10,796 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:18:10,830 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:18:10,863 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:18:10,897 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:18:10,927 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:18:10,963 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:18:11,016 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:18:11,071 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:18:11,130 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:18:11,191 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:18:11,254 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:18:11,311 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:18:11,360 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:18:11,424 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:18:11,481 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:18:11,531 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:18:11,698 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:18:13,683 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15178ms
2014-07-14 01:18:13,683 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15315ms
2014-07-14 01:18:13,683 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15277ms
2014-07-14 01:18:13,684 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15236ms
2014-07-14 01:18:13,684 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15502ms
2014-07-14 01:18:13,684 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15466ms
2014-07-14 01:18:13,685 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15425ms
2014-07-14 01:18:13,685 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15632ms
2014-07-14 01:18:13,685 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15767ms
2014-07-14 01:18:13,686 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15953ms
2014-07-14 01:18:13,686 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15391ms
2014-07-14 01:18:13,686 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15355ms
2014-07-14 01:18:13,687 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16879ms
2014-07-14 01:18:13,687 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16933ms
2014-07-14 01:18:13,687 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15672ms
2014-07-14 01:18:13,687 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16034ms
2014-07-14 01:18:13,687 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16026ms
2014-07-14 01:18:13,688 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15987ms
2014-07-14 01:18:13,688 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16015ms
2014-07-14 01:18:13,688 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15919ms
2014-07-14 01:18:13,689 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15861ms
2014-07-14 01:18:13,689 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15805ms
2014-07-14 01:18:13,689 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15739ms
2014-07-14 01:18:13,690 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15700ms
2014-07-14 01:18:13,690 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15602ms
2014-07-14 01:18:13,690 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15564ms
2014-07-14 01:18:15,683 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:18:15,692 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:18:15,702 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:18:15,727 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:18:15,730 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:18:15,737 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:18:15,767 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:18:15,797 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:18:15,830 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:18:15,863 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:18:15,898 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:18:15,927 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:18:15,964 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:18:16,016 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:18:16,072 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:18:16,130 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:18:16,191 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:18:16,255 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:18:16,311 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:18:16,361 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:18:16,424 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:18:16,481 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:18:16,531 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:18:16,699 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:18:18,683 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20179ms
2014-07-14 01:18:18,683 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20315ms
2014-07-14 01:18:18,684 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20278ms
2014-07-14 01:18:18,685 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20236ms
2014-07-14 01:18:18,685 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20467ms
2014-07-14 01:18:18,685 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20503ms
2014-07-14 01:18:18,685 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20425ms
2014-07-14 01:18:18,685 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20632ms
2014-07-14 01:18:18,686 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20768ms
2014-07-14 01:18:18,686 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20954ms
2014-07-14 01:18:18,687 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20391ms
2014-07-14 01:18:18,688 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20356ms
2014-07-14 01:18:18,688 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21027ms
2014-07-14 01:18:18,688 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21035ms
2014-07-14 01:18:18,688 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20673ms
2014-07-14 01:18:18,688 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21015ms
2014-07-14 01:18:18,689 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21934ms
2014-07-14 01:18:18,689 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20805ms
2014-07-14 01:18:18,690 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21882ms
2014-07-14 01:18:18,690 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20700ms
2014-07-14 01:18:18,690 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20602ms
2014-07-14 01:18:18,691 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20741ms
2014-07-14 01:18:18,691 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20565ms
2014-07-14 01:18:18,691 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20864ms
2014-07-14 01:18:18,691 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20922ms
2014-07-14 01:18:18,691 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20990ms
2014-07-14 01:18:20,410 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7384, memsize=514.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/52f40d66044b4aa98a51f5085be80452
2014-07-14 01:18:20,432 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/52f40d66044b4aa98a51f5085be80452 as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/52f40d66044b4aa98a51f5085be80452
2014-07-14 01:18:20,451 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/52f40d66044b4aa98a51f5085be80452, entries=1873230, sequenceid=7384, filesize=133.3m
2014-07-14 01:18:20,451 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.3g/1366516720, currentsize=31.1m/32615200 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 26355ms, sequenceid=7384, compaction requested=true
2014-07-14 01:18:20,452 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:18:20,452 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 20 blocking
2014-07-14 01:18:20,452 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22751ms
2014-07-14 01:18:20,453 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-14 01:18:20,453 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 92744ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:18:20,453 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:18:20,453 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,453 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:18:20,453 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22684ms
2014-07-14 01:18:20,453 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:18:20,453 DEBUG [MemStoreFlusher.0] regionserver.HRegion: NOT flushing memstore for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., flushing=true, writesEnabled=true
2014-07-14 01:18:20,453 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,453 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22626ms
2014-07-14 01:18:20,453 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,454 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22328ms
2014-07-14 01:18:20,454 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,454 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22504ms
2014-07-14 01:18:20,454 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,454 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22366ms
2014-07-14 01:18:20,454 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,454 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22464ms
2014-07-14 01:18:20,454 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,454 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23646ms
2014-07-14 01:18:20,454 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,454 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22570ms
2014-07-14 01:18:20,455 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,455 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23701ms
2014-07-14 01:18:20,455 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,456 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22783ms
2014-07-14 01:18:20,456 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,456 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22441ms
2014-07-14 01:18:20,456 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,462 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22809ms
2014-07-14 01:18:20,462 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,462 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22801ms
2014-07-14 01:18:20,462 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,462 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22131ms
2014-07-14 01:18:20,463 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,463 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22168ms
2014-07-14 01:18:20,463 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,463 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22731ms
2014-07-14 01:18:20,463 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,473 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22555ms
2014-07-14 01:18:20,473 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,473 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22420ms
2014-07-14 01:18:20,473 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,473 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22213ms
2014-07-14 01:18:20,473 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,473 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22291ms
2014-07-14 01:18:20,473 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,474 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22256ms
2014-07-14 01:18:20,474 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,474 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22026ms
2014-07-14 01:18:20,474 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,474 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22068ms
2014-07-14 01:18:20,474 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,475 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22106ms
2014-07-14 01:18:20,475 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,479 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21975ms
2014-07-14 01:18:20,479 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,479 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23783ms
2014-07-14 01:18:20,479 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,479 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23949ms
2014-07-14 01:18:20,479 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,479 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24000ms
2014-07-14 01:18:20,479 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,479 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24057ms
2014-07-14 01:18:20,479 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,479 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24120ms
2014-07-14 01:18:20,480 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,480 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24170ms
2014-07-14 01:18:20,480 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,481 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24227ms
2014-07-14 01:18:20,481 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,481 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24292ms
2014-07-14 01:18:20,481 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,481 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24353ms
2014-07-14 01:18:20,481 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,481 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24411ms
2014-07-14 01:18:20,481 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,482 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24468ms
2014-07-14 01:18:20,482 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,482 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24520ms
2014-07-14 01:18:20,483 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,483 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24557ms
2014-07-14 01:18:20,483 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,484 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24588ms
2014-07-14 01:18:20,484 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,484 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24622ms
2014-07-14 01:18:20,484 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,484 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24656ms
2014-07-14 01:18:20,484 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,485 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24689ms
2014-07-14 01:18:20,485 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,485 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24720ms
2014-07-14 01:18:20,485 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,485 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24749ms
2014-07-14 01:18:20,485 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,485 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24756ms
2014-07-14 01:18:20,485 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,485 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24760ms
2014-07-14 01:18:20,485 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,487 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24787ms
2014-07-14 01:18:20,487 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,487 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24797ms
2014-07-14 01:18:20,487 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,493 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24813ms
2014-07-14 01:18:20,493 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:18:20,967 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25413,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325875553,"queuetimems":0,"class":"HRegionServer","responsesize":15806,"method":"Multi"}
2014-07-14 01:18:20,967 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25470,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325875496,"queuetimems":0,"class":"HRegionServer","responsesize":15972,"method":"Multi"}
2014-07-14 01:18:21,041 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7375, memsize=526.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/3dd13f6e2f3a47b08512d7bd11a73d9f
2014-07-14 01:18:21,055 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/3dd13f6e2f3a47b08512d7bd11a73d9f as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/3dd13f6e2f3a47b08512d7bd11a73d9f
2014-07-14 01:18:21,076 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/3dd13f6e2f3a47b08512d7bd11a73d9f, entries=1915040, sequenceid=7375, filesize=136.3m
2014-07-14 01:18:21,077 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.3g/1390522560, currentsize=52.7m/55308640 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 27536ms, sequenceid=7375, compaction requested=true
2014-07-14 01:18:21,077 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:18:21,077 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 20 blocking
2014-07-14 01:18:21,077 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-14 01:18:21,077 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:18:21,077 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:18:21,078 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:18:21,079 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25417,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325875661,"queuetimems":0,"class":"HRegionServer","responsesize":15948,"method":"Multi"}
2014-07-14 01:18:21,079 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25469,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325875609,"queuetimems":0,"class":"HRegionServer","responsesize":15658,"method":"Multi"}
2014-07-14 01:18:21,256 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:18:21,273 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29628 synced till here 29592
2014-07-14 01:18:22,470 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325875313 with entries=114, filesize=96.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325901256
2014-07-14 01:18:22,471 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325777927
2014-07-14 01:18:22,471 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325779191
2014-07-14 01:18:22,471 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325798003
2014-07-14 01:18:22,473 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325799894
2014-07-14 01:18:22,473 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325801792
2014-07-14 01:18:22,473 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325803469
2014-07-14 01:18:22,473 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325805455
2014-07-14 01:18:22,473 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325807350
2014-07-14 01:18:22,473 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325809196
2014-07-14 01:18:22,473 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325810606
2014-07-14 01:18:22,473 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325812269
2014-07-14 01:18:22,473 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325813655
2014-07-14 01:18:22,473 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325815181
2014-07-14 01:18:22,473 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325816420
2014-07-14 01:18:22,875 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25108,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325877767,"queuetimems":1,"class":"HRegionServer","responsesize":15811,"method":"Multi"}
2014-07-14 01:18:22,876 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24759,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325878116,"queuetimems":0,"class":"HRegionServer","responsesize":15889,"method":"Multi"}
2014-07-14 01:18:23,276 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25227,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325878049,"queuetimems":1,"class":"HRegionServer","responsesize":15264,"method":"Multi"}
2014-07-14 01:18:23,276 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25063,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325878213,"queuetimems":1,"class":"HRegionServer","responsesize":15745,"method":"Multi"}
2014-07-14 01:18:23,276 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25605,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325877671,"queuetimems":0,"class":"HRegionServer","responsesize":16117,"method":"Multi"}
2014-07-14 01:18:23,276 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24912,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325878364,"queuetimems":0,"class":"HRegionServer","responsesize":15948,"method":"Multi"}
2014-07-14 01:18:23,277 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27353,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325875924,"queuetimems":1,"class":"HRegionServer","responsesize":15617,"method":"Multi"}
2014-07-14 01:18:23,278 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24876,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325878401,"queuetimems":0,"class":"HRegionServer","responsesize":15631,"method":"Multi"}
2014-07-14 01:18:23,278 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25452,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325877825,"queuetimems":1,"class":"HRegionServer","responsesize":15653,"method":"Multi"}
2014-07-14 01:18:23,278 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24986,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325878292,"queuetimems":1,"class":"HRegionServer","responsesize":15462,"method":"Multi"}
2014-07-14 01:18:23,279 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25196,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325878083,"queuetimems":1,"class":"HRegionServer","responsesize":15525,"method":"Multi"}
2014-07-14 01:18:23,276 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24950,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325878325,"queuetimems":0,"class":"HRegionServer","responsesize":15951,"method":"Multi"}
2014-07-14 01:18:23,279 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25579,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325877700,"queuetimems":0,"class":"HRegionServer","responsesize":15917,"method":"Multi"}
2014-07-14 01:18:23,278 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27417,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325875860,"queuetimems":0,"class":"HRegionServer","responsesize":15877,"method":"Multi"}
2014-07-14 01:18:23,278 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25021,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325878256,"queuetimems":0,"class":"HRegionServer","responsesize":15972,"method":"Multi"}
2014-07-14 01:18:24,338 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:18:24,341 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26363,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325877977,"queuetimems":0,"class":"HRegionServer","responsesize":16123,"method":"Multi"}
2014-07-14 01:18:24,341 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26682,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325877659,"queuetimems":0,"class":"HRegionServer","responsesize":15922,"method":"Multi"}
2014-07-14 01:18:24,341 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28548,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325875793,"queuetimems":0,"class":"HRegionServer","responsesize":15462,"method":"Multi"}
2014-07-14 01:18:24,342 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26329,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325878012,"queuetimems":0,"class":"HRegionServer","responsesize":15658,"method":"Multi"}
2014-07-14 01:18:24,342 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26459,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325877882,"queuetimems":0,"class":"HRegionServer","responsesize":16321,"method":"Multi"}
2014-07-14 01:18:24,342 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25840,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325878501,"queuetimems":0,"class":"HRegionServer","responsesize":15897,"method":"Multi"}
2014-07-14 01:18:24,342 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27813,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325876528,"queuetimems":0,"class":"HRegionServer","responsesize":15898,"method":"Multi"}
2014-07-14 01:18:24,342 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27588,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325876753,"queuetimems":0,"class":"HRegionServer","responsesize":15956,"method":"Multi"}
2014-07-14 01:18:24,342 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28034,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325876308,"queuetimems":1,"class":"HRegionServer","responsesize":15951,"method":"Multi"}
2014-07-14 01:18:24,358 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29748 synced till here 29714
2014-07-14 01:18:24,612 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27917,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325876695,"queuetimems":1,"class":"HRegionServer","responsesize":15660,"method":"Multi"}
2014-07-14 01:18:24,613 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28487,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325876125,"queuetimems":0,"class":"HRegionServer","responsesize":15653,"method":"Multi"}
2014-07-14 01:18:24,613 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28719,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325875894,"queuetimems":1,"class":"HRegionServer","responsesize":15525,"method":"Multi"}
2014-07-14 01:18:24,612 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28425,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325876187,"queuetimems":0,"class":"HRegionServer","responsesize":15842,"method":"Multi"}
2014-07-14 01:18:24,612 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28192,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325876420,"queuetimems":0,"class":"HRegionServer","responsesize":16321,"method":"Multi"}
2014-07-14 01:18:24,613 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28256,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325876357,"queuetimems":1,"class":"HRegionServer","responsesize":15716,"method":"Multi"}
2014-07-14 01:18:24,614 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28851,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325875763,"queuetimems":0,"class":"HRegionServer","responsesize":15264,"method":"Multi"}
2014-07-14 01:18:24,614 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26884,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325877730,"queuetimems":0,"class":"HRegionServer","responsesize":15590,"method":"Multi"}
2014-07-14 01:18:24,614 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26702,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325877912,"queuetimems":1,"class":"HRegionServer","responsesize":15904,"method":"Multi"}
2014-07-14 01:18:24,615 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28546,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325876068,"queuetimems":0,"class":"HRegionServer","responsesize":15745,"method":"Multi"}
2014-07-14 01:18:24,615 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26171,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325878443,"queuetimems":1,"class":"HRegionServer","responsesize":15528,"method":"Multi"}
2014-07-14 01:18:24,615 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28881,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325875734,"queuetimems":1,"class":"HRegionServer","responsesize":15821,"method":"Multi"}
2014-07-14 01:18:24,615 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28789,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325875826,"queuetimems":0,"class":"HRegionServer","responsesize":15631,"method":"Multi"}
2014-07-14 01:18:24,617 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28919,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325875698,"queuetimems":0,"class":"HRegionServer","responsesize":15889,"method":"Multi"}
2014-07-14 01:18:24,621 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26444,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325878177,"queuetimems":0,"class":"HRegionServer","responsesize":15877,"method":"Multi"}
2014-07-14 01:18:24,629 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27761,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325876868,"queuetimems":1,"class":"HRegionServer","responsesize":15643,"method":"Multi"}
2014-07-14 01:18:24,637 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26693,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325877944,"queuetimems":0,"class":"HRegionServer","responsesize":15716,"method":"Multi"}
2014-07-14 01:18:24,637 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27830,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325876807,"queuetimems":0,"class":"HRegionServer","responsesize":15396,"method":"Multi"}
2014-07-14 01:18:24,659 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325901256 with entries=120, filesize=100.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325904338
2014-07-14 01:18:24,669 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28656,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325876012,"queuetimems":0,"class":"HRegionServer","responsesize":15897,"method":"Multi"}
2014-07-14 01:18:24,669 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28708,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325875960,"queuetimems":0,"class":"HRegionServer","responsesize":15528,"method":"Multi"}
2014-07-14 01:18:24,940 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28462,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325876477,"queuetimems":0,"class":"HRegionServer","responsesize":16123,"method":"Multi"}
2014-07-14 01:18:24,940 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28689,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325876250,"queuetimems":0,"class":"HRegionServer","responsesize":15904,"method":"Multi"}
2014-07-14 01:18:25,301 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:18:25,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29861 synced till here 29825
2014-07-14 01:18:26,001 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325904338 with entries=113, filesize=97.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325905301
2014-07-14 01:18:26,725 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:18:26,800 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29970 synced till here 29935
2014-07-14 01:18:26,994 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325905301 with entries=109, filesize=93.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325906725
2014-07-14 01:18:28,539 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:18:28,691 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30065 synced till here 30042
2014-07-14 01:18:28,937 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325906725 with entries=95, filesize=81.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325908541
2014-07-14 01:18:29,151 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:18:29,151 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files; delaying flush up to 90000ms
2014-07-14 01:18:29,152 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:18:29,152 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 20 blocking
2014-07-14 01:18:29,153 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-14 01:18:29,153 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:18:29,153 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:18:29,153 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:18:30,669 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:18:30,669 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files; delaying flush up to 90000ms
2014-07-14 01:18:30,670 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 20 blocking
2014-07-14 01:18:30,670 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-14 01:18:30,670 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:18:30,670 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:18:30,670 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:18:30,670 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:18:30,876 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:18:31,043 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30190 synced till here 30149
2014-07-14 01:18:31,393 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325908541 with entries=125, filesize=107.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325910876
2014-07-14 01:18:33,087 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:18:33,140 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30309 synced till here 30280
2014-07-14 01:18:33,562 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325910876 with entries=119, filesize=102.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325913088
2014-07-14 01:18:35,265 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:18:35,614 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30437 synced till here 30424
2014-07-14 01:18:35,805 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325913088 with entries=128, filesize=109.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325915265
2014-07-14 01:18:37,399 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:18:37,471 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30548 synced till here 30528
2014-07-14 01:18:37,663 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325915265 with entries=111, filesize=95.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325917399
2014-07-14 01:18:37,664 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:18:38,581 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:18:39,318 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30653 synced till here 30633
2014-07-14 01:18:39,520 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325917399 with entries=105, filesize=90.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325918581
2014-07-14 01:18:39,521 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:18:40,419 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:18:40,437 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30762 synced till here 30729
2014-07-14 01:18:42,000 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325918581 with entries=109, filesize=93.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325920420
2014-07-14 01:18:42,001 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:18:42,739 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:18:42,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30853 synced till here 30835
2014-07-14 01:18:42,911 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325920420 with entries=91, filesize=78.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325922739
2014-07-14 01:18:42,913 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:18:44,991 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:18:45,027 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30947 synced till here 30934
2014-07-14 01:18:45,711 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325922739 with entries=94, filesize=80.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325924991
2014-07-14 01:18:45,732 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:18:50,824 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:18:50,923 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325924991 with entries=73, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325930825
2014-07-14 01:18:50,923 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:18:52,774 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:18:52,775 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files, but is 1.4g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:18:52,775 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. due to global heap pressure
2014-07-14 01:18:52,775 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 1.4g
2014-07-14 01:18:53,465 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:18:53,465 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files, but is 1.1g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:18:53,465 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. due to global heap pressure
2014-07-14 01:18:53,466 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 1.1g
2014-07-14 01:18:53,627 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:18:53,676 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31109 synced till here 31094
2014-07-14 01:18:53,806 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325930825 with entries=89, filesize=76.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325933628
2014-07-14 01:18:54,564 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:18:54,579 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31188 synced till here 31183
2014-07-14 01:18:55,104 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325933628 with entries=79, filesize=67.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325934564
2014-07-14 01:18:55,125 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:18:55,330 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:55,330 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:55,333 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:55,333 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:55,349 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:55,351 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:55,353 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:55,367 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:55,367 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:55,371 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:55,373 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:55,402 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:18:55,416 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:55,457 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:55,510 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:55,539 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:55,589 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:55,618 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:55,648 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:55,677 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:55,707 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:55,736 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:55,787 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:55,813 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:55,842 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:55,870 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:55,899 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:55,928 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:55,959 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:55,985 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:56,014 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:58,210 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:58,254 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:58,280 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:58,318 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:58,351 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:58,385 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:58,416 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:58,450 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:59,480 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:59,490 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:59,504 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:59,534 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:59,563 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:59,593 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:59,623 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:59,659 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:59,689 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:59,723 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:18:59,760 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:00,330 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:00,330 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:00,333 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:00,334 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:00,349 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:00,352 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:00,353 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:00,367 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:00,367 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:00,372 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:00,374 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:00,418 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-14 01:19:00,457 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:00,510 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:00,539 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:00,590 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:00,618 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:00,648 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:00,677 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:00,707 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:00,737 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:00,788 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:00,813 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:00,842 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:00,870 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:00,899 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:00,929 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:00,959 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:00,985 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:01,015 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:01,623 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:03,210 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:03,254 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:03,280 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:03,318 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:03,352 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:03,386 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:03,416 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:03,451 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:04,481 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:04,490 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:04,505 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:04,537 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:04,564 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:04,594 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:04,623 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:04,660 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:04,691 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:04,724 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:04,761 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:05,330 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:19:05,331 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:05,334 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:05,334 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:05,349 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:05,352 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:05,354 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:19:05,367 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:05,368 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:19:05,372 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:05,374 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:05,419 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-14 01:19:05,458 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:05,510 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:19:05,539 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:19:05,590 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:05,618 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:19:05,649 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:05,678 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:19:05,708 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:19:05,737 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:05,789 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:05,814 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:19:05,842 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:19:06,669 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5046ms
2014-07-14 01:19:06,669 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10684ms
2014-07-14 01:19:06,670 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10799ms
2014-07-14 01:19:06,670 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10656ms
2014-07-14 01:19:06,670 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10772ms
2014-07-14 01:19:06,670 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10742ms
2014-07-14 01:19:06,672 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10712ms
2014-07-14 01:19:08,211 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:08,255 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:19:08,281 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:08,319 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:19:08,352 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:08,386 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:08,417 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:08,452 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:09,481 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:09,491 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:09,506 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:19:09,538 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10004ms
2014-07-14 01:19:09,565 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:19:09,594 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:09,624 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:09,661 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:19:09,692 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10004ms
2014-07-14 01:19:09,725 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-14 01:19:09,762 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:19:10,331 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:19:10,331 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:19:10,334 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:19:10,335 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:19:10,350 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:19:10,353 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:19:10,354 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:19:10,367 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:19:10,368 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:19:10,372 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:19:10,374 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:19:10,419 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15003ms
2014-07-14 01:19:10,458 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:19:10,511 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:19:10,540 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 01:19:10,590 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:19:10,618 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 01:19:10,649 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:19:10,678 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:19:10,708 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:19:10,738 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:19:10,789 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:19:10,814 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:19:10,842 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 01:19:11,670 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10046ms
2014-07-14 01:19:11,670 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15800ms
2014-07-14 01:19:11,670 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15685ms
2014-07-14 01:19:11,671 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15773ms
2014-07-14 01:19:11,671 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15657ms
2014-07-14 01:19:11,672 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15744ms
2014-07-14 01:19:11,672 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15713ms
2014-07-14 01:19:11,810 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7809, memsize=378.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/cf600bcbcb894296b399853fba8bb01a
2014-07-14 01:19:11,824 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/cf600bcbcb894296b399853fba8bb01a as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/cf600bcbcb894296b399853fba8bb01a
2014-07-14 01:19:11,834 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/cf600bcbcb894296b399853fba8bb01a, entries=1379360, sequenceid=7809, filesize=98.3m
2014-07-14 01:19:11,834 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.1g/1198327840, currentsize=48.1m/50464720 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 18368ms, sequenceid=7809, compaction requested=true
2014-07-14 01:19:11,835 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:19:11,835 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 20 blocking
2014-07-14 01:19:11,835 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-14 01:19:11,835 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15876ms
2014-07-14 01:19:11,835 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:19:11,835 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,835 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:19:11,835 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15907ms
2014-07-14 01:19:11,835 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,835 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:19:11,836 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15821ms
2014-07-14 01:19:11,836 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,836 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15938ms
2014-07-14 01:19:11,836 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,836 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15851ms
2014-07-14 01:19:11,836 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,837 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15967ms
2014-07-14 01:19:11,837 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,837 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10214ms
2014-07-14 01:19:11,837 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,849 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16007ms
2014-07-14 01:19:11,849 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,849 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16036ms
2014-07-14 01:19:11,849 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,858 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16071ms
2014-07-14 01:19:11,858 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,858 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16122ms
2014-07-14 01:19:11,859 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,859 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16152ms
2014-07-14 01:19:11,859 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,859 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16182ms
2014-07-14 01:19:11,859 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,860 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16213ms
2014-07-14 01:19:11,860 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,862 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16244ms
2014-07-14 01:19:11,862 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,862 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16273ms
2014-07-14 01:19:11,862 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,862 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16323ms
2014-07-14 01:19:11,863 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,864 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16354ms
2014-07-14 01:19:11,864 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,873 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16416ms
2014-07-14 01:19:11,873 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,877 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16461ms
2014-07-14 01:19:11,877 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,877 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16504ms
2014-07-14 01:19:11,877 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,882 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16511ms
2014-07-14 01:19:11,882 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,882 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16515ms
2014-07-14 01:19:11,882 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,885 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16519ms
2014-07-14 01:19:11,885 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,885 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16532ms
2014-07-14 01:19:11,885 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,893 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16542ms
2014-07-14 01:19:11,893 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,893 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16545ms
2014-07-14 01:19:11,893 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,893 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16560ms
2014-07-14 01:19:11,893 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,894 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16561ms
2014-07-14 01:19:11,894 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,894 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16564ms
2014-07-14 01:19:11,894 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,895 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16564ms
2014-07-14 01:19:11,895 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,895 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12135ms
2014-07-14 01:19:11,895 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,905 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12183ms
2014-07-14 01:19:11,905 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,909 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12221ms
2014-07-14 01:19:11,910 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,910 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12251ms
2014-07-14 01:19:11,910 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,910 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12287ms
2014-07-14 01:19:11,910 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,911 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12318ms
2014-07-14 01:19:11,911 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,911 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12348ms
2014-07-14 01:19:11,911 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,911 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12377ms
2014-07-14 01:19:11,912 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,912 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12408ms
2014-07-14 01:19:11,912 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,912 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12422ms
2014-07-14 01:19:11,912 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,916 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12436ms
2014-07-14 01:19:11,916 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,919 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13467ms
2014-07-14 01:19:11,920 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,921 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13505ms
2014-07-14 01:19:11,921 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,928 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13543ms
2014-07-14 01:19:11,928 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,929 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13578ms
2014-07-14 01:19:11,929 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,930 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13613ms
2014-07-14 01:19:11,931 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,931 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13651ms
2014-07-14 01:19:11,931 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,931 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13678ms
2014-07-14 01:19:11,931 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:11,933 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13721ms
2014-07-14 01:19:11,933 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:19:13,118 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18024,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325935094,"queuetimems":1,"class":"HRegionServer","responsesize":15903,"method":"Multi"}
2014-07-14 01:19:13,118 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18605,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325934513,"queuetimems":1,"class":"HRegionServer","responsesize":15895,"method":"Multi"}
2014-07-14 01:19:13,439 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:19:13,506 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31296 synced till here 31276
2014-07-14 01:19:13,726 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18578,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325935147,"queuetimems":1,"class":"HRegionServer","responsesize":15574,"method":"Multi"}
2014-07-14 01:19:13,746 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325934564 with entries=108, filesize=90.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325953440
2014-07-14 01:19:13,932 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90341ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:19:13,933 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 110.7m
2014-07-14 01:19:14,319 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19111,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325935208,"queuetimems":0,"class":"HRegionServer","responsesize":15450,"method":"Multi"}
2014-07-14 01:19:14,319 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19199,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325935120,"queuetimems":0,"class":"HRegionServer","responsesize":16071,"method":"Multi"}
2014-07-14 01:19:14,319 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19139,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325935180,"queuetimems":0,"class":"HRegionServer","responsesize":15791,"method":"Multi"}
2014-07-14 01:19:14,319 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19216,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325935103,"queuetimems":0,"class":"HRegionServer","responsesize":15779,"method":"Multi"}
2014-07-14 01:19:14,347 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19026,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325935320,"queuetimems":0,"class":"HRegionServer","responsesize":16122,"method":"Multi"}
2014-07-14 01:19:15,015 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:19:15,062 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:19:15,064 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19050,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325936013,"queuetimems":1,"class":"HRegionServer","responsesize":15998,"method":"Multi"}
2014-07-14 01:19:15,064 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19788,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325935275,"queuetimems":0,"class":"HRegionServer","responsesize":15553,"method":"Multi"}
2014-07-14 01:19:15,064 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19137,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325935926,"queuetimems":0,"class":"HRegionServer","responsesize":15779,"method":"Multi"}
2014-07-14 01:19:15,064 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19166,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325935897,"queuetimems":0,"class":"HRegionServer","responsesize":15895,"method":"Multi"}
2014-07-14 01:19:15,064 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19823,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325935241,"queuetimems":0,"class":"HRegionServer","responsesize":15875,"method":"Multi"}
2014-07-14 01:19:15,064 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19109,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325935955,"queuetimems":0,"class":"HRegionServer","responsesize":16071,"method":"Multi"}
2014-07-14 01:19:15,127 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31403 synced till here 31368
2014-07-14 01:19:15,365 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19554,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325935811,"queuetimems":0,"class":"HRegionServer","responsesize":15751,"method":"Multi"}
2014-07-14 01:19:15,366 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19661,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325935704,"queuetimems":1,"class":"HRegionServer","responsesize":15740,"method":"Multi"}
2014-07-14 01:19:15,366 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19750,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325935616,"queuetimems":0,"class":"HRegionServer","responsesize":15785,"method":"Multi"}
2014-07-14 01:19:15,369 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19831,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325935537,"queuetimems":0,"class":"HRegionServer","responsesize":16060,"method":"Multi"}
2014-07-14 01:19:15,373 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19590,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325935783,"queuetimems":0,"class":"HRegionServer","responsesize":15741,"method":"Multi"}
2014-07-14 01:19:15,381 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19398,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325935983,"queuetimems":0,"class":"HRegionServer","responsesize":15668,"method":"Multi"}
2014-07-14 01:19:15,381 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19513,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325935868,"queuetimems":0,"class":"HRegionServer","responsesize":15903,"method":"Multi"}
2014-07-14 01:19:15,385 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20013,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325935372,"queuetimems":0,"class":"HRegionServer","responsesize":16025,"method":"Multi"}
2014-07-14 01:19:15,389 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19743,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325935646,"queuetimems":1,"class":"HRegionServer","responsesize":15858,"method":"Multi"}
2014-07-14 01:19:15,390 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13768,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325941621,"queuetimems":1,"class":"HRegionServer","responsesize":15663,"method":"Multi"}
2014-07-14 01:19:15,396 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19809,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325935587,"queuetimems":0,"class":"HRegionServer","responsesize":16028,"method":"Multi"}
2014-07-14 01:19:15,397 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19941,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325935455,"queuetimems":0,"class":"HRegionServer","responsesize":15802,"method":"Multi"}
2014-07-14 01:19:15,397 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19983,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325935414,"queuetimems":0,"class":"HRegionServer","responsesize":15879,"method":"Multi"}
2014-07-14 01:19:15,397 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15640,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325939757,"queuetimems":0,"class":"HRegionServer","responsesize":16071,"method":"Multi"}
2014-07-14 01:19:15,401 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19561,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325935840,"queuetimems":0,"class":"HRegionServer","responsesize":16052,"method":"Multi"}
2014-07-14 01:19:15,396 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19721,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325935675,"queuetimems":1,"class":"HRegionServer","responsesize":15819,"method":"Multi"}
2014-07-14 01:19:15,405 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15720,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325939685,"queuetimems":0,"class":"HRegionServer","responsesize":15574,"method":"Multi"}
2014-07-14 01:19:15,447 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15915,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325939531,"queuetimems":0,"class":"HRegionServer","responsesize":15751,"method":"Multi"}
2014-07-14 01:19:15,405 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19889,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325935507,"queuetimems":0,"class":"HRegionServer","responsesize":15670,"method":"Multi"}
2014-07-14 01:19:15,504 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325953440 with entries=107, filesize=91.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325955062
2014-07-14 01:19:15,804 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16325,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325939478,"queuetimems":0,"class":"HRegionServer","responsesize":15663,"method":"Multi"}
2014-07-14 01:19:15,810 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16307,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325939502,"queuetimems":1,"class":"HRegionServer","responsesize":15741,"method":"Multi"}
2014-07-14 01:19:15,813 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16194,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325939619,"queuetimems":0,"class":"HRegionServer","responsesize":15668,"method":"Multi"}
2014-07-14 01:19:15,813 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17537,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325938276,"queuetimems":1,"class":"HRegionServer","responsesize":15779,"method":"Multi"}
2014-07-14 01:19:15,819 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16098,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325939720,"queuetimems":1,"class":"HRegionServer","responsesize":15998,"method":"Multi"}
2014-07-14 01:19:15,820 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16230,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325939590,"queuetimems":2,"class":"HRegionServer","responsesize":15785,"method":"Multi"}
2014-07-14 01:19:15,820 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17613,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325938207,"queuetimems":0,"class":"HRegionServer","responsesize":15791,"method":"Multi"}
2014-07-14 01:19:15,825 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17511,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325938314,"queuetimems":0,"class":"HRegionServer","responsesize":15895,"method":"Multi"}
2014-07-14 01:19:15,829 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17450,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325938379,"queuetimems":0,"class":"HRegionServer","responsesize":15450,"method":"Multi"}
2014-07-14 01:19:15,837 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16349,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325939488,"queuetimems":0,"class":"HRegionServer","responsesize":15875,"method":"Multi"}
2014-07-14 01:19:15,840 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16279,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325939560,"queuetimems":0,"class":"HRegionServer","responsesize":15740,"method":"Multi"}
2014-07-14 01:19:15,841 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17393,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325938448,"queuetimems":1,"class":"HRegionServer","responsesize":16060,"method":"Multi"}
2014-07-14 01:19:15,845 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17432,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325938413,"queuetimems":1,"class":"HRegionServer","responsesize":15553,"method":"Multi"}
2014-07-14 01:19:15,849 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16192,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325939657,"queuetimems":0,"class":"HRegionServer","responsesize":15819,"method":"Multi"}
2014-07-14 01:19:15,853 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17609,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325938244,"queuetimems":0,"class":"HRegionServer","responsesize":15903,"method":"Multi"}
2014-07-14 01:19:15,914 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17566,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325938347,"queuetimems":0,"class":"HRegionServer","responsesize":16052,"method":"Multi"}
2014-07-14 01:19:15,919 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20185,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325935734,"queuetimems":1,"class":"HRegionServer","responsesize":15663,"method":"Multi"}
2014-07-14 01:19:16,866 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:19:16,920 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31512 synced till here 31479
2014-07-14 01:19:17,262 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325955062 with entries=109, filesize=93.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325956867
2014-07-14 01:19:18,968 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7789, memsize=432.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/4769da1df81c47e399d465797e4019cd
2014-07-14 01:19:18,977 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:19:18,997 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/4769da1df81c47e399d465797e4019cd as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/4769da1df81c47e399d465797e4019cd
2014-07-14 01:19:19,051 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/4769da1df81c47e399d465797e4019cd, entries=1573100, sequenceid=7789, filesize=112.0m
2014-07-14 01:19:19,062 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.4g/1501918400, currentsize=160.4m/168208800 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 26287ms, sequenceid=7789, compaction requested=true
2014-07-14 01:19:19,062 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:19:19,062 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 20 blocking
2014-07-14 01:19:19,062 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-14 01:19:19,062 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:19:19,062 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:19:19,063 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:19:19,114 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31630 synced till here 31595
2014-07-14 01:19:19,251 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7886, memsize=73.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/d886fcd4915447cbbf30386ca5a34a22
2014-07-14 01:19:19,263 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/d886fcd4915447cbbf30386ca5a34a22 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/d886fcd4915447cbbf30386ca5a34a22
2014-07-14 01:19:19,274 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/d886fcd4915447cbbf30386ca5a34a22, entries=266870, sequenceid=7886, filesize=19.0m
2014-07-14 01:19:19,275 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~115.4m/121008880, currentsize=96.5m/101146320 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 5342ms, sequenceid=7886, compaction requested=true
2014-07-14 01:19:19,275 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:19:19,275 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 20 blocking
2014-07-14 01:19:19,275 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-14 01:19:19,275 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:19:19,275 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:19:19,275 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:19:19,453 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325956867 with entries=118, filesize=101.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325958977
2014-07-14 01:19:19,453 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325817809
2014-07-14 01:19:19,453 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325819232
2014-07-14 01:19:19,453 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325820805
2014-07-14 01:19:19,453 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325823655
2014-07-14 01:19:19,453 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325825596
2014-07-14 01:19:19,453 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325828531
2014-07-14 01:19:19,453 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325830695
2014-07-14 01:19:19,453 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325833034
2014-07-14 01:19:19,453 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325835079
2014-07-14 01:19:19,453 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325837630
2014-07-14 01:19:19,453 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325853480
2014-07-14 01:19:19,453 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325854727
2014-07-14 01:19:19,454 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325855797
2014-07-14 01:19:19,454 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325857334
2014-07-14 01:19:19,454 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325858478
2014-07-14 01:19:19,454 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325862202
2014-07-14 01:19:19,454 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325863616
2014-07-14 01:19:19,454 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325864927
2014-07-14 01:19:19,454 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325866339
2014-07-14 01:19:19,454 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325867158
2014-07-14 01:19:19,454 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325868977
2014-07-14 01:19:19,454 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325870669
2014-07-14 01:19:21,075 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:19:21,161 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31733 synced till here 31713
2014-07-14 01:19:21,413 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325958977 with entries=103, filesize=87.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325961075
2014-07-14 01:19:22,153 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:19:22,914 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31834 synced till here 31805
2014-07-14 01:19:22,955 DEBUG [RpcServer.handler=30,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:19:22,955 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files; delaying flush up to 90000ms
2014-07-14 01:19:22,956 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:19:22,956 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 20 blocking
2014-07-14 01:19:22,956 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-14 01:19:22,956 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:19:22,956 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:19:22,956 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:19:23,068 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325961075 with entries=101, filesize=86.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325962154
2014-07-14 01:19:23,783 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:19:23,798 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31908 synced till here 31905
2014-07-14 01:19:23,817 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325962154 with entries=74, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325963783
2014-07-14 01:19:26,699 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:19:26,751 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31985 synced till here 31982
2014-07-14 01:19:26,792 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325963783 with entries=77, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325966700
2014-07-14 01:19:28,225 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:19:28,225 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:19:28,226 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files; delaying flush up to 90000ms
2014-07-14 01:19:28,227 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:19:28,227 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 20 blocking
2014-07-14 01:19:28,228 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-14 01:19:28,228 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:19:28,228 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:19:28,228 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:19:28,249 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32060 synced till here 32058
2014-07-14 01:19:28,294 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325966700 with entries=75, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325968225
2014-07-14 01:19:29,272 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:19:29,311 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325968225 with entries=71, filesize=60.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325969273
2014-07-14 01:19:30,716 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:19:30,792 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32206 synced till here 32204
2014-07-14 01:19:30,831 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325969273 with entries=75, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325970716
2014-07-14 01:19:32,375 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:19:32,975 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32322 synced till here 32321
2014-07-14 01:19:32,999 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325970716 with entries=116, filesize=99.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325972375
2014-07-14 01:19:34,557 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:19:34,583 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325972375 with entries=73, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325974557
2014-07-14 01:19:35,827 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:19:36,173 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32496 synced till here 32494
2014-07-14 01:19:36,198 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325974557 with entries=101, filesize=86.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325975828
2014-07-14 01:19:37,677 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:19:37,699 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32572 synced till here 32569
2014-07-14 01:19:37,757 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325975828 with entries=76, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325977678
2014-07-14 01:19:37,757 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:19:39,149 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:19:39,179 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32646 synced till here 32644
2014-07-14 01:19:39,193 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325977678 with entries=74, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325979152
2014-07-14 01:19:39,195 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:19:40,208 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:19:40,347 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32726 synced till here 32724
2014-07-14 01:19:40,370 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325979152 with entries=80, filesize=68.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325980209
2014-07-14 01:19:40,370 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:19:41,903 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:19:41,919 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32801 synced till here 32799
2014-07-14 01:19:41,949 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325980209 with entries=75, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325981903
2014-07-14 01:19:41,951 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:19:42,102 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:19:42,102 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files, but is 1.3g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:19:42,102 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. due to global heap pressure
2014-07-14 01:19:42,103 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 1.3g
2014-07-14 01:19:42,899 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:19:42,900 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files, but is 1.3g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:19:42,900 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. due to global heap pressure
2014-07-14 01:19:42,900 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 1.3g
2014-07-14 01:19:43,317 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:19:43,758 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32921 synced till here 32919
2014-07-14 01:19:43,790 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325981903 with entries=120, filesize=103.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325983318
2014-07-14 01:19:43,791 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:19:44,047 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:19:45,009 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:45,010 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:45,010 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:45,039 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:45,039 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:45,057 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:45,067 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:45,070 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:45,138 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:45,182 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:45,237 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:45,313 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:45,369 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:45,420 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:45,469 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:45,516 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:45,558 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:45,604 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:45,640 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:45,682 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:45,736 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:45,780 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:45,824 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:45,874 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:45,931 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:45,987 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:46,044 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:46,083 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:46,116 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:46,163 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:46,208 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:46,250 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:46,298 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:46,331 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:47,265 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:47,296 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:47,336 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:47,371 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:47,406 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:47,440 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:47,478 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:47,503 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:47,540 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:47,575 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:47,620 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:47,665 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:48,846 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1007ms
GC pool 'ParNew' had collection(s): count=1 time=1049ms
2014-07-14 01:19:48,857 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:48,874 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:48,885 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:48,913 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:19:50,010 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:50,010 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:50,011 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:50,040 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:50,040 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:50,057 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:50,067 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:50,070 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:50,138 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:50,182 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:50,238 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:50,314 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:50,369 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:50,421 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:50,470 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:50,517 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:50,559 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:50,605 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:50,640 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:50,682 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:50,737 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:50,781 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:50,824 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:50,874 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:50,973 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5042ms
2014-07-14 01:19:50,987 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:51,045 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:51,083 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:51,116 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:51,164 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:51,209 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:51,251 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:51,298 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:51,332 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:52,265 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:52,297 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:52,337 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:52,372 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:52,407 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:52,441 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:52,478 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:52,504 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:52,541 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:52,575 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:52,621 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:52,666 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:53,858 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:53,874 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:19:53,886 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:53,914 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:19:55,884 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10010ms
2014-07-14 01:19:55,956 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10132ms
2014-07-14 01:19:55,956 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10947ms
2014-07-14 01:19:55,956 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10946ms
2014-07-14 01:19:55,956 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10946ms
2014-07-14 01:19:55,957 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10918ms
2014-07-14 01:19:55,957 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10918ms
2014-07-14 01:19:55,957 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10900ms
2014-07-14 01:19:55,958 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10890ms
2014-07-14 01:19:55,958 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10888ms
2014-07-14 01:19:55,958 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10820ms
2014-07-14 01:19:55,958 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10777ms
2014-07-14 01:19:55,959 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10721ms
2014-07-14 01:19:55,959 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10646ms
2014-07-14 01:19:55,959 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10590ms
2014-07-14 01:19:55,960 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10540ms
2014-07-14 01:19:55,960 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10491ms
2014-07-14 01:19:55,960 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10444ms
2014-07-14 01:19:55,961 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10402ms
2014-07-14 01:19:55,961 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10357ms
2014-07-14 01:19:55,961 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10321ms
2014-07-14 01:19:55,961 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10279ms
2014-07-14 01:19:55,961 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10225ms
2014-07-14 01:19:55,962 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10182ms
2014-07-14 01:19:55,974 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10043ms
2014-07-14 01:19:55,987 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:19:56,045 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:56,083 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:19:56,117 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:56,164 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:56,209 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:56,251 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:56,298 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:19:56,332 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:57,266 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:57,297 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:57,337 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:57,372 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:57,407 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:57,441 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:57,478 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:19:57,504 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:57,541 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:57,575 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:19:57,621 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:57,666 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:58,858 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:19:58,875 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:58,886 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:19:58,914 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:20:00,956 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15082ms
2014-07-14 01:20:00,956 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15132ms
2014-07-14 01:20:00,956 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15947ms
2014-07-14 01:20:00,956 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15946ms
2014-07-14 01:20:00,957 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15947ms
2014-07-14 01:20:00,957 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15918ms
2014-07-14 01:20:00,957 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15918ms
2014-07-14 01:20:00,957 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15900ms
2014-07-14 01:20:00,958 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15891ms
2014-07-14 01:20:00,958 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15888ms
2014-07-14 01:20:00,958 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15820ms
2014-07-14 01:20:00,959 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15778ms
2014-07-14 01:20:00,959 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15722ms
2014-07-14 01:20:00,959 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15646ms
2014-07-14 01:20:00,960 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15591ms
2014-07-14 01:20:00,960 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15540ms
2014-07-14 01:20:00,960 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15491ms
2014-07-14 01:20:00,960 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15444ms
2014-07-14 01:20:00,961 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15403ms
2014-07-14 01:20:00,961 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15357ms
2014-07-14 01:20:00,961 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15321ms
2014-07-14 01:20:00,961 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15279ms
2014-07-14 01:20:00,962 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15226ms
2014-07-14 01:20:00,962 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15182ms
2014-07-14 01:20:00,974 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15043ms
2014-07-14 01:20:00,988 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:20:01,046 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:20:01,084 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 01:20:01,117 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:20:01,164 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:20:01,210 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:20:01,251 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:20:01,298 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 01:20:01,333 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:20:02,523 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15020ms
2014-07-14 01:20:02,523 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15258ms
2014-07-14 01:20:02,524 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15228ms
2014-07-14 01:20:02,524 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15046ms
2014-07-14 01:20:02,525 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15189ms
2014-07-14 01:20:02,525 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15154ms
2014-07-14 01:20:02,526 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15119ms
2014-07-14 01:20:02,526 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15086ms
2014-07-14 01:20:02,541 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:20:02,576 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:20:02,621 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:20:02,666 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:20:03,859 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:20:03,875 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:20:03,886 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:20:03,914 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:20:05,956 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20082ms
2014-07-14 01:20:05,957 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20947ms
2014-07-14 01:20:05,957 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20948ms
2014-07-14 01:20:05,957 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20918ms
2014-07-14 01:20:05,957 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20133ms
2014-07-14 01:20:05,958 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20900ms
2014-07-14 01:20:05,958 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20919ms
2014-07-14 01:20:05,958 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20891ms
2014-07-14 01:20:05,959 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20888ms
2014-07-14 01:20:05,959 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20949ms
2014-07-14 01:20:05,959 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20646ms
2014-07-14 01:20:05,960 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20722ms
2014-07-14 01:20:05,960 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20779ms
2014-07-14 01:20:05,961 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20823ms
2014-07-14 01:20:05,961 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20445ms
2014-07-14 01:20:05,961 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20492ms
2014-07-14 01:20:05,961 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20541ms
2014-07-14 01:20:05,961 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20357ms
2014-07-14 01:20:05,962 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20592ms
2014-07-14 01:20:05,962 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20280ms
2014-07-14 01:20:05,962 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20322ms
2014-07-14 01:20:05,962 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20404ms
2014-07-14 01:20:05,962 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20182ms
2014-07-14 01:20:05,962 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20226ms
2014-07-14 01:20:05,974 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20043ms
2014-07-14 01:20:05,988 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:20:06,046 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:20:06,084 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:20:06,117 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:20:06,165 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:20:06,210 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:20:06,252 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:20:06,299 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:20:06,333 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:20:06,513 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8234, memsize=488.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/ed933617490d454da35c5b3452f2ea8f
2014-07-14 01:20:06,514 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8235, memsize=477.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/4b23068044484f078143696961bcef75
2014-07-14 01:20:06,565 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/ed933617490d454da35c5b3452f2ea8f as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/ed933617490d454da35c5b3452f2ea8f
2014-07-14 01:20:06,569 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/4b23068044484f078143696961bcef75 as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/4b23068044484f078143696961bcef75
2014-07-14 01:20:06,608 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/ed933617490d454da35c5b3452f2ea8f, entries=1776740, sequenceid=8234, filesize=126.6m
2014-07-14 01:20:06,608 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.3g/1388714960, currentsize=49.6m/52060160 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 24505ms, sequenceid=8234, compaction requested=true
2014-07-14 01:20:06,610 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:20:06,610 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 20 blocking
2014-07-14 01:20:06,610 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20279ms
2014-07-14 01:20:06,610 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-14 01:20:06,610 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,610 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:20:06,610 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20312ms
2014-07-14 01:20:06,610 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:20:06,611 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,611 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:20:06,611 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20361ms
2014-07-14 01:20:06,611 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,611 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20403ms
2014-07-14 01:20:06,611 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,612 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20449ms
2014-07-14 01:20:06,612 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,612 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20496ms
2014-07-14 01:20:06,612 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,612 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20529ms
2014-07-14 01:20:06,612 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,613 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20569ms
2014-07-14 01:20:06,613 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,613 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20626ms
2014-07-14 01:20:06,613 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,613 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20682ms
2014-07-14 01:20:06,613 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,625 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20889ms
2014-07-14 01:20:06,625 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,625 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20845ms
2014-07-14 01:20:06,625 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,625 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21067ms
2014-07-14 01:20:06,625 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,625 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20985ms
2014-07-14 01:20:06,625 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,625 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20943ms
2014-07-14 01:20:06,626 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,626 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21257ms
2014-07-14 01:20:06,626 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,626 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21022ms
2014-07-14 01:20:06,626 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,627 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21207ms
2014-07-14 01:20:06,627 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,631 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21162ms
2014-07-14 01:20:06,631 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,631 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21115ms
2014-07-14 01:20:06,631 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,631 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21493ms
2014-07-14 01:20:06,631 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,631 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21450ms
2014-07-14 01:20:06,631 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,631 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21394ms
2014-07-14 01:20:06,632 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,633 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21320ms
2014-07-14 01:20:06,633 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,634 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21624ms
2014-07-14 01:20:06,634 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,636 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21566ms
2014-07-14 01:20:06,636 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,636 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21569ms
2014-07-14 01:20:06,636 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,641 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21602ms
2014-07-14 01:20:06,641 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,641 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21584ms
2014-07-14 01:20:06,641 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,645 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20821ms
2014-07-14 01:20:06,645 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,645 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21606ms
2014-07-14 01:20:06,645 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,645 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21636ms
2014-07-14 01:20:06,645 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,645 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21635ms
2014-07-14 01:20:06,645 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,645 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20771ms
2014-07-14 01:20:06,645 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,647 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17734ms
2014-07-14 01:20:06,647 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,648 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17763ms
2014-07-14 01:20:06,648 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,648 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17774ms
2014-07-14 01:20:06,648 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,648 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17792ms
2014-07-14 01:20:06,648 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,650 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18985ms
2014-07-14 01:20:06,650 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,650 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/4b23068044484f078143696961bcef75, entries=1736590, sequenceid=8235, filesize=123.7m
2014-07-14 01:20:06,650 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 19030ms
2014-07-14 01:20:06,650 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,650 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 19075ms
2014-07-14 01:20:06,650 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,651 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 19110ms
2014-07-14 01:20:06,651 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,651 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 19211ms
2014-07-14 01:20:06,651 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,651 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 19245ms
2014-07-14 01:20:06,651 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,651 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 19280ms
2014-07-14 01:20:06,651 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,651 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 19315ms
2014-07-14 01:20:06,651 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,651 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 19173ms
2014-07-14 01:20:06,651 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,652 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 19356ms
2014-07-14 01:20:06,653 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,653 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 19388ms
2014-07-14 01:20:06,653 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,653 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 19150ms
2014-07-14 01:20:06,653 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:20:06,654 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.3g/1385699520, currentsize=40.3m/42242480 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 23754ms, sequenceid=8235, compaction requested=true
2014-07-14 01:20:06,655 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 20 blocking
2014-07-14 01:20:06,655 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-14 01:20:06,655 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:20:06,655 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:20:06,655 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:20:06,657 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:20:06,870 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22065,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325984804,"queuetimems":2,"class":"HRegionServer","responsesize":15961,"method":"Multi"}
2014-07-14 01:20:06,872 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22417,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325984455,"queuetimems":0,"class":"HRegionServer","responsesize":15704,"method":"Multi"}
2014-07-14 01:20:06,872 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22080,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325984792,"queuetimems":1,"class":"HRegionServer","responsesize":15667,"method":"Multi"}
2014-07-14 01:20:06,914 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22068,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325984845,"queuetimems":0,"class":"HRegionServer","responsesize":15807,"method":"Multi"}
2014-07-14 01:20:08,439 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:20:08,445 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23490,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325984953,"queuetimems":1,"class":"HRegionServer","responsesize":16229,"method":"Multi"}
2014-07-14 01:20:08,477 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33009 synced till here 32994
2014-07-14 01:20:08,582 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23683,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325984899,"queuetimems":0,"class":"HRegionServer","responsesize":15634,"method":"Multi"}
2014-07-14 01:20:08,603 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325983318 with entries=88, filesize=75.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326008440
2014-07-14 01:20:08,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325872370
2014-07-14 01:20:08,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325873778
2014-07-14 01:20:08,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325875313
2014-07-14 01:20:08,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325901256
2014-07-14 01:20:08,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325904338
2014-07-14 01:20:08,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325905301
2014-07-14 01:20:08,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325906725
2014-07-14 01:20:08,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325908541
2014-07-14 01:20:08,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325910876
2014-07-14 01:20:08,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325913088
2014-07-14 01:20:08,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325915265
2014-07-14 01:20:08,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325917399
2014-07-14 01:20:08,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325918581
2014-07-14 01:20:08,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325920420
2014-07-14 01:20:08,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325922739
2014-07-14 01:20:08,605 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325924991
2014-07-14 01:20:09,336 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23090,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325986245,"queuetimems":0,"class":"HRegionServer","responsesize":16213,"method":"Multi"}
2014-07-14 01:20:09,336 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23175,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325986160,"queuetimems":0,"class":"HRegionServer","responsesize":15959,"method":"Multi"}
2014-07-14 01:20:09,444 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:20:09,516 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33120 synced till here 33084
2014-07-14 01:20:10,493 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24206,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325986286,"queuetimems":0,"class":"HRegionServer","responsesize":15779,"method":"Multi"}
2014-07-14 01:20:10,493 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24891,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325985601,"queuetimems":0,"class":"HRegionServer","responsesize":15836,"method":"Multi"}
2014-07-14 01:20:10,493 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25076,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325985416,"queuetimems":0,"class":"HRegionServer","responsesize":15772,"method":"Multi"}
2014-07-14 01:20:10,494 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24760,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325985733,"queuetimems":0,"class":"HRegionServer","responsesize":15724,"method":"Multi"}
2014-07-14 01:20:10,685 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326008440 with entries=111, filesize=92.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326009444
2014-07-14 01:20:10,983 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24955,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325986028,"queuetimems":0,"class":"HRegionServer","responsesize":15806,"method":"Multi"}
2014-07-14 01:20:10,984 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25162,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325985821,"queuetimems":0,"class":"HRegionServer","responsesize":15578,"method":"Multi"}
2014-07-14 01:20:10,984 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23689,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325987294,"queuetimems":1,"class":"HRegionServer","responsesize":15779,"method":"Multi"}
2014-07-14 01:20:10,984 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25115,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325985868,"queuetimems":0,"class":"HRegionServer","responsesize":15251,"method":"Multi"}
2014-07-14 01:20:10,984 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25933,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325985051,"queuetimems":0,"class":"HRegionServer","responsesize":15802,"method":"Multi"}
2014-07-14 01:20:10,985 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25674,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325985310,"queuetimems":1,"class":"HRegionServer","responsesize":15670,"method":"Multi"}
2014-07-14 01:20:10,985 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24661,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325986323,"queuetimems":0,"class":"HRegionServer","responsesize":15818,"method":"Multi"}
2014-07-14 01:20:10,983 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22130,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325988853,"queuetimems":1,"class":"HRegionServer","responsesize":15704,"method":"Multi"}
2014-07-14 01:20:10,983 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25304,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325985679,"queuetimems":1,"class":"HRegionServer","responsesize":15518,"method":"Multi"}
2014-07-14 01:20:10,984 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22072,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325988911,"queuetimems":0,"class":"HRegionServer","responsesize":15634,"method":"Multi"}
2014-07-14 01:20:10,984 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24876,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325986108,"queuetimems":0,"class":"HRegionServer","responsesize":16096,"method":"Multi"}
2014-07-14 01:20:11,013 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24811,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325986202,"queuetimems":0,"class":"HRegionServer","responsesize":15982,"method":"Multi"}
2014-07-14 01:20:10,984 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25000,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325985984,"queuetimems":0,"class":"HRegionServer","responsesize":15631,"method":"Multi"}
2014-07-14 01:20:11,017 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25466,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325985551,"queuetimems":0,"class":"HRegionServer","responsesize":16122,"method":"Multi"}
2014-07-14 01:20:11,021 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25093,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325985928,"queuetimems":1,"class":"HRegionServer","responsesize":15444,"method":"Multi"}
2014-07-14 01:20:11,159 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23756,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325987403,"queuetimems":0,"class":"HRegionServer","responsesize":15667,"method":"Multi"}
2014-07-14 01:20:11,159 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23659,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325987500,"queuetimems":0,"class":"HRegionServer","responsesize":15828,"method":"Multi"}
2014-07-14 01:20:11,159 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26154,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325985005,"queuetimems":1,"class":"HRegionServer","responsesize":16025,"method":"Multi"}
2014-07-14 01:20:11,160 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25793,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325985366,"queuetimems":0,"class":"HRegionServer","responsesize":15759,"method":"Multi"}
2014-07-14 01:20:11,160 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25930,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325985230,"queuetimems":0,"class":"HRegionServer","responsesize":15795,"method":"Multi"}
2014-07-14 01:20:11,160 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26029,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325985131,"queuetimems":0,"class":"HRegionServer","responsesize":15452,"method":"Multi"}
2014-07-14 01:20:11,160 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25695,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325985465,"queuetimems":0,"class":"HRegionServer","responsesize":15879,"method":"Multi"}
2014-07-14 01:20:11,160 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25384,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325985776,"queuetimems":0,"class":"HRegionServer","responsesize":15905,"method":"Multi"}
2014-07-14 01:20:11,161 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23498,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325987662,"queuetimems":0,"class":"HRegionServer","responsesize":15959,"method":"Multi"}
2014-07-14 01:20:11,161 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23589,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325987571,"queuetimems":0,"class":"HRegionServer","responsesize":15795,"method":"Multi"}
2014-07-14 01:20:11,161 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25649,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325985512,"queuetimems":0,"class":"HRegionServer","responsesize":15456,"method":"Multi"}
2014-07-14 01:20:11,159 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23793,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325987366,"queuetimems":1,"class":"HRegionServer","responsesize":15836,"method":"Multi"}
2014-07-14 01:20:11,161 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23901,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325987260,"queuetimems":1,"class":"HRegionServer","responsesize":15759,"method":"Multi"}
2014-07-14 01:20:11,161 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23546,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325987615,"queuetimems":1,"class":"HRegionServer","responsesize":16213,"method":"Multi"}
2014-07-14 01:20:11,159 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23624,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325987535,"queuetimems":0,"class":"HRegionServer","responsesize":15807,"method":"Multi"}
2014-07-14 01:20:11,159 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22289,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325988870,"queuetimems":0,"class":"HRegionServer","responsesize":16229,"method":"Multi"}
2014-07-14 01:20:11,159 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25086,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325986073,"queuetimems":1,"class":"HRegionServer","responsesize":15526,"method":"Multi"}
2014-07-14 01:20:11,161 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23695,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325987466,"queuetimems":0,"class":"HRegionServer","responsesize":15452,"method":"Multi"}
2014-07-14 01:20:11,161 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23723,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325987437,"queuetimems":0,"class":"HRegionServer","responsesize":15565,"method":"Multi"}
2014-07-14 01:20:11,160 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25525,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325985635,"queuetimems":0,"class":"HRegionServer","responsesize":15828,"method":"Multi"}
2014-07-14 01:20:11,160 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25985,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325985175,"queuetimems":0,"class":"HRegionServer","responsesize":15565,"method":"Multi"}
2014-07-14 01:20:11,159 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23828,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325987331,"queuetimems":0,"class":"HRegionServer","responsesize":15982,"method":"Multi"}
2014-07-14 01:20:11,370 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:20:11,372 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22489,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47132","starttimems":1405325988882,"queuetimems":0,"class":"HRegionServer","responsesize":15518,"method":"Multi"}
2014-07-14 01:20:11,450 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33229 synced till here 33195
2014-07-14 01:20:12,779 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326009444 with entries=109, filesize=93.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326011370
2014-07-14 01:20:13,485 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:20:13,498 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33340 synced till here 33310
2014-07-14 01:20:13,738 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326011370 with entries=111, filesize=94.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326013486
2014-07-14 01:20:15,213 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:20:15,269 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33422 synced till here 33418
2014-07-14 01:20:15,315 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326013486 with entries=82, filesize=66.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326015213
2014-07-14 01:20:16,982 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:20:17,123 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33529 synced till here 33499
2014-07-14 01:20:17,193 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:20:17,193 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files; delaying flush up to 90000ms
2014-07-14 01:20:17,193 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:20:17,194 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 20 blocking
2014-07-14 01:20:17,194 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-14 01:20:17,194 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:20:17,194 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:20:17,194 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:20:17,343 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326015213 with entries=107, filesize=88.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326016982
2014-07-14 01:20:17,598 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:20:17,598 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files; delaying flush up to 90000ms
2014-07-14 01:20:17,598 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 20 blocking
2014-07-14 01:20:17,599 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-14 01:20:17,599 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:20:17,599 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:20:17,599 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:20:17,599 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:20:18,733 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:20:18,781 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33628 synced till here 33601
2014-07-14 01:20:19,024 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326016982 with entries=99, filesize=82.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326018734
2014-07-14 01:20:19,966 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:20:20,021 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33729 synced till here 33702
2014-07-14 01:20:20,280 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326018734 with entries=101, filesize=86.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326019967
2014-07-14 01:20:21,934 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:20:21,978 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33844 synced till here 33810
2014-07-14 01:20:22,601 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326019967 with entries=115, filesize=98.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326021935
2014-07-14 01:20:24,997 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:20:25,024 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33917 synced till here 33916
2014-07-14 01:20:25,047 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326021935 with entries=73, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326024998
2014-07-14 01:20:28,758 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:20:28,785 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33995 synced till here 33990
2014-07-14 01:20:28,876 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326024998 with entries=78, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326028759
2014-07-14 01:20:29,817 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:20:29,846 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34083 synced till here 34074
2014-07-14 01:20:30,515 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326028759 with entries=88, filesize=75.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326029818
2014-07-14 01:20:30,516 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:20:31,237 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:20:31,277 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34163 synced till here 34154
2014-07-14 01:20:31,372 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326029818 with entries=80, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326031237
2014-07-14 01:20:31,372 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:20:32,695 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:20:32,895 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326031237 with entries=83, filesize=71.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326032696
2014-07-14 01:20:32,896 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:20:34,446 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:20:34,670 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34324 synced till here 34323
2014-07-14 01:20:34,697 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326032696 with entries=78, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326034446
2014-07-14 01:20:34,697 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:20:37,269 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:20:37,292 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326034446 with entries=71, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326037270
2014-07-14 01:20:37,292 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:20:40,563 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:20:40,592 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326037270 with entries=71, filesize=60.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326040564
2014-07-14 01:20:40,592 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:20:53,012 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90056ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:20:53,012 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 1.3g
2014-07-14 01:20:53,987 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:20:58,486 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90261ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:20:58,487 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 1.2g
2014-07-14 01:20:59,464 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:21:03,221 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:21:03,242 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34540 synced till here 34539
2014-07-14 01:21:03,270 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326040564 with entries=74, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326063222
2014-07-14 01:21:07,212 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:21:07,231 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34614 synced till here 34613
2014-07-14 01:21:07,246 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326063222 with entries=74, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326067213
2014-07-14 01:21:07,936 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:07,945 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:07,971 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:07,986 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:08,093 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:08,141 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:08,185 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:08,230 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:11,238 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:11,267 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:11,296 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:11,331 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:11,368 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:11,432 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:11,481 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:11,529 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:11,580 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:11,633 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:11,691 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:11,731 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:11,772 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:12,044 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:12,234 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:12,936 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:21:12,945 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:21:12,971 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:21:12,986 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:21:13,094 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:21:13,141 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:21:13,186 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:21:13,230 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:21:14,238 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:14,272 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:14,311 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:14,355 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:14,407 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:14,476 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:14,555 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:14,619 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:14,664 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:14,715 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:14,756 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:14,805 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:14,977 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:15,035 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:15,088 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:15,133 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:15,183 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:15,224 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:15,272 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:15,307 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:15,347 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:15,397 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:15,441 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:15,474 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:15,510 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:15,545 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:15,582 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:21:16,238 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:21:16,267 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:21:16,296 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:21:16,331 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:21:16,368 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:21:16,433 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:21:16,481 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:21:16,529 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:21:16,581 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:21:16,633 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:21:16,692 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:21:16,732 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:21:16,772 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:21:17,825 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5591ms
2014-07-14 01:21:17,829 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5786ms
2014-07-14 01:21:17,937 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:21:17,946 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:21:17,971 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:21:17,986 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:21:18,094 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:21:18,142 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:21:18,187 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:21:18,230 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:21:18,610 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8644, memsize=644.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/ca565b0107d24355a42a1b9d1c0dcb16
2014-07-14 01:21:18,633 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/ca565b0107d24355a42a1b9d1c0dcb16 as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/ca565b0107d24355a42a1b9d1c0dcb16
2014-07-14 01:21:18,656 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/ca565b0107d24355a42a1b9d1c0dcb16, entries=2346440, sequenceid=8644, filesize=167.1m
2014-07-14 01:21:18,657 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.3g/1383872960, currentsize=67.2m/70468800 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 25645ms, sequenceid=8644, compaction requested=true
2014-07-14 01:21:18,657 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:21:18,657 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 20 blocking
2014-07-14 01:21:18,657 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10428ms
2014-07-14 01:21:18,658 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,658 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-14 01:21:18,658 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:21:18,658 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10473ms
2014-07-14 01:21:18,658 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,658 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:21:18,658 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10517ms
2014-07-14 01:21:18,658 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,658 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:21:18,658 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10565ms
2014-07-14 01:21:18,658 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,658 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10672ms
2014-07-14 01:21:18,659 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,659 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10688ms
2014-07-14 01:21:18,659 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,664 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10719ms
2014-07-14 01:21:18,664 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,664 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10728ms
2014-07-14 01:21:18,664 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,672 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6629ms
2014-07-14 01:21:18,672 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,672 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6438ms
2014-07-14 01:21:18,672 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,673 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6901ms
2014-07-14 01:21:18,673 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,673 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6942ms
2014-07-14 01:21:18,673 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,673 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6982ms
2014-07-14 01:21:18,673 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,673 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7041ms
2014-07-14 01:21:18,673 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,673 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7093ms
2014-07-14 01:21:18,673 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,674 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7145ms
2014-07-14 01:21:18,674 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,674 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7193ms
2014-07-14 01:21:18,674 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,677 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7245ms
2014-07-14 01:21:18,677 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,677 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7309ms
2014-07-14 01:21:18,677 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,679 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7347ms
2014-07-14 01:21:18,679 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,680 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7384ms
2014-07-14 01:21:18,680 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,681 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7414ms
2014-07-14 01:21:18,681 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,681 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7444ms
2014-07-14 01:21:18,681 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,681 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3099ms
2014-07-14 01:21:18,681 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,683 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3138ms
2014-07-14 01:21:18,683 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,684 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3173ms
2014-07-14 01:21:18,684 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,685 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3210ms
2014-07-14 01:21:18,685 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,686 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3245ms
2014-07-14 01:21:18,686 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,686 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3289ms
2014-07-14 01:21:18,686 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,686 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3339ms
2014-07-14 01:21:18,686 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,690 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3383ms
2014-07-14 01:21:18,690 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,690 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3418ms
2014-07-14 01:21:18,690 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,691 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3466ms
2014-07-14 01:21:18,691 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,691 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3508ms
2014-07-14 01:21:18,691 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,692 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3558ms
2014-07-14 01:21:18,692 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,692 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3604ms
2014-07-14 01:21:18,693 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,693 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3658ms
2014-07-14 01:21:18,693 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,693 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3716ms
2014-07-14 01:21:18,693 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,694 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3890ms
2014-07-14 01:21:18,694 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,694 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3938ms
2014-07-14 01:21:18,694 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,694 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3980ms
2014-07-14 01:21:18,694 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,694 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4030ms
2014-07-14 01:21:18,694 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,695 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4076ms
2014-07-14 01:21:18,695 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,696 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4141ms
2014-07-14 01:21:18,696 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,697 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4220ms
2014-07-14 01:21:18,697 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,698 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4291ms
2014-07-14 01:21:18,699 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,699 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4344ms
2014-07-14 01:21:18,699 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,699 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4388ms
2014-07-14 01:21:18,699 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,700 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4427ms
2014-07-14 01:21:18,700 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,701 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4463ms
2014-07-14 01:21:18,701 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:21:18,742 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10955,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47133","starttimems":1405326067787,"queuetimems":0,"class":"HRegionServer","responsesize":15867,"method":"Multi"}
2014-07-14 01:21:18,778 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11040,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47133","starttimems":1405326067738,"queuetimems":0,"class":"HRegionServer","responsesize":15814,"method":"Multi"}
2014-07-14 01:21:18,905 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11062,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47133","starttimems":1405326067842,"queuetimems":1,"class":"HRegionServer","responsesize":15924,"method":"Multi"}
2014-07-14 01:21:18,974 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:21:19,008 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34722 synced till here 34691
2014-07-14 01:21:19,262 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326067213 with entries=108, filesize=87.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326078974
2014-07-14 01:21:19,263 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325930825
2014-07-14 01:21:19,263 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325933628
2014-07-14 01:21:19,263 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325934564
2014-07-14 01:21:19,360 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11469,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47133","starttimems":1405326067890,"queuetimems":0,"class":"HRegionServer","responsesize":16078,"method":"Multi"}
2014-07-14 01:21:21,384 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:21:21,409 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34837 synced till here 34809
2014-07-14 01:21:21,581 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13352,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47133","starttimems":1405326068228,"queuetimems":1,"class":"HRegionServer","responsesize":15916,"method":"Multi"}
2014-07-14 01:21:21,581 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13442,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47133","starttimems":1405326068139,"queuetimems":0,"class":"HRegionServer","responsesize":16061,"method":"Multi"}
2014-07-14 01:21:21,584 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13398,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47133","starttimems":1405326068184,"queuetimems":0,"class":"HRegionServer","responsesize":15613,"method":"Multi"}
2014-07-14 01:21:21,597 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10058,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47133","starttimems":1405326071524,"queuetimems":0,"class":"HRegionServer","responsesize":15814,"method":"Multi"}
2014-07-14 01:21:21,601 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13509,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47133","starttimems":1405326068091,"queuetimems":0,"class":"HRegionServer","responsesize":15635,"method":"Multi"}
2014-07-14 01:21:21,874 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326078974 with entries=115, filesize=95.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326081384
2014-07-14 01:21:23,098 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11060,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47133","starttimems":1405326072037,"queuetimems":1,"class":"HRegionServer","responsesize":15699,"method":"Multi"}
2014-07-14 01:21:23,098 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11733,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47133","starttimems":1405326071365,"queuetimems":0,"class":"HRegionServer","responsesize":16078,"method":"Multi"}
2014-07-14 01:21:23,106 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11777,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47133","starttimems":1405326071329,"queuetimems":0,"class":"HRegionServer","responsesize":15867,"method":"Multi"}
2014-07-14 01:21:23,106 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11338,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47133","starttimems":1405326071768,"queuetimems":1,"class":"HRegionServer","responsesize":15772,"method":"Multi"}
2014-07-14 01:21:23,107 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11871,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47133","starttimems":1405326071236,"queuetimems":0,"class":"HRegionServer","responsesize":16061,"method":"Multi"}
2014-07-14 01:21:23,121 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10891,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47133","starttimems":1405326072230,"queuetimems":2,"class":"HRegionServer","responsesize":15613,"method":"Multi"}
2014-07-14 01:21:23,129 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11500,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47133","starttimems":1405326071629,"queuetimems":1,"class":"HRegionServer","responsesize":15467,"method":"Multi"}
2014-07-14 01:21:23,130 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11552,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47133","starttimems":1405326071577,"queuetimems":0,"class":"HRegionServer","responsesize":15758,"method":"Multi"}
2014-07-14 01:21:23,129 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11650,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47133","starttimems":1405326071479,"queuetimems":0,"class":"HRegionServer","responsesize":15916,"method":"Multi"}
2014-07-14 01:21:23,150 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11467,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47133","starttimems":1405326071683,"queuetimems":0,"class":"HRegionServer","responsesize":15664,"method":"Multi"}
2014-07-14 01:21:23,151 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11425,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47133","starttimems":1405326071726,"queuetimems":0,"class":"HRegionServer","responsesize":15593,"method":"Multi"}
2014-07-14 01:21:23,150 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11722,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47133","starttimems":1405326071428,"queuetimems":0,"class":"HRegionServer","responsesize":15924,"method":"Multi"}
2014-07-14 01:21:23,267 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11972,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47133","starttimems":1405326071294,"queuetimems":0,"class":"HRegionServer","responsesize":15635,"method":"Multi"}
2014-07-14 01:21:23,267 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12000,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47133","starttimems":1405326071266,"queuetimems":0,"class":"HRegionServer","responsesize":15870,"method":"Multi"}
2014-07-14 01:21:23,352 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=10829, hits=3899, hitRatio=36.00%, , cachingAccesses=3903, cachingHits=3898, cachingHitsRatio=99.87%, evictions=0, evicted=3, evictedPerRun=Infinity
2014-07-14 01:21:23,429 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:21:23,464 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34938 synced till here 34908
2014-07-14 01:21:23,681 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326081384 with entries=101, filesize=86.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326083429
2014-07-14 01:21:25,261 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1405ms
GC pool 'ParNew' had collection(s): count=1 time=1297ms
2014-07-14 01:21:26,036 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:21:26,213 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35054 synced till here 35021
2014-07-14 01:21:27,513 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326083429 with entries=116, filesize=98.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326086036
2014-07-14 01:21:29,859 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:21:29,954 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35173 synced till here 35152
2014-07-14 01:21:30,151 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326086036 with entries=119, filesize=99.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326089859
2014-07-14 01:21:30,712 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8654, memsize=608.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/b7401a77b7ef48129866e44054d59a4b
2014-07-14 01:21:30,749 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/b7401a77b7ef48129866e44054d59a4b as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/b7401a77b7ef48129866e44054d59a4b
2014-07-14 01:21:30,792 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/b7401a77b7ef48129866e44054d59a4b, entries=2213950, sequenceid=8654, filesize=157.7m
2014-07-14 01:21:30,792 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.2g/1246605200, currentsize=316.7m/332069200 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 32305ms, sequenceid=8654, compaction requested=true
2014-07-14 01:21:30,793 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:21:30,793 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 20 blocking
2014-07-14 01:21:30,794 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-14 01:21:30,794 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:21:30,794 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:21:30,794 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:21:32,300 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1086ms
GC pool 'ParNew' had collection(s): count=1 time=1418ms
2014-07-14 01:21:32,326 DEBUG [RpcServer.handler=5,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:21:32,326 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files; delaying flush up to 90000ms
2014-07-14 01:21:32,333 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 20 blocking
2014-07-14 01:21:32,333 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-14 01:21:32,333 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:21:32,333 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:21:32,333 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:21:32,336 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:21:32,595 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:21:32,601 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:21:32,601 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files; delaying flush up to 90000ms
2014-07-14 01:21:32,602 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 20 blocking
2014-07-14 01:21:32,602 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-14 01:21:32,602 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:21:32,602 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:21:32,602 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:21:32,602 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:21:32,631 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35276 synced till here 35253
2014-07-14 01:21:32,788 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326089859 with entries=103, filesize=88.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326092595
2014-07-14 01:21:32,788 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325953440
2014-07-14 01:21:32,788 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325955062
2014-07-14 01:21:32,788 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325956867
2014-07-14 01:21:32,788 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325958977
2014-07-14 01:21:32,788 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325961075
2014-07-14 01:21:32,788 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325962154
2014-07-14 01:21:32,788 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325963783
2014-07-14 01:21:32,788 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325966700
2014-07-14 01:21:32,789 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325968225
2014-07-14 01:21:32,789 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325969273
2014-07-14 01:21:32,789 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325970716
2014-07-14 01:21:32,789 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325972375
2014-07-14 01:21:32,789 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325974557
2014-07-14 01:21:32,789 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325975828
2014-07-14 01:21:32,789 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325977678
2014-07-14 01:21:32,789 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325979152
2014-07-14 01:21:32,789 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325980209
2014-07-14 01:21:34,763 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1462ms
GC pool 'ParNew' had collection(s): count=1 time=1506ms
2014-07-14 01:21:34,944 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:21:35,020 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326092595 with entries=71, filesize=60.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326094945
2014-07-14 01:21:37,834 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1570ms
GC pool 'ParNew' had collection(s): count=1 time=1843ms
2014-07-14 01:21:37,969 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:21:38,171 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35462 synced till here 35422
2014-07-14 01:21:38,437 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326094945 with entries=115, filesize=97.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326097971
2014-07-14 01:21:40,574 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1238ms
GC pool 'ParNew' had collection(s): count=1 time=1466ms
2014-07-14 01:21:40,764 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:21:40,833 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35548 synced till here 35534
2014-07-14 01:21:40,963 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326097971 with entries=86, filesize=72.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326100765
2014-07-14 01:21:43,183 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:21:43,309 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326100765 with entries=89, filesize=76.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326103184
2014-07-14 01:21:44,643 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:21:48,190 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3234ms
GC pool 'ParNew' had collection(s): count=1 time=3510ms
2014-07-14 01:21:48,190 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90997ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:21:48,191 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 1.1g
2014-07-14 01:21:48,191 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90593ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:21:48,191 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 1.0g
2014-07-14 01:21:48,235 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35746 synced till here 35711
2014-07-14 01:21:48,462 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15537 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:48,464 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:48,465 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15532 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:48,465 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:48,526 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326103184 with entries=109, filesize=91.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326104643
2014-07-14 01:21:49,146 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15569 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,146 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,147 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15601 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,148 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,148 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15534 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,148 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,148 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15572 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,148 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,250 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15567 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,250 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,250 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15594 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,250 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,250 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15599 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,250 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,250 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15561 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,251 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,251 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15608 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,251 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,251 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15562 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,251 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,251 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15607 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,251 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,251 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15609 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,252 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,252 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15603 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,252 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,252 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15606 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,252 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,252 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15605 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,253 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,253 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15565 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,253 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,253 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15611 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,253 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,254 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15597 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,254 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,254 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15598 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,254 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,254 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15570 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,254 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,254 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15604 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,254 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,254 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15566 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,254 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,255 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15559 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,255 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,255 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15574 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,255 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,255 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15536 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,255 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,255 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15528 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,255 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,255 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15563 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,255 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,256 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15560 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,256 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,256 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15530 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,256 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,256 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15558 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,256 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,256 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15593 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,256 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,256 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15576 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,257 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,257 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15610 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,257 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,257 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15531 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,257 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,257 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15535 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,257 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,257 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15557 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,257 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,257 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15575 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,258 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,258 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15573 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,258 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,258 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15600 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,258 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,258 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15613 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,258 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,258 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15527 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,258 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,258 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15564 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,259 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,259 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15568 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,259 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,259 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15602 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,259 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,259 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15571 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,259 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,293 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15612 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,294 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,294 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15595 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,294 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,294 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 15596 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47133: output error
2014-07-14 01:21:49,294 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:21:49,386 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:21:49,697 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35825 synced till here 35821
2014-07-14 01:21:49,791 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326104643 with entries=79, filesize=67.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326109386
2014-07-14 01:21:49,923 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:21:49,979 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:21:51,763 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:21:51,977 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35904 synced till here 35901
2014-07-14 01:21:51,999 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326109386 with entries=79, filesize=67.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326111764
2014-07-14 01:21:56,098 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:21:56,115 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35978 synced till here 35977
2014-07-14 01:21:56,324 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326111764 with entries=74, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326116099
2014-07-14 01:21:57,790 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:21:57,811 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36052 synced till here 36050
2014-07-14 01:21:57,823 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326116099 with entries=74, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326117790
2014-07-14 01:21:59,598 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:21:59,617 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36134 synced till here 36129
2014-07-14 01:21:59,664 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326117790 with entries=82, filesize=70.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326119599
2014-07-14 01:22:01,364 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:22:01,379 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36211 synced till here 36207
2014-07-14 01:22:01,443 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326119599 with entries=77, filesize=65.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326121364
2014-07-14 01:22:02,789 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:22:02,820 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326121364 with entries=72, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326122789
2014-07-14 01:22:03,184 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:03,185 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:03,185 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:03,185 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:03,185 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:03,186 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:03,198 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:03,199 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:05,362 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:05,396 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:05,435 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:05,472 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:05,511 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:05,556 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:05,592 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:05,624 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:07,236 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:07,292 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:07,339 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:07,377 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:07,506 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:07,544 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:07,588 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:07,970 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:08,080 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:08,080 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:08,114 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:08,154 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:08,185 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:22:08,185 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:22:08,185 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:22:08,186 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-14 01:22:08,186 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-14 01:22:08,186 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:22:08,189 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:08,199 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:22:08,199 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:22:08,243 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:09,226 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:09,236 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:09,471 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:09,501 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:09,530 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:09,581 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:09,613 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:09,643 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:09,678 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:09,710 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:09,767 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:09,809 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:09,851 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:09,895 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:09,934 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:09,969 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:10,002 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:10,039 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:22:10,363 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:22:10,373 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8975, memsize=393.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/ae1faf4d65174a029da43f45351418b6
2014-07-14 01:22:10,388 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8976, memsize=388.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/0942fcfb90f34fb69d5c5d9eb6437603
2014-07-14 01:22:10,396 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:22:10,403 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/ae1faf4d65174a029da43f45351418b6 as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/ae1faf4d65174a029da43f45351418b6
2014-07-14 01:22:10,429 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/0942fcfb90f34fb69d5c5d9eb6437603 as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/0942fcfb90f34fb69d5c5d9eb6437603
2014-07-14 01:22:10,435 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:22:10,448 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/ae1faf4d65174a029da43f45351418b6, entries=1433620, sequenceid=8975, filesize=102.2m
2014-07-14 01:22:10,449 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.1g/1186090320, currentsize=201.9m/211656240 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 22258ms, sequenceid=8975, compaction requested=true
2014-07-14 01:22:10,450 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:22:10,450 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 20 blocking
2014-07-14 01:22:10,450 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5016ms
2014-07-14 01:22:10,450 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-14 01:22:10,450 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,450 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:22:10,450 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5054ms
2014-07-14 01:22:10,450 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,450 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:22:10,450 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5089ms
2014-07-14 01:22:10,451 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,451 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:22:10,451 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 412ms
2014-07-14 01:22:10,451 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,451 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 449ms
2014-07-14 01:22:10,451 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,452 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 483ms
2014-07-14 01:22:10,452 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,452 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 518ms
2014-07-14 01:22:10,452 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,457 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 562ms
2014-07-14 01:22:10,457 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,469 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 618ms
2014-07-14 01:22:10,469 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,469 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 660ms
2014-07-14 01:22:10,470 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,470 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 703ms
2014-07-14 01:22:10,470 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,470 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 760ms
2014-07-14 01:22:10,470 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,473 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:22:10,473 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,481 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/0942fcfb90f34fb69d5c5d9eb6437603, entries=1413230, sequenceid=8976, filesize=100.7m
2014-07-14 01:22:10,482 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.1g/1192511520, currentsize=194.0m/203469440 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 22291ms, sequenceid=8976, compaction requested=true
2014-07-14 01:22:10,482 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 804ms
2014-07-14 01:22:10,482 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,482 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 20 blocking
2014-07-14 01:22:10,482 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-14 01:22:10,483 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:22:10,483 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:22:10,483 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:22:10,482 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 839ms
2014-07-14 01:22:10,483 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,483 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 871ms
2014-07-14 01:22:10,483 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,485 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 904ms
2014-07-14 01:22:10,485 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,488 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 958ms
2014-07-14 01:22:10,488 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,488 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 987ms
2014-07-14 01:22:10,488 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,489 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1018ms
2014-07-14 01:22:10,489 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,489 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1253ms
2014-07-14 01:22:10,489 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,497 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1271ms
2014-07-14 01:22:10,497 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,497 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2254ms
2014-07-14 01:22:10,497 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,497 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7299ms
2014-07-14 01:22:10,497 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,497 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7299ms
2014-07-14 01:22:10,497 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,497 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:22:10,509 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2320ms
2014-07-14 01:22:10,509 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,509 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7323ms
2014-07-14 01:22:10,509 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,509 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7325ms
2014-07-14 01:22:10,509 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,509 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7325ms
2014-07-14 01:22:10,509 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,509 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7325ms
2014-07-14 01:22:10,509 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,510 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7325ms
2014-07-14 01:22:10,510 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,517 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5006ms
2014-07-14 01:22:10,517 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,521 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7337ms
2014-07-14 01:22:10,521 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,521 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2367ms
2014-07-14 01:22:10,521 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,529 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2415ms
2014-07-14 01:22:10,529 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,529 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2499ms
2014-07-14 01:22:10,529 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,529 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2450ms
2014-07-14 01:22:10,529 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,529 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2559ms
2014-07-14 01:22:10,529 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,537 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2949ms
2014-07-14 01:22:10,537 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,537 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2993ms
2014-07-14 01:22:10,537 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,537 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3031ms
2014-07-14 01:22:10,537 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,538 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3160ms
2014-07-14 01:22:10,538 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,539 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3199ms
2014-07-14 01:22:10,539 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,539 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3247ms
2014-07-14 01:22:10,539 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,539 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3303ms
2014-07-14 01:22:10,539 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,540 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4915ms
2014-07-14 01:22:10,540 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,541 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4948ms
2014-07-14 01:22:10,541 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:10,541 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4985ms
2014-07-14 01:22:10,541 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:22:11,136 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:22:11,251 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36369 synced till here 36356
2014-07-14 01:22:11,413 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326122789 with entries=86, filesize=71.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326131137
2014-07-14 01:22:11,414 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325981903
2014-07-14 01:22:11,414 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405325983318
2014-07-14 01:22:11,414 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326008440
2014-07-14 01:22:11,414 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326009444
2014-07-14 01:22:11,414 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326011370
2014-07-14 01:22:11,414 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326013486
2014-07-14 01:22:11,414 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326015213
2014-07-14 01:22:11,414 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326016982
2014-07-14 01:22:11,414 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326018734
2014-07-14 01:22:11,414 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326019967
2014-07-14 01:22:11,414 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326021935
2014-07-14 01:22:11,414 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326024998
2014-07-14 01:22:11,414 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326028759
2014-07-14 01:22:11,414 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326029818
2014-07-14 01:22:11,414 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326031237
2014-07-14 01:22:11,414 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326032696
2014-07-14 01:22:11,414 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326034446
2014-07-14 01:22:11,414 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326037270
2014-07-14 01:22:12,632 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1014ms
GC pool 'ParNew' had collection(s): count=1 time=1174ms
2014-07-14 01:22:13,159 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:22:13,159 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files; delaying flush up to 90000ms
2014-07-14 01:22:13,161 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 20 blocking
2014-07-14 01:22:13,161 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-14 01:22:13,161 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:22:13,161 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:22:13,161 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:22:13,162 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:22:13,449 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:22:13,463 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36477 synced till here 36448
2014-07-14 01:22:13,715 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326131137 with entries=108, filesize=90.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326133449
2014-07-14 01:22:13,718 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:22:13,718 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files; delaying flush up to 90000ms
2014-07-14 01:22:13,718 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:22:13,718 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 20 blocking
2014-07-14 01:22:13,718 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-14 01:22:13,719 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:22:13,719 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:22:13,719 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:22:15,274 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:22:15,315 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36564 synced till here 36552
2014-07-14 01:22:15,397 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326133449 with entries=87, filesize=71.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326135275
2014-07-14 01:22:16,570 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:22:16,601 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36639 synced till here 36636
2014-07-14 01:22:16,885 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326135275 with entries=75, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326136570
2014-07-14 01:22:18,103 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:22:18,127 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36713 synced till here 36711
2014-07-14 01:22:18,308 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326136570 with entries=74, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326138104
2014-07-14 01:22:19,807 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:22:19,827 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36790 synced till here 36786
2014-07-14 01:22:19,888 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326138104 with entries=77, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326139807
2014-07-14 01:22:23,096 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:22:23,118 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36863 synced till here 36862
2014-07-14 01:22:23,135 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326139807 with entries=73, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326143096
2014-07-14 01:22:24,576 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:22:24,641 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326143096 with entries=73, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326144577
2014-07-14 01:22:25,593 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:22:25,761 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326144577 with entries=79, filesize=67.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326145593
2014-07-14 01:22:46,472 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:22:46,508 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326145593 with entries=73, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326166473
2014-07-14 01:22:48,862 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:22:48,882 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37166 synced till here 37160
2014-07-14 01:22:48,976 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326166473 with entries=78, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326168862
2014-07-14 01:22:50,305 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:22:50,343 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37240 synced till here 37238
2014-07-14 01:22:50,366 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326168862 with entries=74, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326170306
2014-07-14 01:22:57,134 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:22:57,167 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37314 synced till here 37311
2014-07-14 01:22:57,198 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326170306 with entries=74, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326177135
2014-07-14 01:22:57,199 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 2 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72, 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:22:58,774 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:22:58,798 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37391 synced till here 37387
2014-07-14 01:22:58,848 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326177135 with entries=77, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326178775
2014-07-14 01:22:58,848 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 2 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72, 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:23:00,877 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:23:00,934 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37470 synced till here 37465
2014-07-14 01:23:01,054 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326178775 with entries=79, filesize=67.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326180877
2014-07-14 01:23:01,055 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 2 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72, 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:23:02,802 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90477ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:23:02,803 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 1.1g
2014-07-14 01:23:02,802 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90201ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:23:02,803 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 1.1g
2014-07-14 01:23:03,467 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:23:03,612 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37551 synced till here 37549
2014-07-14 01:23:03,637 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326180877 with entries=81, filesize=69.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326183467
2014-07-14 01:23:04,261 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:23:04,302 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:23:05,030 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:23:05,054 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37624 synced till here 37623
2014-07-14 01:23:05,073 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326183467 with entries=73, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326185030
2014-07-14 01:23:06,815 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:23:06,857 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326185030 with entries=73, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326186816
2014-07-14 01:23:09,281 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:23:09,285 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:09,300 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:09,313 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:09,323 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:09,329 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:09,819 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326186816 with entries=74, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326189282
2014-07-14 01:23:27,663 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18334ms
2014-07-14 01:23:27,664 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18364ms
2014-07-14 01:23:27,666 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18342ms
2014-07-14 01:23:27,666 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18381ms
2014-07-14 01:23:27,668 WARN  [regionserver60020.periodicFlusher] util.Sleeper: We slept 26522ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-14 01:23:27,669 WARN  [regionserver60020.compactionChecker] util.Sleeper: We slept 26524ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-14 01:23:27,670 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18357ms
2014-07-14 01:23:27,672 WARN  [regionserver60020] util.Sleeper: We slept 20123ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-14 01:23:27,672 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 17062ms
GC pool 'ParNew' had collection(s): count=1 time=683ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=16474ms
2014-07-14 01:23:27,687 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:27,701 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:29,861 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:29,914 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:29,953 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:30,019 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:30,088 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:30,165 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:30,231 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:30,282 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:30,576 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:30,635 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:30,680 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:30,735 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:30,792 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:30,848 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:30,897 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:30,951 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:30,991 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:31,241 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:31,961 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:32,013 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:32,069 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:32,114 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:32,147 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:32,664 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23335ms
2014-07-14 01:23:32,664 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23364ms
2014-07-14 01:23:32,666 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23343ms
2014-07-14 01:23:32,667 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23382ms
2014-07-14 01:23:32,670 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23358ms
2014-07-14 01:23:32,687 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:23:32,702 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:23:34,010 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:34,042 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:34,075 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:34,109 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:34,940 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5026ms
2014-07-14 01:23:34,940 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5079ms
2014-07-14 01:23:34,952 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:34,954 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:23:34,976 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:35,006 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:35,019 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:23:35,088 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:23:35,139 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:35,165 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:23:35,231 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:23:35,282 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:23:35,576 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:23:35,635 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:23:35,680 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:23:35,735 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:23:35,792 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:23:35,848 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:23:35,898 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:23:35,951 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:23:35,991 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:23:36,052 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:36,092 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:36,124 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:36,173 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:36,209 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:36,241 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:23:36,586 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:36,616 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:36,655 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:36,693 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:36,736 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:36,772 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:36,811 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:23:36,962 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:23:37,014 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:23:37,070 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:23:37,115 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:23:37,148 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:23:37,665 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28335ms
2014-07-14 01:23:37,674 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28362ms
2014-07-14 01:23:37,674 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28389ms
2014-07-14 01:23:37,674 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28351ms
2014-07-14 01:23:37,674 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28374ms
2014-07-14 01:23:37,687 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:23:37,702 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:23:39,010 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:23:39,043 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:23:39,076 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:23:39,109 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:23:39,940 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10026ms
2014-07-14 01:23:39,941 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10080ms
2014-07-14 01:23:39,952 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:23:39,954 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:23:39,977 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:23:40,006 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:23:40,019 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:23:40,089 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:23:40,139 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:23:40,166 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:23:40,232 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:23:40,283 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:23:40,577 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:23:40,636 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:23:40,681 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:23:40,735 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:23:40,792 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:23:40,848 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:23:40,898 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:23:40,951 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:23:40,992 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:23:41,053 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:23:41,093 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:23:42,000 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10039ms
2014-07-14 01:23:42,000 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5414ms
2014-07-14 01:23:42,001 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5385ms
2014-07-14 01:23:42,001 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5346ms
2014-07-14 01:23:42,002 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5308ms
2014-07-14 01:23:42,002 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5266ms
2014-07-14 01:23:42,002 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5191ms
2014-07-14 01:23:42,003 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5232ms
2014-07-14 01:23:42,003 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5879ms
2014-07-14 01:23:42,003 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5830ms
2014-07-14 01:23:42,003 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5794ms
2014-07-14 01:23:42,003 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10762ms
2014-07-14 01:23:42,014 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:23:42,070 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:23:42,115 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:23:42,148 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:23:42,674 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33345ms
2014-07-14 01:23:42,702 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15014ms
2014-07-14 01:23:42,702 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33402ms
2014-07-14 01:23:42,702 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33379ms
2014-07-14 01:23:42,703 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33418ms
2014-07-14 01:23:42,704 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33391ms
2014-07-14 01:23:42,704 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15003ms
2014-07-14 01:23:43,744 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9400, memsize=584.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/bb029fdaa9164a46b35045a2aec5cb03
2014-07-14 01:23:43,765 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/bb029fdaa9164a46b35045a2aec5cb03 as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/bb029fdaa9164a46b35045a2aec5cb03
2014-07-14 01:23:43,777 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/bb029fdaa9164a46b35045a2aec5cb03, entries=2127550, sequenceid=9400, filesize=151.5m
2014-07-14 01:23:43,778 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.1g/1222004640, currentsize=96.8m/101464480 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 40975ms, sequenceid=9400, compaction requested=true
2014-07-14 01:23:43,778 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:23:43,778 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 20 blocking
2014-07-14 01:23:43,779 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-14 01:23:43,779 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:23:43,779 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90620ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:23:43,779 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:23:43,779 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16078ms
2014-07-14 01:23:43,779 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 759.4m
2014-07-14 01:23:43,779 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:23:43,779 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,779 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 34467ms
2014-07-14 01:23:43,779 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,780 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 34495ms
2014-07-14 01:23:43,780 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,780 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 34457ms
2014-07-14 01:23:43,780 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,780 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 34480ms
2014-07-14 01:23:43,780 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,781 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16094ms
2014-07-14 01:23:43,781 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,785 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 34456ms
2014-07-14 01:23:43,785 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,786 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11639ms
2014-07-14 01:23:43,786 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,786 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11672ms
2014-07-14 01:23:43,786 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,786 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11717ms
2014-07-14 01:23:43,786 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,786 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11773ms
2014-07-14 01:23:43,787 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,791 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12549ms
2014-07-14 01:23:43,791 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,793 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7584ms
2014-07-14 01:23:43,793 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,794 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7621ms
2014-07-14 01:23:43,794 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,805 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7681ms
2014-07-14 01:23:43,805 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,805 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7034ms
2014-07-14 01:23:43,805 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,805 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6994ms
2014-07-14 01:23:43,805 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,806 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7069ms
2014-07-14 01:23:43,806 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,806 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7113ms
2014-07-14 01:23:43,806 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,806 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7151ms
2014-07-14 01:23:43,806 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,807 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7190ms
2014-07-14 01:23:43,807 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,807 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7221ms
2014-07-14 01:23:43,807 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,807 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11846ms
2014-07-14 01:23:43,808 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,808 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7716ms
2014-07-14 01:23:43,808 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,810 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7758ms
2014-07-14 01:23:43,810 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,810 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12819ms
2014-07-14 01:23:43,811 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,811 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12860ms
2014-07-14 01:23:43,811 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,812 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12914ms
2014-07-14 01:23:43,812 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,815 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12968ms
2014-07-14 01:23:43,815 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,819 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13027ms
2014-07-14 01:23:43,819 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,822 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13087ms
2014-07-14 01:23:43,822 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,830 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13149ms
2014-07-14 01:23:43,830 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,830 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13195ms
2014-07-14 01:23:43,831 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,836 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13260ms
2014-07-14 01:23:43,836 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,837 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13554ms
2014-07-14 01:23:43,837 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,837 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13606ms
2014-07-14 01:23:43,837 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,845 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13680ms
2014-07-14 01:23:43,845 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,845 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8706ms
2014-07-14 01:23:43,845 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,845 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13757ms
2014-07-14 01:23:43,845 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,845 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13826ms
2014-07-14 01:23:43,846 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,846 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8840ms
2014-07-14 01:23:43,846 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,851 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8874ms
2014-07-14 01:23:43,851 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,851 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13898ms
2014-07-14 01:23:43,851 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,852 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8899ms
2014-07-14 01:23:43,852 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,852 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13991ms
2014-07-14 01:23:43,852 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,853 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13939ms
2014-07-14 01:23:43,853 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,857 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9748ms
2014-07-14 01:23:43,857 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,857 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9782ms
2014-07-14 01:23:43,857 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,857 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9815ms
2014-07-14 01:23:43,857 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,857 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9847ms
2014-07-14 01:23:43,857 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:23:43,900 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34810,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47135","starttimems":1405326189090,"queuetimems":0,"class":"HRegionServer","responsesize":15733,"method":"Multi"}
2014-07-14 01:23:43,901 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 16555 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47135: output error
2014-07-14 01:23:43,904 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:23:44,268 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":35128,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47135","starttimems":1405326189139,"queuetimems":1,"class":"HRegionServer","responsesize":15893,"method":"Multi"}
2014-07-14 01:23:44,268 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 16556 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47135: output error
2014-07-14 01:23:44,269 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:23:44,411 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:23:44,434 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":35195,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47135","starttimems":1405326189238,"queuetimems":1,"class":"HRegionServer","responsesize":15850,"method":"Multi"}
2014-07-14 01:23:44,435 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 16561 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47135: output error
2014-07-14 01:23:44,435 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:23:44,443 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":35244,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47135","starttimems":1405326189188,"queuetimems":0,"class":"HRegionServer","responsesize":15828,"method":"Multi"}
2014-07-14 01:23:44,443 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 16558 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47135: output error
2014-07-14 01:23:44,443 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:23:45,727 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:23:45,743 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37881 synced till here 37850
2014-07-14 01:23:46,130 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326189282 with entries=110, filesize=91.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326225728
2014-07-14 01:23:46,331 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16169,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326210161,"queuetimems":1,"class":"HRegionServer","responsesize":15559,"method":"Multi"}
2014-07-14 01:23:46,331 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12290,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326214040,"queuetimems":1,"class":"HRegionServer","responsesize":15810,"method":"Multi"}
2014-07-14 01:23:46,331 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18630,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47135","starttimems":1405326207700,"queuetimems":0,"class":"HRegionServer","responsesize":15915,"method":"Multi"}
2014-07-14 01:23:46,352 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 16578 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47135: output error
2014-07-14 01:23:46,352 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:23:46,694 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15849,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326210844,"queuetimems":1,"class":"HRegionServer","responsesize":15687,"method":"Multi"}
2014-07-14 01:23:46,701 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15712,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326210989,"queuetimems":1,"class":"HRegionServer","responsesize":15615,"method":"Multi"}
2014-07-14 01:23:46,702 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10089,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326216613,"queuetimems":0,"class":"HRegionServer","responsesize":15618,"method":"Multi"}
2014-07-14 01:23:46,701 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15753,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326210948,"queuetimems":0,"class":"HRegionServer","responsesize":15616,"method":"Multi"}
2014-07-14 01:23:46,847 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9420, memsize=591.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/17b56b897f3541fb86f6b274fff6af56
2014-07-14 01:23:47,383 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/17b56b897f3541fb86f6b274fff6af56 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/17b56b897f3541fb86f6b274fff6af56
2014-07-14 01:23:47,576 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:23:47,586 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16798,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326210788,"queuetimems":0,"class":"HRegionServer","responsesize":15715,"method":"Multi"}
2014-07-14 01:23:47,586 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11010,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326216576,"queuetimems":1,"class":"HRegionServer","responsesize":15491,"method":"Multi"}
2014-07-14 01:23:47,587 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17014,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326210572,"queuetimems":1,"class":"HRegionServer","responsesize":15917,"method":"Multi"}
2014-07-14 01:23:47,587 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13514,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326214073,"queuetimems":1,"class":"HRegionServer","responsesize":16041,"method":"Multi"}
2014-07-14 01:23:47,589 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11430,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326216159,"queuetimems":0,"class":"HRegionServer","responsesize":15144,"method":"Multi"}
2014-07-14 01:23:47,601 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15590,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326212010,"queuetimems":1,"class":"HRegionServer","responsesize":16229,"method":"Multi"}
2014-07-14 01:23:47,601 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16971,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326210630,"queuetimems":2,"class":"HRegionServer","responsesize":15329,"method":"Multi"}
2014-07-14 01:23:47,602 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17653,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326209949,"queuetimems":1,"class":"HRegionServer","responsesize":15916,"method":"Multi"}
2014-07-14 01:23:47,613 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15470,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326212143,"queuetimems":1,"class":"HRegionServer","responsesize":15559,"method":"Multi"}
2014-07-14 01:23:47,613 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16377,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326211236,"queuetimems":1,"class":"HRegionServer","responsesize":15565,"method":"Multi"}
2014-07-14 01:23:47,615 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11532,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326216083,"queuetimems":1,"class":"HRegionServer","responsesize":15722,"method":"Multi"}
2014-07-14 01:23:47,615 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17388,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326210227,"queuetimems":0,"class":"HRegionServer","responsesize":16229,"method":"Multi"}
2014-07-14 01:23:47,621 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13614,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326214007,"queuetimems":1,"class":"HRegionServer","responsesize":15916,"method":"Multi"}
2014-07-14 01:23:47,613 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10844,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326216769,"queuetimems":1,"class":"HRegionServer","responsesize":15746,"method":"Multi"}
2014-07-14 01:23:47,628 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17769,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326209858,"queuetimems":1,"class":"HRegionServer","responsesize":15568,"method":"Multi"}
2014-07-14 01:23:47,629 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13522,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326214106,"queuetimems":0,"class":"HRegionServer","responsesize":15565,"method":"Multi"}
2014-07-14 01:23:47,629 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15518,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326212111,"queuetimems":0,"class":"HRegionServer","responsesize":15722,"method":"Multi"}
2014-07-14 01:23:47,621 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17607,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326210014,"queuetimems":0,"class":"HRegionServer","responsesize":15144,"method":"Multi"}
2014-07-14 01:23:47,615 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10965,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326216650,"queuetimems":0,"class":"HRegionServer","responsesize":15955,"method":"Multi"}
2014-07-14 01:23:47,615 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":38307,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47135","starttimems":1405326189308,"queuetimems":0,"class":"HRegionServer","responsesize":15821,"method":"Multi"}
2014-07-14 01:23:47,634 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 16562 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47135: output error
2014-07-14 01:23:47,634 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:23:47,637 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11435,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326216202,"queuetimems":1,"class":"HRegionServer","responsesize":16229,"method":"Multi"}
2014-07-14 01:23:47,638 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12664,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326214973,"queuetimems":1,"class":"HRegionServer","responsesize":16076,"method":"Multi"}
2014-07-14 01:23:47,637 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11591,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326216046,"queuetimems":0,"class":"HRegionServer","responsesize":15559,"method":"Multi"}
2014-07-14 01:23:47,638 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12688,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326214950,"queuetimems":0,"class":"HRegionServer","responsesize":16179,"method":"Multi"}
2014-07-14 01:23:47,653 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10849,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326216804,"queuetimems":0,"class":"HRegionServer","responsesize":15630,"method":"Multi"}
2014-07-14 01:23:47,661 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11540,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326216121,"queuetimems":0,"class":"HRegionServer","responsesize":15568,"method":"Multi"}
2014-07-14 01:23:47,638 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12502,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326215135,"queuetimems":0,"class":"HRegionServer","responsesize":15859,"method":"Multi"}
2014-07-14 01:23:47,662 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10929,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326216733,"queuetimems":0,"class":"HRegionServer","responsesize":15485,"method":"Multi"}
2014-07-14 01:23:47,662 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15596,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326212066,"queuetimems":0,"class":"HRegionServer","responsesize":15568,"method":"Multi"}
2014-07-14 01:23:47,677 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12675,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326215002,"queuetimems":0,"class":"HRegionServer","responsesize":16066,"method":"Multi"}
2014-07-14 01:23:47,677 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19992,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47135","starttimems":1405326207685,"queuetimems":0,"class":"HRegionServer","responsesize":15955,"method":"Multi"}
2014-07-14 01:23:47,678 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17767,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326209911,"queuetimems":0,"class":"HRegionServer","responsesize":15810,"method":"Multi"}
2014-07-14 01:23:47,678 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 16564 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47135: output error
2014-07-14 01:23:47,678 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:23:47,679 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16785,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326210893,"queuetimems":1,"class":"HRegionServer","responsesize":15978,"method":"Multi"}
2014-07-14 01:23:47,682 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15725,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326211957,"queuetimems":0,"class":"HRegionServer","responsesize":15144,"method":"Multi"}
2014-07-14 01:23:47,682 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17598,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326210084,"queuetimems":1,"class":"HRegionServer","responsesize":15722,"method":"Multi"}
2014-07-14 01:23:47,677 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10988,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326216689,"queuetimems":0,"class":"HRegionServer","responsesize":15649,"method":"Multi"}
2014-07-14 01:23:47,679 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17002,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326210676,"queuetimems":0,"class":"HRegionServer","responsesize":15924,"method":"Multi"}
2014-07-14 01:23:47,678 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16947,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326210731,"queuetimems":1,"class":"HRegionServer","responsesize":16077,"method":"Multi"}
2014-07-14 01:23:47,845 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/17b56b897f3541fb86f6b274fff6af56, entries=2154620, sequenceid=9420, filesize=153.4m
2014-07-14 01:23:47,846 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.1g/1229130960, currentsize=183.5m/192442560 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 45043ms, sequenceid=9420, compaction requested=true
2014-07-14 01:23:47,847 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:23:47,847 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 94129ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:23:47,847 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 28 store files, 0 compacting, 28 eligible, 20 blocking
2014-07-14 01:23:47,847 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 28 files from compaction candidates
2014-07-14 01:23:47,847 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 854.7m
2014-07-14 01:23:47,847 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:23:47,847 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:23:47,847 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:23:47,857 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17578,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326210279,"queuetimems":0,"class":"HRegionServer","responsesize":16041,"method":"Multi"}
2014-07-14 01:23:47,883 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38033 synced till here 38009
2014-07-14 01:23:48,065 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326225728 with entries=152, filesize=124.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326227576
2014-07-14 01:23:48,065 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326040564
2014-07-14 01:23:48,065 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326063222
2014-07-14 01:23:48,065 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326067213
2014-07-14 01:23:48,065 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326078974
2014-07-14 01:23:48,065 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326081384
2014-07-14 01:23:48,065 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326083429
2014-07-14 01:23:48,066 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326086036
2014-07-14 01:23:48,066 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326089859
2014-07-14 01:23:48,066 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326092595
2014-07-14 01:23:48,066 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326094945
2014-07-14 01:23:48,066 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326097971
2014-07-14 01:23:48,066 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326100765
2014-07-14 01:23:48,066 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326103184
2014-07-14 01:23:49,546 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:23:49,601 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38144 synced till here 38112
2014-07-14 01:23:49,786 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:23:49,938 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:23:49,990 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326227576 with entries=111, filesize=92.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326229546
2014-07-14 01:23:52,342 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:23:52,430 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38254 synced till here 38225
2014-07-14 01:23:52,559 DEBUG [RpcServer.handler=37,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:23:52,586 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326229546 with entries=110, filesize=94.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326232343
2014-07-14 01:23:54,291 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:23:54,305 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38339 synced till here 38331
2014-07-14 01:23:54,353 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326232343 with entries=85, filesize=72.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326234292
2014-07-14 01:23:58,131 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:23:58,154 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326234292 with entries=71, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326238132
2014-07-14 01:23:59,440 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:23:59,505 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38483 synced till here 38482
2014-07-14 01:23:59,521 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326238132 with entries=73, filesize=62.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326239440
2014-07-14 01:24:01,995 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:24:02,044 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326239440 with entries=74, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326241996
2014-07-14 01:24:03,500 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:24:03,634 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38641 synced till here 38631
2014-07-14 01:24:03,693 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326241996 with entries=84, filesize=71.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326243501
2014-07-14 01:24:05,444 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:24:05,475 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38717 synced till here 38713
2014-07-14 01:24:05,527 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326243501 with entries=76, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326245444
2014-07-14 01:24:06,970 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:24:07,319 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38814 synced till here 38812
2014-07-14 01:24:07,341 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326245444 with entries=97, filesize=83.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326246971
2014-07-14 01:24:08,743 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:24:09,638 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38919 synced till here 38904
2014-07-14 01:24:09,797 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326246971 with entries=105, filesize=89.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326248744
2014-07-14 01:24:12,151 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:24:12,212 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39015 synced till here 38994
2014-07-14 01:24:12,472 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326248744 with entries=96, filesize=81.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326252151
2014-07-14 01:24:14,452 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:24:14,536 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39119 synced till here 39096
2014-07-14 01:24:14,670 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326252151 with entries=104, filesize=88.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326254453
2014-07-14 01:24:16,460 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:24:16,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39202 synced till here 39196
2014-07-14 01:24:16,657 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326254453 with entries=83, filesize=71.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326256461
2014-07-14 01:24:17,555 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:24:17,595 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:18,203 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:18,218 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:18,219 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326256461 with entries=83, filesize=71.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326257556
2014-07-14 01:24:18,229 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:18,251 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:18,262 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:18,295 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:18,298 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:18,304 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:18,325 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:18,354 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:18,383 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:18,413 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:18,443 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:18,474 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:18,504 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:18,534 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:18,564 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:18,593 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:18,622 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:18,652 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:18,681 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:18,710 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:18,739 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:19,773 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9469, memsize=618.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/ece8934bdd424b6eac564802ebc9a687
2014-07-14 01:24:19,792 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/ece8934bdd424b6eac564802ebc9a687 as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/ece8934bdd424b6eac564802ebc9a687
2014-07-14 01:24:19,808 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/ece8934bdd424b6eac564802ebc9a687, entries=2250970, sequenceid=9469, filesize=160.2m
2014-07-14 01:24:19,808 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~759.4m/796298880, currentsize=584.5m/612844080 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 36029ms, sequenceid=9469, compaction requested=true
2014-07-14 01:24:19,809 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:24:19,809 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 28 store files, 0 compacting, 28 eligible, 20 blocking
2014-07-14 01:24:19,809 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 28 files from compaction candidates
2014-07-14 01:24:19,809 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files; delaying flush up to 90000ms
2014-07-14 01:24:19,809 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:24:19,809 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:24:19,809 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-14 01:24:19,809 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:24:19,810 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files; delaying flush up to 90000ms
2014-07-14 01:24:19,810 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1070ms
2014-07-14 01:24:19,810 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-14 01:24:19,810 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 28 store files, 0 compacting, 28 eligible, 20 blocking
2014-07-14 01:24:19,810 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:19,810 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 28 files from compaction candidates
2014-07-14 01:24:19,810 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:24:19,810 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1100ms
2014-07-14 01:24:19,810 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:24:19,810 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:19,810 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:24:19,810 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 20 blocking
2014-07-14 01:24:19,811 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-14 01:24:19,811 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:24:19,811 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:24:19,811 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:24:19,813 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1132ms
2014-07-14 01:24:19,813 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:19,829 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1177ms
2014-07-14 01:24:19,829 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:19,834 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1211ms
2014-07-14 01:24:19,834 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:19,836 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1243ms
2014-07-14 01:24:19,836 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:19,837 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1273ms
2014-07-14 01:24:19,837 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:19,849 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1315ms
2014-07-14 01:24:19,849 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:19,849 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1345ms
2014-07-14 01:24:19,849 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:19,849 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1375ms
2014-07-14 01:24:19,849 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:19,849 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1406ms
2014-07-14 01:24:19,850 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:19,850 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1437ms
2014-07-14 01:24:19,850 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:19,851 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1467ms
2014-07-14 01:24:19,851 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:19,865 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1511ms
2014-07-14 01:24:19,865 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:19,866 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1541ms
2014-07-14 01:24:19,866 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:19,866 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1562ms
2014-07-14 01:24:19,866 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:19,866 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1568ms
2014-07-14 01:24:19,866 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:19,867 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1572ms
2014-07-14 01:24:19,867 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:19,881 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1619ms
2014-07-14 01:24:19,881 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:19,889 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1638ms
2014-07-14 01:24:19,889 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:19,889 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1660ms
2014-07-14 01:24:19,889 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:19,890 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1671ms
2014-07-14 01:24:19,890 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:19,894 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1691ms
2014-07-14 01:24:19,894 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:19,894 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2299ms
2014-07-14 01:24:19,894 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:19,967 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:24:19,969 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files; delaying flush up to 90000ms
2014-07-14 01:24:19,969 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:24:19,969 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 28 store files, 0 compacting, 28 eligible, 20 blocking
2014-07-14 01:24:19,969 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 28 files from compaction candidates
2014-07-14 01:24:19,969 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:24:19,969 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:24:19,969 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:24:21,576 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:24:21,634 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39371 synced till here 39361
2014-07-14 01:24:21,694 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326257556 with entries=86, filesize=73.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326261577
2014-07-14 01:24:22,404 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:24:22,421 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39449 synced till here 39442
2014-07-14 01:24:22,494 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326261577 with entries=78, filesize=67.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326262405
2014-07-14 01:24:24,120 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:24:24,160 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39524 synced till here 39522
2014-07-14 01:24:24,243 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326262405 with entries=75, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326264121
2014-07-14 01:24:26,135 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:24:26,352 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39616 synced till here 39614
2014-07-14 01:24:26,380 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326264121 with entries=92, filesize=78.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326266136
2014-07-14 01:24:27,717 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:24:27,718 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files, but is 824.7m vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:24:27,718 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. due to global heap pressure
2014-07-14 01:24:27,718 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 824.7m
2014-07-14 01:24:28,201 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:24:28,478 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39710 synced till here 39709
2014-07-14 01:24:28,491 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:24:28,513 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326266136 with entries=94, filesize=80.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326268215
2014-07-14 01:24:30,461 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1245ms
GC pool 'ParNew' had collection(s): count=1 time=1722ms
2014-07-14 01:24:30,951 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9533, memsize=674.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/b627597e80ed48ce9d847767c25044ad
2014-07-14 01:24:30,954 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:24:30,958 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:30,959 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:30,959 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:30,961 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:30,961 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:30,961 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:30,961 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:30,962 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:30,962 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:30,967 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/b627597e80ed48ce9d847767c25044ad as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/b627597e80ed48ce9d847767c25044ad
2014-07-14 01:24:30,969 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:30,970 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:30,972 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39790 synced till here 39781
2014-07-14 01:24:30,978 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/b627597e80ed48ce9d847767c25044ad, entries=2455000, sequenceid=9533, filesize=174.8m
2014-07-14 01:24:30,979 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~862.5m/904358080, currentsize=675.0m/707752480 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 43132ms, sequenceid=9533, compaction requested=true
2014-07-14 01:24:30,979 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:24:30,979 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 20 blocking
2014-07-14 01:24:30,979 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9ms
2014-07-14 01:24:30,979 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:30,979 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10ms
2014-07-14 01:24:30,979 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:30,979 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-14 01:24:30,979 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17ms
2014-07-14 01:24:30,980 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:30,980 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:24:30,980 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18ms
2014-07-14 01:24:30,980 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:30,980 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:24:30,980 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:24:30,981 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22ms
2014-07-14 01:24:30,981 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:30,981 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20ms
2014-07-14 01:24:30,981 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:30,981 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20ms
2014-07-14 01:24:30,981 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:30,981 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24ms
2014-07-14 01:24:30,981 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:30,981 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22ms
2014-07-14 01:24:30,981 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:30,989 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30ms
2014-07-14 01:24:30,989 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:30,989 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32ms
2014-07-14 01:24:30,989 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:31,030 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:24:31,033 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files; delaying flush up to 90000ms
2014-07-14 01:24:31,039 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 20 blocking
2014-07-14 01:24:31,039 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-14 01:24:31,039 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:24:31,039 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:24:31,039 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:24:31,040 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:24:31,185 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326268215 with entries=80, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326270955
2014-07-14 01:24:31,185 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326104643
2014-07-14 01:24:31,185 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326109386
2014-07-14 01:24:31,185 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326111764
2014-07-14 01:24:31,185 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326116099
2014-07-14 01:24:31,185 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326117790
2014-07-14 01:24:31,185 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326119599
2014-07-14 01:24:31,185 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326121364
2014-07-14 01:24:31,185 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326122789
2014-07-14 01:24:31,185 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326131137
2014-07-14 01:24:31,185 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326133449
2014-07-14 01:24:31,185 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326135275
2014-07-14 01:24:31,186 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326136570
2014-07-14 01:24:31,186 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326138104
2014-07-14 01:24:31,186 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326139807
2014-07-14 01:24:31,186 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326143096
2014-07-14 01:24:31,186 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326144577
2014-07-14 01:24:31,186 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326145593
2014-07-14 01:24:31,186 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326166473
2014-07-14 01:24:31,186 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326168862
2014-07-14 01:24:31,186 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326170306
2014-07-14 01:24:31,186 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326177135
2014-07-14 01:24:31,186 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326178775
2014-07-14 01:24:32,713 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1251ms
GC pool 'ParNew' had collection(s): count=1 time=1134ms
2014-07-14 01:24:33,244 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:24:33,300 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39877 synced till here 39867
2014-07-14 01:24:33,578 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326270955 with entries=87, filesize=74.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326273244
2014-07-14 01:24:35,650 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:24:35,680 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39972 synced till here 39950
2014-07-14 01:24:36,018 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326273244 with entries=95, filesize=80.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326275650
2014-07-14 01:24:38,157 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:24:38,173 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40077 synced till here 40066
2014-07-14 01:24:38,344 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326275650 with entries=105, filesize=89.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326278158
2014-07-14 01:24:40,418 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:24:40,480 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40180 synced till here 40155
2014-07-14 01:24:42,010 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1007ms
GC pool 'ParNew' had collection(s): count=1 time=1305ms
2014-07-14 01:24:42,064 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326278158 with entries=103, filesize=88.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326280418
2014-07-14 01:24:42,426 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:24:42,427 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files, but is 1013.4m vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:24:42,427 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. due to global heap pressure
2014-07-14 01:24:42,427 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 1013.4m
2014-07-14 01:24:42,832 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:24:42,921 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40286 synced till here 40251
2014-07-14 01:24:44,164 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1152ms
GC pool 'ParNew' had collection(s): count=1 time=1154ms
2014-07-14 01:24:44,548 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326280418 with entries=106, filesize=90.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326282833
2014-07-14 01:24:44,977 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:44,979 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:44,980 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:44,986 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:44,986 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:44,987 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:45,096 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:45,118 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:24:45,193 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:45,194 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:45,200 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:45,201 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:45,372 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:24:45,376 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:45,376 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,679 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1013ms
GC pool 'ParNew' had collection(s): count=1 time=1301ms
2014-07-14 01:24:46,680 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,680 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,681 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,682 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,682 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,687 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,687 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,687 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,688 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,689 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,689 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,690 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,690 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,693 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,693 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,693 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,693 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,694 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40377 synced till here 40367
2014-07-14 01:24:46,760 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,761 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,761 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,767 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,767 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,769 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,769 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,769 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,769 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,769 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,770 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,770 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326282833 with entries=91, filesize=77.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326285373
2014-07-14 01:24:46,770 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,770 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,771 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,771 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,771 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,771 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,772 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,775 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:46,775 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:24:49,977 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:24:49,979 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:24:49,980 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:24:49,986 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:24:49,986 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5009ms
2014-07-14 01:24:49,987 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:24:50,097 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:24:50,193 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:24:50,196 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-14 01:24:50,200 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:24:50,201 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:24:50,376 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:24:50,377 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:24:52,599 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5824ms
2014-07-14 01:24:52,600 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5833ms
2014-07-14 01:24:52,600 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5832ms
2014-07-14 01:24:52,600 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5840ms
2014-07-14 01:24:52,601 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5829ms
2014-07-14 01:24:52,601 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5914ms
2014-07-14 01:24:52,602 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5919ms
2014-07-14 01:24:52,603 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5833ms
2014-07-14 01:24:52,603 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5842ms
2014-07-14 01:24:52,603 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5834ms
2014-07-14 01:24:52,604 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5842ms
2014-07-14 01:24:52,604 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5837ms
2014-07-14 01:24:52,605 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5837ms
2014-07-14 01:24:52,605 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5838ms
2014-07-14 01:24:52,606 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5838ms
2014-07-14 01:24:52,607 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5840ms
2014-07-14 01:24:52,607 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5832ms
2014-07-14 01:24:52,607 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5836ms
2014-07-14 01:24:52,608 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7232ms
2014-07-14 01:24:52,608 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7232ms
2014-07-14 01:24:52,609 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5927ms
2014-07-14 01:24:52,609 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5927ms
2014-07-14 01:24:52,610 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5922ms
2014-07-14 01:24:52,610 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5923ms
2014-07-14 01:24:52,611 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5921ms
2014-07-14 01:24:52,611 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5922ms
2014-07-14 01:24:52,611 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5923ms
2014-07-14 01:24:52,612 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5923ms
2014-07-14 01:24:52,612 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5922ms
2014-07-14 01:24:52,612 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5919ms
2014-07-14 01:24:52,612 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5919ms
2014-07-14 01:24:52,613 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5920ms
2014-07-14 01:24:52,613 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5920ms
2014-07-14 01:24:52,613 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5853ms
2014-07-14 01:24:52,613 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5853ms
2014-07-14 01:24:52,613 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5852ms
2014-07-14 01:24:52,614 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5847ms
2014-07-14 01:24:53,617 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9961, memsize=355.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/866eee253fec46e8b0749898389b6bcc
2014-07-14 01:24:53,638 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/866eee253fec46e8b0749898389b6bcc as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/866eee253fec46e8b0749898389b6bcc
2014-07-14 01:24:53,651 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/866eee253fec46e8b0749898389b6bcc, entries=1295690, sequenceid=9961, filesize=92.4m
2014-07-14 01:24:53,651 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~826.2m/866291280, currentsize=295.3m/309688960 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 25933ms, sequenceid=9961, compaction requested=true
2014-07-14 01:24:53,652 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:24:53,652 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 20 blocking
2014-07-14 01:24:53,652 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6885ms
2014-07-14 01:24:53,652 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-14 01:24:53,652 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,652 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:24:53,652 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6891ms
2014-07-14 01:24:53,652 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,652 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:24:53,653 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6893ms
2014-07-14 01:24:53,653 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,653 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:24:53,657 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6897ms
2014-07-14 01:24:53,657 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,657 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6964ms
2014-07-14 01:24:53,657 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,657 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6964ms
2014-07-14 01:24:53,657 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,657 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6964ms
2014-07-14 01:24:53,657 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,657 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6964ms
2014-07-14 01:24:53,657 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,669 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6979ms
2014-07-14 01:24:53,669 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,669 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6981ms
2014-07-14 01:24:53,669 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,673 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6985ms
2014-07-14 01:24:53,673 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,673 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6984ms
2014-07-14 01:24:53,673 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,673 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6983ms
2014-07-14 01:24:53,673 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,673 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6986ms
2014-07-14 01:24:53,673 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,673 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6986ms
2014-07-14 01:24:53,673 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,673 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6991ms
2014-07-14 01:24:53,674 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,689 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7008ms
2014-07-14 01:24:53,689 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,689 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8313ms
2014-07-14 01:24:53,689 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,689 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8313ms
2014-07-14 01:24:53,689 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,689 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6918ms
2014-07-14 01:24:53,689 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,689 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6914ms
2014-07-14 01:24:53,690 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,690 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6924ms
2014-07-14 01:24:53,690 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,690 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6923ms
2014-07-14 01:24:53,690 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,690 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6923ms
2014-07-14 01:24:53,690 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,690 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6923ms
2014-07-14 01:24:53,690 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,701 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6934ms
2014-07-14 01:24:53,701 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,701 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6940ms
2014-07-14 01:24:53,701 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,701 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6932ms
2014-07-14 01:24:53,701 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,701 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6940ms
2014-07-14 01:24:53,701 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,701 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6932ms
2014-07-14 01:24:53,701 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,709 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7027ms
2014-07-14 01:24:53,709 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,709 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7022ms
2014-07-14 01:24:53,709 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,709 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6937ms
2014-07-14 01:24:53,709 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,709 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6949ms
2014-07-14 01:24:53,709 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,709 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6941ms
2014-07-14 01:24:53,709 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,709 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6942ms
2014-07-14 01:24:53,709 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,710 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6935ms
2014-07-14 01:24:53,710 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,710 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8334ms
2014-07-14 01:24:53,710 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,717 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8341ms
2014-07-14 01:24:53,717 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,717 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8516ms
2014-07-14 01:24:53,717 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,753 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8553ms
2014-07-14 01:24:53,753 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,761 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8567ms
2014-07-14 01:24:53,761 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,761 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8568ms
2014-07-14 01:24:53,761 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,769 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8673ms
2014-07-14 01:24:53,769 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,777 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8790ms
2014-07-14 01:24:53,777 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,777 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8800ms
2014-07-14 01:24:53,777 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,777 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8791ms
2014-07-14 01:24:53,777 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,784 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11158,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326282625,"queuetimems":212,"class":"HRegionServer","responsesize":16129,"method":"Multi"}
2014-07-14 01:24:53,784 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11369,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326282414,"queuetimems":103,"class":"HRegionServer","responsesize":15994,"method":"Multi"}
2014-07-14 01:24:53,784 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8804ms
2014-07-14 01:24:53,784 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,793 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8814ms
2014-07-14 01:24:53,793 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,793 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8816ms
2014-07-14 01:24:53,793 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:24:53,793 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11387,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326282406,"queuetimems":127,"class":"HRegionServer","responsesize":15671,"method":"Multi"}
2014-07-14 01:24:53,811 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11177,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326282633,"queuetimems":119,"class":"HRegionServer","responsesize":16024,"method":"Multi"}
2014-07-14 01:24:53,939 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11317,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326282622,"queuetimems":281,"class":"HRegionServer","responsesize":15795,"method":"Multi"}
2014-07-14 01:24:53,939 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11541,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326282398,"queuetimems":166,"class":"HRegionServer","responsesize":15904,"method":"Multi"}
2014-07-14 01:24:53,939 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11316,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326282623,"queuetimems":253,"class":"HRegionServer","responsesize":15431,"method":"Multi"}
2014-07-14 01:24:54,121 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:24:54,121 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13691,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326280430,"queuetimems":433,"class":"HRegionServer","responsesize":15586,"method":"Multi"}
2014-07-14 01:24:54,121 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files; delaying flush up to 90000ms
2014-07-14 01:24:54,121 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12062,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326282058,"queuetimems":1803,"class":"HRegionServer","responsesize":15705,"method":"Multi"}
2014-07-14 01:24:54,121 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12012,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326282109,"queuetimems":66,"class":"HRegionServer","responsesize":15811,"method":"Multi"}
2014-07-14 01:24:54,122 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12069,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326282053,"queuetimems":1967,"class":"HRegionServer","responsesize":15863,"method":"Multi"}
2014-07-14 01:24:54,122 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12064,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326282058,"queuetimems":1863,"class":"HRegionServer","responsesize":15628,"method":"Multi"}
2014-07-14 01:24:54,129 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:24:54,129 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 20 blocking
2014-07-14 01:24:54,129 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-14 01:24:54,129 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:24:54,129 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:24:54,129 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:24:54,573 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:24:54,575 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10304,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326284270,"queuetimems":50,"class":"HRegionServer","responsesize":15478,"method":"Multi"}
2014-07-14 01:24:54,651 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40486 synced till here 40453
2014-07-14 01:24:54,785 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10532,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326284252,"queuetimems":84,"class":"HRegionServer","responsesize":15964,"method":"Multi"}
2014-07-14 01:24:54,785 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11939,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326282845,"queuetimems":188,"class":"HRegionServer","responsesize":15839,"method":"Multi"}
2014-07-14 01:24:54,785 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11941,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326282843,"queuetimems":211,"class":"HRegionServer","responsesize":15853,"method":"Multi"}
2014-07-14 01:24:54,794 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10419,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326284374,"queuetimems":9,"class":"HRegionServer","responsesize":15701,"method":"Multi"}
2014-07-14 01:24:54,795 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10542,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326284252,"queuetimems":1338,"class":"HRegionServer","responsesize":15901,"method":"Multi"}
2014-07-14 01:24:54,801 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11962,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326282839,"queuetimems":248,"class":"HRegionServer","responsesize":15765,"method":"Multi"}
2014-07-14 01:24:54,794 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10541,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326284252,"queuetimems":1285,"class":"HRegionServer","responsesize":15811,"method":"Multi"}
2014-07-14 01:24:54,802 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10547,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326284255,"queuetimems":77,"class":"HRegionServer","responsesize":15707,"method":"Multi"}
2014-07-14 01:24:54,805 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10554,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326284251,"queuetimems":1427,"class":"HRegionServer","responsesize":15915,"method":"Multi"}
2014-07-14 01:24:54,805 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10447,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326284358,"queuetimems":106,"class":"HRegionServer","responsesize":16208,"method":"Multi"}
2014-07-14 01:24:54,806 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11948,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326282857,"queuetimems":132,"class":"HRegionServer","responsesize":15502,"method":"Multi"}
2014-07-14 01:24:54,806 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10555,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326284251,"queuetimems":1372,"class":"HRegionServer","responsesize":15683,"method":"Multi"}
2014-07-14 01:24:54,801 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10435,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326284366,"queuetimems":32,"class":"HRegionServer","responsesize":15824,"method":"Multi"}
2014-07-14 01:24:54,805 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11956,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326282849,"queuetimems":153,"class":"HRegionServer","responsesize":15845,"method":"Multi"}
2014-07-14 01:24:54,805 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11948,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326282857,"queuetimems":66,"class":"HRegionServer","responsesize":15769,"method":"Multi"}
2014-07-14 01:24:55,561 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10627,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326284934,"queuetimems":458,"class":"HRegionServer","responsesize":15765,"method":"Multi"}
2014-07-14 01:24:55,596 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326285373 with entries=109, filesize=92.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326294574
2014-07-14 01:24:55,799 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10858,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326284941,"queuetimems":405,"class":"HRegionServer","responsesize":15706,"method":"Multi"}
2014-07-14 01:24:55,800 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11541,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326284259,"queuetimems":69,"class":"HRegionServer","responsesize":15839,"method":"Multi"}
2014-07-14 01:24:55,799 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10873,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326284926,"queuetimems":488,"class":"HRegionServer","responsesize":15803,"method":"Multi"}
2014-07-14 01:24:56,007 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11072,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326284934,"queuetimems":428,"class":"HRegionServer","responsesize":15461,"method":"Multi"}
2014-07-14 01:24:56,009 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13151,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326282857,"queuetimems":103,"class":"HRegionServer","responsesize":15661,"method":"Multi"}
2014-07-14 01:24:56,449 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:24:56,464 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40604 synced till here 40574
2014-07-14 01:24:56,691 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10005,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326286685,"queuetimems":1886,"class":"HRegionServer","responsesize":16024,"method":"Multi"}
2014-07-14 01:24:56,691 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11494,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326285197,"queuetimems":521,"class":"HRegionServer","responsesize":15822,"method":"Multi"}
2014-07-14 01:24:56,691 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10004,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326286687,"queuetimems":1768,"class":"HRegionServer","responsesize":15904,"method":"Multi"}
2014-07-14 01:24:56,692 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11500,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326285191,"queuetimems":584,"class":"HRegionServer","responsesize":15705,"method":"Multi"}
2014-07-14 01:24:56,692 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10006,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326286685,"queuetimems":1827,"class":"HRegionServer","responsesize":15795,"method":"Multi"}
2014-07-14 01:24:56,691 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11496,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326285195,"queuetimems":550,"class":"HRegionServer","responsesize":15628,"method":"Multi"}
2014-07-14 01:24:56,691 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10005,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326286685,"queuetimems":1856,"class":"HRegionServer","responsesize":15671,"method":"Multi"}
2014-07-14 01:24:56,691 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11500,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326285190,"queuetimems":618,"class":"HRegionServer","responsesize":15863,"method":"Multi"}
2014-07-14 01:24:56,691 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10007,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326286683,"queuetimems":1978,"class":"HRegionServer","responsesize":15589,"method":"Multi"}
2014-07-14 01:24:56,694 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10003,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326286690,"queuetimems":1623,"class":"HRegionServer","responsesize":16129,"method":"Multi"}
2014-07-14 01:24:56,701 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10017,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326286684,"queuetimems":1949,"class":"HRegionServer","responsesize":15586,"method":"Multi"}
2014-07-14 01:24:56,710 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10019,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326286690,"queuetimems":1578,"class":"HRegionServer","responsesize":15431,"method":"Multi"}
2014-07-14 01:24:56,701 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10010,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326286691,"queuetimems":1372,"class":"HRegionServer","responsesize":15964,"method":"Multi"}
2014-07-14 01:24:56,756 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326294574 with entries=118, filesize=101.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326296450
2014-07-14 01:24:58,187 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:24:58,235 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40705 synced till here 40679
2014-07-14 01:24:58,386 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326296450 with entries=101, filesize=86.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326298188
2014-07-14 01:24:58,634 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:24:58,634 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files, but is 1.1g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:24:58,634 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. due to global heap pressure
2014-07-14 01:24:58,635 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 1.1g
2014-07-14 01:24:59,867 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:25:00,227 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:25:00,329 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326298188 with entries=84, filesize=72.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326300012
2014-07-14 01:25:02,062 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:25:02,090 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40865 synced till here 40862
2014-07-14 01:25:02,102 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:02,147 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:02,147 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:02,151 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326300012 with entries=76, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326302062
2014-07-14 01:25:02,175 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:02,213 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:02,213 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:02,294 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:02,436 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:02,591 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:02,809 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:02,877 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:03,180 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:03,726 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:03,840 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:03,870 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:03,901 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:03,946 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:03,981 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:04,012 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:04,042 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:04,071 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:04,233 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:04,266 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:04,296 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:04,325 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:04,354 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:04,386 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:04,417 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:04,844 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:04,887 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:05,703 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:05,712 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:05,733 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:05,764 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:06,132 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10065, memsize=396.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/a50f4845cab647dd8cfeab3b097a0754
2014-07-14 01:25:06,151 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/a50f4845cab647dd8cfeab3b097a0754 as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/a50f4845cab647dd8cfeab3b097a0754
2014-07-14 01:25:06,164 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/a50f4845cab647dd8cfeab3b097a0754, entries=1444850, sequenceid=10065, filesize=103.0m
2014-07-14 01:25:06,164 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.0g/1080557840, currentsize=262.9m/275689280 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 23737ms, sequenceid=10065, compaction requested=true
2014-07-14 01:25:06,165 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:25:06,165 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 20 blocking
2014-07-14 01:25:06,165 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 401ms
2014-07-14 01:25:06,165 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-14 01:25:06,165 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,165 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:25:06,165 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 432ms
2014-07-14 01:25:06,165 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,165 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:25:06,165 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 454ms
2014-07-14 01:25:06,165 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,165 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:25:06,166 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 463ms
2014-07-14 01:25:06,166 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,166 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1279ms
2014-07-14 01:25:06,166 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,166 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1322ms
2014-07-14 01:25:06,166 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,166 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1749ms
2014-07-14 01:25:06,166 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,171 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1785ms
2014-07-14 01:25:06,171 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,171 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1817ms
2014-07-14 01:25:06,171 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,171 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1846ms
2014-07-14 01:25:06,172 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,172 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1876ms
2014-07-14 01:25:06,172 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,184 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1918ms
2014-07-14 01:25:06,185 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,185 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1952ms
2014-07-14 01:25:06,185 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,185 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2114ms
2014-07-14 01:25:06,185 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,189 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2147ms
2014-07-14 01:25:06,189 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,189 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2177ms
2014-07-14 01:25:06,189 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,200 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2219ms
2014-07-14 01:25:06,200 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,200 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2254ms
2014-07-14 01:25:06,200 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,209 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2308ms
2014-07-14 01:25:06,209 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,209 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2339ms
2014-07-14 01:25:06,209 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,210 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2369ms
2014-07-14 01:25:06,210 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,211 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2484ms
2014-07-14 01:25:06,211 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,213 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3033ms
2014-07-14 01:25:06,213 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,215 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3339ms
2014-07-14 01:25:06,215 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,216 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3406ms
2014-07-14 01:25:06,216 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,216 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3625ms
2014-07-14 01:25:06,216 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,225 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3789ms
2014-07-14 01:25:06,225 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,226 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3931ms
2014-07-14 01:25:06,226 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,227 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4013ms
2014-07-14 01:25:06,227 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,228 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4014ms
2014-07-14 01:25:06,228 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,228 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4053ms
2014-07-14 01:25:06,229 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,233 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4086ms
2014-07-14 01:25:06,233 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,234 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4086ms
2014-07-14 01:25:06,234 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,234 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4133ms
2014-07-14 01:25:06,234 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:06,438 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:25:06,438 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files; delaying flush up to 90000ms
2014-07-14 01:25:06,438 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 20 blocking
2014-07-14 01:25:06,439 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-14 01:25:06,439 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:25:06,439 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:25:06,439 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:25:06,439 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:25:08,090 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:25:08,165 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40975 synced till here 40958
2014-07-14 01:25:08,336 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326302062 with entries=110, filesize=93.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326308091
2014-07-14 01:25:08,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326180877
2014-07-14 01:25:08,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326183467
2014-07-14 01:25:08,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326185030
2014-07-14 01:25:08,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326186816
2014-07-14 01:25:09,084 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:25:09,143 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41071 synced till here 41052
2014-07-14 01:25:09,946 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326308091 with entries=96, filesize=82.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326309085
2014-07-14 01:25:10,791 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:25:10,833 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41154 synced till here 41145
2014-07-14 01:25:10,917 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326309085 with entries=83, filesize=71.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326310791
2014-07-14 01:25:12,320 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:25:12,334 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41227 synced till here 41226
2014-07-14 01:25:12,351 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326310791 with entries=73, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326312320
2014-07-14 01:25:13,665 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:25:13,680 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41300 synced till here 41299
2014-07-14 01:25:13,692 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326312320 with entries=73, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326313666
2014-07-14 01:25:15,413 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:25:15,439 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326313666 with entries=72, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326315413
2014-07-14 01:25:15,963 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:25:15,964 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files, but is 1.3g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:25:15,964 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. due to global heap pressure
2014-07-14 01:25:15,964 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 1.3g
2014-07-14 01:25:16,634 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:25:17,150 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41461 synced till here 41460
2014-07-14 01:25:17,151 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10215, memsize=299.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/030191f9474449e987556ad6437d01f8
2014-07-14 01:25:17,212 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/030191f9474449e987556ad6437d01f8 as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/030191f9474449e987556ad6437d01f8
2014-07-14 01:25:17,314 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326315413 with entries=89, filesize=76.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326316635
2014-07-14 01:25:17,367 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/030191f9474449e987556ad6437d01f8, entries=1091760, sequenceid=10215, filesize=77.8m
2014-07-14 01:25:17,368 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.1g/1206850720, currentsize=273.8m/287095520 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 18734ms, sequenceid=10215, compaction requested=true
2014-07-14 01:25:17,368 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:25:17,368 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 20 blocking
2014-07-14 01:25:17,368 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-14 01:25:17,368 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:25:17,368 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:25:17,368 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:25:17,521 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:25:17,521 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files; delaying flush up to 90000ms
2014-07-14 01:25:17,521 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:25:17,521 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 20 blocking
2014-07-14 01:25:17,522 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-14 01:25:17,522 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:25:17,522 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:25:17,522 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:25:17,609 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:25:18,407 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:25:19,022 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41558 synced till here 41554
2014-07-14 01:25:19,058 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326316635 with entries=97, filesize=82.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326318407
2014-07-14 01:25:19,059 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326189282
2014-07-14 01:25:19,059 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326225728
2014-07-14 01:25:19,931 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:25:20,194 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41645 synced till here 41643
2014-07-14 01:25:20,221 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326318407 with entries=87, filesize=74.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326319932
2014-07-14 01:25:21,702 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:25:21,835 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41728 synced till here 41727
2014-07-14 01:25:21,857 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326319932 with entries=83, filesize=71.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326321703
2014-07-14 01:25:23,206 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:25:23,364 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41809 synced till here 41807
2014-07-14 01:25:23,492 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326321703 with entries=81, filesize=69.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326323207
2014-07-14 01:25:25,357 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:25:25,392 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41894 synced till here 41886
2014-07-14 01:25:25,475 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326323207 with entries=85, filesize=73.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326325358
2014-07-14 01:25:27,192 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:25:27,251 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41978 synced till here 41967
2014-07-14 01:25:27,313 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326325358 with entries=84, filesize=72.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326327193
2014-07-14 01:25:28,890 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:25:28,914 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42062 synced till here 42052
2014-07-14 01:25:29,011 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326327193 with entries=84, filesize=71.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326328891
2014-07-14 01:25:30,729 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:25:30,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42148 synced till here 42134
2014-07-14 01:25:30,841 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:25:30,841 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files, but is 969.9m vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:25:30,841 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. due to global heap pressure
2014-07-14 01:25:30,841 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 969.9m
2014-07-14 01:25:30,890 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326328891 with entries=86, filesize=73.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326330729
2014-07-14 01:25:32,683 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:25:32,770 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42238 synced till here 42220
2014-07-14 01:25:32,918 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:25:32,943 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326330729 with entries=90, filesize=77.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326332684
2014-07-14 01:25:33,353 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,354 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,356 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,358 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,358 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,359 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,360 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,361 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,371 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,400 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,442 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,474 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,475 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,476 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,478 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,478 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,479 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,480 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,480 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,480 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,481 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,481 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,487 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,517 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,558 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,561 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,562 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,562 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,598 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:33,636 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:34,724 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:34,736 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:34,751 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:34,781 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:34,811 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:34,839 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:34,870 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:34,900 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:34,930 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:34,959 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:34,988 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:35,018 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:35,048 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:35,078 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:35,107 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:35,137 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:35,168 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:35,198 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:35,228 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:35,259 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:25:38,354 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:25:38,354 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:25:38,356 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:25:38,358 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:25:38,359 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:25:38,360 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:25:38,360 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:25:38,361 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:25:38,372 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:25:38,400 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:25:38,442 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:25:38,475 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:25:38,475 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:25:38,476 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:25:38,478 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:25:38,478 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:25:38,480 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:25:38,480 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:25:38,480 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:25:38,481 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:25:38,481 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:25:38,482 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:25:38,487 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:25:38,517 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:25:38,559 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:25:38,562 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:25:38,562 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:25:38,562 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:25:38,599 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:25:38,636 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:25:40,164 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5026ms
2014-07-14 01:25:40,165 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5383ms
2014-07-14 01:25:40,165 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5354ms
2014-07-14 01:25:40,165 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5326ms
2014-07-14 01:25:40,165 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5295ms
2014-07-14 01:25:40,166 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5266ms
2014-07-14 01:25:40,166 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5236ms
2014-07-14 01:25:40,166 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5207ms
2014-07-14 01:25:40,166 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5178ms
2014-07-14 01:25:40,166 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5148ms
2014-07-14 01:25:40,167 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5118ms
2014-07-14 01:25:40,167 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5089ms
2014-07-14 01:25:40,167 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5060ms
2014-07-14 01:25:40,167 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5443ms
2014-07-14 01:25:40,167 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5431ms
2014-07-14 01:25:40,168 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5418ms
2014-07-14 01:25:40,168 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:25:40,199 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:25:40,228 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:25:40,259 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:25:43,355 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:25:43,355 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:25:43,357 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:25:43,358 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:25:43,359 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:25:43,360 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:25:43,360 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:25:43,362 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:25:43,372 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:25:43,401 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:25:43,442 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:25:43,475 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:25:43,476 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:25:43,477 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:25:43,478 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:25:43,479 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:25:43,480 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:25:43,480 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:25:43,480 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:25:43,481 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:25:43,481 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:25:43,482 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:25:43,487 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:25:43,518 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:25:43,559 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:25:43,562 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:25:43,562 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:25:43,562 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:25:43,599 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:25:43,637 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:25:44,512 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10378, memsize=430.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/4816b678320946628ed897d109804c4e
2014-07-14 01:25:44,525 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/4816b678320946628ed897d109804c4e as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/4816b678320946628ed897d109804c4e
2014-07-14 01:25:44,540 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/4816b678320946628ed897d109804c4e, entries=1568000, sequenceid=10378, filesize=111.7m
2014-07-14 01:25:44,541 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.3g/1378379120, currentsize=339.9m/356425760 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 28577ms, sequenceid=10378, compaction requested=true
2014-07-14 01:25:44,541 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:25:44,541 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 28 store files, 0 compacting, 28 eligible, 20 blocking
2014-07-14 01:25:44,541 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10905ms
2014-07-14 01:25:44,542 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,542 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 28 files from compaction candidates
2014-07-14 01:25:44,542 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10944ms
2014-07-14 01:25:44,542 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:25:44,542 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,542 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:25:44,542 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:25:44,545 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10984ms
2014-07-14 01:25:44,545 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,545 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10984ms
2014-07-14 01:25:44,545 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,545 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10984ms
2014-07-14 01:25:44,545 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,545 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10987ms
2014-07-14 01:25:44,545 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,545 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11028ms
2014-07-14 01:25:44,545 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,549 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11062ms
2014-07-14 01:25:44,549 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,549 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11068ms
2014-07-14 01:25:44,549 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,549 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11068ms
2014-07-14 01:25:44,549 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,549 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11069ms
2014-07-14 01:25:44,549 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,552 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11072ms
2014-07-14 01:25:44,552 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,552 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11072ms
2014-07-14 01:25:44,552 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,552 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11073ms
2014-07-14 01:25:44,552 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,552 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11074ms
2014-07-14 01:25:44,553 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,558 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11080ms
2014-07-14 01:25:44,558 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,558 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11082ms
2014-07-14 01:25:44,559 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,559 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11084ms
2014-07-14 01:25:44,559 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,559 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11085ms
2014-07-14 01:25:44,559 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,559 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11118ms
2014-07-14 01:25:44,559 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,559 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11159ms
2014-07-14 01:25:44,559 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,567 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11196ms
2014-07-14 01:25:44,567 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,567 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11206ms
2014-07-14 01:25:44,567 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,569 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11209ms
2014-07-14 01:25:44,569 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,569 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11210ms
2014-07-14 01:25:44,569 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,569 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11211ms
2014-07-14 01:25:44,569 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,569 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11211ms
2014-07-14 01:25:44,569 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,577 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11221ms
2014-07-14 01:25:44,577 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,577 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11223ms
2014-07-14 01:25:44,577 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,577 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11224ms
2014-07-14 01:25:44,577 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,577 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9318ms
2014-07-14 01:25:44,577 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,577 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9349ms
2014-07-14 01:25:44,577 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,577 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9379ms
2014-07-14 01:25:44,578 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,584 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9416ms
2014-07-14 01:25:44,584 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,584 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9834ms
2014-07-14 01:25:44,584 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,584 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9848ms
2014-07-14 01:25:44,584 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,584 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9860ms
2014-07-14 01:25:44,584 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,591 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9484ms
2014-07-14 01:25:44,591 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,591 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9513ms
2014-07-14 01:25:44,591 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,591 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9543ms
2014-07-14 01:25:44,592 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,592 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9574ms
2014-07-14 01:25:44,592 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,594 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9606ms
2014-07-14 01:25:44,594 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,594 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9635ms
2014-07-14 01:25:44,594 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,596 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9666ms
2014-07-14 01:25:44,597 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,597 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9697ms
2014-07-14 01:25:44,597 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,598 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9728ms
2014-07-14 01:25:44,598 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,598 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9759ms
2014-07-14 01:25:44,598 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,598 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9787ms
2014-07-14 01:25:44,599 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,605 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9824ms
2014-07-14 01:25:44,605 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:44,606 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9469ms
2014-07-14 01:25:44,606 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:25:45,935 DEBUG [RpcServer.handler=38,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:25:45,935 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files; delaying flush up to 90000ms
2014-07-14 01:25:45,935 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:25:45,936 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 28 store files, 0 compacting, 28 eligible, 20 blocking
2014-07-14 01:25:45,936 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 28 files from compaction candidates
2014-07-14 01:25:45,936 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:25:45,936 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:25:45,936 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:25:46,056 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:25:46,069 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42344 synced till here 42322
2014-07-14 01:25:46,231 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13661,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326332570,"queuetimems":0,"class":"HRegionServer","responsesize":15679,"method":"Multi"}
2014-07-14 01:25:46,231 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14862,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326331369,"queuetimems":1,"class":"HRegionServer","responsesize":15600,"method":"Multi"}
2014-07-14 01:25:46,250 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13755,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326332494,"queuetimems":1,"class":"HRegionServer","responsesize":16020,"method":"Multi"}
2014-07-14 01:25:46,250 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14819,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326331431,"queuetimems":1,"class":"HRegionServer","responsesize":15570,"method":"Multi"}
2014-07-14 01:25:46,250 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13322,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326332927,"queuetimems":0,"class":"HRegionServer","responsesize":15918,"method":"Multi"}
2014-07-14 01:25:46,250 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14916,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326331333,"queuetimems":1,"class":"HRegionServer","responsesize":15629,"method":"Multi"}
2014-07-14 01:25:46,250 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13771,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326332478,"queuetimems":0,"class":"HRegionServer","responsesize":15728,"method":"Multi"}
2014-07-14 01:25:46,258 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14957,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326331300,"queuetimems":0,"class":"HRegionServer","responsesize":16115,"method":"Multi"}
2014-07-14 01:25:46,309 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326332684 with entries=106, filesize=90.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326346057
2014-07-14 01:25:46,309 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326227576
2014-07-14 01:25:46,309 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326229546
2014-07-14 01:25:46,309 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326232343
2014-07-14 01:25:46,309 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326234292
2014-07-14 01:25:46,310 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326238132
2014-07-14 01:25:46,310 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326239440
2014-07-14 01:25:46,310 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326241996
2014-07-14 01:25:46,310 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326243501
2014-07-14 01:25:46,310 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326245444
2014-07-14 01:25:46,310 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326246971
2014-07-14 01:25:46,310 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326248744
2014-07-14 01:25:46,310 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326252151
2014-07-14 01:25:46,310 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326254453
2014-07-14 01:25:46,310 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326256461
2014-07-14 01:25:46,310 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326257556
2014-07-14 01:25:46,310 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326261577
2014-07-14 01:25:46,310 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326262405
2014-07-14 01:25:46,310 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326264121
2014-07-14 01:25:46,626 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13872,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326332753,"queuetimems":15,"class":"HRegionServer","responsesize":15887,"method":"Multi"}
2014-07-14 01:25:46,747 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13924,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326332823,"queuetimems":0,"class":"HRegionServer","responsesize":15908,"method":"Multi"}
2014-07-14 01:25:47,005 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14379,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326332626,"queuetimems":0,"class":"HRegionServer","responsesize":15812,"method":"Multi"}
2014-07-14 01:25:47,006 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14333,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326332672,"queuetimems":1,"class":"HRegionServer","responsesize":15707,"method":"Multi"}
2014-07-14 01:25:47,018 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14483,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326332534,"queuetimems":1,"class":"HRegionServer","responsesize":15672,"method":"Multi"}
2014-07-14 01:25:47,018 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14143,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326332874,"queuetimems":1,"class":"HRegionServer","responsesize":15878,"method":"Multi"}
2014-07-14 01:25:47,018 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13717,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326333300,"queuetimems":0,"class":"HRegionServer","responsesize":15910,"method":"Multi"}
2014-07-14 01:25:47,025 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13940,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326333085,"queuetimems":0,"class":"HRegionServer","responsesize":15458,"method":"Multi"}
2014-07-14 01:25:47,029 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13850,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326333179,"queuetimems":1,"class":"HRegionServer","responsesize":15910,"method":"Multi"}
2014-07-14 01:25:47,033 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13806,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326333227,"queuetimems":0,"class":"HRegionServer","responsesize":15597,"method":"Multi"}
2014-07-14 01:25:47,033 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13699,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326333334,"queuetimems":0,"class":"HRegionServer","responsesize":15519,"method":"Multi"}
2014-07-14 01:25:47,123 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14089,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326333033,"queuetimems":1,"class":"HRegionServer","responsesize":15891,"method":"Multi"}
2014-07-14 01:25:47,130 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13999,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326333130,"queuetimems":0,"class":"HRegionServer","responsesize":15707,"method":"Multi"}
2014-07-14 01:25:47,133 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13501,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326333632,"queuetimems":0,"class":"HRegionServer","responsesize":15887,"method":"Multi"}
2014-07-14 01:25:47,139 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14154,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326332985,"queuetimems":1,"class":"HRegionServer","responsesize":15837,"method":"Multi"}
2014-07-14 01:25:47,178 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:25:47,987 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42428 synced till here 42421
2014-07-14 01:25:48,083 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326346057 with entries=84, filesize=72.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326347179
2014-07-14 01:25:48,306 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13558,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326334748,"queuetimems":0,"class":"HRegionServer","responsesize":16060,"method":"Multi"}
2014-07-14 01:25:48,469 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14874,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326333595,"queuetimems":0,"class":"HRegionServer","responsesize":15878,"method":"Multi"}
2014-07-14 01:25:48,469 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13632,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326334837,"queuetimems":0,"class":"HRegionServer","responsesize":15590,"method":"Multi"}
2014-07-14 01:25:48,561 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14898,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326333663,"queuetimems":0,"class":"HRegionServer","responsesize":15908,"method":"Multi"}
2014-07-14 01:25:48,790 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:25:48,894 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42532 synced till here 42502
2014-07-14 01:25:49,034 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13867,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326335166,"queuetimems":0,"class":"HRegionServer","responsesize":15838,"method":"Multi"}
2014-07-14 01:25:49,034 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13957,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326335076,"queuetimems":0,"class":"HRegionServer","responsesize":16115,"method":"Multi"}
2014-07-14 01:25:49,034 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14048,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326334986,"queuetimems":0,"class":"HRegionServer","responsesize":15710,"method":"Multi"}
2014-07-14 01:25:49,035 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14167,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326334867,"queuetimems":0,"class":"HRegionServer","responsesize":15597,"method":"Multi"}
2014-07-14 01:25:49,034 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13837,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326335196,"queuetimems":0,"class":"HRegionServer","responsesize":16240,"method":"Multi"}
2014-07-14 01:25:49,041 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15527,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326333514,"queuetimems":0,"class":"HRegionServer","responsesize":15672,"method":"Multi"}
2014-07-14 01:25:49,041 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13815,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326335226,"queuetimems":0,"class":"HRegionServer","responsesize":15629,"method":"Multi"}
2014-07-14 01:25:49,034 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13988,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326335046,"queuetimems":0,"class":"HRegionServer","responsesize":15600,"method":"Multi"}
2014-07-14 01:25:49,045 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14311,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326334734,"queuetimems":0,"class":"HRegionServer","responsesize":16240,"method":"Multi"}
2014-07-14 01:25:49,045 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14088,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326334957,"queuetimems":0,"class":"HRegionServer","responsesize":15429,"method":"Multi"}
2014-07-14 01:25:49,045 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14117,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326334928,"queuetimems":0,"class":"HRegionServer","responsesize":15519,"method":"Multi"}
2014-07-14 01:25:49,045 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14236,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326334809,"queuetimems":1,"class":"HRegionServer","responsesize":15670,"method":"Multi"}
2014-07-14 01:25:49,045 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13940,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326335105,"queuetimems":0,"class":"HRegionServer","responsesize":15793,"method":"Multi"}
2014-07-14 01:25:49,046 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13911,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326335135,"queuetimems":0,"class":"HRegionServer","responsesize":15624,"method":"Multi"}
2014-07-14 01:25:49,046 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14148,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326334898,"queuetimems":0,"class":"HRegionServer","responsesize":15910,"method":"Multi"}
2014-07-14 01:25:49,045 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15647,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326333398,"queuetimems":1,"class":"HRegionServer","responsesize":15570,"method":"Multi"}
2014-07-14 01:25:49,045 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14029,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326335016,"queuetimems":0,"class":"HRegionServer","responsesize":15769,"method":"Multi"}
2014-07-14 01:25:49,047 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15491,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326333556,"queuetimems":1,"class":"HRegionServer","responsesize":15679,"method":"Multi"}
2014-07-14 01:25:49,070 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13785,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326335257,"queuetimems":0,"class":"HRegionServer","responsesize":15812,"method":"Multi"}
2014-07-14 01:25:49,843 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326347179 with entries=104, filesize=87.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326348790
2014-07-14 01:25:50,051 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15271,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326334779,"queuetimems":0,"class":"HRegionServer","responsesize":15816,"method":"Multi"}
2014-07-14 01:25:50,056 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16688,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326333368,"queuetimems":1,"class":"HRegionServer","responsesize":15429,"method":"Multi"}
2014-07-14 01:25:50,057 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16618,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326333438,"queuetimems":0,"class":"HRegionServer","responsesize":15728,"method":"Multi"}
2014-07-14 01:25:50,063 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16580,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326333483,"queuetimems":1,"class":"HRegionServer","responsesize":16020,"method":"Multi"}
2014-07-14 01:25:50,730 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90764ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:25:50,730 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 699.2m
2014-07-14 01:25:51,127 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:25:51,240 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42652 synced till here 42629
2014-07-14 01:25:52,272 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326348790 with entries=120, filesize=102.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326351127
2014-07-14 01:25:52,820 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:25:53,304 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:25:53,375 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42759 synced till here 42744
2014-07-14 01:25:53,491 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326351127 with entries=107, filesize=88.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326353305
2014-07-14 01:25:54,843 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:25:55,227 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42879 synced till here 42875
2014-07-14 01:25:55,272 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326353305 with entries=120, filesize=99.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326354843
2014-07-14 01:25:55,320 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10595, memsize=398.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/729f83d2442f4f1d8895657c311d1073
2014-07-14 01:25:55,842 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/729f83d2442f4f1d8895657c311d1073 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/729f83d2442f4f1d8895657c311d1073
2014-07-14 01:25:55,880 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/729f83d2442f4f1d8895657c311d1073, entries=1452310, sequenceid=10595, filesize=103.5m
2014-07-14 01:25:55,880 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~980.6m/1028206960, currentsize=290.0m/304041680 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 25039ms, sequenceid=10595, compaction requested=true
2014-07-14 01:25:55,882 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:25:55,882 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 30 store files, 0 compacting, 30 eligible, 20 blocking
2014-07-14 01:25:55,882 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 30 files from compaction candidates
2014-07-14 01:25:55,882 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:25:55,883 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:25:55,883 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:25:56,743 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:25:56,744 DEBUG [RpcServer.handler=6,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:25:56,744 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files; delaying flush up to 90000ms
2014-07-14 01:25:56,744 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:25:56,744 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 30 store files, 0 compacting, 30 eligible, 20 blocking
2014-07-14 01:25:56,745 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 30 files from compaction candidates
2014-07-14 01:25:56,745 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:25:56,745 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:25:56,745 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:25:56,782 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326354843 with entries=93, filesize=78.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326356744
2014-07-14 01:25:56,782 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326266136
2014-07-14 01:25:56,782 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326268215
2014-07-14 01:25:56,782 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326270955
2014-07-14 01:25:56,783 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326273244
2014-07-14 01:25:56,783 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326275650
2014-07-14 01:25:56,783 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326278158
2014-07-14 01:26:03,067 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:26:03,298 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43047 synced till here 43046
2014-07-14 01:26:03,343 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326356744 with entries=75, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326363068
2014-07-14 01:26:04,934 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:26:04,952 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43122 synced till here 43121
2014-07-14 01:26:04,978 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326363068 with entries=75, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326364935
2014-07-14 01:26:06,346 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:26:06,574 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43207 synced till here 43206
2014-07-14 01:26:06,624 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326364935 with entries=85, filesize=73.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326366346
2014-07-14 01:26:06,624 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:26:07,885 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:26:07,910 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326366346 with entries=72, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326367886
2014-07-14 01:26:07,910 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:26:09,177 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10709, memsize=398.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/717da64014a34decb1d3784bcc2a4753
2014-07-14 01:26:09,190 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/717da64014a34decb1d3784bcc2a4753 as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/717da64014a34decb1d3784bcc2a4753
2014-07-14 01:26:09,200 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/717da64014a34decb1d3784bcc2a4753, entries=1449630, sequenceid=10709, filesize=103.3m
2014-07-14 01:26:09,201 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~766.3m/803535040, currentsize=225.4m/236372400 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 18470ms, sequenceid=10709, compaction requested=true
2014-07-14 01:26:09,201 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:26:09,201 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 30 store files, 0 compacting, 30 eligible, 20 blocking
2014-07-14 01:26:09,201 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 30 files from compaction candidates
2014-07-14 01:26:09,201 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:26:09,202 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:26:09,202 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:26:20,393 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:26:20,415 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326367886 with entries=71, filesize=60.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326380393
2014-07-14 01:26:20,415 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:26:21,556 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:26:21,556 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files; delaying flush up to 90000ms
2014-07-14 01:26:21,557 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 30 store files, 0 compacting, 30 eligible, 20 blocking
2014-07-14 01:26:21,557 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 30 files from compaction candidates
2014-07-14 01:26:21,557 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:26:21,557 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:26:21,557 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:26:21,557 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:26:21,993 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:26:22,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43424 synced till here 43422
2014-07-14 01:26:22,040 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326380393 with entries=74, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326381994
2014-07-14 01:26:22,041 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:26:23,352 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=10829, hits=3899, hitRatio=36.00%, , cachingAccesses=3903, cachingHits=3898, cachingHitsRatio=99.87%, evictions=0, evicted=3, evictedPerRun=Infinity
2014-07-14 01:26:23,532 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:26:23,559 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326381994 with entries=73, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326383532
2014-07-14 01:26:23,560 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:26:24,937 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:26:24,959 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43571 synced till here 43568
2014-07-14 01:26:25,003 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326383532 with entries=74, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326384937
2014-07-14 01:26:25,003 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:26:27,152 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:26:27,172 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43645 synced till here 43643
2014-07-14 01:26:28,088 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326384937 with entries=74, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326387153
2014-07-14 01:26:28,091 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:26:28,931 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:26:29,025 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43732 synced till here 43718
2014-07-14 01:26:29,073 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326387153 with entries=87, filesize=74.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326388932
2014-07-14 01:26:29,073 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:26:30,592 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:26:30,630 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43814 synced till here 43805
2014-07-14 01:26:30,744 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326388932 with entries=82, filesize=70.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326390593
2014-07-14 01:26:30,745 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:26:32,275 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:26:32,303 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43889 synced till here 43887
2014-07-14 01:26:32,345 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326390593 with entries=75, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326392275
2014-07-14 01:26:32,346 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:26:33,877 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:26:33,896 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43962 synced till here 43961
2014-07-14 01:26:33,913 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326392275 with entries=73, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326393878
2014-07-14 01:26:33,917 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:26:34,692 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:26:34,710 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44040 synced till here 44035
2014-07-14 01:26:34,781 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326393878 with entries=78, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326394692
2014-07-14 01:26:34,781 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=44, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:26:36,221 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:26:36,386 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44130 synced till here 44112
2014-07-14 01:26:36,475 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90038ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:26:36,475 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 1.5g
2014-07-14 01:26:36,517 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:26:36,517 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files, but is 1.0g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:26:36,517 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. due to global heap pressure
2014-07-14 01:26:36,518 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 1.0g
2014-07-14 01:26:36,531 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326394692 with entries=90, filesize=77.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326396371
2014-07-14 01:26:36,531 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=45, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:26:38,400 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:26:38,519 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:26:39,422 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44244 synced till here 44230
2014-07-14 01:26:39,553 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326396371 with entries=114, filesize=97.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326398400
2014-07-14 01:26:39,674 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:39,674 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:39,675 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:39,676 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:39,676 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:39,677 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:39,678 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:39,680 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:39,686 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:39,726 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:39,756 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:39,757 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:39,757 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:39,757 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:39,783 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:39,809 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:39,818 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:39,819 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:39,819 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:39,820 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:39,824 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:39,826 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:39,827 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:39,828 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:39,831 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:39,849 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:26:39,856 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:39,888 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:39,936 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:39,977 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:40,022 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:40,054 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:40,101 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:40,135 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:40,174 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:40,213 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:40,254 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:40,290 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:40,332 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:40,379 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:40,423 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:40,454 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:40,493 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:40,534 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:40,578 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:40,615 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:40,651 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:40,694 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:40,731 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:40,777 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:40,815 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:26:44,675 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:26:44,676 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:26:44,676 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:26:44,676 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:26:44,677 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:26:44,678 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:26:44,678 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:26:44,680 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:26:44,686 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:26:44,726 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:26:44,757 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:26:44,757 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:26:44,757 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:26:44,757 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:26:44,784 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:26:44,809 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:26:44,819 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:26:44,819 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:26:44,819 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:26:44,821 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:26:44,824 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:26:44,827 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:26:44,827 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:26:44,828 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:26:44,831 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:26:44,856 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:26:44,888 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:26:44,937 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:26:44,978 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:26:45,022 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:26:45,054 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:26:45,102 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:26:45,135 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:26:45,175 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:26:45,213 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:26:45,254 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:26:45,291 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:26:45,332 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:26:45,379 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:26:45,423 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:26:45,455 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:26:45,494 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:26:45,535 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:26:45,579 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:26:45,616 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:26:45,652 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:26:45,695 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:26:45,731 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:26:45,777 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:26:45,815 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:26:49,675 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:49,676 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:26:49,676 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:49,677 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:49,677 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:49,678 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:49,678 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:49,680 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:26:49,686 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:49,727 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:49,757 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:49,757 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:49,758 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:49,758 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:26:49,784 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:49,810 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:49,819 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:49,819 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:26:49,820 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:49,822 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:26:49,824 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:26:49,828 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:49,828 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:49,828 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:49,831 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:26:49,856 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:49,888 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:49,937 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:49,978 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:50,023 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:50,055 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:26:50,102 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:50,135 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:26:50,175 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:50,213 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:50,255 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:26:50,291 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:50,335 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:26:50,379 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:26:50,424 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:26:50,455 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:50,494 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:50,535 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:50,580 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:50,616 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:50,652 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:50,695 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:50,732 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:50,778 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:50,816 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:26:54,676 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:26:54,676 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:26:54,677 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:26:54,677 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:26:54,677 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:26:54,679 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:26:54,679 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:26:54,681 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:26:54,687 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:26:54,727 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:26:54,758 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:26:54,758 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:26:54,758 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:26:54,759 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15003ms
2014-07-14 01:26:54,784 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:26:54,810 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:26:54,819 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:26:54,820 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:26:54,820 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:26:54,822 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:26:54,825 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:26:54,829 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:26:54,829 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:26:54,830 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:26:54,832 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 01:26:54,857 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:26:54,889 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:26:54,938 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:26:54,980 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:26:55,788 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15011ms
2014-07-14 01:26:55,788 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15409ms
2014-07-14 01:26:55,788 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15534ms
2014-07-14 01:26:55,789 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15735ms
2014-07-14 01:26:55,789 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15366ms
2014-07-14 01:26:55,790 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15335ms
2014-07-14 01:26:55,790 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15297ms
2014-07-14 01:26:55,790 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15256ms
2014-07-14 01:26:55,790 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15212ms
2014-07-14 01:26:55,790 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15175ms
2014-07-14 01:26:55,790 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15139ms
2014-07-14 01:26:55,791 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15096ms
2014-07-14 01:26:55,791 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15060ms
2014-07-14 01:26:55,791 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15769ms
2014-07-14 01:26:55,791 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15690ms
2014-07-14 01:26:55,791 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15656ms
2014-07-14 01:26:55,792 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15617ms
2014-07-14 01:26:55,792 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15580ms
2014-07-14 01:26:55,792 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15502ms
2014-07-14 01:26:55,792 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15460ms
2014-07-14 01:26:55,816 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:26:59,677 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:26:59,678 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:26:59,678 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:26:59,678 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20003ms
2014-07-14 01:26:59,678 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20004ms
2014-07-14 01:26:59,679 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:26:59,679 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:26:59,681 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:26:59,687 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:26:59,727 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:26:59,758 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:26:59,758 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:26:59,759 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20003ms
2014-07-14 01:26:59,759 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20003ms
2014-07-14 01:26:59,784 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:26:59,810 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:26:59,820 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:26:59,820 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:26:59,820 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:26:59,822 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:26:59,826 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:26:59,829 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20003ms
2014-07-14 01:26:59,829 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:26:59,830 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20003ms
2014-07-14 01:26:59,832 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:26:59,857 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:26:59,889 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:26:59,938 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:26:59,980 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20003ms
2014-07-14 01:27:00,789 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20011ms
2014-07-14 01:27:00,789 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20535ms
2014-07-14 01:27:00,789 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20410ms
2014-07-14 01:27:00,789 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20366ms
2014-07-14 01:27:00,789 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20735ms
2014-07-14 01:27:00,790 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20336ms
2014-07-14 01:27:00,790 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20297ms
2014-07-14 01:27:00,790 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20256ms
2014-07-14 01:27:00,791 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20212ms
2014-07-14 01:27:00,791 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20097ms
2014-07-14 01:27:00,791 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20060ms
2014-07-14 01:27:00,791 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20140ms
2014-07-14 01:27:00,791 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20176ms
2014-07-14 01:27:00,791 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20690ms
2014-07-14 01:27:00,791 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20769ms
2014-07-14 01:27:00,792 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20657ms
2014-07-14 01:27:00,792 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20618ms
2014-07-14 01:27:00,792 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20580ms
2014-07-14 01:27:00,793 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20502ms
2014-07-14 01:27:00,793 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20461ms
2014-07-14 01:27:00,816 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:27:03,113 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11060, memsize=582.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/ac5a0f70cb4a4f698e5c58a909d889a6
2014-07-14 01:27:03,195 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/ac5a0f70cb4a4f698e5c58a909d889a6 as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/ac5a0f70cb4a4f698e5c58a909d889a6
2014-07-14 01:27:03,210 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/ac5a0f70cb4a4f698e5c58a909d889a6, entries=2120210, sequenceid=11060, filesize=151.0m
2014-07-14 01:27:03,210 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.0g/1105198080, currentsize=49.3m/51656640 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 26692ms, sequenceid=11060, compaction requested=true
2014-07-14 01:27:03,211 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:27:03,211 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 20 blocking
2014-07-14 01:27:03,211 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files; delaying flush up to 90000ms
2014-07-14 01:27:03,211 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22396ms
2014-07-14 01:27:03,212 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-14 01:27:03,212 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-14 01:27:03,212 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,212 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 105704ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:27:03,212 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22880ms
2014-07-14 01:27:03,212 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:27:03,212 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,212 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 600.2m
2014-07-14 01:27:03,212 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:27:03,212 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22922ms
2014-07-14 01:27:03,212 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,212 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:27:03,212 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23000ms
2014-07-14 01:27:03,213 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 20 blocking
2014-07-14 01:27:03,213 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,213 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-14 01:27:03,213 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23039ms
2014-07-14 01:27:03,213 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,213 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:27:03,213 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:27:03,213 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23078ms
2014-07-14 01:27:03,213 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,213 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:27:03,217 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23195ms
2014-07-14 01:27:03,217 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,221 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23120ms
2014-07-14 01:27:03,221 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,221 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22606ms
2014-07-14 01:27:03,221 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,233 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22582ms
2014-07-14 01:27:03,233 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,233 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22502ms
2014-07-14 01:27:03,233 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,241 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22547ms
2014-07-14 01:27:03,241 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,246 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22668ms
2014-07-14 01:27:03,246 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,253 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22719ms
2014-07-14 01:27:03,253 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,253 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22760ms
2014-07-14 01:27:03,253 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,254 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22799ms
2014-07-14 01:27:03,254 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,254 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23200ms
2014-07-14 01:27:03,254 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,261 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22838ms
2014-07-14 01:27:03,261 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,261 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22882ms
2014-07-14 01:27:03,261 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,261 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23007ms
2014-07-14 01:27:03,261 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,269 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22492ms
2014-07-14 01:27:03,269 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,270 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23293ms
2014-07-14 01:27:03,270 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,270 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23334ms
2014-07-14 01:27:03,270 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,270 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23383ms
2014-07-14 01:27:03,270 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,270 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23415ms
2014-07-14 01:27:03,270 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,270 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23439ms
2014-07-14 01:27:03,270 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,280 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23453ms
2014-07-14 01:27:03,280 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,280 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23453ms
2014-07-14 01:27:03,280 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,280 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23454ms
2014-07-14 01:27:03,280 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,285 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23461ms
2014-07-14 01:27:03,285 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,285 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23465ms
2014-07-14 01:27:03,285 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,285 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23466ms
2014-07-14 01:27:03,285 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,285 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23466ms
2014-07-14 01:27:03,285 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,344 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25151,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326398192,"queuetimems":0,"class":"HRegionServer","responsesize":15980,"method":"Multi"}
2014-07-14 01:27:03,345 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23527ms
2014-07-14 01:27:03,345 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,345 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23536ms
2014-07-14 01:27:03,345 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,349 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23566ms
2014-07-14 01:27:03,349 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,349 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23593ms
2014-07-14 01:27:03,349 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,349 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23593ms
2014-07-14 01:27:03,349 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,349 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23592ms
2014-07-14 01:27:03,349 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,357 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23601ms
2014-07-14 01:27:03,357 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,357 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23631ms
2014-07-14 01:27:03,357 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,358 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23673ms
2014-07-14 01:27:03,358 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,358 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23678ms
2014-07-14 01:27:03,358 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,358 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23681ms
2014-07-14 01:27:03,358 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,365 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23688ms
2014-07-14 01:27:03,366 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,368 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23693ms
2014-07-14 01:27:03,368 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,369 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23693ms
2014-07-14 01:27:03,369 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,370 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23693ms
2014-07-14 01:27:03,370 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,370 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23694ms
2014-07-14 01:27:03,370 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,371 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23697ms
2014-07-14 01:27:03,371 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:03,772 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:27:03,819 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:27:03,820 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25533,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326398287,"queuetimems":0,"class":"HRegionServer","responsesize":15708,"method":"Multi"}
2014-07-14 01:27:03,820 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25593,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326398227,"queuetimems":0,"class":"HRegionServer","responsesize":15325,"method":"Multi"}
2014-07-14 01:27:03,820 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25563,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326398257,"queuetimems":1,"class":"HRegionServer","responsesize":15941,"method":"Multi"}
2014-07-14 01:27:03,825 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25358,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326398467,"queuetimems":1,"class":"HRegionServer","responsesize":15742,"method":"Multi"}
2014-07-14 01:27:03,834 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25685,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326398148,"queuetimems":0,"class":"HRegionServer","responsesize":15821,"method":"Multi"}
2014-07-14 01:27:03,884 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44336 synced till here 44325
2014-07-14 01:27:04,002 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326398400 with entries=92, filesize=78.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326423819
2014-07-14 01:27:04,005 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25600,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326398405,"queuetimems":0,"class":"HRegionServer","responsesize":15693,"method":"Multi"}
2014-07-14 01:27:04,007 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25569,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326398437,"queuetimems":0,"class":"HRegionServer","responsesize":16074,"method":"Multi"}
2014-07-14 01:27:04,013 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25510,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326398502,"queuetimems":1,"class":"HRegionServer","responsesize":15706,"method":"Multi"}
2014-07-14 01:27:04,013 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25638,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326398375,"queuetimems":0,"class":"HRegionServer","responsesize":15575,"method":"Multi"}
2014-07-14 01:27:04,014 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25666,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326398347,"queuetimems":0,"class":"HRegionServer","responsesize":15733,"method":"Multi"}
2014-07-14 01:27:04,300 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24668,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326399632,"queuetimems":0,"class":"HRegionServer","responsesize":15765,"method":"Multi"}
2014-07-14 01:27:04,300 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25982,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326398318,"queuetimems":1,"class":"HRegionServer","responsesize":16187,"method":"Multi"}
2014-07-14 01:27:04,300 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24715,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326399585,"queuetimems":0,"class":"HRegionServer","responsesize":15822,"method":"Multi"}
2014-07-14 01:27:04,535 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25013,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326399521,"queuetimems":1,"class":"HRegionServer","responsesize":15647,"method":"Multi"}
2014-07-14 01:27:04,535 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25045,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326399490,"queuetimems":0,"class":"HRegionServer","responsesize":15808,"method":"Multi"}
2014-07-14 01:27:04,535 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25145,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326399389,"queuetimems":0,"class":"HRegionServer","responsesize":15919,"method":"Multi"}
2014-07-14 01:27:05,313 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25913,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326399400,"queuetimems":0,"class":"HRegionServer","responsesize":15679,"method":"Multi"}
2014-07-14 01:27:05,313 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25858,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326399455,"queuetimems":1,"class":"HRegionServer","responsesize":16065,"method":"Multi"}
2014-07-14 01:27:05,314 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24501,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326400812,"queuetimems":0,"class":"HRegionServer","responsesize":15325,"method":"Multi"}
2014-07-14 01:27:05,325 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26793,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326398532,"queuetimems":0,"class":"HRegionServer","responsesize":16199,"method":"Multi"}
2014-07-14 01:27:05,325 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25914,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326399411,"queuetimems":0,"class":"HRegionServer","responsesize":15608,"method":"Multi"}
2014-07-14 01:27:05,457 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:27:05,487 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44446 synced till here 44411
2014-07-14 01:27:05,690 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25716,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326399973,"queuetimems":0,"class":"HRegionServer","responsesize":15773,"method":"Multi"}
2014-07-14 01:27:05,717 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326423819 with entries=110, filesize=94.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326425457
2014-07-14 01:27:06,082 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25708,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326400374,"queuetimems":0,"class":"HRegionServer","responsesize":16199,"method":"Multi"}
2014-07-14 01:27:06,082 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25353,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326400729,"queuetimems":1,"class":"HRegionServer","responsesize":15808,"method":"Multi"}
2014-07-14 01:27:06,083 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25394,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326400689,"queuetimems":0,"class":"HRegionServer","responsesize":15710,"method":"Multi"}
2014-07-14 01:27:06,083 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25796,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326400287,"queuetimems":0,"class":"HRegionServer","responsesize":15742,"method":"Multi"}
2014-07-14 01:27:06,082 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25992,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326400090,"queuetimems":1,"class":"HRegionServer","responsesize":15575,"method":"Multi"}
2014-07-14 01:27:06,089 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25441,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326400647,"queuetimems":0,"class":"HRegionServer","responsesize":15647,"method":"Multi"}
2014-07-14 01:27:06,089 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26407,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326399682,"queuetimems":0,"class":"HRegionServer","responsesize":15743,"method":"Multi"}
2014-07-14 01:27:06,251 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26320,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326399930,"queuetimems":1,"class":"HRegionServer","responsesize":15773,"method":"Multi"}
2014-07-14 01:27:06,251 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25836,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326400414,"queuetimems":0,"class":"HRegionServer","responsesize":15679,"method":"Multi"}
2014-07-14 01:27:06,251 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26121,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326400130,"queuetimems":1,"class":"HRegionServer","responsesize":16187,"method":"Multi"}
2014-07-14 01:27:06,251 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25760,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326400491,"queuetimems":0,"class":"HRegionServer","responsesize":16065,"method":"Multi"}
2014-07-14 01:27:06,252 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26041,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326400210,"queuetimems":1,"class":"HRegionServer","responsesize":15733,"method":"Multi"}
2014-07-14 01:27:06,252 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26366,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326399885,"queuetimems":1,"class":"HRegionServer","responsesize":15850,"method":"Multi"}
2014-07-14 01:27:06,252 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26202,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326400050,"queuetimems":1,"class":"HRegionServer","responsesize":15693,"method":"Multi"}
2014-07-14 01:27:06,252 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25721,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326400531,"queuetimems":0,"class":"HRegionServer","responsesize":15821,"method":"Multi"}
2014-07-14 01:27:06,252 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26533,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326399719,"queuetimems":0,"class":"HRegionServer","responsesize":15838,"method":"Multi"}
2014-07-14 01:27:06,252 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25682,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326400570,"queuetimems":0,"class":"HRegionServer","responsesize":15608,"method":"Multi"}
2014-07-14 01:27:06,265 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26015,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326400249,"queuetimems":1,"class":"HRegionServer","responsesize":15941,"method":"Multi"}
2014-07-14 01:27:06,269 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26498,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326399771,"queuetimems":0,"class":"HRegionServer","responsesize":15623,"method":"Multi"}
2014-07-14 01:27:06,270 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26420,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326399850,"queuetimems":0,"class":"HRegionServer","responsesize":15394,"method":"Multi"}
2014-07-14 01:27:06,270 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26465,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326399805,"queuetimems":0,"class":"HRegionServer","responsesize":16054,"method":"Multi"}
2014-07-14 01:27:06,271 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26100,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326400171,"queuetimems":1,"class":"HRegionServer","responsesize":15708,"method":"Multi"}
2014-07-14 01:27:06,271 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25944,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326400327,"queuetimems":0,"class":"HRegionServer","responsesize":15919,"method":"Multi"}
2014-07-14 01:27:06,272 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25665,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326400607,"queuetimems":0,"class":"HRegionServer","responsesize":16074,"method":"Multi"}
2014-07-14 01:27:06,272 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26258,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326400014,"queuetimems":1,"class":"HRegionServer","responsesize":15729,"method":"Multi"}
2014-07-14 01:27:06,270 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25819,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326400451,"queuetimems":1,"class":"HRegionServer","responsesize":15706,"method":"Multi"}
2014-07-14 01:27:06,281 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25498,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47139","starttimems":1405326400773,"queuetimems":0,"class":"HRegionServer","responsesize":15980,"method":"Multi"}
2014-07-14 01:27:06,863 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:27:06,941 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44563 synced till here 44524
2014-07-14 01:27:07,179 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326425457 with entries=117, filesize=100.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326426863
2014-07-14 01:27:08,756 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:27:09,098 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44724 synced till here 44694
2014-07-14 01:27:09,336 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326426863 with entries=161, filesize=138.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326428756
2014-07-14 01:27:10,651 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:27:10,757 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44827 synced till here 44826
2014-07-14 01:27:10,792 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326428756 with entries=103, filesize=86.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326430651
2014-07-14 01:27:11,225 DEBUG [RpcServer.handler=37,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:27:12,047 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:27:12,393 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44952 synced till here 44937
2014-07-14 01:27:12,442 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:12,443 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:12,443 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:12,445 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:12,446 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:12,447 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:12,449 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:12,449 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:12,449 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:12,452 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:12,464 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:12,465 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:12,470 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326430651 with entries=125, filesize=102.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326432048
2014-07-14 01:27:12,493 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:12,608 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:12,609 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:12,609 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:12,609 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:12,609 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:12,609 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:12,610 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:12,610 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:12,610 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:12,610 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:12,612 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:12,612 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:12,613 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:12,613 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:12,613 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:12,613 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:13,398 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:13,430 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:13,459 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:13,490 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:13,520 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:13,550 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:27:14,035 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11053, memsize=740.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/467fd1e4aa984a07a894d6c39ffb2413
2014-07-14 01:27:14,047 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/467fd1e4aa984a07a894d6c39ffb2413 as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/467fd1e4aa984a07a894d6c39ffb2413
2014-07-14 01:27:14,056 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/467fd1e4aa984a07a894d6c39ffb2413, entries=2696610, sequenceid=11053, filesize=192.1m
2014-07-14 01:27:14,056 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.5g/1596953760, currentsize=319.2m/334752800 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 37581ms, sequenceid=11053, compaction requested=true
2014-07-14 01:27:14,057 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:27:14,057 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 507ms
2014-07-14 01:27:14,057 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 28 store files, 0 compacting, 28 eligible, 20 blocking
2014-07-14 01:27:14,057 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,057 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 28 files from compaction candidates
2014-07-14 01:27:14,057 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 537ms
2014-07-14 01:27:14,057 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files; delaying flush up to 90000ms
2014-07-14 01:27:14,057 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:27:14,057 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-14 01:27:14,057 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,057 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:27:14,058 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 568ms
2014-07-14 01:27:14,058 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,058 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:27:14,058 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 599ms
2014-07-14 01:27:14,058 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,058 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 20 blocking
2014-07-14 01:27:14,058 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 629ms
2014-07-14 01:27:14,058 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,058 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-14 01:27:14,058 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 660ms
2014-07-14 01:27:14,058 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,058 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:27:14,061 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:27:14,061 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:27:14,061 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1448ms
2014-07-14 01:27:14,061 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,061 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1448ms
2014-07-14 01:27:14,061 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,062 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1449ms
2014-07-14 01:27:14,062 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,065 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1453ms
2014-07-14 01:27:14,065 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,065 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1453ms
2014-07-14 01:27:14,065 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,073 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1461ms
2014-07-14 01:27:14,073 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,073 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1463ms
2014-07-14 01:27:14,073 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,073 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1463ms
2014-07-14 01:27:14,073 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,078 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1468ms
2014-07-14 01:27:14,078 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,078 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1468ms
2014-07-14 01:27:14,078 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,078 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1469ms
2014-07-14 01:27:14,079 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,079 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1470ms
2014-07-14 01:27:14,079 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,079 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1470ms
2014-07-14 01:27:14,079 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,079 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1470ms
2014-07-14 01:27:14,079 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,079 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1471ms
2014-07-14 01:27:14,079 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,079 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1471ms
2014-07-14 01:27:14,079 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,079 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1586ms
2014-07-14 01:27:14,080 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,080 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1615ms
2014-07-14 01:27:14,080 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,080 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1616ms
2014-07-14 01:27:14,080 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,089 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1637ms
2014-07-14 01:27:14,089 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,097 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1648ms
2014-07-14 01:27:14,097 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,097 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1648ms
2014-07-14 01:27:14,097 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,097 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1648ms
2014-07-14 01:27:14,097 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,097 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1650ms
2014-07-14 01:27:14,097 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,101 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1655ms
2014-07-14 01:27:14,101 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,101 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1656ms
2014-07-14 01:27:14,101 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,101 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1658ms
2014-07-14 01:27:14,101 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,101 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1658ms
2014-07-14 01:27:14,101 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:14,109 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1667ms
2014-07-14 01:27:14,109 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:27:15,225 DEBUG [RpcServer.handler=6,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:27:15,247 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:27:15,411 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45053 synced till here 45032
2014-07-14 01:27:15,552 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326432048 with entries=101, filesize=86.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326435247
2014-07-14 01:27:15,552 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326280418
2014-07-14 01:27:15,552 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326282833
2014-07-14 01:27:15,552 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326285373
2014-07-14 01:27:15,552 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326294574
2014-07-14 01:27:15,552 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326296450
2014-07-14 01:27:15,552 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326298188
2014-07-14 01:27:15,552 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326300012
2014-07-14 01:27:15,552 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326302062
2014-07-14 01:27:15,552 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326308091
2014-07-14 01:27:15,552 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326309085
2014-07-14 01:27:15,552 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326310791
2014-07-14 01:27:15,552 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326312320
2014-07-14 01:27:15,553 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326313666
2014-07-14 01:27:15,553 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326315413
2014-07-14 01:27:15,553 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326316635
2014-07-14 01:27:15,553 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326318407
2014-07-14 01:27:15,553 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326319932
2014-07-14 01:27:15,553 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326321703
2014-07-14 01:27:15,553 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326323207
2014-07-14 01:27:15,553 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326325358
2014-07-14 01:27:15,553 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326327193
2014-07-14 01:27:15,553 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326328891
2014-07-14 01:27:17,140 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:27:17,158 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45129 synced till here 45125
2014-07-14 01:27:17,194 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326435247 with entries=76, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326437140
2014-07-14 01:27:19,675 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:27:19,705 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326437140 with entries=72, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326439675
2014-07-14 01:27:19,706 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:27:20,713 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:27:21,353 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45295 synced till here 45294
2014-07-14 01:27:21,767 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326439675 with entries=94, filesize=80.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326440713
2014-07-14 01:27:21,768 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:27:22,621 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:27:22,915 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326440713 with entries=87, filesize=74.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326442621
2014-07-14 01:27:22,915 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:27:23,416 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11097, memsize=415.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/b6d39a502007401fa8a99b7a92cfd3ac
2014-07-14 01:27:23,434 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/b6d39a502007401fa8a99b7a92cfd3ac as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/b6d39a502007401fa8a99b7a92cfd3ac
2014-07-14 01:27:23,450 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/b6d39a502007401fa8a99b7a92cfd3ac, entries=1511400, sequenceid=11097, filesize=107.7m
2014-07-14 01:27:23,451 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~600.2m/629344000, currentsize=432.1m/453073840 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 20239ms, sequenceid=11097, compaction requested=true
2014-07-14 01:27:23,451 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:27:23,451 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 20 blocking
2014-07-14 01:27:23,452 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-14 01:27:23,452 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:27:23,452 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:27:23,452 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:27:23,541 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:27:23,541 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files; delaying flush up to 90000ms
2014-07-14 01:27:23,542 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:27:23,542 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 20 blocking
2014-07-14 01:27:23,542 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-14 01:27:23,542 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:27:23,542 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:27:23,542 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:27:25,158 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:27:25,175 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45454 synced till here 45453
2014-07-14 01:27:25,187 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326442621 with entries=72, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326445158
2014-07-14 01:27:25,188 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:27:26,865 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90121ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:27:26,865 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 1.3g
2014-07-14 01:27:26,927 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:27:27,293 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45562 synced till here 45554
2014-07-14 01:27:27,365 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326445158 with entries=108, filesize=92.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326446927
2014-07-14 01:27:28,339 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:27:28,361 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45637 synced till here 45633
2014-07-14 01:27:28,388 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326446927 with entries=75, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326448340
2014-07-14 01:27:28,418 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:27:29,978 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:27:30,018 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326448340 with entries=72, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326449979
2014-07-14 01:27:36,591 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:27:36,641 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326449979 with entries=72, filesize=61.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326456592
2014-07-14 01:27:39,127 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:27:39,161 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45857 synced till here 45855
2014-07-14 01:27:39,213 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326456592 with entries=76, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326459127
2014-07-14 01:27:41,584 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:27:41,600 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45931 synced till here 45930
2014-07-14 01:27:41,614 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326459127 with entries=74, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326461585
2014-07-14 01:27:48,912 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:27:48,951 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326461585 with entries=71, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326468912
2014-07-14 01:27:50,241 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11432, memsize=556.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/4b1524c87b1e4fd3b92a5ef88e534134
2014-07-14 01:27:50,257 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/4b1524c87b1e4fd3b92a5ef88e534134 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/4b1524c87b1e4fd3b92a5ef88e534134
2014-07-14 01:27:50,268 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/4b1524c87b1e4fd3b92a5ef88e534134, entries=2026720, sequenceid=11432, filesize=144.4m
2014-07-14 01:27:50,270 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.3g/1358102160, currentsize=189.5m/198695120 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 23405ms, sequenceid=11432, compaction requested=true
2014-07-14 01:27:50,271 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:27:50,271 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 20 blocking
2014-07-14 01:27:50,271 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-14 01:27:50,271 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:27:50,271 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:27:50,271 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:27:52,430 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90874ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:27:52,430 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 686.4m
2014-07-14 01:27:52,833 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:28:05,139 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11542, memsize=375.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/095ca09c2a8749e59c31449cb7839e67
2014-07-14 01:28:05,349 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/095ca09c2a8749e59c31449cb7839e67 as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/095ca09c2a8749e59c31449cb7839e67
2014-07-14 01:28:05,357 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/095ca09c2a8749e59c31449cb7839e67, entries=1367760, sequenceid=11542, filesize=97.5m
2014-07-14 01:28:05,357 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~686.4m/719753200, currentsize=3.2m/3335040 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 12927ms, sequenceid=11542, compaction requested=true
2014-07-14 01:28:05,358 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:28:05,358 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 32 store files, 0 compacting, 32 eligible, 20 blocking
2014-07-14 01:28:05,358 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 32 files from compaction candidates
2014-07-14 01:28:05,358 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:28:05,359 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:28:05,359 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:28:06,833 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90302ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:28:06,833 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 757.7m
2014-07-14 01:28:07,293 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:28:08,970 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:28:08,985 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46074 synced till here 46073
2014-07-14 01:28:08,994 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326468912 with entries=72, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326488971
2014-07-14 01:28:08,994 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326330729
2014-07-14 01:28:08,994 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326332684
2014-07-14 01:28:08,994 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326346057
2014-07-14 01:28:08,995 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326347179
2014-07-14 01:28:08,995 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326348790
2014-07-14 01:28:08,995 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326351127
2014-07-14 01:28:08,995 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326353305
2014-07-14 01:28:08,995 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326354843
2014-07-14 01:28:08,995 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326356744
2014-07-14 01:28:08,995 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326363068
2014-07-14 01:28:08,995 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326364935
2014-07-14 01:28:08,995 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326366346
2014-07-14 01:28:08,995 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326367886
2014-07-14 01:28:08,995 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326380393
2014-07-14 01:28:08,995 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326381994
2014-07-14 01:28:08,995 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326383532
2014-07-14 01:28:08,995 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326384937
2014-07-14 01:28:08,995 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326387153
2014-07-14 01:28:08,995 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326388932
2014-07-14 01:28:08,996 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326390593
2014-07-14 01:28:08,996 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326392275
2014-07-14 01:28:08,996 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326393878
2014-07-14 01:28:08,996 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326394692
2014-07-14 01:28:10,326 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:28:10,344 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46147 synced till here 46146
2014-07-14 01:28:10,364 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326488971 with entries=73, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326490326
2014-07-14 01:28:14,716 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:28:14,717 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files; delaying flush up to 90000ms
2014-07-14 01:28:14,717 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:28:14,717 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 20 blocking
2014-07-14 01:28:14,718 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-14 01:28:14,718 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:28:14,718 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:28:14,718 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:28:14,788 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:28:14,804 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46222 synced till here 46220
2014-07-14 01:28:14,847 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326490326 with entries=75, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326494789
2014-07-14 01:28:16,748 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:28:16,762 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46301 synced till here 46293
2014-07-14 01:28:16,855 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326494789 with entries=79, filesize=67.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326496748
2014-07-14 01:28:18,475 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:28:18,503 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326496748 with entries=71, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326498475
2014-07-14 01:28:21,771 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11545, memsize=385.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/d865be57c4764d57a5e660dfb5dd6ba0
2014-07-14 01:28:21,802 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/d865be57c4764d57a5e660dfb5dd6ba0 as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/d865be57c4764d57a5e660dfb5dd6ba0
2014-07-14 01:28:21,826 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/d865be57c4764d57a5e660dfb5dd6ba0, entries=1403620, sequenceid=11545, filesize=100.0m
2014-07-14 01:28:21,826 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~757.7m/794461360, currentsize=130.6m/136925920 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 14993ms, sequenceid=11545, compaction requested=true
2014-07-14 01:28:21,827 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:28:21,827 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 20 blocking
2014-07-14 01:28:21,827 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-14 01:28:21,828 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:28:21,828 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:28:21,828 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:28:24,873 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:28:24,910 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326498475 with entries=73, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326504873
2014-07-14 01:28:26,984 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:28:27,025 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326504873 with entries=72, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326506984
2014-07-14 01:28:28,394 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:28:28,456 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46589 synced till here 46588
2014-07-14 01:28:28,479 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326506984 with entries=72, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326508395
2014-07-14 01:28:30,988 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:28:31,019 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46662 synced till here 46661
2014-07-14 01:28:31,040 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326508395 with entries=73, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326510989
2014-07-14 01:28:32,963 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:28:32,963 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files; delaying flush up to 90000ms
2014-07-14 01:28:32,964 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:28:32,964 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 32 store files, 0 compacting, 32 eligible, 20 blocking
2014-07-14 01:28:32,964 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 32 files from compaction candidates
2014-07-14 01:28:32,964 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:28:32,964 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:28:32,964 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:28:33,241 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:28:33,256 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46737 synced till here 46734
2014-07-14 01:28:33,321 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326510989 with entries=75, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326513241
2014-07-14 01:28:33,427 DEBUG [RpcServer.handler=15,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:28:33,428 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files; delaying flush up to 90000ms
2014-07-14 01:28:33,428 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 20 blocking
2014-07-14 01:28:33,428 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-14 01:28:33,428 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:28:33,428 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:28:33,429 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:28:33,429 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:28:36,239 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:28:36,268 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46810 synced till here 46809
2014-07-14 01:28:36,297 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326513241 with entries=73, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326516240
2014-07-14 01:28:41,240 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90015ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:28:41,241 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 1.0g
2014-07-14 01:28:42,363 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:28:42,793 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:28:43,002 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326516240 with entries=73, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326522793
2014-07-14 01:28:44,132 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:28:44,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46960 synced till here 46957
2014-07-14 01:28:44,654 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326522793 with entries=77, filesize=66.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326524133
2014-07-14 01:28:45,605 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:28:45,635 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326524133 with entries=74, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326525606
2014-07-14 01:28:47,916 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:28:47,942 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47110 synced till here 47107
2014-07-14 01:28:48,016 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326525606 with entries=76, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326527917
2014-07-14 01:28:49,412 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:28:49,445 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326527917 with entries=73, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326529413
2014-07-14 01:28:50,788 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:28:50,809 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47258 synced till here 47257
2014-07-14 01:28:50,851 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326529413 with entries=75, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326530789
2014-07-14 01:28:51,777 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:28:51,801 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326530789 with entries=71, filesize=60.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326531777
2014-07-14 01:28:53,027 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:28:53,044 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47404 synced till here 47403
2014-07-14 01:28:53,071 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326531777 with entries=75, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326533027
2014-07-14 01:28:54,174 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90633ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:28:54,174 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 545.7m
2014-07-14 01:28:54,353 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:28:54,377 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47479 synced till here 47478
2014-07-14 01:28:54,407 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326533027 with entries=75, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326534354
2014-07-14 01:28:54,544 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:28:55,722 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:28:55,738 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47551 synced till here 47550
2014-07-14 01:28:55,753 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326534354 with entries=72, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326535722
2014-07-14 01:28:58,027 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:28:58,066 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326535722 with entries=73, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326538028
2014-07-14 01:28:59,275 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:28:59,302 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326538028 with entries=72, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326539276
2014-07-14 01:29:00,979 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:29:00,994 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47770 synced till here 47768
2014-07-14 01:29:01,034 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326539276 with entries=74, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326540980
2014-07-14 01:29:02,129 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:29:02,149 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47845 synced till here 47841
2014-07-14 01:29:03,298 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326540980 with entries=75, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326542129
2014-07-14 01:29:05,489 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:29:06,305 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47933 synced till here 47932
2014-07-14 01:29:06,344 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326542129 with entries=88, filesize=75.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326545490
2014-07-14 01:29:07,220 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:29:07,358 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326545490 with entries=77, filesize=66.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326547220
2014-07-14 01:29:08,788 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:08,788 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:08,788 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:08,791 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:08,835 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:08,836 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:08,854 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:08,905 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:08,972 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:09,030 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:09,085 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:09,142 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:09,183 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:09,242 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:09,290 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:09,334 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:09,381 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:09,432 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:09,475 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:09,515 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:09,560 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:09,644 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:09,700 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:09,762 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:09,801 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:09,869 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:09,914 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:09,950 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:09,984 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:10,025 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:10,055 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:10,098 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:11,165 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:11,196 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:11,226 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:11,255 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:11,284 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:11,314 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:11,346 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:11,377 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:11,408 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:11,439 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:11,474 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:11,503 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:11,532 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:11,563 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:11,593 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:11,623 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:11,653 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:11,684 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:13,687 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11743, memsize=677.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/65389252f1f349eba4073fbaf6799424
2014-07-14 01:29:13,708 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/65389252f1f349eba4073fbaf6799424 as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/65389252f1f349eba4073fbaf6799424
2014-07-14 01:29:13,727 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/65389252f1f349eba4073fbaf6799424, entries=2465550, sequenceid=11743, filesize=175.6m
2014-07-14 01:29:13,728 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.0g/1108540720, currentsize=469.9m/492693600 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 32487ms, sequenceid=11743, compaction requested=true
2014-07-14 01:29:13,728 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:29:13,728 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 30 store files, 0 compacting, 30 eligible, 20 blocking
2014-07-14 01:29:13,728 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 30 files from compaction candidates
2014-07-14 01:29:13,728 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2044ms
2014-07-14 01:29:13,729 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:29:13,729 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,729 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:29:13,729 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2076ms
2014-07-14 01:29:13,729 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,729 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:29:13,729 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2106ms
2014-07-14 01:29:13,729 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,729 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2136ms
2014-07-14 01:29:13,729 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,730 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2166ms
2014-07-14 01:29:13,730 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,730 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2198ms
2014-07-14 01:29:13,730 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,730 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2227ms
2014-07-14 01:29:13,730 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,730 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2256ms
2014-07-14 01:29:13,731 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,731 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2292ms
2014-07-14 01:29:13,731 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,731 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2323ms
2014-07-14 01:29:13,731 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,731 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2355ms
2014-07-14 01:29:13,731 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,733 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2387ms
2014-07-14 01:29:13,733 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,733 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2420ms
2014-07-14 01:29:13,733 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,734 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2450ms
2014-07-14 01:29:13,734 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,737 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2482ms
2014-07-14 01:29:13,737 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,737 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2511ms
2014-07-14 01:29:13,737 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,745 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2549ms
2014-07-14 01:29:13,745 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,745 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2580ms
2014-07-14 01:29:13,746 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,746 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3648ms
2014-07-14 01:29:13,746 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,752 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3697ms
2014-07-14 01:29:13,752 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,753 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3728ms
2014-07-14 01:29:13,753 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,753 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3769ms
2014-07-14 01:29:13,754 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,754 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3804ms
2014-07-14 01:29:13,755 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,755 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3841ms
2014-07-14 01:29:13,755 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,765 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3896ms
2014-07-14 01:29:13,765 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,765 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3964ms
2014-07-14 01:29:13,765 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,766 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4003ms
2014-07-14 01:29:13,766 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,766 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4067ms
2014-07-14 01:29:13,766 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,767 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4122ms
2014-07-14 01:29:13,767 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,767 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4207ms
2014-07-14 01:29:13,767 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,768 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4252ms
2014-07-14 01:29:13,768 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,768 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4293ms
2014-07-14 01:29:13,768 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,777 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4345ms
2014-07-14 01:29:13,777 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,778 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4396ms
2014-07-14 01:29:13,778 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,778 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4444ms
2014-07-14 01:29:13,778 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,785 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4495ms
2014-07-14 01:29:13,785 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,788 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:29:13,788 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,788 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4546ms
2014-07-14 01:29:13,788 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,789 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4606ms
2014-07-14 01:29:13,789 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,789 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4647ms
2014-07-14 01:29:13,789 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,791 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:29:13,791 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,794 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4708ms
2014-07-14 01:29:13,794 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,794 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4764ms
2014-07-14 01:29:13,794 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,805 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4833ms
2014-07-14 01:29:13,805 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,805 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4900ms
2014-07-14 01:29:13,805 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,805 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4951ms
2014-07-14 01:29:13,805 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,805 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4969ms
2014-07-14 01:29:13,805 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,806 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4971ms
2014-07-14 01:29:13,806 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,806 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5018ms
2014-07-14 01:29:13,806 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,817 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5029ms
2014-07-14 01:29:13,817 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:29:13,951 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:29:13,952 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:29:13,953 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files; delaying flush up to 90000ms
2014-07-14 01:29:13,953 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 30 store files, 0 compacting, 30 eligible, 20 blocking
2014-07-14 01:29:13,953 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 30 files from compaction candidates
2014-07-14 01:29:13,954 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:29:13,954 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:29:13,954 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:29:13,954 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:29:15,162 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48133 synced till here 48124
2014-07-14 01:29:15,293 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326547220 with entries=123, filesize=101.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326553951
2014-07-14 01:29:15,293 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326396371
2014-07-14 01:29:15,293 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326398400
2014-07-14 01:29:15,293 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326423819
2014-07-14 01:29:15,293 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326425457
2014-07-14 01:29:15,293 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326426863
2014-07-14 01:29:15,293 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326428756
2014-07-14 01:29:15,293 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326430651
2014-07-14 01:29:15,293 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326432048
2014-07-14 01:29:15,293 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326435247
2014-07-14 01:29:15,293 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326437140
2014-07-14 01:29:15,294 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326439675
2014-07-14 01:29:15,294 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326440713
2014-07-14 01:29:15,294 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326442621
2014-07-14 01:29:15,461 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:29:17,261 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:29:17,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48254 synced till here 48220
2014-07-14 01:29:17,646 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326553951 with entries=121, filesize=101.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326557261
2014-07-14 01:29:17,647 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:29:18,400 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11896, memsize=534.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/1656345618e04fb393cd83fb5a40b619
2014-07-14 01:29:18,444 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/1656345618e04fb393cd83fb5a40b619 as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/1656345618e04fb393cd83fb5a40b619
2014-07-14 01:29:18,470 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/1656345618e04fb393cd83fb5a40b619, entries=1947160, sequenceid=11896, filesize=138.7m
2014-07-14 01:29:18,471 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~548.8m/575467360, currentsize=324.4m/340181280 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 24297ms, sequenceid=11896, compaction requested=true
2014-07-14 01:29:18,472 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:29:18,472 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 33 store files, 0 compacting, 33 eligible, 20 blocking
2014-07-14 01:29:18,472 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 33 files from compaction candidates
2014-07-14 01:29:18,473 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:29:18,473 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:29:18,473 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:29:19,682 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:29:19,682 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files; delaying flush up to 90000ms
2014-07-14 01:29:19,683 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:29:19,683 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 33 store files, 0 compacting, 33 eligible, 20 blocking
2014-07-14 01:29:19,683 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10309,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326549374,"queuetimems":0,"class":"HRegionServer","responsesize":15903,"method":"Multi"}
2014-07-14 01:29:19,683 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 33 files from compaction candidates
2014-07-14 01:29:19,684 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:29:19,684 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:29:19,684 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:29:19,698 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10460,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326549237,"queuetimems":0,"class":"HRegionServer","responsesize":15303,"method":"Multi"}
2014-07-14 01:29:19,698 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10913,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326548784,"queuetimems":0,"class":"HRegionServer","responsesize":15717,"method":"Multi"}
2014-07-14 01:29:19,699 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10142,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326549556,"queuetimems":0,"class":"HRegionServer","responsesize":15667,"method":"Multi"}
2014-07-14 01:29:19,701 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10185,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326549511,"queuetimems":0,"class":"HRegionServer","responsesize":15447,"method":"Multi"}
2014-07-14 01:29:19,709 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10571,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326549138,"queuetimems":0,"class":"HRegionServer","responsesize":15887,"method":"Multi"}
2014-07-14 01:29:19,741 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10774,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326548967,"queuetimems":0,"class":"HRegionServer","responsesize":15783,"method":"Multi"}
2014-07-14 01:29:19,741 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10101,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326549640,"queuetimems":1,"class":"HRegionServer","responsesize":16026,"method":"Multi"}
2014-07-14 01:29:20,026 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:29:20,090 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48360 synced till here 48333
2014-07-14 01:29:20,302 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11400,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326548901,"queuetimems":0,"class":"HRegionServer","responsesize":15648,"method":"Multi"}
2014-07-14 01:29:20,412 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326557261 with entries=106, filesize=87.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326560026
2014-07-14 01:29:20,418 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:29:20,742 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11891,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326548851,"queuetimems":1,"class":"HRegionServer","responsesize":15868,"method":"Multi"}
2014-07-14 01:29:22,386 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1235ms
GC pool 'ParNew' had collection(s): count=1 time=1367ms
2014-07-14 01:29:22,886 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:29:22,983 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48461 synced till here 48450
2014-07-14 01:29:23,169 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326560026 with entries=101, filesize=84.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326562886
2014-07-14 01:29:23,169 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:29:25,564 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:29:25,619 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48571 synced till here 48546
2014-07-14 01:29:25,881 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326562886 with entries=110, filesize=94.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326565564
2014-07-14 01:29:25,881 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:29:27,862 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1269ms
GC pool 'ParNew' had collection(s): count=1 time=1565ms
2014-07-14 01:29:28,401 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:29:28,436 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48681 synced till here 48653
2014-07-14 01:29:28,934 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326565564 with entries=110, filesize=94.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326568402
2014-07-14 01:29:28,934 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:29:31,495 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:29:31,542 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326568402 with entries=112, filesize=94.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326571495
2014-07-14 01:29:31,543 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:29:53,013 WARN  [regionserver60020.compactionChecker] util.Sleeper: We slept 22414ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-14 01:29:53,013 WARN  [regionserver60020.periodicFlusher] util.Sleeper: We slept 22414ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-14 01:29:53,015 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 20416ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=20839ms
2014-07-14 01:29:53,015 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 98299ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:29:53,015 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 1.3g
2014-07-14 01:29:53,016 WARN  [regionserver60020] util.Sleeper: We slept 23797ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-14 01:29:53,228 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24814,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326568413,"queuetimems":2899,"class":"HRegionServer","responsesize":15447,"method":"Multi"}
2014-07-14 01:29:53,228 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24535,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326568693,"queuetimems":3110,"class":"HRegionServer","responsesize":15542,"method":"Multi"}
2014-07-14 01:29:53,229 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21259 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:53,228 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24531,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326568697,"queuetimems":2984,"class":"HRegionServer","responsesize":15783,"method":"Multi"}
2014-07-14 01:29:53,228 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24821,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326568406,"queuetimems":2953,"class":"HRegionServer","responsesize":15633,"method":"Multi"}
2014-07-14 01:29:53,228 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24490,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326568738,"queuetimems":2990,"class":"HRegionServer","responsesize":16026,"method":"Multi"}
2014-07-14 01:29:53,233 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24535,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326568693,"queuetimems":3149,"class":"HRegionServer","responsesize":15648,"method":"Multi"}
2014-07-14 01:29:53,230 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21283 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:53,234 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24483,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326568750,"queuetimems":2971,"class":"HRegionServer","responsesize":15629,"method":"Multi"}
2014-07-14 01:29:53,234 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21284 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:53,234 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:53,234 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21279 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:53,234 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:53,234 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21280 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:53,234 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:53,234 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21261 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:53,234 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:53,235 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21281 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:53,235 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:53,230 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:53,234 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:53,234 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24536,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326568697,"queuetimems":3043,"class":"HRegionServer","responsesize":15868,"method":"Multi"}
2014-07-14 01:29:53,237 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21282 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:53,237 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:53,445 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:29:53,448 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22779,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326570669,"queuetimems":4709,"class":"HRegionServer","responsesize":15813,"method":"Multi"}
2014-07-14 01:29:53,448 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24078,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326569370,"queuetimems":3479,"class":"HRegionServer","responsesize":15620,"method":"Multi"}
2014-07-14 01:29:53,449 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21275 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:53,449 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:53,449 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21276 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:53,450 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:53,497 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48900 synced till here 48879
2014-07-14 01:29:53,727 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23056,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326570671,"queuetimems":4570,"class":"HRegionServer","responsesize":15958,"method":"Multi"}
2014-07-14 01:29:53,727 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24364,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326569363,"queuetimems":3554,"class":"HRegionServer","responsesize":15972,"method":"Multi"}
2014-07-14 01:29:53,728 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21289 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:53,728 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:53,728 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21278 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:53,728 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:53,728 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23034,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326570694,"queuetimems":2698,"class":"HRegionServer","responsesize":15778,"method":"Multi"}
2014-07-14 01:29:53,727 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24362,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326569365,"queuetimems":3521,"class":"HRegionServer","responsesize":15877,"method":"Multi"}
2014-07-14 01:29:53,729 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21292 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:53,729 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:53,729 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21277 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:53,729 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:53,727 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23058,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326570669,"queuetimems":4663,"class":"HRegionServer","responsesize":15887,"method":"Multi"}
2014-07-14 01:29:53,729 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21274 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:53,729 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:53,733 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23016,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326570705,"queuetimems":2354,"class":"HRegionServer","responsesize":15853,"method":"Multi"}
2014-07-14 01:29:53,734 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21294 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:53,734 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:53,739 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326571495 with entries=107, filesize=90.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326593446
2014-07-14 01:29:53,888 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23216,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326570671,"queuetimems":4602,"class":"HRegionServer","responsesize":15903,"method":"Multi"}
2014-07-14 01:29:53,888 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21272 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:53,888 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,002 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23313,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326570689,"queuetimems":4493,"class":"HRegionServer","responsesize":15635,"method":"Multi"}
2014-07-14 01:29:54,003 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21286 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,003 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23322,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326570681,"queuetimems":4514,"class":"HRegionServer","responsesize":16007,"method":"Multi"}
2014-07-14 01:29:54,003 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21287 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,003 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,003 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,005 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23310,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326570694,"queuetimems":4469,"class":"HRegionServer","responsesize":15849,"method":"Multi"}
2014-07-14 01:29:54,005 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23335,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326570669,"queuetimems":4631,"class":"HRegionServer","responsesize":15877,"method":"Multi"}
2014-07-14 01:29:54,005 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22903,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326571101,"queuetimems":2549,"class":"HRegionServer","responsesize":15981,"method":"Multi"}
2014-07-14 01:29:54,006 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22904,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326571101,"queuetimems":2509,"class":"HRegionServer","responsesize":15887,"method":"Multi"}
2014-07-14 01:29:54,006 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23334,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326570671,"queuetimems":4534,"class":"HRegionServer","responsesize":15642,"method":"Multi"}
2014-07-14 01:29:54,007 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22905,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326571102,"queuetimems":2472,"class":"HRegionServer","responsesize":15926,"method":"Multi"}
2014-07-14 01:29:54,008 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23306,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326570701,"queuetimems":2606,"class":"HRegionServer","responsesize":15447,"method":"Multi"}
2014-07-14 01:29:54,005 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22903,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326571101,"queuetimems":2604,"class":"HRegionServer","responsesize":15667,"method":"Multi"}
2014-07-14 01:29:54,008 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22905,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326571102,"queuetimems":2439,"class":"HRegionServer","responsesize":15687,"method":"Multi"}
2014-07-14 01:29:54,005 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21285 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,008 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,008 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21304 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,008 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,008 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21293 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,009 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,009 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21308 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,009 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,009 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21305 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,009 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,009 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21288 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,009 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,009 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21306 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,010 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,010 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21307 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,010 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,010 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21273 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,010 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,169 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23289,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326570879,"queuetimems":2470,"class":"HRegionServer","responsesize":15620,"method":"Multi"}
2014-07-14 01:29:54,170 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21309 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,170 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,226 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22704,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326571521,"queuetimems":2732,"class":"HRegionServer","responsesize":15744,"method":"Multi"}
2014-07-14 01:29:54,226 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22616,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326571609,"queuetimems":2554,"class":"HRegionServer","responsesize":15717,"method":"Multi"}
2014-07-14 01:29:54,226 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21302 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,226 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,227 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22702,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326571525,"queuetimems":2674,"class":"HRegionServer","responsesize":15633,"method":"Multi"}
2014-07-14 01:29:54,228 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21301 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,228 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,229 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21298 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,229 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,271 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22654,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326571617,"queuetimems":2524,"class":"HRegionServer","responsesize":15303,"method":"Multi"}
2014-07-14 01:29:54,271 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22483,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326571788,"queuetimems":2618,"class":"HRegionServer","responsesize":15666,"method":"Multi"}
2014-07-14 01:29:54,272 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21297 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,272 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,272 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21295 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,272 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,272 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22759,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326571512,"queuetimems":2784,"class":"HRegionServer","responsesize":15510,"method":"Multi"}
2014-07-14 01:29:54,273 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21303 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,273 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,308 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22690,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326571618,"queuetimems":2482,"class":"HRegionServer","responsesize":15903,"method":"Multi"}
2014-07-14 01:29:54,309 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22775,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326571533,"queuetimems":2567,"class":"HRegionServer","responsesize":15813,"method":"Multi"}
2014-07-14 01:29:54,309 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21296 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,309 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,309 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22520,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326571788,"queuetimems":2581,"class":"HRegionServer","responsesize":15877,"method":"Multi"}
2014-07-14 01:29:54,309 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22700,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326571608,"queuetimems":2607,"class":"HRegionServer","responsesize":15822,"method":"Multi"}
2014-07-14 01:29:54,309 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22456,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326571852,"queuetimems":1243,"class":"HRegionServer","responsesize":15972,"method":"Multi"}
2014-07-14 01:29:54,309 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21300 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,310 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,310 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21299 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,310 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,310 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21310 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,310 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,310 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21311 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,310 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,321 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22158,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326572162,"queuetimems":69,"class":"HRegionServer","responsesize":15648,"method":"Multi"}
2014-07-14 01:29:54,321 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21323 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,321 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,326 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21320 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,327 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,373 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22211,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326572162,"queuetimems":98,"class":"HRegionServer","responsesize":16007,"method":"Multi"}
2014-07-14 01:29:54,374 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21324 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,374 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,379 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22226,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326572152,"queuetimems":1426,"class":"HRegionServer","responsesize":15629,"method":"Multi"}
2014-07-14 01:29:54,379 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21312 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,380 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,390 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22227,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326572162,"queuetimems":39,"class":"HRegionServer","responsesize":15849,"method":"Multi"}
2014-07-14 01:29:54,390 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22238,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326572151,"queuetimems":1514,"class":"HRegionServer","responsesize":15877,"method":"Multi"}
2014-07-14 01:29:54,390 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21322 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,390 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,390 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21313 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,390 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,391 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22227,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326572162,"queuetimems":127,"class":"HRegionServer","responsesize":15958,"method":"Multi"}
2014-07-14 01:29:54,391 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21325 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,391 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,436 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22273,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326572162,"queuetimems":9,"class":"HRegionServer","responsesize":15783,"method":"Multi"}
2014-07-14 01:29:54,436 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21319 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,436 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,436 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22277,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47140","starttimems":1405326572158,"queuetimems":153,"class":"HRegionServer","responsesize":16026,"method":"Multi"}
2014-07-14 01:29:54,436 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21321 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,436 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,436 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21316 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47140: output error
2014-07-14 01:29:54,436 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:29:54,672 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:29:56,655 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:29:57,054 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48973 synced till here 48971
2014-07-14 01:29:57,075 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326593446 with entries=73, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326596655
2014-07-14 01:29:57,278 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:29:57,279 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files, but is 1.1g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:29:57,279 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. due to global heap pressure
2014-07-14 01:29:57,279 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 1.1g
2014-07-14 01:29:57,955 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:29:57,981 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49048 synced till here 49047
2014-07-14 01:29:58,020 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326596655 with entries=75, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326597955
2014-07-14 01:29:58,722 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:29:59,812 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:59,824 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:29:59,834 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:59,835 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:59,846 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:59,848 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49121 synced till here 49120
2014-07-14 01:29:59,853 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:59,855 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326597955 with entries=73, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326599824
2014-07-14 01:29:59,906 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:59,936 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:29:59,976 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:00,013 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:00,057 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:00,115 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:00,164 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:00,193 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:00,237 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:00,285 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:00,314 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:00,351 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:00,395 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:00,443 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:00,486 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:00,525 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:00,565 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:00,615 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:00,659 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:01,434 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:01,443 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:01,454 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:01,482 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:02,012 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:02,042 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:02,074 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:02,109 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:02,150 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:02,181 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:02,214 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:04,009 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:04,051 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:04,094 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:04,136 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:04,173 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:04,219 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:04,260 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:04,303 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:04,349 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:04,390 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:04,438 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:04,487 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:04,812 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:04,834 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:04,835 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:04,846 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:04,853 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:04,906 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:04,936 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:04,976 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:30:05,014 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:30:05,057 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:05,116 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:30:05,164 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:05,193 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:05,237 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:05,286 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:30:05,315 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:30:05,351 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:05,396 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:30:05,443 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:05,486 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:05,525 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:05,565 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:05,615 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:05,659 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:06,114 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:06,158 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:06,199 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:30:06,434 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:06,444 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:06,455 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:30:06,483 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:30:07,012 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:30:07,043 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:30:07,075 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:07,110 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:07,151 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:30:07,182 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:30:07,214 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:30:09,010 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:30:09,052 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:09,095 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:09,136 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:30:09,173 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:09,219 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:09,260 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:09,303 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:09,350 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:09,390 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:30:09,438 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:09,487 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:09,813 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:09,835 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:30:09,836 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:09,847 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:09,853 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:30:09,906 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:30:09,936 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:30:09,976 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:10,015 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:30:10,058 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:30:10,116 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:10,164 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:30:10,194 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:10,237 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:30:10,286 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:10,315 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:10,352 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:30:10,396 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:10,444 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:10,487 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:30:10,525 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:30:10,565 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:30:10,616 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:10,660 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:11,115 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:30:11,159 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:30:11,199 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:30:11,435 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:30:11,444 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:11,455 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:11,483 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:12,012 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:12,043 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:12,075 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:12,110 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:12,152 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:12,182 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:12,214 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:14,010 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:14,052 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:14,095 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:14,137 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:14,174 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:30:14,220 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:30:14,261 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:30:14,303 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:30:14,350 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:14,390 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:14,439 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:15,175 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15011ms
2014-07-14 01:30:15,175 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15322ms
2014-07-14 01:30:15,175 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10688ms
2014-07-14 01:30:15,175 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15363ms
2014-07-14 01:30:15,176 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15330ms
2014-07-14 01:30:15,176 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15341ms
2014-07-14 01:30:15,176 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15342ms
2014-07-14 01:30:15,176 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15270ms
2014-07-14 01:30:15,176 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15240ms
2014-07-14 01:30:15,177 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15202ms
2014-07-14 01:30:15,177 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15120ms
2014-07-14 01:30:15,177 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15164ms
2014-07-14 01:30:15,177 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15062ms
2014-07-14 01:30:15,194 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:15,238 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:15,286 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:15,315 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:15,352 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:15,396 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:15,444 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:15,487 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:15,525 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 01:30:15,565 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 01:30:15,616 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:15,660 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:16,115 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:16,159 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:16,200 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:30:16,435 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:16,444 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:16,455 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:16,483 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:17,013 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:30:17,044 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:30:17,075 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:17,111 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:17,152 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:30:17,182 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:17,214 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:19,011 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:30:19,052 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:19,095 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:19,137 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:30:19,174 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:19,220 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:19,261 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:19,304 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 01:30:19,350 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:19,390 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:19,439 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:20,175 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20011ms
2014-07-14 01:30:20,175 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20322ms
2014-07-14 01:30:20,176 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15689ms
2014-07-14 01:30:20,176 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20364ms
2014-07-14 01:30:20,176 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20330ms
2014-07-14 01:30:20,176 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20341ms
2014-07-14 01:30:20,176 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20342ms
2014-07-14 01:30:20,177 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20271ms
2014-07-14 01:30:20,177 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20241ms
2014-07-14 01:30:20,177 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20202ms
2014-07-14 01:30:20,177 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20120ms
2014-07-14 01:30:20,177 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20164ms
2014-07-14 01:30:20,177 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20062ms
2014-07-14 01:30:20,194 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:30:20,238 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:30:20,287 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:30:20,315 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:30:20,352 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:30:20,397 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:30:20,444 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:30:20,487 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:30:20,526 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:30:20,566 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:30:20,616 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:30:20,660 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:30:21,116 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:21,160 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:21,200 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:30:21,435 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:30:21,444 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:30:21,455 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:30:21,484 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:30:22,013 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:30:22,044 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:30:22,075 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:30:22,111 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:30:22,658 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20445ms
2014-07-14 01:30:22,658 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20508ms
2014-07-14 01:30:22,659 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20477ms
2014-07-14 01:30:24,011 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:30:24,052 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:30:24,096 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:30:24,137 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:30:24,174 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:30:24,220 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:30:24,261 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:30:24,304 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:30:24,350 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:30:24,391 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:30:24,439 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:30:25,176 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25012ms
2014-07-14 01:30:25,176 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20689ms
2014-07-14 01:30:25,176 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25364ms
2014-07-14 01:30:25,177 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25324ms
2014-07-14 01:30:25,177 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25271ms
2014-07-14 01:30:25,177 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25343ms
2014-07-14 01:30:25,178 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25342ms
2014-07-14 01:30:25,178 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25332ms
2014-07-14 01:30:25,178 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25063ms
2014-07-14 01:30:25,178 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25165ms
2014-07-14 01:30:25,178 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25121ms
2014-07-14 01:30:25,179 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25204ms
2014-07-14 01:30:25,179 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25243ms
2014-07-14 01:30:25,194 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:30:25,238 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:30:25,287 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:30:25,316 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:30:25,352 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:30:25,397 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:30:25,444 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:30:25,487 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:30:25,526 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:30:25,566 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:30:25,616 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:30:25,660 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:30:26,116 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:30:26,160 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:30:26,201 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:30:26,436 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:30:26,445 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:30:26,456 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:30:26,484 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:30:26,760 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12272, memsize=790.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/a9f0b948fbb742dbb58697bf7f9532f0
2014-07-14 01:30:26,783 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/a9f0b948fbb742dbb58697bf7f9532f0 as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/a9f0b948fbb742dbb58697bf7f9532f0
2014-07-14 01:30:26,801 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/a9f0b948fbb742dbb58697bf7f9532f0, entries=2878560, sequenceid=12272, filesize=205.0m
2014-07-14 01:30:26,802 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.1g/1177001920, currentsize=54.4m/57007920 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 29523ms, sequenceid=12272, compaction requested=true
2014-07-14 01:30:26,802 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:30:26,803 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 30 store files, 0 compacting, 30 eligible, 20 blocking
2014-07-14 01:30:26,803 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25321ms
2014-07-14 01:30:26,803 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 30 files from compaction candidates
2014-07-14 01:30:26,803 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 113840ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:30:26,803 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:30:26,803 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:30:26,803 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,803 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:30:26,803 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 635.0m
2014-07-14 01:30:26,804 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25350ms
2014-07-14 01:30:26,804 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,804 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25361ms
2014-07-14 01:30:26,804 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,805 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25371ms
2014-07-14 01:30:26,805 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,805 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20606ms
2014-07-14 01:30:26,805 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,805 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20647ms
2014-07-14 01:30:26,805 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,805 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20691ms
2014-07-14 01:30:26,805 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,808 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26149ms
2014-07-14 01:30:26,808 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,808 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26193ms
2014-07-14 01:30:26,809 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,809 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26244ms
2014-07-14 01:30:26,809 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,809 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26284ms
2014-07-14 01:30:26,809 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,809 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26323ms
2014-07-14 01:30:26,809 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,809 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26366ms
2014-07-14 01:30:26,810 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,810 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26415ms
2014-07-14 01:30:26,810 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,810 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26459ms
2014-07-14 01:30:26,810 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,810 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26496ms
2014-07-14 01:30:26,810 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,810 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26525ms
2014-07-14 01:30:26,810 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,814 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26577ms
2014-07-14 01:30:26,814 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,815 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26622ms
2014-07-14 01:30:26,815 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,815 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26879ms
2014-07-14 01:30:26,815 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,815 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26840ms
2014-07-14 01:30:26,815 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,815 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26758ms
2014-07-14 01:30:26,815 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,815 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26802ms
2014-07-14 01:30:26,815 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,816 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26701ms
2014-07-14 01:30:26,816 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,816 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26970ms
2014-07-14 01:30:26,816 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,816 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26981ms
2014-07-14 01:30:26,816 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,816 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26982ms
2014-07-14 01:30:26,816 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,816 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26910ms
2014-07-14 01:30:26,816 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,817 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26964ms
2014-07-14 01:30:26,817 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,819 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27007ms
2014-07-14 01:30:26,819 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,819 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22332ms
2014-07-14 01:30:26,820 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,820 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26656ms
2014-07-14 01:30:26,820 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,820 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22382ms
2014-07-14 01:30:26,820 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,820 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22431ms
2014-07-14 01:30:26,820 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,820 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22471ms
2014-07-14 01:30:26,820 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,820 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22517ms
2014-07-14 01:30:26,820 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,821 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22560ms
2014-07-14 01:30:26,821 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,821 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22602ms
2014-07-14 01:30:26,821 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,821 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22648ms
2014-07-14 01:30:26,821 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,821 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22686ms
2014-07-14 01:30:26,821 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,821 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22727ms
2014-07-14 01:30:26,821 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,821 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22770ms
2014-07-14 01:30:26,821 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,821 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22812ms
2014-07-14 01:30:26,822 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,822 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24641ms
2014-07-14 01:30:26,822 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,822 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24672ms
2014-07-14 01:30:26,822 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,822 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24609ms
2014-07-14 01:30:26,822 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,822 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24713ms
2014-07-14 01:30:26,822 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,822 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24748ms
2014-07-14 01:30:26,823 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,823 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24781ms
2014-07-14 01:30:26,823 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,823 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24812ms
2014-07-14 01:30:26,823 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:30:26,911 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27289,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326599621,"queuetimems":1,"class":"HRegionServer","responsesize":15609,"method":"Multi"}
2014-07-14 01:30:27,323 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27644,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326599678,"queuetimems":0,"class":"HRegionServer","responsesize":15776,"method":"Multi"}
2014-07-14 01:30:28,103 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28352,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326599751,"queuetimems":0,"class":"HRegionServer","responsesize":16064,"method":"Multi"}
2014-07-14 01:30:28,107 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:30:28,248 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28454,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326599794,"queuetimems":1,"class":"HRegionServer","responsesize":15833,"method":"Multi"}
2014-07-14 01:30:28,540 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:30:28,702 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49264 synced till here 49249
2014-07-14 01:30:28,799 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26729,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326602070,"queuetimems":0,"class":"HRegionServer","responsesize":15609,"method":"Multi"}
2014-07-14 01:30:28,799 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22686,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326606112,"queuetimems":1,"class":"HRegionServer","responsesize":15830,"method":"Multi"}
2014-07-14 01:30:28,800 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22604,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326606196,"queuetimems":0,"class":"HRegionServer","responsesize":15609,"method":"Multi"}
2014-07-14 01:30:28,800 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27347,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326601453,"queuetimems":1,"class":"HRegionServer","responsesize":15874,"method":"Multi"}
2014-07-14 01:30:28,799 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24667,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326604131,"queuetimems":0,"class":"HRegionServer","responsesize":15833,"method":"Multi"}
2014-07-14 01:30:28,799 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27357,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326601441,"queuetimems":0,"class":"HRegionServer","responsesize":15877,"method":"Multi"}
2014-07-14 01:30:28,799 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27317,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326601481,"queuetimems":0,"class":"HRegionServer","responsesize":15671,"method":"Multi"}
2014-07-14 01:30:28,799 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28487,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326600311,"queuetimems":0,"class":"HRegionServer","responsesize":15819,"method":"Multi"}
2014-07-14 01:30:28,800 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27369,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326601431,"queuetimems":711,"class":"HRegionServer","responsesize":15576,"method":"Multi"}
2014-07-14 01:30:28,829 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326599824 with entries=143, filesize=113.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326628541
2014-07-14 01:30:29,725 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25468,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326604256,"queuetimems":1,"class":"HRegionServer","responsesize":15576,"method":"Multi"}
2014-07-14 01:30:29,730 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29573,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326600156,"queuetimems":0,"class":"HRegionServer","responsesize":15928,"method":"Multi"}
2014-07-14 01:30:29,733 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25646,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326604087,"queuetimems":1,"class":"HRegionServer","responsesize":15671,"method":"Multi"}
2014-07-14 01:30:29,733 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25249,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326604484,"queuetimems":0,"class":"HRegionServer","responsesize":15877,"method":"Multi"}
2014-07-14 01:30:29,734 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27522,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326602211,"queuetimems":0,"class":"HRegionServer","responsesize":15776,"method":"Multi"}
2014-07-14 01:30:29,737 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29901,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326599836,"queuetimems":0,"class":"HRegionServer","responsesize":15881,"method":"Multi"}
2014-07-14 01:30:29,737 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29174,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326600563,"queuetimems":0,"class":"HRegionServer","responsesize":15818,"method":"Multi"}
2014-07-14 01:30:29,737 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29083,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326600654,"queuetimems":0,"class":"HRegionServer","responsesize":16047,"method":"Multi"}
2014-07-14 01:30:29,741 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29508,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326600233,"queuetimems":0,"class":"HRegionServer","responsesize":15679,"method":"Multi"}
2014-07-14 01:30:29,742 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25395,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326604346,"queuetimems":0,"class":"HRegionServer","responsesize":15819,"method":"Multi"}
2014-07-14 01:30:29,745 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27638,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326602107,"queuetimems":1,"class":"HRegionServer","responsesize":15680,"method":"Multi"}
2014-07-14 01:30:29,746 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29306,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326600440,"queuetimems":0,"class":"HRegionServer","responsesize":15830,"method":"Multi"}
2014-07-14 01:30:29,757 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29702,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326600055,"queuetimems":0,"class":"HRegionServer","responsesize":15874,"method":"Multi"}
2014-07-14 01:30:29,757 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27577,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326602180,"queuetimems":1,"class":"HRegionServer","responsesize":15651,"method":"Multi"}
2014-07-14 01:30:29,757 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29408,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326600349,"queuetimems":0,"class":"HRegionServer","responsesize":15881,"method":"Multi"}
2014-07-14 01:30:29,757 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29145,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326600612,"queuetimems":1,"class":"HRegionServer","responsesize":15668,"method":"Multi"}
2014-07-14 01:30:29,758 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29480,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326600277,"queuetimems":0,"class":"HRegionServer","responsesize":16064,"method":"Multi"}
2014-07-14 01:30:29,757 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25752,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326604005,"queuetimems":0,"class":"HRegionServer","responsesize":15722,"method":"Multi"}
2014-07-14 01:30:29,764 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29575,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326600189,"queuetimems":0,"class":"HRegionServer","responsesize":15722,"method":"Multi"}
2014-07-14 01:30:30,035 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:30:30,037 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29552,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326600484,"queuetimems":0,"class":"HRegionServer","responsesize":15680,"method":"Multi"}
2014-07-14 01:30:30,038 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30141,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326599897,"queuetimems":0,"class":"HRegionServer","responsesize":15819,"method":"Multi"}
2014-07-14 01:30:30,041 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29653,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326600388,"queuetimems":1,"class":"HRegionServer","responsesize":15833,"method":"Multi"}
2014-07-14 01:30:30,050 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25751,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326604298,"queuetimems":0,"class":"HRegionServer","responsesize":16064,"method":"Multi"}
2014-07-14 01:30:30,051 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28011,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326602039,"queuetimems":0,"class":"HRegionServer","responsesize":15830,"method":"Multi"}
2014-07-14 01:30:30,053 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26005,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326604047,"queuetimems":0,"class":"HRegionServer","responsesize":16047,"method":"Multi"}
2014-07-14 01:30:30,065 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25850,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326604215,"queuetimems":0,"class":"HRegionServer","responsesize":15874,"method":"Multi"}
2014-07-14 01:30:30,067 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30095,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326599971,"queuetimems":0,"class":"HRegionServer","responsesize":15928,"method":"Multi"}
2014-07-14 01:30:30,073 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25687,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326604386,"queuetimems":0,"class":"HRegionServer","responsesize":15679,"method":"Multi"}
2014-07-14 01:30:30,077 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28067,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326602010,"queuetimems":1,"class":"HRegionServer","responsesize":15818,"method":"Multi"}
2014-07-14 01:30:30,089 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30082,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326600007,"queuetimems":1,"class":"HRegionServer","responsesize":15877,"method":"Multi"}
2014-07-14 01:30:30,147 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49382 synced till here 49352
2014-07-14 01:30:30,235 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12285, memsize=986.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/9c73c3ef6c14408abc59683fc4b72e6c
2014-07-14 01:30:30,249 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/9c73c3ef6c14408abc59683fc4b72e6c as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/9c73c3ef6c14408abc59683fc4b72e6c
2014-07-14 01:30:30,260 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/9c73c3ef6c14408abc59683fc4b72e6c, entries=3591590, sequenceid=12285, filesize=255.7m
2014-07-14 01:30:30,260 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.3g/1375480080, currentsize=160.7m/168524080 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 37245ms, sequenceid=12285, compaction requested=true
2014-07-14 01:30:30,260 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30328,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326599932,"queuetimems":0,"class":"HRegionServer","responsesize":15679,"method":"Multi"}
2014-07-14 01:30:30,260 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24106,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326606154,"queuetimems":0,"class":"HRegionServer","responsesize":15818,"method":"Multi"}
2014-07-14 01:30:30,261 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:30:30,261 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 32 store files, 0 compacting, 32 eligible, 20 blocking
2014-07-14 01:30:30,261 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 32 files from compaction candidates
2014-07-14 01:30:30,261 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25828,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326604433,"queuetimems":0,"class":"HRegionServer","responsesize":15881,"method":"Multi"}
2014-07-14 01:30:30,262 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30151,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326600110,"queuetimems":1,"class":"HRegionServer","responsesize":15576,"method":"Multi"}
2014-07-14 01:30:30,260 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26091,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326604169,"queuetimems":1,"class":"HRegionServer","responsesize":15928,"method":"Multi"}
2014-07-14 01:30:30,261 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:30:30,273 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:30:30,274 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:30:30,288 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28140,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326602147,"queuetimems":0,"class":"HRegionServer","responsesize":15668,"method":"Multi"}
2014-07-14 01:30:30,312 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326628541 with entries=118, filesize=98.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326630035
2014-07-14 01:30:30,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326445158
2014-07-14 01:30:30,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326446927
2014-07-14 01:30:30,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326448340
2014-07-14 01:30:30,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326449979
2014-07-14 01:30:30,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326456592
2014-07-14 01:30:30,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326459127
2014-07-14 01:30:30,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326461585
2014-07-14 01:30:30,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326468912
2014-07-14 01:30:30,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326488971
2014-07-14 01:30:30,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326490326
2014-07-14 01:30:30,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326494789
2014-07-14 01:30:30,313 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326496748
2014-07-14 01:30:30,313 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326498475
2014-07-14 01:30:30,313 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326504873
2014-07-14 01:30:30,313 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326506984
2014-07-14 01:30:30,313 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326508395
2014-07-14 01:30:30,313 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326510989
2014-07-14 01:30:30,313 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326513241
2014-07-14 01:30:30,602 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30081,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326600520,"queuetimems":0,"class":"HRegionServer","responsesize":15651,"method":"Multi"}
2014-07-14 01:30:31,595 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:30:31,666 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49484 synced till here 49455
2014-07-14 01:30:32,089 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326630035 with entries=102, filesize=87.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326631595
2014-07-14 01:30:33,477 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:30:33,500 DEBUG [RpcServer.handler=15,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:30:33,501 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files; delaying flush up to 90000ms
2014-07-14 01:30:33,501 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 32 store files, 0 compacting, 32 eligible, 20 blocking
2014-07-14 01:30:33,501 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 32 files from compaction candidates
2014-07-14 01:30:33,502 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:30:33,502 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:30:33,502 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:30:33,502 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:30:33,541 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49603 synced till here 49583
2014-07-14 01:30:33,713 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326631595 with entries=119, filesize=101.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326633478
2014-07-14 01:30:35,037 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:30:35,134 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49706 synced till here 49681
2014-07-14 01:30:35,326 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326633478 with entries=103, filesize=88.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326635038
2014-07-14 01:30:35,650 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:30:35,651 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files; delaying flush up to 90000ms
2014-07-14 01:30:35,652 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:30:35,652 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 30 store files, 0 compacting, 30 eligible, 20 blocking
2014-07-14 01:30:35,652 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 30 files from compaction candidates
2014-07-14 01:30:35,652 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:30:35,652 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:30:35,652 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:30:36,031 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:30:36,224 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49792 synced till here 49791
2014-07-14 01:30:36,235 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326635038 with entries=86, filesize=73.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326636032
2014-07-14 01:30:36,235 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:30:40,029 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12309, memsize=270.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/be166cd65a4c45a196504c3b4ef33c30
2014-07-14 01:30:40,078 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/be166cd65a4c45a196504c3b4ef33c30 as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/be166cd65a4c45a196504c3b4ef33c30
2014-07-14 01:30:40,091 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/be166cd65a4c45a196504c3b4ef33c30, entries=984400, sequenceid=12309, filesize=70.1m
2014-07-14 01:30:40,091 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~635.0m/665894320, currentsize=256.7m/269185040 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 13288ms, sequenceid=12309, compaction requested=true
2014-07-14 01:30:40,092 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:30:40,092 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 34 store files, 0 compacting, 34 eligible, 20 blocking
2014-07-14 01:30:40,092 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 34 files from compaction candidates
2014-07-14 01:30:40,092 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:30:40,092 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:30:40,092 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:30:42,875 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:30:42,875 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files; delaying flush up to 90000ms
2014-07-14 01:30:42,876 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 34 store files, 0 compacting, 34 eligible, 20 blocking
2014-07-14 01:30:42,876 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 34 files from compaction candidates
2014-07-14 01:30:42,876 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:30:42,876 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:30:42,876 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:30:42,876 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:30:43,713 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:30:43,729 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49865 synced till here 49863
2014-07-14 01:30:43,748 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326636032 with entries=73, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326643713
2014-07-14 01:30:43,749 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:30:44,753 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90801ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:30:44,754 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 1.1g
2014-07-14 01:30:44,910 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:30:44,936 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49943 synced till here 49937
2014-07-14 01:30:45,003 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326643713 with entries=78, filesize=66.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326644910
2014-07-14 01:30:46,309 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:30:46,364 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:30:46,402 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50026 synced till here 50015
2014-07-14 01:30:46,541 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326644910 with entries=83, filesize=71.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326646309
2014-07-14 01:30:47,611 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:30:47,626 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50100 synced till here 50098
2014-07-14 01:30:47,654 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326646309 with entries=74, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326647611
2014-07-14 01:30:48,325 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:30:48,389 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50178 synced till here 50177
2014-07-14 01:30:48,405 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326647611 with entries=78, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326648325
2014-07-14 01:30:49,664 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:30:49,686 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50255 synced till here 50252
2014-07-14 01:30:49,723 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326648325 with entries=77, filesize=66.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326649665
2014-07-14 01:30:50,538 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90856ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:30:50,538 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 457.5m
2014-07-14 01:30:50,899 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:30:51,006 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:30:51,174 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50339 synced till here 50338
2014-07-14 01:30:51,206 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326649665 with entries=84, filesize=72.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326651006
2014-07-14 01:30:52,429 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:30:52,843 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50440 synced till here 50437
2014-07-14 01:30:52,881 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326651006 with entries=101, filesize=86.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326652430
2014-07-14 01:30:55,437 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:30:55,477 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326652430 with entries=73, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326655437
2014-07-14 01:30:57,640 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:30:57,660 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50589 synced till here 50587
2014-07-14 01:30:57,701 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326655437 with entries=76, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326657640
2014-07-14 01:30:59,049 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:30:59,070 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50662 synced till here 50661
2014-07-14 01:30:59,087 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326657640 with entries=73, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326659049
2014-07-14 01:31:00,927 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:31:00,963 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50745 synced till here 50734
2014-07-14 01:31:01,111 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326659049 with entries=83, filesize=71.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326660927
2014-07-14 01:31:01,241 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12610, memsize=202.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/0e52cfa28e5e460ba2dbf1154af6448c
2014-07-14 01:31:01,261 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/0e52cfa28e5e460ba2dbf1154af6448c as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/0e52cfa28e5e460ba2dbf1154af6448c
2014-07-14 01:31:01,584 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/0e52cfa28e5e460ba2dbf1154af6448c, entries=736930, sequenceid=12610, filesize=52.5m
2014-07-14 01:31:01,585 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~459.1m/481414400, currentsize=192.6m/201955440 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 11047ms, sequenceid=12610, compaction requested=true
2014-07-14 01:31:01,585 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:31:01,586 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 35 store files, 0 compacting, 35 eligible, 20 blocking
2014-07-14 01:31:01,586 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 35 files from compaction candidates
2014-07-14 01:31:01,586 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:31:01,586 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:31:01,586 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:31:02,366 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:31:02,391 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50819 synced till here 50818
2014-07-14 01:31:02,411 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326660927 with entries=74, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326662366
2014-07-14 01:31:03,570 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:31:03,592 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50895 synced till here 50893
2014-07-14 01:31:03,641 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326662366 with entries=76, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326663571
2014-07-14 01:31:04,964 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:31:04,966 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:31:04,966 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files; delaying flush up to 90000ms
2014-07-14 01:31:04,966 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:31:04,966 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 35 store files, 0 compacting, 35 eligible, 20 blocking
2014-07-14 01:31:04,966 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 35 files from compaction candidates
2014-07-14 01:31:04,967 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:31:04,967 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:31:04,967 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:31:05,248 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50986 synced till here 50983
2014-07-14 01:31:05,289 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326663571 with entries=91, filesize=77.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326664964
2014-07-14 01:31:06,701 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:31:06,732 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326664964 with entries=73, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326666702
2014-07-14 01:31:08,720 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:31:08,740 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51132 synced till here 51131
2014-07-14 01:31:08,757 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326666702 with entries=73, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326668720
2014-07-14 01:31:10,518 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:31:10,557 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51209 synced till here 51206
2014-07-14 01:31:10,598 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326668720 with entries=77, filesize=66.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326670519
2014-07-14 01:31:12,048 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:31:12,049 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files, but is 897.2m vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:31:12,049 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. due to global heap pressure
2014-07-14 01:31:12,049 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 897.2m
2014-07-14 01:31:12,161 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:31:12,174 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51288 synced till here 51283
2014-07-14 01:31:12,210 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326670519 with entries=79, filesize=67.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326672161
2014-07-14 01:31:13,574 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:31:13,633 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:31:13,682 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51374 synced till here 51363
2014-07-14 01:31:13,778 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326672161 with entries=86, filesize=73.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326673633
2014-07-14 01:31:14,099 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,100 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,103 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,103 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,104 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,108 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,109 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,139 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,168 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,169 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,170 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,174 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,177 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,181 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,222 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,227 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,228 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,229 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,229 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,230 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,273 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,315 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,352 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,396 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,446 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,485 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,523 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,564 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,618 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,659 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,695 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,740 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,780 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,813 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,862 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,904 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,939 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:14,979 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:15,013 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:16,535 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:16,569 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:16,599 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:16,627 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:16,658 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:16,691 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:16,719 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:16,750 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:16,780 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:16,812 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:16,843 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:16,852 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12510, memsize=559.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/fc543d0f45a646c0865889c6db6334eb
2014-07-14 01:31:16,866 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/fc543d0f45a646c0865889c6db6334eb as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/fc543d0f45a646c0865889c6db6334eb
2014-07-14 01:31:16,875 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/fc543d0f45a646c0865889c6db6334eb, entries=2036690, sequenceid=12510, filesize=145.0m
2014-07-14 01:31:16,875 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.2g/1236830480, currentsize=574.7m/602614080 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 32121ms, sequenceid=12510, compaction requested=true
2014-07-14 01:31:16,876 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:31:16,876 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 20 blocking
2014-07-14 01:31:16,876 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33ms
2014-07-14 01:31:16,876 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,876 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-14 01:31:16,876 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 64ms
2014-07-14 01:31:16,876 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,876 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:31:16,876 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 96ms
2014-07-14 01:31:16,876 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,876 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:31:16,877 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:31:16,877 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 127ms
2014-07-14 01:31:16,877 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,881 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 162ms
2014-07-14 01:31:16,881 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,881 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 190ms
2014-07-14 01:31:16,882 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,882 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 224ms
2014-07-14 01:31:16,883 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,883 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 256ms
2014-07-14 01:31:16,883 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,883 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 284ms
2014-07-14 01:31:16,883 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,883 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 314ms
2014-07-14 01:31:16,883 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,883 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 349ms
2014-07-14 01:31:16,883 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,883 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1870ms
2014-07-14 01:31:16,883 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,884 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1905ms
2014-07-14 01:31:16,884 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,884 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1946ms
2014-07-14 01:31:16,884 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,884 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1980ms
2014-07-14 01:31:16,884 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,884 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2023ms
2014-07-14 01:31:16,884 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,891 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2078ms
2014-07-14 01:31:16,891 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,891 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2111ms
2014-07-14 01:31:16,891 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,891 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2151ms
2014-07-14 01:31:16,891 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,892 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2197ms
2014-07-14 01:31:16,892 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,892 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2233ms
2014-07-14 01:31:16,892 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,892 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2274ms
2014-07-14 01:31:16,892 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,892 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2328ms
2014-07-14 01:31:16,892 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,892 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2370ms
2014-07-14 01:31:16,893 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,893 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2408ms
2014-07-14 01:31:16,893 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,897 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2451ms
2014-07-14 01:31:16,897 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,897 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2501ms
2014-07-14 01:31:16,897 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,897 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2545ms
2014-07-14 01:31:16,897 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,897 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2582ms
2014-07-14 01:31:16,897 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,897 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2624ms
2014-07-14 01:31:16,898 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,898 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2669ms
2014-07-14 01:31:16,898 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,898 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2669ms
2014-07-14 01:31:16,898 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,905 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2676ms
2014-07-14 01:31:16,905 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,905 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2677ms
2014-07-14 01:31:16,905 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,905 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2678ms
2014-07-14 01:31:16,905 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,906 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2684ms
2014-07-14 01:31:16,906 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,906 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2725ms
2014-07-14 01:31:16,906 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,913 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2736ms
2014-07-14 01:31:16,913 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,913 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2739ms
2014-07-14 01:31:16,913 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,913 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2743ms
2014-07-14 01:31:16,913 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,913 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2744ms
2014-07-14 01:31:16,913 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,913 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2745ms
2014-07-14 01:31:16,913 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,917 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2778ms
2014-07-14 01:31:16,917 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,917 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2809ms
2014-07-14 01:31:16,917 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,925 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2817ms
2014-07-14 01:31:16,925 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,925 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2821ms
2014-07-14 01:31:16,925 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,925 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2822ms
2014-07-14 01:31:16,925 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,932 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2829ms
2014-07-14 01:31:16,932 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,932 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2832ms
2014-07-14 01:31:16,932 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:16,932 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2833ms
2014-07-14 01:31:16,932 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:17,388 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:31:17,389 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:31:17,390 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files; delaying flush up to 90000ms
2014-07-14 01:31:17,392 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:31:17,392 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 20 blocking
2014-07-14 01:31:17,393 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-14 01:31:17,393 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:31:17,393 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:31:17,393 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:31:17,410 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51468 synced till here 51460
2014-07-14 01:31:17,516 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326673633 with entries=94, filesize=79.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326677389
2014-07-14 01:31:17,516 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326516240
2014-07-14 01:31:17,516 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326522793
2014-07-14 01:31:17,516 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326524133
2014-07-14 01:31:17,516 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326525606
2014-07-14 01:31:17,516 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326527917
2014-07-14 01:31:17,516 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326529413
2014-07-14 01:31:17,516 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326530789
2014-07-14 01:31:17,516 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326531777
2014-07-14 01:31:17,516 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326533027
2014-07-14 01:31:17,517 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326534354
2014-07-14 01:31:17,517 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326535722
2014-07-14 01:31:17,517 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326538028
2014-07-14 01:31:17,517 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326539276
2014-07-14 01:31:17,517 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326540980
2014-07-14 01:31:17,517 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326542129
2014-07-14 01:31:17,517 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326545490
2014-07-14 01:31:17,517 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326547220
2014-07-14 01:31:17,517 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326553951
2014-07-14 01:31:17,517 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326557261
2014-07-14 01:31:17,517 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326560026
2014-07-14 01:31:17,517 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326562886
2014-07-14 01:31:17,517 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326565564
2014-07-14 01:31:17,517 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326568402
2014-07-14 01:31:19,185 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:31:19,395 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51584 synced till here 51552
2014-07-14 01:31:19,638 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326677389 with entries=116, filesize=96.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326679186
2014-07-14 01:31:21,427 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:31:21,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51691 synced till here 51674
2014-07-14 01:31:21,551 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326679186 with entries=107, filesize=91.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326681427
2014-07-14 01:31:23,159 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:31:23,176 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51796 synced till here 51765
2014-07-14 01:31:23,352 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=10829, hits=3899, hitRatio=36.00%, , cachingAccesses=3903, cachingHits=3898, cachingHitsRatio=99.87%, evictions=0, evicted=3, evictedPerRun=Infinity
2014-07-14 01:31:24,616 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326681427 with entries=105, filesize=89.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326683160
2014-07-14 01:31:25,533 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:31:25,762 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51914 synced till here 51893
2014-07-14 01:31:26,619 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326683160 with entries=118, filesize=100.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326685533
2014-07-14 01:31:27,496 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:31:27,513 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51999 synced till here 51988
2014-07-14 01:31:27,953 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326685533 with entries=85, filesize=72.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326687497
2014-07-14 01:31:28,789 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:31:28,789 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files, but is 1.2g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:31:28,790 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. due to global heap pressure
2014-07-14 01:31:28,790 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 1.2g
2014-07-14 01:31:29,296 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:31:29,309 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52074 synced till here 52071
2014-07-14 01:31:29,345 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326687497 with entries=75, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326689296
2014-07-14 01:31:29,779 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:31:31,011 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:31:31,043 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52149 synced till here 52148
2014-07-14 01:31:31,090 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326689296 with entries=75, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326691011
2014-07-14 01:31:31,346 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:31,366 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:31,374 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:31,402 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:31,403 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:31,405 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:31,419 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:31,420 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:31,454 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:31,476 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:31,525 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:31,562 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:31,616 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:31,676 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:31,829 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:31,863 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:31,897 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:31,930 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:31,966 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:31,999 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:32,034 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:32,067 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:32,102 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:32,135 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:32,982 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:32,990 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:33,000 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:33,030 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:33,058 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:33,087 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:33,117 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:33,145 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:33,174 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:33,203 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:33,232 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:33,521 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:33,550 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:33,580 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:33,610 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:33,641 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:33,670 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:33,701 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:33,732 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:33,762 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:33,791 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:33,821 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:33,852 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:33,882 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:33,913 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:33,944 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:36,347 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:31:36,367 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:36,374 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:36,403 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:31:36,403 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:36,406 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:36,419 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:36,421 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:31:36,455 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:31:36,476 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:36,525 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:36,563 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:31:36,617 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:31:36,677 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:36,830 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:31:36,863 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:36,897 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:36,931 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:36,967 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:37,000 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:31:37,035 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:37,067 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:37,103 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:31:37,135 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:31:38,906 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5024ms
2014-07-14 01:31:38,907 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5732ms
2014-07-14 01:31:38,907 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5704ms
2014-07-14 01:31:38,908 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5675ms
2014-07-14 01:31:38,908 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5387ms
2014-07-14 01:31:38,909 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5359ms
2014-07-14 01:31:38,909 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5268ms
2014-07-14 01:31:38,909 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5299ms
2014-07-14 01:31:38,909 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5329ms
2014-07-14 01:31:38,909 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5208ms
2014-07-14 01:31:38,910 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5240ms
2014-07-14 01:31:38,910 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5178ms
2014-07-14 01:31:38,910 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5148ms
2014-07-14 01:31:38,910 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5119ms
2014-07-14 01:31:38,910 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5089ms
2014-07-14 01:31:38,911 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5058ms
2014-07-14 01:31:38,911 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5929ms
2014-07-14 01:31:38,911 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5921ms
2014-07-14 01:31:38,911 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5911ms
2014-07-14 01:31:38,912 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5882ms
2014-07-14 01:31:38,912 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5854ms
2014-07-14 01:31:38,912 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5825ms
2014-07-14 01:31:38,913 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5795ms
2014-07-14 01:31:38,913 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5768ms
2014-07-14 01:31:38,913 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:38,944 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:31:41,347 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:31:41,367 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:31:41,374 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:31:41,403 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:31:41,404 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:31:41,406 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:31:41,419 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:31:41,421 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:31:41,455 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:31:41,476 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:31:41,501 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12872, memsize=573.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/0d6dc3a6363744f2abd9305106565c40
2014-07-14 01:31:41,518 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/0d6dc3a6363744f2abd9305106565c40 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/0d6dc3a6363744f2abd9305106565c40
2014-07-14 01:31:41,526 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:31:41,527 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/0d6dc3a6363744f2abd9305106565c40, entries=2089710, sequenceid=12872, filesize=148.8m
2014-07-14 01:31:41,528 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~901.9m/945673520, currentsize=342.4m/359053440 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 29479ms, sequenceid=12872, compaction requested=true
2014-07-14 01:31:41,528 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:31:41,528 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 33 store files, 0 compacting, 33 eligible, 20 blocking
2014-07-14 01:31:41,528 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-14 01:31:41,528 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 33 files from compaction candidates
2014-07-14 01:31:41,528 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,529 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:31:41,529 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10053ms
2014-07-14 01:31:41,529 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,529 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:31:41,529 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10075ms
2014-07-14 01:31:41,529 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,529 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:31:41,529 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10109ms
2014-07-14 01:31:41,529 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,529 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10110ms
2014-07-14 01:31:41,529 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,530 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10124ms
2014-07-14 01:31:41,530 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,530 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10127ms
2014-07-14 01:31:41,530 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,530 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10128ms
2014-07-14 01:31:41,530 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,530 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10156ms
2014-07-14 01:31:41,530 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,530 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10164ms
2014-07-14 01:31:41,530 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,533 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10187ms
2014-07-14 01:31:41,533 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,533 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7590ms
2014-07-14 01:31:41,533 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,533 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7620ms
2014-07-14 01:31:41,533 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,537 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8392ms
2014-07-14 01:31:41,537 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,545 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8428ms
2014-07-14 01:31:41,545 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,545 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8458ms
2014-07-14 01:31:41,545 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,545 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8487ms
2014-07-14 01:31:41,545 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,545 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8515ms
2014-07-14 01:31:41,546 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,546 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8546ms
2014-07-14 01:31:41,546 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,546 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8556ms
2014-07-14 01:31:41,546 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,557 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8575ms
2014-07-14 01:31:41,557 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,557 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7705ms
2014-07-14 01:31:41,557 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,557 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7736ms
2014-07-14 01:31:41,557 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,560 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7768ms
2014-07-14 01:31:41,560 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,563 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:31:41,563 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,567 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7805ms
2014-07-14 01:31:41,567 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,567 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7835ms
2014-07-14 01:31:41,567 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,567 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7897ms
2014-07-14 01:31:41,567 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,568 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7866ms
2014-07-14 01:31:41,569 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,570 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7989ms
2014-07-14 01:31:41,570 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,570 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7960ms
2014-07-14 01:31:41,570 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,570 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7929ms
2014-07-14 01:31:41,570 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,570 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8020ms
2014-07-14 01:31:41,570 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,576 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8055ms
2014-07-14 01:31:41,576 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,577 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8344ms
2014-07-14 01:31:41,577 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,577 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8374ms
2014-07-14 01:31:41,578 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,578 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8404ms
2014-07-14 01:31:41,578 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,580 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7697ms
2014-07-14 01:31:41,580 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,583 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9448ms
2014-07-14 01:31:41,583 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,584 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9481ms
2014-07-14 01:31:41,584 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,584 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9517ms
2014-07-14 01:31:41,584 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,584 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9550ms
2014-07-14 01:31:41,585 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,585 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9586ms
2014-07-14 01:31:41,585 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,585 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9619ms
2014-07-14 01:31:41,585 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,589 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9659ms
2014-07-14 01:31:41,589 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,589 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9692ms
2014-07-14 01:31:41,589 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,589 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9726ms
2014-07-14 01:31:41,589 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,590 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10617,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326690972,"queuetimems":1,"class":"HRegionServer","responsesize":15536,"method":"Multi"}
2014-07-14 01:31:41,594 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9764ms
2014-07-14 01:31:41,594 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,594 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9918ms
2014-07-14 01:31:41,594 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,594 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9978ms
2014-07-14 01:31:41,594 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:31:41,643 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10618,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326691024,"queuetimems":0,"class":"HRegionServer","responsesize":15628,"method":"Multi"}
2014-07-14 01:31:41,649 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10579,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326691070,"queuetimems":0,"class":"HRegionServer","responsesize":15754,"method":"Multi"}
2014-07-14 01:31:41,649 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10510,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326691139,"queuetimems":0,"class":"HRegionServer","responsesize":16088,"method":"Multi"}
2014-07-14 01:31:41,951 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10752,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326691198,"queuetimems":1,"class":"HRegionServer","responsesize":15541,"method":"Multi"}
2014-07-14 01:31:42,133 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:31:42,134 DEBUG [RpcServer.handler=5,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:31:42,134 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files; delaying flush up to 90000ms
2014-07-14 01:31:42,134 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 33 store files, 0 compacting, 33 eligible, 20 blocking
2014-07-14 01:31:42,134 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 33 files from compaction candidates
2014-07-14 01:31:42,135 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:31:42,135 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:31:42,135 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:31:42,135 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:31:43,085 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52235 synced till here 52233
2014-07-14 01:31:43,095 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11840,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326691255,"queuetimems":1,"class":"HRegionServer","responsesize":15465,"method":"Multi"}
2014-07-14 01:31:43,098 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11772,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326691326,"queuetimems":0,"class":"HRegionServer","responsesize":15840,"method":"Multi"}
2014-07-14 01:31:43,112 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326691011 with entries=86, filesize=72.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326702133
2014-07-14 01:31:43,113 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326571495
2014-07-14 01:31:43,114 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326593446
2014-07-14 01:31:43,944 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10714,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326693230,"queuetimems":0,"class":"HRegionServer","responsesize":15805,"method":"Multi"}
2014-07-14 01:31:44,147 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:31:45,080 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52343 synced till here 52327
2014-07-14 01:31:45,204 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13342,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326691861,"queuetimems":0,"class":"HRegionServer","responsesize":15723,"method":"Multi"}
2014-07-14 01:31:45,215 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326702133 with entries=108, filesize=90.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326704148
2014-07-14 01:31:45,219 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11670,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326693548,"queuetimems":0,"class":"HRegionServer","responsesize":15657,"method":"Multi"}
2014-07-14 01:31:45,589 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11800,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326693789,"queuetimems":0,"class":"HRegionServer","responsesize":15696,"method":"Multi"}
2014-07-14 01:31:45,589 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11950,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326693639,"queuetimems":1,"class":"HRegionServer","responsesize":15928,"method":"Multi"}
2014-07-14 01:31:45,589 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12446,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326693143,"queuetimems":0,"class":"HRegionServer","responsesize":15702,"method":"Multi"}
2014-07-14 01:31:45,591 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11771,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326693819,"queuetimems":1,"class":"HRegionServer","responsesize":15556,"method":"Multi"}
2014-07-14 01:31:45,589 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14116,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326691473,"queuetimems":0,"class":"HRegionServer","responsesize":16430,"method":"Multi"}
2014-07-14 01:31:45,601 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11993,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326693608,"queuetimems":0,"class":"HRegionServer","responsesize":15754,"method":"Multi"}
2014-07-14 01:31:45,601 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12486,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326693115,"queuetimems":1,"class":"HRegionServer","responsesize":15765,"method":"Multi"}
2014-07-14 01:31:45,601 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12544,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326693057,"queuetimems":1,"class":"HRegionServer","responsesize":15744,"method":"Multi"}
2014-07-14 01:31:45,602 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13501,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326692100,"queuetimems":0,"class":"HRegionServer","responsesize":15950,"method":"Multi"}
2014-07-14 01:31:45,601 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11721,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326693880,"queuetimems":0,"class":"HRegionServer","responsesize":15673,"method":"Multi"}
2014-07-14 01:31:45,605 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11906,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326693699,"queuetimems":1,"class":"HRegionServer","responsesize":15960,"method":"Multi"}
2014-07-14 01:31:45,605 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12577,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326693028,"queuetimems":1,"class":"HRegionServer","responsesize":15960,"method":"Multi"}
2014-07-14 01:31:45,609 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12610,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326692999,"queuetimems":1,"class":"HRegionServer","responsesize":15406,"method":"Multi"}
2014-07-14 01:31:45,609 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13683,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326691926,"queuetimems":0,"class":"HRegionServer","responsesize":15556,"method":"Multi"}
2014-07-14 01:31:45,610 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12621,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326692988,"queuetimems":0,"class":"HRegionServer","responsesize":15637,"method":"Multi"}
2014-07-14 01:31:45,609 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13784,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326691825,"queuetimems":0,"class":"HRegionServer","responsesize":15928,"method":"Multi"}
2014-07-14 01:31:45,614 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13940,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326691674,"queuetimems":0,"class":"HRegionServer","responsesize":15992,"method":"Multi"}
2014-07-14 01:31:45,614 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12636,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326692978,"queuetimems":0,"class":"HRegionServer","responsesize":15565,"method":"Multi"}
2014-07-14 01:31:45,609 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13545,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326692064,"queuetimems":1,"class":"HRegionServer","responsesize":15673,"method":"Multi"}
2014-07-14 01:31:45,617 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14003,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326691614,"queuetimems":1,"class":"HRegionServer","responsesize":15892,"method":"Multi"}
2014-07-14 01:31:45,621 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14258,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326691363,"queuetimems":1,"class":"HRegionServer","responsesize":15698,"method":"Multi"}
2014-07-14 01:31:45,621 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11891,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326693730,"queuetimems":0,"class":"HRegionServer","responsesize":15834,"method":"Multi"}
2014-07-14 01:31:45,622 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12450,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326693172,"queuetimems":0,"class":"HRegionServer","responsesize":15874,"method":"Multi"}
2014-07-14 01:31:45,622 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14101,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326691521,"queuetimems":0,"class":"HRegionServer","responsesize":15501,"method":"Multi"}
2014-07-14 01:31:45,629 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13496,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326692132,"queuetimems":1,"class":"HRegionServer","responsesize":15869,"method":"Multi"}
2014-07-14 01:31:45,637 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12436,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326693201,"queuetimems":0,"class":"HRegionServer","responsesize":15883,"method":"Multi"}
2014-07-14 01:31:45,641 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14225,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326691415,"queuetimems":1,"class":"HRegionServer","responsesize":15751,"method":"Multi"}
2014-07-14 01:31:45,643 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12558,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326693085,"queuetimems":0,"class":"HRegionServer","responsesize":15962,"method":"Multi"}
2014-07-14 01:31:45,647 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11796,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326693850,"queuetimems":1,"class":"HRegionServer","responsesize":15765,"method":"Multi"}
2014-07-14 01:31:45,647 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13753,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326691894,"queuetimems":1,"class":"HRegionServer","responsesize":15696,"method":"Multi"}
2014-07-14 01:31:45,647 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11979,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326693668,"queuetimems":0,"class":"HRegionServer","responsesize":15744,"method":"Multi"}
2014-07-14 01:31:45,643 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12065,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326693578,"queuetimems":1,"class":"HRegionServer","responsesize":15702,"method":"Multi"}
2014-07-14 01:31:45,648 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13618,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326692030,"queuetimems":0,"class":"HRegionServer","responsesize":15834,"method":"Multi"}
2014-07-14 01:31:45,650 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12130,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326693519,"queuetimems":1,"class":"HRegionServer","responsesize":15736,"method":"Multi"}
2014-07-14 01:31:45,651 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13653,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326691997,"queuetimems":0,"class":"HRegionServer","responsesize":15657,"method":"Multi"}
2014-07-14 01:31:45,651 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11709,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326693942,"queuetimems":0,"class":"HRegionServer","responsesize":15723,"method":"Multi"}
2014-07-14 01:31:45,652 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11892,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326693759,"queuetimems":0,"class":"HRegionServer","responsesize":15674,"method":"Multi"}
2014-07-14 01:31:45,663 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11751,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326693911,"queuetimems":0,"class":"HRegionServer","responsesize":15962,"method":"Multi"}
2014-07-14 01:31:45,711 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14153,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326691558,"queuetimems":1,"class":"HRegionServer","responsesize":15674,"method":"Multi"}
2014-07-14 01:31:45,711 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13747,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326691964,"queuetimems":0,"class":"HRegionServer","responsesize":15736,"method":"Multi"}
2014-07-14 01:31:45,960 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:31:46,845 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52443 synced till here 52418
2014-07-14 01:31:47,099 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326704148 with entries=100, filesize=82.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326705960
2014-07-14 01:31:47,989 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:31:48,036 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52553 synced till here 52520
2014-07-14 01:31:48,986 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326705960 with entries=110, filesize=91.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326707989
2014-07-14 01:31:49,613 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:31:49,662 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52646 synced till here 52626
2014-07-14 01:31:49,848 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:31:49,853 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files, but is 1.0g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:31:49,853 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. due to global heap pressure
2014-07-14 01:31:49,853 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 1.0g
2014-07-14 01:31:49,858 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326707989 with entries=93, filesize=79.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326709613
2014-07-14 01:31:51,805 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:31:51,894 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52739 synced till here 52721
2014-07-14 01:31:52,205 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326709613 with entries=93, filesize=78.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326711805
2014-07-14 01:31:52,208 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:31:52,673 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:52,673 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:52,674 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:52,674 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:52,678 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,348 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,352 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,353 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,353 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,354 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,354 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,356 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,357 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,357 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,358 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,358 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,358 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,358 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,359 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,359 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,359 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,360 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,360 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,360 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,360 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,364 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,386 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,440 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,442 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,442 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,444 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,445 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,452 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,452 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,453 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,454 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,454 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,454 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,457 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,459 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,459 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,459 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,459 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,459 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,459 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,460 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,460 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,462 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,462 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:53,474 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:31:57,673 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:57,674 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:31:57,674 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:57,674 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:57,678 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:31:58,348 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:31:58,394 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5009ms
2014-07-14 01:31:58,395 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5031ms
2014-07-14 01:31:58,395 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5042ms
2014-07-14 01:31:58,396 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5036ms
2014-07-14 01:31:58,397 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5036ms
2014-07-14 01:31:58,397 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5049ms
2014-07-14 01:31:58,397 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5049ms
2014-07-14 01:31:58,397 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5048ms
2014-07-14 01:31:58,398 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5039ms
2014-07-14 01:31:58,398 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5048ms
2014-07-14 01:31:58,399 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5048ms
2014-07-14 01:31:58,399 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5048ms
2014-07-14 01:31:58,400 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5048ms
2014-07-14 01:31:58,400 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5043ms
2014-07-14 01:31:58,400 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5043ms
2014-07-14 01:31:58,400 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5044ms
2014-07-14 01:31:58,400 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5046ms
2014-07-14 01:31:58,401 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5047ms
2014-07-14 01:31:58,401 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5049ms
2014-07-14 01:31:58,401 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5049ms
2014-07-14 01:31:58,401 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5053ms
2014-07-14 01:31:58,441 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:31:58,442 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:58,442 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:58,444 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:58,445 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:58,452 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:58,452 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:58,454 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:31:58,454 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:58,454 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:58,455 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:58,457 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:31:58,459 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:31:58,459 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:58,460 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:58,460 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5003ms
2014-07-14 01:31:58,460 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:31:58,460 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:31:58,461 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:31:58,461 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:31:58,462 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:31:58,462 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:31:58,475 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:32:00,913 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13036, memsize=607.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/1392d596d97d4b06a5708f39abd8772f
2014-07-14 01:32:00,927 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/1392d596d97d4b06a5708f39abd8772f as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/1392d596d97d4b06a5708f39abd8772f
2014-07-14 01:32:00,936 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/1392d596d97d4b06a5708f39abd8772f, entries=2210540, sequenceid=13036, filesize=157.3m
2014-07-14 01:32:00,937 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.2g/1239786240, currentsize=249.7m/261832320 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 32147ms, sequenceid=13036, compaction requested=true
2014-07-14 01:32:00,937 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:32:00,937 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 20 blocking
2014-07-14 01:32:00,938 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-14 01:32:00,938 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:32:00,938 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:32:00,938 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7464ms
2014-07-14 01:32:00,938 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:32:00,938 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:00,938 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7476ms
2014-07-14 01:32:00,938 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:00,939 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7477ms
2014-07-14 01:32:00,939 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:00,939 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7479ms
2014-07-14 01:32:00,939 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:00,939 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7480ms
2014-07-14 01:32:00,939 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:00,940 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7480ms
2014-07-14 01:32:00,940 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:00,945 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7486ms
2014-07-14 01:32:00,945 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:00,946 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7489ms
2014-07-14 01:32:01,005 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,005 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7546ms
2014-07-14 01:32:01,006 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,006 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7547ms
2014-07-14 01:32:01,006 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,007 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7548ms
2014-07-14 01:32:01,007 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,014 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7558ms
2014-07-14 01:32:01,014 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,014 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7560ms
2014-07-14 01:32:01,014 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,015 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7561ms
2014-07-14 01:32:01,015 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,025 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7571ms
2014-07-14 01:32:01,025 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,026 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7572ms
2014-07-14 01:32:01,026 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,026 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7574ms
2014-07-14 01:32:01,026 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,026 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7574ms
2014-07-14 01:32:01,026 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,026 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7581ms
2014-07-14 01:32:01,027 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,027 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7583ms
2014-07-14 01:32:01,027 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,028 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7585ms
2014-07-14 01:32:01,028 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,029 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7586ms
2014-07-14 01:32:01,029 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,029 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7589ms
2014-07-14 01:32:01,029 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,030 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7681ms
2014-07-14 01:32:01,030 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,030 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7678ms
2014-07-14 01:32:01,031 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,031 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7679ms
2014-07-14 01:32:01,031 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,033 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7678ms
2014-07-14 01:32:01,033 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,033 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7679ms
2014-07-14 01:32:01,033 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,034 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7677ms
2014-07-14 01:32:01,034 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,034 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7677ms
2014-07-14 01:32:01,035 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,035 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7678ms
2014-07-14 01:32:01,035 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,035 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7684ms
2014-07-14 01:32:01,035 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,037 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7684ms
2014-07-14 01:32:01,037 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,037 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7687ms
2014-07-14 01:32:01,037 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,038 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7687ms
2014-07-14 01:32:01,038 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,039 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7679ms
2014-07-14 01:32:01,039 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,039 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7690ms
2014-07-14 01:32:01,039 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,041 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7692ms
2014-07-14 01:32:01,041 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,046 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7697ms
2014-07-14 01:32:01,046 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,046 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7686ms
2014-07-14 01:32:01,047 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,047 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7688ms
2014-07-14 01:32:01,047 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,048 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7694ms
2014-07-14 01:32:01,048 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,049 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7684ms
2014-07-14 01:32:01,049 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,050 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7664ms
2014-07-14 01:32:01,050 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,050 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7703ms
2014-07-14 01:32:01,050 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,051 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8373ms
2014-07-14 01:32:01,051 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,057 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8383ms
2014-07-14 01:32:01,057 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,058 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8384ms
2014-07-14 01:32:01,058 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,059 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8385ms
2014-07-14 01:32:01,059 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,065 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8391ms
2014-07-14 01:32:01,065 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:01,277 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11866,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709411,"queuetimems":3751,"class":"HRegionServer","responsesize":16088,"method":"Multi"}
2014-07-14 01:32:01,285 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11657,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709628,"queuetimems":493,"class":"HRegionServer","responsesize":15637,"method":"Multi"}
2014-07-14 01:32:01,285 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11915,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709370,"queuetimems":4083,"class":"HRegionServer","responsesize":15556,"method":"Multi"}
2014-07-14 01:32:01,289 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11731,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709558,"queuetimems":570,"class":"HRegionServer","responsesize":15406,"method":"Multi"}
2014-07-14 01:32:01,401 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:32:01,401 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files; delaying flush up to 90000ms
2014-07-14 01:32:01,401 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:32:01,401 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 20 blocking
2014-07-14 01:32:01,402 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-14 01:32:01,402 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10088,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326711313,"queuetimems":2046,"class":"HRegionServer","responsesize":15565,"method":"Multi"}
2014-07-14 01:32:01,402 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:32:01,402 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:32:01,402 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:32:01,404 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:32:01,492 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12112,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709380,"queuetimems":4056,"class":"HRegionServer","responsesize":15723,"method":"Multi"}
2014-07-14 01:32:01,492 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52851 synced till here 52825
2014-07-14 01:32:01,671 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12265,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709406,"queuetimems":3829,"class":"HRegionServer","responsesize":15673,"method":"Multi"}
2014-07-14 01:32:01,673 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12302,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709370,"queuetimems":4115,"class":"HRegionServer","responsesize":15962,"method":"Multi"}
2014-07-14 01:32:01,695 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12068,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709626,"queuetimems":520,"class":"HRegionServer","responsesize":15950,"method":"Multi"}
2014-07-14 01:32:01,701 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12075,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709626,"queuetimems":550,"class":"HRegionServer","responsesize":15556,"method":"Multi"}
2014-07-14 01:32:01,709 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12303,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709406,"queuetimems":3860,"class":"HRegionServer","responsesize":15883,"method":"Multi"}
2014-07-14 01:32:01,709 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12169,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709540,"queuetimems":3656,"class":"HRegionServer","responsesize":15637,"method":"Multi"}
2014-07-14 01:32:01,717 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12350,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709367,"queuetimems":4216,"class":"HRegionServer","responsesize":15928,"method":"Multi"}
2014-07-14 01:32:01,717 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12178,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709539,"queuetimems":3769,"class":"HRegionServer","responsesize":15874,"method":"Multi"}
2014-07-14 01:32:01,725 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12359,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709366,"queuetimems":4294,"class":"HRegionServer","responsesize":15657,"method":"Multi"}
2014-07-14 01:32:01,730 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326711805 with entries=112, filesize=93.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326721404
2014-07-14 01:32:01,730 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326596655
2014-07-14 01:32:01,730 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326597955
2014-07-14 01:32:01,730 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326599824
2014-07-14 01:32:01,730 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326628541
2014-07-14 01:32:01,730 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326630035
2014-07-14 01:32:01,730 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326631595
2014-07-14 01:32:01,730 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326633478
2014-07-14 01:32:01,730 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326635038
2014-07-14 01:32:01,731 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326636032
2014-07-14 01:32:01,735 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12337,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709398,"queuetimems":3911,"class":"HRegionServer","responsesize":15754,"method":"Multi"}
2014-07-14 01:32:01,735 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12190,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709545,"queuetimems":2681,"class":"HRegionServer","responsesize":15805,"method":"Multi"}
2014-07-14 01:32:01,735 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12368,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709367,"queuetimems":4280,"class":"HRegionServer","responsesize":15702,"method":"Multi"}
2014-07-14 01:32:01,735 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12338,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709397,"queuetimems":3940,"class":"HRegionServer","responsesize":15674,"method":"Multi"}
2014-07-14 01:32:01,736 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12351,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709385,"queuetimems":4021,"class":"HRegionServer","responsesize":15736,"method":"Multi"}
2014-07-14 01:32:01,736 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11889,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709847,"queuetimems":641,"class":"HRegionServer","responsesize":15805,"method":"Multi"}
2014-07-14 01:32:01,737 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10430,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326711307,"queuetimems":2069,"class":"HRegionServer","responsesize":15992,"method":"Multi"}
2014-07-14 01:32:01,749 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12358,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709391,"queuetimems":3967,"class":"HRegionServer","responsesize":15744,"method":"Multi"}
2014-07-14 01:32:01,749 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12201,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709548,"queuetimems":1464,"class":"HRegionServer","responsesize":15960,"method":"Multi"}
2014-07-14 01:32:01,763 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12351,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709411,"queuetimems":3766,"class":"HRegionServer","responsesize":15834,"method":"Multi"}
2014-07-14 01:32:01,763 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12357,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709405,"queuetimems":3888,"class":"HRegionServer","responsesize":15892,"method":"Multi"}
2014-07-14 01:32:01,763 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11915,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709847,"queuetimems":682,"class":"HRegionServer","responsesize":15869,"method":"Multi"}
2014-07-14 01:32:01,849 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12292,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709557,"queuetimems":587,"class":"HRegionServer","responsesize":15723,"method":"Multi"}
2014-07-14 01:32:01,850 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12297,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709553,"queuetimems":1294,"class":"HRegionServer","responsesize":15962,"method":"Multi"}
2014-07-14 01:32:01,849 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12224,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709625,"queuetimems":581,"class":"HRegionServer","responsesize":15702,"method":"Multi"}
2014-07-14 01:32:02,025 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12485,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709540,"queuetimems":3711,"class":"HRegionServer","responsesize":15950,"method":"Multi"}
2014-07-14 01:32:02,025 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12473,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709552,"queuetimems":1351,"class":"HRegionServer","responsesize":15928,"method":"Multi"}
2014-07-14 01:32:02,025 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12480,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709545,"queuetimems":2692,"class":"HRegionServer","responsesize":15992,"method":"Multi"}
2014-07-14 01:32:03,429 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13878,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326709551,"queuetimems":1390,"class":"HRegionServer","responsesize":15892,"method":"Multi"}
2014-07-14 01:32:03,616 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:32:03,839 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52965 synced till here 52929
2014-07-14 01:32:03,861 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11196,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326712665,"queuetimems":3203,"class":"HRegionServer","responsesize":15883,"method":"Multi"}
2014-07-14 01:32:04,076 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11402,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326712673,"queuetimems":1125,"class":"HRegionServer","responsesize":15536,"method":"Multi"}
2014-07-14 01:32:04,249 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326721404 with entries=114, filesize=97.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326723617
2014-07-14 01:32:04,512 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11057,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326713454,"queuetimems":41,"class":"HRegionServer","responsesize":15673,"method":"Multi"}
2014-07-14 01:32:04,618 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11236,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326713381,"queuetimems":1,"class":"HRegionServer","responsesize":15960,"method":"Multi"}
2014-07-14 01:32:04,618 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11951,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326712666,"queuetimems":3167,"class":"HRegionServer","responsesize":15754,"method":"Multi"}
2014-07-14 01:32:04,618 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11959,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326712658,"queuetimems":3257,"class":"HRegionServer","responsesize":15834,"method":"Multi"}
2014-07-14 01:32:04,618 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11956,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326712661,"queuetimems":3235,"class":"HRegionServer","responsesize":15744,"method":"Multi"}
2014-07-14 01:32:04,619 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11948,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326712670,"queuetimems":3095,"class":"HRegionServer","responsesize":15657,"method":"Multi"}
2014-07-14 01:32:04,619 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11948,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326712671,"queuetimems":3069,"class":"HRegionServer","responsesize":15696,"method":"Multi"}
2014-07-14 01:32:04,618 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11947,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326712670,"queuetimems":3138,"class":"HRegionServer","responsesize":15736,"method":"Multi"}
2014-07-14 01:32:04,619 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11161,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326713457,"queuetimems":15,"class":"HRegionServer","responsesize":15805,"method":"Multi"}
2014-07-14 01:32:04,672 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12000,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326712671,"queuetimems":1173,"class":"HRegionServer","responsesize":15628,"method":"Multi"}
2014-07-14 01:32:05,610 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:32:05,624 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53066 synced till here 53049
2014-07-14 01:32:05,732 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326723617 with entries=101, filesize=85.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326725610
2014-07-14 01:32:08,531 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:32:08,596 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326725610 with entries=72, filesize=61.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326728533
2014-07-14 01:32:10,652 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:32:10,673 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53211 synced till here 53210
2014-07-14 01:32:10,696 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326728533 with entries=73, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326730652
2014-07-14 01:32:12,112 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:32:12,315 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53302 synced till here 53299
2014-07-14 01:32:12,362 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326730652 with entries=91, filesize=78.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326732113
2014-07-14 01:32:12,932 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90057ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:32:12,933 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 1.1g
2014-07-14 01:32:13,689 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:32:13,717 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53379 synced till here 53375
2014-07-14 01:32:13,773 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326732113 with entries=77, filesize=66.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326733689
2014-07-14 01:32:14,063 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:32:15,328 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:32:15,350 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53457 synced till here 53451
2014-07-14 01:32:15,406 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326733689 with entries=78, filesize=66.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326735329
2014-07-14 01:32:16,578 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:32:16,584 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:16,587 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:16,593 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:16,594 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:16,596 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53538 synced till here 53529
2014-07-14 01:32:16,618 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:16,672 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:16,682 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:16,682 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:16,682 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:16,683 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:16,683 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:16,687 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:16,691 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326735329 with entries=81, filesize=69.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326736578
2014-07-14 01:32:16,724 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:16,728 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:16,745 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:16,781 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:16,837 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:16,883 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:16,940 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:16,985 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:17,054 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:17,102 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:17,139 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:17,189 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:17,294 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:17,624 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:17,730 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:18,153 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:18,853 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:18,884 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:18,893 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13184, memsize=569.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/e70b70c27ade4011ba6538a8995bd037
2014-07-14 01:32:18,905 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/e70b70c27ade4011ba6538a8995bd037 as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/e70b70c27ade4011ba6538a8995bd037
2014-07-14 01:32:18,915 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/e70b70c27ade4011ba6538a8995bd037, entries=2074870, sequenceid=13184, filesize=147.7m
2014-07-14 01:32:18,915 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.0g/1084139280, currentsize=347.4m/364301440 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 29062ms, sequenceid=13184, compaction requested=true
2014-07-14 01:32:18,916 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:32:18,916 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 32 store files, 0 compacting, 32 eligible, 20 blocking
2014-07-14 01:32:18,916 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32ms
2014-07-14 01:32:18,916 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,916 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 32 files from compaction candidates
2014-07-14 01:32:18,916 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 64ms
2014-07-14 01:32:18,916 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,916 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:32:18,917 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 764ms
2014-07-14 01:32:18,917 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:32:18,917 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,917 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:32:18,917 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1188ms
2014-07-14 01:32:18,917 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,917 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1293ms
2014-07-14 01:32:18,917 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,918 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1624ms
2014-07-14 01:32:18,918 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,918 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1729ms
2014-07-14 01:32:18,918 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,921 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1782ms
2014-07-14 01:32:18,921 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,928 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1826ms
2014-07-14 01:32:18,928 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,928 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1874ms
2014-07-14 01:32:18,928 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,929 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1944ms
2014-07-14 01:32:18,929 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,929 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1989ms
2014-07-14 01:32:18,930 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,930 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2047ms
2014-07-14 01:32:18,930 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,930 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2093ms
2014-07-14 01:32:18,930 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,933 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2152ms
2014-07-14 01:32:18,933 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,933 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2188ms
2014-07-14 01:32:18,933 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,933 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2205ms
2014-07-14 01:32:18,933 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,933 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2209ms
2014-07-14 01:32:18,934 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,934 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2248ms
2014-07-14 01:32:18,934 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,934 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2251ms
2014-07-14 01:32:18,934 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,934 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2252ms
2014-07-14 01:32:18,934 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,937 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2255ms
2014-07-14 01:32:18,937 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,941 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2259ms
2014-07-14 01:32:18,941 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,944 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2263ms
2014-07-14 01:32:18,944 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,945 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2274ms
2014-07-14 01:32:18,945 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,945 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2327ms
2014-07-14 01:32:18,945 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,945 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2351ms
2014-07-14 01:32:18,945 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,945 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2352ms
2014-07-14 01:32:18,945 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,945 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2358ms
2014-07-14 01:32:18,945 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:18,945 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2361ms
2014-07-14 01:32:18,946 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:19,046 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:32:19,046 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files; delaying flush up to 90000ms
2014-07-14 01:32:19,047 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 32 store files, 0 compacting, 32 eligible, 20 blocking
2014-07-14 01:32:19,047 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 32 files from compaction candidates
2014-07-14 01:32:19,047 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:32:19,047 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:32:19,048 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:32:19,048 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:32:20,872 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:32:20,916 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53649 synced till here 53618
2014-07-14 01:32:21,941 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326736578 with entries=111, filesize=95.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326740872
2014-07-14 01:32:21,942 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326643713
2014-07-14 01:32:21,942 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326644910
2014-07-14 01:32:21,942 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326646309
2014-07-14 01:32:21,942 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326647611
2014-07-14 01:32:21,942 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326648325
2014-07-14 01:32:22,723 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:32:22,794 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53749 synced till here 53725
2014-07-14 01:32:23,954 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326740872 with entries=100, filesize=85.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326742723
2014-07-14 01:32:24,934 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:32:25,056 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53842 synced till here 53839
2014-07-14 01:32:25,100 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326742723 with entries=93, filesize=80.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326744934
2014-07-14 01:32:26,976 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:32:26,999 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53933 synced till here 53923
2014-07-14 01:32:27,084 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326744934 with entries=91, filesize=78.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326746976
2014-07-14 01:32:28,558 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:32:28,584 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54012 synced till here 54005
2014-07-14 01:32:28,659 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326746976 with entries=79, filesize=68.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326748558
2014-07-14 01:32:30,099 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:32:30,100 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files, but is 1.0g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:32:30,100 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. due to global heap pressure
2014-07-14 01:32:30,100 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 1.0g
2014-07-14 01:32:30,386 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:32:30,434 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54088 synced till here 54086
2014-07-14 01:32:30,479 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326748558 with entries=76, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326750386
2014-07-14 01:32:31,169 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:32:31,857 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:32:32,008 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54175 synced till here 54162
2014-07-14 01:32:32,112 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326750386 with entries=87, filesize=74.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326751859
2014-07-14 01:32:32,321 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:32,323 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:32,335 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:32,379 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:32,398 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:32,398 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:32,398 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:32,402 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:32,419 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:32,477 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:32,570 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:32,641 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:32,705 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:32,757 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:32,815 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:32,881 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:32,920 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:32,964 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:32,997 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:33,123 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:33,162 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:33,411 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:33,720 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:34,593 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:34,623 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:34,660 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:34,696 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:34,736 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:35,533 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:35,543 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:35,556 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:35,584 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:36,335 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:36,371 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:36,404 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:36,440 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:36,475 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:36,508 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:36,550 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:36,591 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:36,641 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:36,678 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:36,716 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:36,766 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:36,805 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:36,848 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:36,890 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:36,943 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:36,984 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:37,031 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:32:37,322 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:32:37,323 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:32:37,335 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:32:37,379 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:32:37,398 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:32:37,398 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:32:37,399 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:32:37,402 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:32:37,419 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:32:37,478 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:32:37,570 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:32:37,641 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:32:37,706 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:32:37,757 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:32:37,815 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:32:37,882 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:32:37,920 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:32:37,965 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:32:37,998 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:32:38,123 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:32:38,162 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:32:38,411 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:32:38,721 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:32:39,593 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:32:39,623 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:32:39,660 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:32:39,697 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:32:39,737 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:32:40,534 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:32:40,543 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:32:40,556 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:32:40,585 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:32:41,335 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:32:41,372 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:32:41,404 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:32:41,438 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13367, memsize=472.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/09997fea27ee4ca69ba813d8d9e16ab0
2014-07-14 01:32:41,440 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:32:42,324 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:32:42,324 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5774ms
2014-07-14 01:32:42,324 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5733ms
2014-07-14 01:32:42,324 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5683ms
2014-07-14 01:32:42,325 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5646ms
2014-07-14 01:32:42,325 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5609ms
2014-07-14 01:32:42,325 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5559ms
2014-07-14 01:32:42,325 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5520ms
2014-07-14 01:32:42,325 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5477ms
2014-07-14 01:32:42,326 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5435ms
2014-07-14 01:32:42,326 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5383ms
2014-07-14 01:32:42,326 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5343ms
2014-07-14 01:32:42,327 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5296ms
2014-07-14 01:32:42,327 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10004ms
2014-07-14 01:32:42,327 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5852ms
2014-07-14 01:32:42,327 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5819ms
2014-07-14 01:32:42,335 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:32:42,339 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/09997fea27ee4ca69ba813d8d9e16ab0 as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/09997fea27ee4ca69ba813d8d9e16ab0
2014-07-14 01:32:42,360 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/09997fea27ee4ca69ba813d8d9e16ab0, entries=1718560, sequenceid=13367, filesize=122.4m
2014-07-14 01:32:42,360 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.1g/1225725760, currentsize=340.5m/357022080 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 29427ms, sequenceid=13367, compaction requested=true
2014-07-14 01:32:42,362 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:32:42,362 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 36 store files, 0 compacting, 36 eligible, 20 blocking
2014-07-14 01:32:42,362 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10027ms
2014-07-14 01:32:42,362 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,362 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 36 files from compaction candidates
2014-07-14 01:32:42,362 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5854ms
2014-07-14 01:32:42,362 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:32:42,362 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,362 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:32:42,362 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5887ms
2014-07-14 01:32:42,362 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:32:42,362 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,363 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10040ms
2014-07-14 01:32:42,363 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,363 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5332ms
2014-07-14 01:32:42,363 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,363 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5380ms
2014-07-14 01:32:42,363 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,363 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5420ms
2014-07-14 01:32:42,363 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,363 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5473ms
2014-07-14 01:32:42,363 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,363 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5515ms
2014-07-14 01:32:42,363 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,363 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5558ms
2014-07-14 01:32:42,364 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,364 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5598ms
2014-07-14 01:32:42,364 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,364 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5648ms
2014-07-14 01:32:42,364 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,364 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5686ms
2014-07-14 01:32:42,364 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,364 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5723ms
2014-07-14 01:32:42,364 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,364 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5773ms
2014-07-14 01:32:42,364 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,364 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5814ms
2014-07-14 01:32:42,364 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,365 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10044ms
2014-07-14 01:32:42,365 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,365 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5925ms
2014-07-14 01:32:42,365 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,367 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5963ms
2014-07-14 01:32:42,367 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,367 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5996ms
2014-07-14 01:32:42,367 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,369 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6034ms
2014-07-14 01:32:42,369 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,371 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6787ms
2014-07-14 01:32:42,371 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,371 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6815ms
2014-07-14 01:32:42,371 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,371 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6828ms
2014-07-14 01:32:42,371 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,374 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6841ms
2014-07-14 01:32:42,374 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,374 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7638ms
2014-07-14 01:32:42,374 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,374 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7678ms
2014-07-14 01:32:42,374 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,377 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7717ms
2014-07-14 01:32:42,377 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,378 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7755ms
2014-07-14 01:32:42,378 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,380 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7788ms
2014-07-14 01:32:42,381 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,381 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8661ms
2014-07-14 01:32:42,381 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,381 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8970ms
2014-07-14 01:32:42,381 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,381 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9219ms
2014-07-14 01:32:42,381 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,381 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9258ms
2014-07-14 01:32:42,382 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,382 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9385ms
2014-07-14 01:32:42,382 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,382 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9418ms
2014-07-14 01:32:42,382 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,382 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9462ms
2014-07-14 01:32:42,382 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,382 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9501ms
2014-07-14 01:32:42,382 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,385 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9570ms
2014-07-14 01:32:42,385 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,385 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9629ms
2014-07-14 01:32:42,385 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,386 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9681ms
2014-07-14 01:32:42,386 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,386 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9745ms
2014-07-14 01:32:42,387 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,387 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9818ms
2014-07-14 01:32:42,387 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,387 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9910ms
2014-07-14 01:32:42,387 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,388 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9968ms
2014-07-14 01:32:42,388 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,389 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9986ms
2014-07-14 01:32:42,389 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,390 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9991ms
2014-07-14 01:32:42,390 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,391 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9992ms
2014-07-14 01:32:42,391 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,391 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9993ms
2014-07-14 01:32:42,391 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,393 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10014ms
2014-07-14 01:32:42,393 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:32:42,477 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:32:42,477 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files; delaying flush up to 90000ms
2014-07-14 01:32:42,480 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:32:42,480 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 36 store files, 0 compacting, 36 eligible, 20 blocking
2014-07-14 01:32:42,480 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 36 files from compaction candidates
2014-07-14 01:32:42,480 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:32:42,480 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:32:42,480 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:32:42,566 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10601,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326751964,"queuetimems":1,"class":"HRegionServer","responsesize":15709,"method":"Multi"}
2014-07-14 01:32:42,699 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10775,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326751923,"queuetimems":0,"class":"HRegionServer","responsesize":15985,"method":"Multi"}
2014-07-14 01:32:42,699 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10675,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326752023,"queuetimems":0,"class":"HRegionServer","responsesize":16128,"method":"Multi"}
2014-07-14 01:32:42,699 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10596,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326752102,"queuetimems":1,"class":"HRegionServer","responsesize":15508,"method":"Multi"}
2014-07-14 01:32:43,032 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:32:43,098 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10919,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326752179,"queuetimems":1,"class":"HRegionServer","responsesize":15889,"method":"Multi"}
2014-07-14 01:32:43,300 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54275 synced till here 54273
2014-07-14 01:32:43,374 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326751859 with entries=100, filesize=83.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326763033
2014-07-14 01:32:43,375 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326649665
2014-07-14 01:32:43,377 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326651006
2014-07-14 01:32:43,377 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326652430
2014-07-14 01:32:43,378 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326655437
2014-07-14 01:32:43,378 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326657640
2014-07-14 01:32:43,378 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326659049
2014-07-14 01:32:43,378 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326660927
2014-07-14 01:32:43,378 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326662366
2014-07-14 01:32:43,378 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326663571
2014-07-14 01:32:43,378 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326664964
2014-07-14 01:32:43,378 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326666702
2014-07-14 01:32:43,378 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326668720
2014-07-14 01:32:43,599 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11360,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326752239,"queuetimems":1,"class":"HRegionServer","responsesize":16174,"method":"Multi"}
2014-07-14 01:32:45,008 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12690,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326752317,"queuetimems":0,"class":"HRegionServer","responsesize":16081,"method":"Multi"}
2014-07-14 01:32:45,076 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:32:45,153 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54385 synced till here 54349
2014-07-14 01:32:45,318 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12157,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326753160,"queuetimems":0,"class":"HRegionServer","responsesize":15993,"method":"Multi"}
2014-07-14 01:32:45,360 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326763033 with entries=110, filesize=92.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326765076
2014-07-14 01:32:46,665 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13964,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326752700,"queuetimems":1,"class":"HRegionServer","responsesize":15856,"method":"Multi"}
2014-07-14 01:32:46,665 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10300,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326756364,"queuetimems":0,"class":"HRegionServer","responsesize":15869,"method":"Multi"}
2014-07-14 01:32:46,665 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10332,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326756333,"queuetimems":1,"class":"HRegionServer","responsesize":16088,"method":"Multi"}
2014-07-14 01:32:46,665 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10266,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326756399,"queuetimems":1,"class":"HRegionServer","responsesize":15673,"method":"Multi"}
2014-07-14 01:32:46,665 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10120,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326756545,"queuetimems":0,"class":"HRegionServer","responsesize":15992,"method":"Multi"}
2014-07-14 01:32:46,681 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13763,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326752917,"queuetimems":1,"class":"HRegionServer","responsesize":15688,"method":"Multi"}
2014-07-14 01:32:46,681 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13722,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326752959,"queuetimems":0,"class":"HRegionServer","responsesize":15943,"method":"Multi"}
2014-07-14 01:32:46,689 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12971,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326753718,"queuetimems":0,"class":"HRegionServer","responsesize":15857,"method":"Multi"}
2014-07-14 01:32:46,689 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14122,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326752567,"queuetimems":1,"class":"HRegionServer","responsesize":15820,"method":"Multi"}
2014-07-14 01:32:46,689 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10015,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326756674,"queuetimems":1,"class":"HRegionServer","responsesize":15820,"method":"Multi"}
2014-07-14 01:32:46,689 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10102,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326756587,"queuetimems":0,"class":"HRegionServer","responsesize":15985,"method":"Multi"}
2014-07-14 01:32:46,689 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11956,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326754733,"queuetimems":0,"class":"HRegionServer","responsesize":15724,"method":"Multi"}
2014-07-14 01:32:46,701 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13952,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326752749,"queuetimems":0,"class":"HRegionServer","responsesize":15681,"method":"Multi"}
2014-07-14 01:32:46,701 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12082,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326754619,"queuetimems":0,"class":"HRegionServer","responsesize":15508,"method":"Multi"}
2014-07-14 01:32:46,705 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11174,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326755531,"queuetimems":1,"class":"HRegionServer","responsesize":15751,"method":"Multi"}
2014-07-14 01:32:46,706 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12052,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326754654,"queuetimems":0,"class":"HRegionServer","responsesize":15709,"method":"Multi"}
2014-07-14 01:32:46,713 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14299,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326752414,"queuetimems":0,"class":"HRegionServer","responsesize":15751,"method":"Multi"}
2014-07-14 01:32:46,701 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14325,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326752376,"queuetimems":1,"class":"HRegionServer","responsesize":15637,"method":"Multi"}
2014-07-14 01:32:46,706 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11165,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326755541,"queuetimems":0,"class":"HRegionServer","responsesize":15820,"method":"Multi"}
2014-07-14 01:32:46,721 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13727,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326752994,"queuetimems":0,"class":"HRegionServer","responsesize":15820,"method":"Multi"}
2014-07-14 01:32:46,725 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12036,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326754689,"queuetimems":0,"class":"HRegionServer","responsesize":15889,"method":"Multi"}
2014-07-14 01:32:46,725 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13316,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326753409,"queuetimems":0,"class":"HRegionServer","responsesize":15622,"method":"Multi"}
2014-07-14 01:32:46,705 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13893,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326752812,"queuetimems":0,"class":"HRegionServer","responsesize":15635,"method":"Multi"}
2014-07-14 01:32:46,721 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13850,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326752871,"queuetimems":1,"class":"HRegionServer","responsesize":15696,"method":"Multi"}
2014-07-14 01:32:46,728 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13609,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326753119,"queuetimems":0,"class":"HRegionServer","responsesize":15628,"method":"Multi"}
2014-07-14 01:32:46,721 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11167,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326755554,"queuetimems":1,"class":"HRegionServer","responsesize":15378,"method":"Multi"}
2014-07-14 01:32:46,729 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12144,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326754585,"queuetimems":1,"class":"HRegionServer","responsesize":16128,"method":"Multi"}
2014-07-14 01:32:46,729 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14255,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326752474,"queuetimems":0,"class":"HRegionServer","responsesize":15378,"method":"Multi"}
2014-07-14 01:32:46,756 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13574, memsize=249.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/c180be88e7fa4f44958ab1eafa5452d8
2014-07-14 01:32:46,790 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/c180be88e7fa4f44958ab1eafa5452d8 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/c180be88e7fa4f44958ab1eafa5452d8
2014-07-14 01:32:46,809 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/c180be88e7fa4f44958ab1eafa5452d8, entries=907220, sequenceid=13574, filesize=64.6m
2014-07-14 01:32:46,810 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.1g/1133141840, currentsize=139.2m/145984560 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 16710ms, sequenceid=13574, compaction requested=true
2014-07-14 01:32:46,810 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:32:46,810 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 34 store files, 0 compacting, 34 eligible, 20 blocking
2014-07-14 01:32:46,811 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 34 files from compaction candidates
2014-07-14 01:32:46,811 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:32:46,811 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:32:46,811 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:32:46,831 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:32:46,857 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54485 synced till here 54462
2014-07-14 01:32:47,162 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326765076 with entries=100, filesize=81.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326766832
2014-07-14 01:32:47,162 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326670519
2014-07-14 01:32:47,162 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326672161
2014-07-14 01:32:47,162 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326673633
2014-07-14 01:32:47,162 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326677389
2014-07-14 01:32:47,162 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326679186
2014-07-14 01:32:47,162 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326681427
2014-07-14 01:32:47,162 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326683160
2014-07-14 01:32:47,162 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326685533
2014-07-14 01:32:47,212 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11629,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326755582,"queuetimems":0,"class":"HRegionServer","responsesize":15637,"method":"Multi"}
2014-07-14 01:32:47,212 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14574,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326752637,"queuetimems":0,"class":"HRegionServer","responsesize":15724,"method":"Multi"}
2014-07-14 01:32:48,815 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:32:49,005 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54604 synced till here 54597
2014-07-14 01:32:49,119 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326766832 with entries=119, filesize=98.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326768815
2014-07-14 01:32:49,806 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:32:49,819 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54683 synced till here 54678
2014-07-14 01:32:49,873 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326768815 with entries=79, filesize=66.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326769807
2014-07-14 01:32:51,939 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:32:51,970 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:32:51,970 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files; delaying flush up to 90000ms
2014-07-14 01:32:51,971 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:32:51,971 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 34 store files, 0 compacting, 34 eligible, 20 blocking
2014-07-14 01:32:51,971 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 34 files from compaction candidates
2014-07-14 01:32:51,971 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:32:51,971 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:32:51,971 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:32:52,653 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54786 synced till here 54779
2014-07-14 01:32:52,721 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326769807 with entries=103, filesize=88.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326771939
2014-07-14 01:32:53,386 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:32:54,408 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54882 synced till here 54869
2014-07-14 01:32:54,492 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326771939 with entries=96, filesize=82.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326773386
2014-07-14 01:32:55,315 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:32:56,315 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54978 synced till here 54971
2014-07-14 01:32:56,391 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326773386 with entries=96, filesize=82.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326775316
2014-07-14 01:32:57,271 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:32:57,337 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55059 synced till here 55049
2014-07-14 01:32:57,432 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326775316 with entries=81, filesize=69.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326777272
2014-07-14 01:32:57,432 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:32:59,197 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:32:59,643 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55168 synced till here 55167
2014-07-14 01:32:59,701 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326777272 with entries=109, filesize=93.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326779198
2014-07-14 01:32:59,702 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:33:01,035 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:33:01,081 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55243 synced till here 55240
2014-07-14 01:33:01,124 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326779198 with entries=75, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326781036
2014-07-14 01:33:01,126 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:33:02,653 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:33:02,671 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55319 synced till here 55315
2014-07-14 01:33:02,712 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326781036 with entries=76, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326782654
2014-07-14 01:33:02,713 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:33:04,134 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:33:04,170 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55402 synced till here 55397
2014-07-14 01:33:04,219 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326782654 with entries=83, filesize=71.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326784134
2014-07-14 01:33:04,220 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:33:06,301 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:33:06,330 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55475 synced till here 55474
2014-07-14 01:33:06,370 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326784134 with entries=73, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326786301
2014-07-14 01:33:06,370 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72
2014-07-14 01:33:06,875 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:33:06,876 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files, but is 1.3g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:33:06,876 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. due to global heap pressure
2014-07-14 01:33:06,876 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 1.3g
2014-07-14 01:33:06,979 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:33:06,980 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files, but is 1.1g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:33:06,980 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. due to global heap pressure
2014-07-14 01:33:06,980 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 1.1g
2014-07-14 01:33:07,115 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:33:07,320 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55566 synced till here 55564
2014-07-14 01:33:07,360 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326786301 with entries=91, filesize=77.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326787115
2014-07-14 01:33:08,808 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:33:08,858 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:33:08,951 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:33:09,051 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55662 synced till here 55654
2014-07-14 01:33:09,068 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:09,068 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:09,071 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:09,071 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:09,075 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:09,187 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326787115 with entries=96, filesize=82.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326788951
2014-07-14 01:33:09,204 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:09,235 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:09,236 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:09,237 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:09,237 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:09,238 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:09,241 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:09,251 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:09,267 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:10,168 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:10,182 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:10,197 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:10,222 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:10,252 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:10,283 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:10,316 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:11,267 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:11,311 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:11,354 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:11,396 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:11,435 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:11,473 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:11,514 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:11,563 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:11,601 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:11,650 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:11,682 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:11,716 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:11,754 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:11,791 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:11,826 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:11,878 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:13,232 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:13,267 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:13,308 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:13,345 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:13,380 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:14,068 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:14,069 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:33:14,071 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:14,072 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:14,075 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:14,205 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:14,235 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:14,236 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:14,237 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-14 01:33:14,237 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:33:14,238 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:14,241 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:14,252 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:14,267 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:15,168 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:15,182 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:15,197 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:33:15,222 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:33:15,252 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:15,283 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:33:15,316 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:15,391 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:15,444 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:15,482 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:15,524 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:15,559 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:15,602 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:15,640 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:15,689 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:16,636 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5035ms
2014-07-14 01:33:16,637 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5370ms
2014-07-14 01:33:16,637 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5326ms
2014-07-14 01:33:16,637 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5283ms
2014-07-14 01:33:16,637 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5241ms
2014-07-14 01:33:16,638 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5203ms
2014-07-14 01:33:16,639 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5165ms
2014-07-14 01:33:16,639 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5125ms
2014-07-14 01:33:16,639 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5076ms
2014-07-14 01:33:16,651 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:16,683 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:33:16,717 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:33:16,755 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:33:16,792 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:16,827 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:33:16,878 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:18,232 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:18,267 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:18,309 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:18,346 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:18,380 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:19,069 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:19,070 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:19,072 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:19,072 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:19,076 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:33:19,205 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:19,237 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:19,237 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:19,238 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:19,238 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-14 01:33:19,238 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:33:19,241 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:33:19,252 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:19,267 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:33:20,169 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:20,183 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:20,198 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:20,223 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:33:20,253 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:20,283 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:20,317 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:20,391 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:20,444 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:20,483 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:33:20,524 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:20,560 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:20,603 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:20,640 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:20,689 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:33:21,637 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10036ms
2014-07-14 01:33:21,638 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10326ms
2014-07-14 01:33:21,638 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10371ms
2014-07-14 01:33:21,638 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10203ms
2014-07-14 01:33:21,639 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10242ms
2014-07-14 01:33:21,639 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10285ms
2014-07-14 01:33:21,639 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10166ms
2014-07-14 01:33:21,639 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10125ms
2014-07-14 01:33:21,640 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10077ms
2014-07-14 01:33:21,651 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:21,683 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:21,717 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:21,755 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:21,792 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:21,827 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:21,878 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:33:23,233 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:23,268 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:33:23,309 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:23,346 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:23,381 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:24,528 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15261ms
2014-07-14 01:33:24,528 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15293ms
2014-07-14 01:33:24,528 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15292ms
2014-07-14 01:33:24,529 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15292ms
2014-07-14 01:33:24,529 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15294ms
2014-07-14 01:33:24,530 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15292ms
2014-07-14 01:33:24,530 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15289ms
2014-07-14 01:33:24,530 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15279ms
2014-07-14 01:33:24,531 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15463ms
2014-07-14 01:33:24,533 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15465ms
2014-07-14 01:33:24,533 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15462ms
2014-07-14 01:33:24,533 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15462ms
2014-07-14 01:33:24,533 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15458ms
2014-07-14 01:33:24,534 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15330ms
2014-07-14 01:33:25,169 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:33:25,183 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:33:25,198 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:33:25,224 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15003ms
2014-07-14 01:33:25,253 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:33:25,284 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:33:25,317 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:33:25,392 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:33:25,444 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:33:25,484 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:25,524 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:33:25,560 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:25,603 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:25,641 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:33:25,690 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:33:26,638 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15036ms
2014-07-14 01:33:26,638 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15327ms
2014-07-14 01:33:26,638 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15371ms
2014-07-14 01:33:26,639 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15204ms
2014-07-14 01:33:26,640 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15166ms
2014-07-14 01:33:26,640 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15126ms
2014-07-14 01:33:26,640 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15244ms
2014-07-14 01:33:26,643 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15286ms
2014-07-14 01:33:26,643 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15080ms
2014-07-14 01:33:26,652 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:33:26,683 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:33:26,717 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:33:26,755 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:33:26,792 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:33:26,827 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:33:26,879 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 01:33:28,234 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:33:28,268 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:33:28,309 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:33:28,346 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:33:28,381 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:33:29,209 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13913, memsize=472.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/35b8da31a59d4896b55dbd568eb5f424
2014-07-14 01:33:29,229 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/35b8da31a59d4896b55dbd568eb5f424 as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/35b8da31a59d4896b55dbd568eb5f424
2014-07-14 01:33:29,239 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/35b8da31a59d4896b55dbd568eb5f424, entries=1718750, sequenceid=13913, filesize=122.3m
2014-07-14 01:33:29,239 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.1g/1176548240, currentsize=46.7m/48944000 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 22259ms, sequenceid=13913, compaction requested=true
2014-07-14 01:33:29,240 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:33:29,240 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 33 store files, 0 compacting, 33 eligible, 20 blocking
2014-07-14 01:33:29,240 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15860ms
2014-07-14 01:33:29,240 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 33 files from compaction candidates
2014-07-14 01:33:29,240 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,240 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:33:29,240 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15895ms
2014-07-14 01:33:29,240 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,240 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:33:29,240 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15932ms
2014-07-14 01:33:29,241 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,240 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:33:29,241 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15974ms
2014-07-14 01:33:29,241 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,243 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16011ms
2014-07-14 01:33:29,244 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,244 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17366ms
2014-07-14 01:33:29,244 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,245 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17419ms
2014-07-14 01:33:29,245 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,246 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17455ms
2014-07-14 01:33:29,246 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,247 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17493ms
2014-07-14 01:33:29,247 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,248 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17532ms
2014-07-14 01:33:29,249 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,257 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17575ms
2014-07-14 01:33:29,257 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,261 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17611ms
2014-07-14 01:33:29,261 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,269 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17706ms
2014-07-14 01:33:29,269 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,269 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17915ms
2014-07-14 01:33:29,269 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,269 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17873ms
2014-07-14 01:33:29,269 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,269 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17755ms
2014-07-14 01:33:29,270 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,270 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17797ms
2014-07-14 01:33:29,270 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,270 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17835ms
2014-07-14 01:33:29,270 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,270 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18003ms
2014-07-14 01:33:29,270 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,275 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17964ms
2014-07-14 01:33:29,275 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,275 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17674ms
2014-07-14 01:33:29,275 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,276 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13587ms
2014-07-14 01:33:29,276 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,276 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13636ms
2014-07-14 01:33:29,276 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,276 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13674ms
2014-07-14 01:33:29,276 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,276 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13717ms
2014-07-14 01:33:29,276 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,277 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13753ms
2014-07-14 01:33:29,277 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,277 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13795ms
2014-07-14 01:33:29,277 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,277 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13833ms
2014-07-14 01:33:29,277 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,279 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13886ms
2014-07-14 01:33:29,279 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,279 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18963ms
2014-07-14 01:33:29,280 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,281 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18999ms
2014-07-14 01:33:29,281 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,281 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 19029ms
2014-07-14 01:33:29,281 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,282 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 19061ms
2014-07-14 01:33:29,282 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,282 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 19086ms
2014-07-14 01:33:29,282 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,282 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 19100ms
2014-07-14 01:33:29,282 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,283 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 19115ms
2014-07-14 01:33:29,283 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,284 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20080ms
2014-07-14 01:33:29,284 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,285 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20210ms
2014-07-14 01:33:29,285 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,285 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20214ms
2014-07-14 01:33:29,285 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,285 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20214ms
2014-07-14 01:33:29,285 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,286 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20217ms
2014-07-14 01:33:29,286 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,297 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20229ms
2014-07-14 01:33:29,297 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,298 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20046ms
2014-07-14 01:33:29,298 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,299 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20058ms
2014-07-14 01:33:29,299 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,300 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20061ms
2014-07-14 01:33:29,300 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,305 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20070ms
2014-07-14 01:33:29,305 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,305 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20069ms
2014-07-14 01:33:29,305 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,313 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20077ms
2014-07-14 01:33:29,313 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,313 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20078ms
2014-07-14 01:33:29,313 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,314 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20046ms
2014-07-14 01:33:29,315 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:33:29,415 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20954,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326788460,"queuetimems":0,"class":"HRegionServer","responsesize":16068,"method":"Multi"}
2014-07-14 01:33:29,417 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20872,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326788545,"queuetimems":1,"class":"HRegionServer","responsesize":15503,"method":"Multi"}
2014-07-14 01:33:29,502 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21071,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326788430,"queuetimems":0,"class":"HRegionServer","responsesize":16101,"method":"Multi"}
2014-07-14 01:33:29,505 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20988,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326788516,"queuetimems":0,"class":"HRegionServer","responsesize":15841,"method":"Multi"}
2014-07-14 01:33:30,642 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21717,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326788924,"queuetimems":0,"class":"HRegionServer","responsesize":15868,"method":"Multi"}
2014-07-14 01:33:30,642 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21995,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326788647,"queuetimems":1,"class":"HRegionServer","responsesize":15718,"method":"Multi"}
2014-07-14 01:33:30,645 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21762,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326788883,"queuetimems":0,"class":"HRegionServer","responsesize":15718,"method":"Multi"}
2014-07-14 01:33:30,657 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21869,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326788788,"queuetimems":0,"class":"HRegionServer","responsesize":15855,"method":"Multi"}
2014-07-14 01:33:31,088 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:33:31,251 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55796 synced till here 55754
2014-07-14 01:33:31,335 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22302,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326789032,"queuetimems":0,"class":"HRegionServer","responsesize":15594,"method":"Multi"}
2014-07-14 01:33:31,439 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90038ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:33:31,440 DEBUG [MemStoreFlusher.1] regionserver.HRegion: NOT flushing memstore for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., flushing=true, writesEnabled=true
2014-07-14 01:33:31,486 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22528,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326788957,"queuetimems":1,"class":"HRegionServer","responsesize":15742,"method":"Multi"}
2014-07-14 01:33:31,487 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18109,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326793377,"queuetimems":0,"class":"HRegionServer","responsesize":15505,"method":"Multi"}
2014-07-14 01:33:31,486 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22493,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326788992,"queuetimems":0,"class":"HRegionServer","responsesize":15759,"method":"Multi"}
2014-07-14 01:33:31,506 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326788951 with entries=134, filesize=110.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326811089
2014-07-14 01:33:31,752 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18447,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326793305,"queuetimems":0,"class":"HRegionServer","responsesize":15641,"method":"Multi"}
2014-07-14 01:33:31,753 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18489,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326793263,"queuetimems":0,"class":"HRegionServer","responsesize":15785,"method":"Multi"}
2014-07-14 01:33:31,753 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19881,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326791871,"queuetimems":0,"class":"HRegionServer","responsesize":15742,"method":"Multi"}
2014-07-14 01:33:31,753 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18410,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326793342,"queuetimems":0,"class":"HRegionServer","responsesize":16084,"method":"Multi"}
2014-07-14 01:33:32,549 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21240,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326791308,"queuetimems":1,"class":"HRegionServer","responsesize":15718,"method":"Multi"}
2014-07-14 01:33:32,557 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20913,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326791644,"queuetimems":1,"class":"HRegionServer","responsesize":15841,"method":"Multi"}
2014-07-14 01:33:32,565 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20777,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326791788,"queuetimems":1,"class":"HRegionServer","responsesize":15759,"method":"Multi"}
2014-07-14 01:33:32,824 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:33:32,827 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21562,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326791264,"queuetimems":1,"class":"HRegionServer","responsesize":15679,"method":"Multi"}
2014-07-14 01:33:32,827 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21147,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326791679,"queuetimems":1,"class":"HRegionServer","responsesize":16101,"method":"Multi"}
2014-07-14 01:33:32,827 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22631,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326790195,"queuetimems":1,"class":"HRegionServer","responsesize":16084,"method":"Multi"}
2014-07-14 01:33:32,827 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22546,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326790280,"queuetimems":1,"class":"HRegionServer","responsesize":15641,"method":"Multi"}
2014-07-14 01:33:32,839 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22526,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326790313,"queuetimems":0,"class":"HRegionServer","responsesize":15505,"method":"Multi"}
2014-07-14 01:33:32,839 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21490,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326791349,"queuetimems":0,"class":"HRegionServer","responsesize":15718,"method":"Multi"}
2014-07-14 01:33:32,839 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21243,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326791596,"queuetimems":0,"class":"HRegionServer","responsesize":15594,"method":"Multi"}
2014-07-14 01:33:32,839 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21279,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326791560,"queuetimems":1,"class":"HRegionServer","responsesize":16068,"method":"Multi"}
2014-07-14 01:33:32,840 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21090,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326791749,"queuetimems":1,"class":"HRegionServer","responsesize":15868,"method":"Multi"}
2014-07-14 01:33:32,840 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17320,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326795520,"queuetimems":0,"class":"HRegionServer","responsesize":15718,"method":"Multi"}
2014-07-14 01:33:32,840 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22621,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326790219,"queuetimems":0,"class":"HRegionServer","responsesize":15785,"method":"Multi"}
2014-07-14 01:33:32,840 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21409,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326791431,"queuetimems":0,"class":"HRegionServer","responsesize":15855,"method":"Multi"}
2014-07-14 01:33:32,840 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19617,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326793223,"queuetimems":0,"class":"HRegionServer","responsesize":15820,"method":"Multi"}
2014-07-14 01:33:32,841 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21449,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326791391,"queuetimems":1,"class":"HRegionServer","responsesize":15503,"method":"Multi"}
2014-07-14 01:33:32,846 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21134,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326791712,"queuetimems":1,"class":"HRegionServer","responsesize":16056,"method":"Multi"}
2014-07-14 01:33:32,846 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21023,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326791823,"queuetimems":1,"class":"HRegionServer","responsesize":15632,"method":"Multi"}
2014-07-14 01:33:32,847 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22666,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326790180,"queuetimems":0,"class":"HRegionServer","responsesize":15985,"method":"Multi"}
2014-07-14 01:33:32,846 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21379,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326791467,"queuetimems":0,"class":"HRegionServer","responsesize":15644,"method":"Multi"}
2014-07-14 01:33:32,846 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21335,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326791511,"queuetimems":0,"class":"HRegionServer","responsesize":15944,"method":"Multi"}
2014-07-14 01:33:33,215 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24012,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326789202,"queuetimems":0,"class":"HRegionServer","responsesize":15632,"method":"Multi"}
2014-07-14 01:33:33,222 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22972,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326790250,"queuetimems":0,"class":"HRegionServer","responsesize":15944,"method":"Multi"}
2014-07-14 01:33:33,223 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23056,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326790166,"queuetimems":1,"class":"HRegionServer","responsesize":15644,"method":"Multi"}
2014-07-14 01:33:33,222 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17666,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326795556,"queuetimems":1,"class":"HRegionServer","responsesize":15855,"method":"Multi"}
2014-07-14 01:33:33,229 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23965,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326789264,"queuetimems":1,"class":"HRegionServer","responsesize":15679,"method":"Multi"}
2014-07-14 01:33:33,229 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17751,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326795478,"queuetimems":1,"class":"HRegionServer","responsesize":15644,"method":"Multi"}
2014-07-14 01:33:33,230 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17545,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326795685,"queuetimems":0,"class":"HRegionServer","responsesize":16101,"method":"Multi"}
2014-07-14 01:33:33,241 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17642,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326795599,"queuetimems":0,"class":"HRegionServer","responsesize":15679,"method":"Multi"}
2014-07-14 01:33:33,241 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17853,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326795388,"queuetimems":0,"class":"HRegionServer","responsesize":15503,"method":"Multi"}
2014-07-14 01:33:33,241 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17605,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326795636,"queuetimems":0,"class":"HRegionServer","responsesize":15718,"method":"Multi"}
2014-07-14 01:33:33,252 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55922 synced till here 55896
2014-07-14 01:33:33,370 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24297,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326789073,"queuetimems":0,"class":"HRegionServer","responsesize":16056,"method":"Multi"}
2014-07-14 01:33:33,413 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13903, memsize=514.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/b82858cb8cae437abb0c3b2688c03f66
2014-07-14 01:33:33,420 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326811089 with entries=126, filesize=106.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326812824
2014-07-14 01:33:33,462 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/b82858cb8cae437abb0c3b2688c03f66 as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/b82858cb8cae437abb0c3b2688c03f66
2014-07-14 01:33:33,475 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/b82858cb8cae437abb0c3b2688c03f66, entries=1874640, sequenceid=13903, filesize=133.5m
2014-07-14 01:33:33,476 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.3g/1402109280, currentsize=139.8m/146614720 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 26600ms, sequenceid=13903, compaction requested=true
2014-07-14 01:33:33,476 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:33:33,476 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 32 store files, 0 compacting, 32 eligible, 20 blocking
2014-07-14 01:33:33,477 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 32 files from compaction candidates
2014-07-14 01:33:33,477 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:33:33,477 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:33:33,477 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:33:34,470 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19030,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326795439,"queuetimems":0,"class":"HRegionServer","responsesize":15944,"method":"Multi"}
2014-07-14 01:33:34,791 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:33:35,053 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56035 synced till here 55996
2014-07-14 01:33:35,311 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326812824 with entries=113, filesize=97.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326814791
2014-07-14 01:33:35,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326687497
2014-07-14 01:33:35,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326689296
2014-07-14 01:33:35,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326691011
2014-07-14 01:33:35,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326702133
2014-07-14 01:33:35,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326704148
2014-07-14 01:33:35,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326705960
2014-07-14 01:33:35,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326707989
2014-07-14 01:33:35,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326709613
2014-07-14 01:33:35,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326711805
2014-07-14 01:33:35,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326721404
2014-07-14 01:33:35,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326723617
2014-07-14 01:33:35,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326725610
2014-07-14 01:33:35,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326728533
2014-07-14 01:33:35,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326730652
2014-07-14 01:33:36,462 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:33:36,539 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56141 synced till here 56109
2014-07-14 01:33:36,993 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326814791 with entries=106, filesize=90.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326816463
2014-07-14 01:33:37,482 DEBUG [RpcServer.handler=7,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:33:37,482 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files; delaying flush up to 90000ms
2014-07-14 01:33:37,483 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:33:37,483 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 32 store files, 0 compacting, 32 eligible, 20 blocking
2014-07-14 01:33:37,483 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 32 files from compaction candidates
2014-07-14 01:33:37,483 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:33:37,483 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:33:37,483 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:33:39,580 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:33:39,581 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files; delaying flush up to 90000ms
2014-07-14 01:33:39,581 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:33:39,581 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 33 store files, 0 compacting, 33 eligible, 20 blocking
2014-07-14 01:33:39,581 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 33 files from compaction candidates
2014-07-14 01:33:39,581 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:33:39,581 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:33:39,582 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:33:39,589 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:33:39,620 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326816463 with entries=73, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326819590
2014-07-14 01:33:41,284 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:33:41,574 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56300 synced till here 56288
2014-07-14 01:33:41,725 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326819590 with entries=86, filesize=73.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326821285
2014-07-14 01:33:42,929 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:33:42,949 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56374 synced till here 56373
2014-07-14 01:33:42,993 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326821285 with entries=74, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326822930
2014-07-14 01:33:42,993 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 6e2bf74487f61672cd8bc06d8b34f003
2014-07-14 01:33:44,233 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:33:44,254 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56447 synced till here 56445
2014-07-14 01:33:44,309 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326822930 with entries=73, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326824233
2014-07-14 01:33:44,310 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 6e2bf74487f61672cd8bc06d8b34f003
2014-07-14 01:33:45,281 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:33:45,702 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326824233 with entries=74, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326825282
2014-07-14 01:33:45,703 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 6e2bf74487f61672cd8bc06d8b34f003
2014-07-14 01:33:46,732 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:33:46,749 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56594 synced till here 56592
2014-07-14 01:33:46,774 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326825282 with entries=73, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326826732
2014-07-14 01:33:46,774 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 6e2bf74487f61672cd8bc06d8b34f003
2014-07-14 01:33:47,997 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:33:48,016 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56671 synced till here 56668
2014-07-14 01:33:48,057 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326826732 with entries=77, filesize=65.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326827998
2014-07-14 01:33:48,058 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 6e2bf74487f61672cd8bc06d8b34f003
2014-07-14 01:33:49,639 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:33:49,687 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56748 synced till here 56745
2014-07-14 01:33:49,752 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326827998 with entries=77, filesize=66.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326829639
2014-07-14 01:33:49,752 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 6e2bf74487f61672cd8bc06d8b34f003
2014-07-14 01:33:51,292 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:33:51,753 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56864 synced till here 56860
2014-07-14 01:33:51,820 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326829639 with entries=116, filesize=99.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326831293
2014-07-14 01:33:51,821 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 6e2bf74487f61672cd8bc06d8b34f003
2014-07-14 01:33:53,112 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:33:53,130 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56942 synced till here 56938
2014-07-14 01:33:53,174 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326831293 with entries=78, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326833114
2014-07-14 01:33:53,174 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 6e2bf74487f61672cd8bc06d8b34f003
2014-07-14 01:33:55,475 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:33:55,509 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57017 synced till here 57014
2014-07-14 01:33:55,562 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326833114 with entries=75, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326835476
2014-07-14 01:33:55,562 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 6e2bf74487f61672cd8bc06d8b34f003
2014-07-14 01:33:56,263 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:33:56,931 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57110 synced till here 57109
2014-07-14 01:33:56,954 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326835476 with entries=93, filesize=79.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326836263
2014-07-14 01:33:56,954 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 6e2bf74487f61672cd8bc06d8b34f003
2014-07-14 01:33:57,012 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:33:57,013 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files, but is 1.4g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:33:57,013 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. due to global heap pressure
2014-07-14 01:33:57,013 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 1.4g
2014-07-14 01:33:57,808 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:33:57,818 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:33:57,818 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files, but is 1.2g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:33:57,818 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. due to global heap pressure
2014-07-14 01:33:57,819 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 1.2g
2014-07-14 01:33:57,826 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57184 synced till here 57182
2014-07-14 01:33:57,861 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326836263 with entries=74, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326837808
2014-07-14 01:33:59,158 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:33:59,326 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:59,326 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:59,349 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:59,378 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:59,380 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:59,381 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:59,382 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:59,396 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:59,396 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:59,424 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:59,473 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:59,506 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:59,543 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:59,590 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:59,642 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:33:59,643 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:59,682 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:59,721 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:59,751 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:59,784 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:59,813 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:59,841 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:59,870 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:59,898 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:59,927 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:59,956 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:33:59,984 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:00,013 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:00,041 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:00,069 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:00,098 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:00,418 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:00,461 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:00,939 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:01,454 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:01,635 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:02,727 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:02,738 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:02,753 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:02,782 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:02,810 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:02,839 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:02,876 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:02,922 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:02,954 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:02,989 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:03,021 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:03,060 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:03,090 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:03,126 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:03,158 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:04,327 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:34:04,327 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:34:04,350 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:34:04,378 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:34:04,381 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:34:04,382 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:34:04,382 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:34:04,396 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:34:04,397 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:34:04,424 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:34:04,474 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:34:04,507 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:34:04,544 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:34:04,591 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:34:04,644 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:34:04,682 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:34:04,721 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:34:04,751 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:34:04,784 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:34:04,813 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:34:04,842 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:34:04,870 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:34:04,899 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:34:04,927 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:34:04,956 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:34:04,985 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:34:05,013 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:34:05,041 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:34:05,070 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:34:05,099 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:34:05,419 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:34:05,461 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:34:05,940 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:34:06,455 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:34:06,636 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:34:07,728 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:34:07,738 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:34:07,753 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:34:08,657 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5498ms
2014-07-14 01:34:08,657 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5875ms
2014-07-14 01:34:08,658 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5847ms
2014-07-14 01:34:08,658 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5819ms
2014-07-14 01:34:08,658 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5782ms
2014-07-14 01:34:08,659 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5736ms
2014-07-14 01:34:08,659 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5705ms
2014-07-14 01:34:08,660 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5670ms
2014-07-14 01:34:08,660 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5600ms
2014-07-14 01:34:08,660 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5639ms
2014-07-14 01:34:08,660 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5570ms
2014-07-14 01:34:08,660 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5534ms
2014-07-14 01:34:09,327 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:09,327 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:09,350 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:09,378 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:34:09,381 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:09,382 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:09,382 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:34:09,397 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:09,397 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:09,425 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:34:09,474 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:09,507 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:09,544 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:09,591 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:09,644 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:09,683 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:09,722 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:09,751 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:09,785 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:09,814 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:09,842 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:09,870 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:34:09,899 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:09,927 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:34:09,957 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:34:09,985 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:10,014 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:10,042 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:10,070 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:10,099 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:10,419 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:10,461 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:10,941 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:11,455 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:11,636 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:12,729 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:34:12,739 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:34:12,754 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:34:13,657 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10499ms
2014-07-14 01:34:13,658 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10876ms
2014-07-14 01:34:13,658 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10848ms
2014-07-14 01:34:13,658 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10819ms
2014-07-14 01:34:13,659 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10783ms
2014-07-14 01:34:13,659 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10737ms
2014-07-14 01:34:13,659 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10705ms
2014-07-14 01:34:13,660 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10671ms
2014-07-14 01:34:13,660 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10600ms
2014-07-14 01:34:13,660 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10639ms
2014-07-14 01:34:13,660 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10570ms
2014-07-14 01:34:13,661 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10534ms
2014-07-14 01:34:15,071 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:34:15,072 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15288ms
2014-07-14 01:34:15,072 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15482ms
2014-07-14 01:34:15,072 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15031ms
2014-07-14 01:34:15,073 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15677ms
2014-07-14 01:34:15,074 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15747ms
2014-07-14 01:34:15,075 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15262ms
2014-07-14 01:34:15,076 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15235ms
2014-07-14 01:34:15,076 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15206ms
2014-07-14 01:34:15,077 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15178ms
2014-07-14 01:34:15,077 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15150ms
2014-07-14 01:34:15,077 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15121ms
2014-07-14 01:34:15,078 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15094ms
2014-07-14 01:34:15,078 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15065ms
2014-07-14 01:34:15,078 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15752ms
2014-07-14 01:34:15,078 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15729ms
2014-07-14 01:34:15,079 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15701ms
2014-07-14 01:34:15,079 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15699ms
2014-07-14 01:34:15,079 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15698ms
2014-07-14 01:34:15,080 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15698ms
2014-07-14 01:34:15,081 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15684ms
2014-07-14 01:34:15,081 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15657ms
2014-07-14 01:34:15,081 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15608ms
2014-07-14 01:34:15,082 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15576ms
2014-07-14 01:34:15,082 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15539ms
2014-07-14 01:34:15,082 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15439ms
2014-07-14 01:34:15,082 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15401ms
2014-07-14 01:34:15,082 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15361ms
2014-07-14 01:34:15,082 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15332ms
2014-07-14 01:34:15,100 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:34:15,419 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:34:15,461 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:34:15,941 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:34:16,455 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:34:16,636 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:34:17,729 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:34:17,739 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:34:17,754 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:34:18,658 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15500ms
2014-07-14 01:34:18,658 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15819ms
2014-07-14 01:34:18,659 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15849ms
2014-07-14 01:34:18,659 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15877ms
2014-07-14 01:34:18,659 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15783ms
2014-07-14 01:34:18,660 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15737ms
2014-07-14 01:34:18,660 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15706ms
2014-07-14 01:34:18,660 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15671ms
2014-07-14 01:34:18,661 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15571ms
2014-07-14 01:34:18,661 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15640ms
2014-07-14 01:34:18,661 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15601ms
2014-07-14 01:34:18,662 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15536ms
2014-07-14 01:34:20,072 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20003ms
2014-07-14 01:34:20,072 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20288ms
2014-07-14 01:34:20,072 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20482ms
2014-07-14 01:34:20,073 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20032ms
2014-07-14 01:34:20,073 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20677ms
2014-07-14 01:34:20,074 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20748ms
2014-07-14 01:34:20,076 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20263ms
2014-07-14 01:34:20,076 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20235ms
2014-07-14 01:34:20,077 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20207ms
2014-07-14 01:34:20,077 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20179ms
2014-07-14 01:34:20,077 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20150ms
2014-07-14 01:34:20,078 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20122ms
2014-07-14 01:34:20,078 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20094ms
2014-07-14 01:34:20,078 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20065ms
2014-07-14 01:34:20,078 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20752ms
2014-07-14 01:34:20,079 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20730ms
2014-07-14 01:34:20,079 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20701ms
2014-07-14 01:34:20,079 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20699ms
2014-07-14 01:34:20,080 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20699ms
2014-07-14 01:34:20,080 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20698ms
2014-07-14 01:34:20,081 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20685ms
2014-07-14 01:34:20,082 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20657ms
2014-07-14 01:34:20,082 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20609ms
2014-07-14 01:34:20,082 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20576ms
2014-07-14 01:34:20,082 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20539ms
2014-07-14 01:34:20,083 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20439ms
2014-07-14 01:34:20,083 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20333ms
2014-07-14 01:34:20,083 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20362ms
2014-07-14 01:34:20,083 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20402ms
2014-07-14 01:34:20,100 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:34:20,420 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:34:20,462 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:34:21,573 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20119ms
2014-07-14 01:34:21,573 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20634ms
2014-07-14 01:34:21,636 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:34:22,729 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:34:22,739 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:34:22,754 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:34:23,659 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20501ms
2014-07-14 01:34:23,660 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20784ms
2014-07-14 01:34:23,660 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20821ms
2014-07-14 01:34:23,660 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20878ms
2014-07-14 01:34:23,660 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20706ms
2014-07-14 01:34:23,660 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20850ms
2014-07-14 01:34:23,661 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20739ms
2014-07-14 01:34:23,661 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20672ms
2014-07-14 01:34:23,661 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20571ms
2014-07-14 01:34:23,661 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20640ms
2014-07-14 01:34:23,662 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20602ms
2014-07-14 01:34:23,662 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20536ms
2014-07-14 01:34:25,073 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25003ms
2014-07-14 01:34:25,073 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25483ms
2014-07-14 01:34:25,073 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25289ms
2014-07-14 01:34:25,074 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25032ms
2014-07-14 01:34:25,074 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25678ms
2014-07-14 01:34:25,075 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25749ms
2014-07-14 01:34:25,076 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25263ms
2014-07-14 01:34:25,076 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25235ms
2014-07-14 01:34:25,077 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25207ms
2014-07-14 01:34:25,077 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25179ms
2014-07-14 01:34:25,077 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25150ms
2014-07-14 01:34:25,078 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25122ms
2014-07-14 01:34:25,078 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25094ms
2014-07-14 01:34:25,078 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25065ms
2014-07-14 01:34:25,079 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25753ms
2014-07-14 01:34:25,079 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25730ms
2014-07-14 01:34:25,079 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25701ms
2014-07-14 01:34:25,080 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25700ms
2014-07-14 01:34:25,080 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25699ms
2014-07-14 01:34:25,081 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25699ms
2014-07-14 01:34:25,082 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25685ms
2014-07-14 01:34:25,082 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25658ms
2014-07-14 01:34:25,082 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25609ms
2014-07-14 01:34:25,083 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25539ms
2014-07-14 01:34:25,083 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25577ms
2014-07-14 01:34:25,083 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25440ms
2014-07-14 01:34:25,083 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25333ms
2014-07-14 01:34:25,083 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25402ms
2014-07-14 01:34:25,083 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25362ms
2014-07-14 01:34:25,100 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:34:25,420 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:34:25,462 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:34:26,573 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25119ms
2014-07-14 01:34:26,574 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25635ms
2014-07-14 01:34:26,637 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:34:27,373 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14355, memsize=672.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/9c9bde828e914623bba01c01b62e2220
2014-07-14 01:34:27,387 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/9c9bde828e914623bba01c01b62e2220 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/9c9bde828e914623bba01c01b62e2220
2014-07-14 01:34:27,396 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/9c9bde828e914623bba01c01b62e2220, entries=2448650, sequenceid=14355, filesize=174.4m
2014-07-14 01:34:27,396 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.2g/1248050240, currentsize=28.0m/29374960 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 29577ms, sequenceid=14355, compaction requested=true
2014-07-14 01:34:27,398 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:34:27,398 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 35 store files, 0 compacting, 35 eligible, 20 blocking
2014-07-14 01:34:27,398 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 95428ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:34:27,398 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 35 files from compaction candidates
2014-07-14 01:34:27,398 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25763ms
2014-07-14 01:34:27,398 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:34:27,398 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 28.0m
2014-07-14 01:34:27,398 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,398 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:34:27,399 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26460ms
2014-07-14 01:34:27,399 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,399 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:34:27,399 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25945ms
2014-07-14 01:34:27,399 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,399 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26939ms
2014-07-14 01:34:27,399 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,401 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26983ms
2014-07-14 01:34:27,401 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,401 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27303ms
2014-07-14 01:34:27,401 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,401 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27680ms
2014-07-14 01:34:27,401 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,405 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27724ms
2014-07-14 01:34:27,405 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,405 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27655ms
2014-07-14 01:34:27,405 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,406 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27762ms
2014-07-14 01:34:27,406 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,406 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27900ms
2014-07-14 01:34:27,406 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,406 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27863ms
2014-07-14 01:34:27,406 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,408 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:34:27,411 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27937ms
2014-07-14 01:34:27,411 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,414 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27990ms
2014-07-14 01:34:27,414 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,414 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28018ms
2014-07-14 01:34:27,414 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,414 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28032ms
2014-07-14 01:34:27,414 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,425 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28044ms
2014-07-14 01:34:27,425 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,425 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28045ms
2014-07-14 01:34:27,425 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,428 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28050ms
2014-07-14 01:34:27,428 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,437 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28088ms
2014-07-14 01:34:27,437 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,437 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28111ms
2014-07-14 01:34:27,437 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,437 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27424ms
2014-07-14 01:34:27,437 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,437 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27453ms
2014-07-14 01:34:27,437 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,438 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27482ms
2014-07-14 01:34:27,438 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,443 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27516ms
2014-07-14 01:34:27,443 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,443 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27545ms
2014-07-14 01:34:27,443 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,443 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27573ms
2014-07-14 01:34:27,443 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,445 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27604ms
2014-07-14 01:34:27,445 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,446 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27633ms
2014-07-14 01:34:27,446 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,447 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28120ms
2014-07-14 01:34:27,447 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,448 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28051ms
2014-07-14 01:34:27,448 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,449 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27407ms
2014-07-14 01:34:27,449 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,449 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27665ms
2014-07-14 01:34:27,449 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,450 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27860ms
2014-07-14 01:34:27,450 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,451 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27382ms
2014-07-14 01:34:27,451 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,457 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24331ms
2014-07-14 01:34:27,457 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,457 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24397ms
2014-07-14 01:34:27,457 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,457 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24436ms
2014-07-14 01:34:27,457 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,457 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24367ms
2014-07-14 01:34:27,458 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,460 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24471ms
2014-07-14 01:34:27,460 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,460 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24538ms
2014-07-14 01:34:27,460 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,461 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24650ms
2014-07-14 01:34:27,461 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,461 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24507ms
2014-07-14 01:34:27,461 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,462 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24680ms
2014-07-14 01:34:27,462 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:34:27,462 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,462 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24623ms
2014-07-14 01:34:27,462 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,462 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24586ms
2014-07-14 01:34:27,462 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,463 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24305ms
2014-07-14 01:34:27,463 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,463 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24710ms
2014-07-14 01:34:27,463 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,463 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24726ms
2014-07-14 01:34:27,463 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,463 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24736ms
2014-07-14 01:34:27,464 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:34:27,506 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57261 synced till here 57258
2014-07-14 01:34:27,582 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326837808 with entries=77, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326867462
2014-07-14 01:34:27,674 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28570,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326839103,"queuetimems":1,"class":"HRegionServer","responsesize":15791,"method":"Multi"}
2014-07-14 01:34:28,444 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29300,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326839143,"queuetimems":1,"class":"HRegionServer","responsesize":15955,"method":"Multi"}
2014-07-14 01:34:28,677 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29468,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326839208,"queuetimems":0,"class":"HRegionServer","responsesize":15516,"method":"Multi"}
2014-07-14 01:34:28,686 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29509,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326839176,"queuetimems":0,"class":"HRegionServer","responsesize":15491,"method":"Multi"}
2014-07-14 01:34:28,822 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29552,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326839269,"queuetimems":0,"class":"HRegionServer","responsesize":15390,"method":"Multi"}
2014-07-14 01:34:28,822 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29582,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326839240,"queuetimems":1,"class":"HRegionServer","responsesize":15884,"method":"Multi"}
2014-07-14 01:34:28,822 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29521,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326839300,"queuetimems":0,"class":"HRegionServer","responsesize":15985,"method":"Multi"}
2014-07-14 01:34:29,171 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14374, memsize=28.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/031c2727f7a84951b5ee7f782cce343c
2014-07-14 01:34:29,183 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/031c2727f7a84951b5ee7f782cce343c as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/031c2727f7a84951b5ee7f782cce343c
2014-07-14 01:34:29,194 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/031c2727f7a84951b5ee7f782cce343c, entries=102000, sequenceid=14374, filesize=7.3m
2014-07-14 01:34:29,195 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~28.0m/29374960, currentsize=73.0m/76565840 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 1797ms, sequenceid=14374, compaction requested=true
2014-07-14 01:34:29,195 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:34:29,195 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 36 store files, 0 compacting, 36 eligible, 20 blocking
2014-07-14 01:34:29,195 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 106718ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:34:29,195 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 36 files from compaction candidates
2014-07-14 01:34:29,195 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:34:29,195 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:34:29,195 DEBUG [MemStoreFlusher.1] regionserver.HRegion: NOT flushing memstore for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., flushing=true, writesEnabled=true
2014-07-14 01:34:29,196 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:34:29,203 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:34:29,268 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57379 synced till here 57360
2014-07-14 01:34:29,445 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326867462 with entries=118, filesize=96.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326869203
2014-07-14 01:34:30,352 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30880,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326839471,"queuetimems":1,"class":"HRegionServer","responsesize":15861,"method":"Multi"}
2014-07-14 01:34:30,360 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29901,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326840459,"queuetimems":0,"class":"HRegionServer","responsesize":15692,"method":"Multi"}
2014-07-14 01:34:30,364 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28736,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326841628,"queuetimems":0,"class":"HRegionServer","responsesize":15666,"method":"Multi"}
2014-07-14 01:34:30,364 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29427,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326840937,"queuetimems":0,"class":"HRegionServer","responsesize":15949,"method":"Multi"}
2014-07-14 01:34:30,373 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28922,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326841451,"queuetimems":1,"class":"HRegionServer","responsesize":15655,"method":"Multi"}
2014-07-14 01:34:30,462 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27372,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326843089,"queuetimems":1,"class":"HRegionServer","responsesize":15847,"method":"Multi"}
2014-07-14 01:34:30,640 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:34:30,642 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30773,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326839868,"queuetimems":1,"class":"HRegionServer","responsesize":15653,"method":"Multi"}
2014-07-14 01:34:30,642 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30802,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326839839,"queuetimems":0,"class":"HRegionServer","responsesize":15955,"method":"Multi"}
2014-07-14 01:34:30,642 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30923,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326839719,"queuetimems":0,"class":"HRegionServer","responsesize":15701,"method":"Multi"}
2014-07-14 01:34:30,642 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30660,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326839982,"queuetimems":0,"class":"HRegionServer","responsesize":15390,"method":"Multi"}
2014-07-14 01:34:30,642 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30831,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326839811,"queuetimems":0,"class":"HRegionServer","responsesize":15491,"method":"Multi"}
2014-07-14 01:34:30,642 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30226,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326840416,"queuetimems":0,"class":"HRegionServer","responsesize":15581,"method":"Multi"}
2014-07-14 01:34:30,643 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27518,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326843124,"queuetimems":0,"class":"HRegionServer","responsesize":15643,"method":"Multi"}
2014-07-14 01:34:30,643 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31295,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326839347,"queuetimems":0,"class":"HRegionServer","responsesize":15515,"method":"Multi"}
2014-07-14 01:34:30,643 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30603,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326840039,"queuetimems":0,"class":"HRegionServer","responsesize":15516,"method":"Multi"}
2014-07-14 01:34:30,643 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31139,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326839504,"queuetimems":0,"class":"HRegionServer","responsesize":15883,"method":"Multi"}
2014-07-14 01:34:30,644 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27835,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326842808,"queuetimems":1,"class":"HRegionServer","responsesize":15353,"method":"Multi"}
2014-07-14 01:34:30,644 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30965,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326839679,"queuetimems":0,"class":"HRegionServer","responsesize":15669,"method":"Multi"}
2014-07-14 01:34:30,642 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27656,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326842986,"queuetimems":0,"class":"HRegionServer","responsesize":15810,"method":"Multi"}
2014-07-14 01:34:30,643 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31267,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326839376,"queuetimems":0,"class":"HRegionServer","responsesize":15739,"method":"Multi"}
2014-07-14 01:34:30,654 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30699,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326839954,"queuetimems":0,"class":"HRegionServer","responsesize":16024,"method":"Multi"}
2014-07-14 01:34:30,665 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31124,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326839541,"queuetimems":0,"class":"HRegionServer","responsesize":16172,"method":"Multi"}
2014-07-14 01:34:30,667 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30656,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326840011,"queuetimems":0,"class":"HRegionServer","responsesize":15884,"method":"Multi"}
2014-07-14 01:34:30,667 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30918,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326839749,"queuetimems":1,"class":"HRegionServer","responsesize":15993,"method":"Multi"}
2014-07-14 01:34:30,667 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30771,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326839896,"queuetimems":0,"class":"HRegionServer","responsesize":15932,"method":"Multi"}
2014-07-14 01:34:30,677 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30580,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326840097,"queuetimems":0,"class":"HRegionServer","responsesize":16000,"method":"Multi"}
2014-07-14 01:34:30,677 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27725,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326842952,"queuetimems":1,"class":"HRegionServer","responsesize":15791,"method":"Multi"}
2014-07-14 01:34:30,677 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30609,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326840068,"queuetimems":1,"class":"HRegionServer","responsesize":15695,"method":"Multi"}
2014-07-14 01:34:30,679 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31254,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326839423,"queuetimems":1,"class":"HRegionServer","responsesize":15666,"method":"Multi"}
2014-07-14 01:34:30,683 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57481 synced till here 57464
2014-07-14 01:34:30,801 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27646,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326843155,"queuetimems":0,"class":"HRegionServer","responsesize":15669,"method":"Multi"}
2014-07-14 01:34:30,801 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28021,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326842780,"queuetimems":1,"class":"HRegionServer","responsesize":16172,"method":"Multi"}
2014-07-14 01:34:30,801 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29132,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326841669,"queuetimems":0,"class":"HRegionServer","responsesize":15883,"method":"Multi"}
2014-07-14 01:34:30,802 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27783,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326843018,"queuetimems":1,"class":"HRegionServer","responsesize":15701,"method":"Multi"}
2014-07-14 01:34:30,801 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31162,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326839639,"queuetimems":0,"class":"HRegionServer","responsesize":15810,"method":"Multi"}
2014-07-14 01:34:30,802 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31214,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326839588,"queuetimems":0,"class":"HRegionServer","responsesize":15690,"method":"Multi"}
2014-07-14 01:34:30,806 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27748,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326843058,"queuetimems":0,"class":"HRegionServer","responsesize":15690,"method":"Multi"}
2014-07-14 01:34:30,807 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30882,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326839925,"queuetimems":0,"class":"HRegionServer","responsesize":15515,"method":"Multi"}
2014-07-14 01:34:30,808 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28050,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326842751,"queuetimems":2,"class":"HRegionServer","responsesize":15695,"method":"Multi"}
2014-07-14 01:34:30,812 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27975,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326842837,"queuetimems":0,"class":"HRegionServer","responsesize":15739,"method":"Multi"}
2014-07-14 01:34:30,812 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27939,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326842873,"queuetimems":0,"class":"HRegionServer","responsesize":15861,"method":"Multi"}
2014-07-14 01:34:30,812 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28076,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326842736,"queuetimems":0,"class":"HRegionServer","responsesize":15993,"method":"Multi"}
2014-07-14 01:34:30,812 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31030,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326839782,"queuetimems":0,"class":"HRegionServer","responsesize":15353,"method":"Multi"}
2014-07-14 01:34:30,971 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326869203 with entries=102, filesize=86.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326870640
2014-07-14 01:34:30,983 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28063,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326842919,"queuetimems":1,"class":"HRegionServer","responsesize":16000,"method":"Multi"}
2014-07-14 01:34:31,216 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14315, memsize=726.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/716c5a0f1a0c42dba513a064b079734d
2014-07-14 01:34:31,277 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/716c5a0f1a0c42dba513a064b079734d as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/716c5a0f1a0c42dba513a064b079734d
2014-07-14 01:34:31,292 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/716c5a0f1a0c42dba513a064b079734d, entries=2644720, sequenceid=14315, filesize=188.3m
2014-07-14 01:34:31,292 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.4g/1540677280, currentsize=131.3m/137701200 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 34279ms, sequenceid=14315, compaction requested=true
2014-07-14 01:34:31,292 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:34:31,293 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 37 store files, 0 compacting, 37 eligible, 20 blocking
2014-07-14 01:34:31,293 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 37 files from compaction candidates
2014-07-14 01:34:31,293 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:34:31,293 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:34:31,293 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:34:32,279 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:34:32,294 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57588 synced till here 57561
2014-07-14 01:34:32,463 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326870640 with entries=107, filesize=90.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326872280
2014-07-14 01:34:32,464 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326732113
2014-07-14 01:34:32,464 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326733689
2014-07-14 01:34:32,464 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326735329
2014-07-14 01:34:32,464 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326736578
2014-07-14 01:34:32,464 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326740872
2014-07-14 01:34:32,464 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326742723
2014-07-14 01:34:32,464 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326744934
2014-07-14 01:34:32,464 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326746976
2014-07-14 01:34:32,464 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326748558
2014-07-14 01:34:32,464 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326750386
2014-07-14 01:34:32,464 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326751859
2014-07-14 01:34:32,464 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326763033
2014-07-14 01:34:32,464 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326765076
2014-07-14 01:34:32,464 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326766832
2014-07-14 01:34:32,464 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326768815
2014-07-14 01:34:32,464 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326769807
2014-07-14 01:34:32,464 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326771939
2014-07-14 01:34:32,464 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326773386
2014-07-14 01:34:32,464 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326775316
2014-07-14 01:34:32,465 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326777272
2014-07-14 01:34:32,465 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326779198
2014-07-14 01:34:32,465 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326781036
2014-07-14 01:34:32,465 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326782654
2014-07-14 01:34:32,465 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326784134
2014-07-14 01:34:34,220 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:34:34,267 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57696 synced till here 57665
2014-07-14 01:34:34,466 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326872280 with entries=108, filesize=88.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326874221
2014-07-14 01:34:35,427 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:34:36,048 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326874221 with entries=97, filesize=81.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326875427
2014-07-14 01:34:36,520 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:34:36,522 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files; delaying flush up to 90000ms
2014-07-14 01:34:36,523 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 37 store files, 0 compacting, 37 eligible, 20 blocking
2014-07-14 01:34:36,523 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 37 files from compaction candidates
2014-07-14 01:34:36,523 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:34:36,523 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:34:36,523 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:34:36,523 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:34:36,556 DEBUG [RpcServer.handler=38,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:34:36,557 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files; delaying flush up to 90000ms
2014-07-14 01:34:36,557 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 36 store files, 0 compacting, 36 eligible, 20 blocking
2014-07-14 01:34:36,557 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 36 files from compaction candidates
2014-07-14 01:34:36,557 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:34:36,557 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:34:36,557 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:34:36,558 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:34:36,833 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:34:36,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57902 synced till here 57870
2014-07-14 01:34:37,036 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326875427 with entries=109, filesize=91.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326876834
2014-07-14 01:34:38,641 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:34:38,660 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58021 synced till here 57982
2014-07-14 01:34:38,957 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326876834 with entries=119, filesize=100.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326878642
2014-07-14 01:34:40,533 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:34:40,584 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58138 synced till here 58109
2014-07-14 01:34:40,779 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326878642 with entries=117, filesize=90.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326880533
2014-07-14 01:34:41,712 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:34:41,740 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326880533 with entries=73, filesize=61.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326881712
2014-07-14 01:34:43,502 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:34:43,559 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326881712 with entries=74, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326883502
2014-07-14 01:34:45,226 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:34:45,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58362 synced till here 58359
2014-07-14 01:34:45,332 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326883502 with entries=77, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326885227
2014-07-14 01:34:47,331 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:34:48,842 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58508 synced till here 58498
2014-07-14 01:34:48,926 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326885227 with entries=146, filesize=125.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326887332
2014-07-14 01:34:49,764 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:34:49,791 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58583 synced till here 58582
2014-07-14 01:34:49,813 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326887332 with entries=75, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326889764
2014-07-14 01:34:49,814 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 2 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72, b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:34:51,190 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:34:51,207 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58659 synced till here 58657
2014-07-14 01:34:51,242 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326889764 with entries=76, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326891190
2014-07-14 01:34:51,243 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 2 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72, b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:34:52,809 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:34:52,830 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58734 synced till here 58732
2014-07-14 01:34:52,878 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326891190 with entries=75, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326892809
2014-07-14 01:34:52,878 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 2 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72, b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:34:53,955 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:34:54,365 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58811 synced till here 58808
2014-07-14 01:34:54,418 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326892809 with entries=77, filesize=66.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326893956
2014-07-14 01:34:54,419 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 2 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72, b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:34:55,688 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:34:55,688 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files, but is 1.3g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:34:55,688 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. due to global heap pressure
2014-07-14 01:34:55,688 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 1.3g
2014-07-14 01:34:55,803 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:34:55,827 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58883 synced till here 58882
2014-07-14 01:34:55,852 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326893956 with entries=72, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326895804
2014-07-14 01:34:55,852 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:34:56,757 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:34:56,758 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files, but is 1.3g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:34:56,758 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. due to global heap pressure
2014-07-14 01:34:56,758 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 1.3g
2014-07-14 01:34:57,303 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:34:57,501 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:34:57,539 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326895804 with entries=72, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326897502
2014-07-14 01:34:57,948 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:34:58,429 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:58,437 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:58,446 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:58,501 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:58,745 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:58,790 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:58,824 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:58,862 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:58,894 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:58,935 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:58,973 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:59,012 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:59,046 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:59,076 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:59,110 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:59,148 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:59,183 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:59,221 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:59,256 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:59,292 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:59,335 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:59,372 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:59,429 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:59,815 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:59,892 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:34:59,933 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:35:00,095 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:35:00,143 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:35:00,215 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:35:00,354 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:35:00,583 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:35:00,855 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:35:00,890 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:35:00,927 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:35:00,964 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:35:00,996 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:35:01,029 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:35:01,064 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:35:01,095 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:35:01,125 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:35:01,156 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:35:01,187 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:35:02,857 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:35:02,894 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:35:02,930 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:35:03,965 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5030ms
2014-07-14 01:35:03,965 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5464ms
2014-07-14 01:35:03,965 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5519ms
2014-07-14 01:35:03,966 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5536ms
2014-07-14 01:35:03,966 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5529ms
2014-07-14 01:35:03,966 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5177ms
2014-07-14 01:35:03,967 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5222ms
2014-07-14 01:35:03,967 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5143ms
2014-07-14 01:35:03,967 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5105ms
2014-07-14 01:35:03,968 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5074ms
2014-07-14 01:35:03,973 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:35:03,974 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:35:03,984 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:35:03,998 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:35:04,013 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:35:04,028 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:35:04,047 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:35:04,057 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:35:04,076 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:35:04,110 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:35:04,148 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:35:04,184 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:35:04,222 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:35:04,257 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:35:04,293 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:35:04,335 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:35:04,372 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:35:04,429 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:35:04,816 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:35:04,893 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:35:04,933 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:35:05,095 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:35:05,143 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:35:05,215 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:35:05,354 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:35:05,584 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:35:05,855 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:35:05,890 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:35:05,927 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:35:05,964 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:35:05,996 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:35:06,030 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:35:06,064 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:35:06,095 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:35:06,126 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:35:06,157 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:35:06,187 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:35:07,858 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:35:07,895 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:35:07,930 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:35:08,965 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10031ms
2014-07-14 01:35:08,967 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10520ms
2014-07-14 01:35:08,967 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10466ms
2014-07-14 01:35:08,967 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10178ms
2014-07-14 01:35:08,967 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10530ms
2014-07-14 01:35:08,967 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10538ms
2014-07-14 01:35:08,967 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10222ms
2014-07-14 01:35:08,968 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10144ms
2014-07-14 01:35:08,968 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10106ms
2014-07-14 01:35:08,968 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10074ms
2014-07-14 01:35:08,973 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:35:08,975 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:35:08,984 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:35:08,998 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:35:09,013 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:35:09,029 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:35:09,047 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:35:09,057 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:35:09,076 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:35:09,111 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:35:09,148 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:35:09,184 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:35:09,222 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:35:09,257 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:35:09,293 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:35:09,336 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:35:09,372 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:35:10,310 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10094ms
2014-07-14 01:35:10,310 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10881ms
2014-07-14 01:35:10,311 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10496ms
2014-07-14 01:35:10,311 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10419ms
2014-07-14 01:35:10,311 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10379ms
2014-07-14 01:35:10,312 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10217ms
2014-07-14 01:35:10,312 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10169ms
2014-07-14 01:35:10,354 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:35:10,584 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:35:10,856 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:35:10,891 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:35:10,928 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:35:10,964 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:35:10,996 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:35:11,030 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:35:11,065 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:35:11,096 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:35:11,126 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:35:11,157 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:35:11,187 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:35:12,858 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:35:12,895 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:35:12,930 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:35:13,966 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15032ms
2014-07-14 01:35:13,967 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15521ms
2014-07-14 01:35:13,967 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15466ms
2014-07-14 01:35:13,967 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15178ms
2014-07-14 01:35:13,967 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15530ms
2014-07-14 01:35:13,968 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15538ms
2014-07-14 01:35:13,968 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15223ms
2014-07-14 01:35:13,968 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15144ms
2014-07-14 01:35:13,968 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15106ms
2014-07-14 01:35:13,968 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15074ms
2014-07-14 01:35:13,973 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 01:35:13,975 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:35:13,985 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:35:13,998 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:35:14,013 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:35:14,029 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:35:14,047 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:35:14,058 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:35:14,077 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:35:14,111 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:35:14,149 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:35:14,184 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:35:14,222 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:35:14,257 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:35:14,294 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:35:14,336 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:35:14,373 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:35:15,310 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15095ms
2014-07-14 01:35:15,311 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15882ms
2014-07-14 01:35:15,311 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15496ms
2014-07-14 01:35:15,312 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15419ms
2014-07-14 01:35:15,312 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15380ms
2014-07-14 01:35:15,312 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15217ms
2014-07-14 01:35:15,312 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15169ms
2014-07-14 01:35:15,355 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:35:15,585 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:35:16,622 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15435ms
2014-07-14 01:35:16,623 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15627ms
2014-07-14 01:35:16,623 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15594ms
2014-07-14 01:35:16,623 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15559ms
2014-07-14 01:35:16,623 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15528ms
2014-07-14 01:35:16,624 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15498ms
2014-07-14 01:35:16,624 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15468ms
2014-07-14 01:35:16,624 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15769ms
2014-07-14 01:35:16,624 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15734ms
2014-07-14 01:35:16,624 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15697ms
2014-07-14 01:35:16,624 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15661ms
2014-07-14 01:35:17,859 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:35:17,895 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:35:17,930 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 01:35:18,967 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20033ms
2014-07-14 01:35:18,968 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20466ms
2014-07-14 01:35:18,968 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20522ms
2014-07-14 01:35:18,968 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20539ms
2014-07-14 01:35:18,968 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20531ms
2014-07-14 01:35:18,968 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20106ms
2014-07-14 01:35:18,969 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20179ms
2014-07-14 01:35:18,969 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20075ms
2014-07-14 01:35:18,969 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20145ms
2014-07-14 01:35:18,969 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20224ms
2014-07-14 01:35:18,974 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:35:18,975 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:35:18,985 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:35:18,999 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:35:19,013 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:35:19,029 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:35:19,048 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:35:19,058 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:35:19,077 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:35:19,112 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:35:19,149 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:35:19,185 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:35:19,223 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:35:19,258 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:35:19,294 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:35:19,336 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:35:19,373 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:35:20,311 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20096ms
2014-07-14 01:35:20,312 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20497ms
2014-07-14 01:35:20,312 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20883ms
2014-07-14 01:35:20,312 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20217ms
2014-07-14 01:35:20,312 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20380ms
2014-07-14 01:35:20,312 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20420ms
2014-07-14 01:35:20,313 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20170ms
2014-07-14 01:35:20,355 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:35:20,585 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:35:21,623 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20436ms
2014-07-14 01:35:21,624 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20529ms
2014-07-14 01:35:21,625 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20560ms
2014-07-14 01:35:21,625 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20596ms
2014-07-14 01:35:21,626 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20629ms
2014-07-14 01:35:21,627 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20663ms
2014-07-14 01:35:21,627 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20700ms
2014-07-14 01:35:21,627 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20737ms
2014-07-14 01:35:21,628 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20773ms
2014-07-14 01:35:21,628 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20472ms
2014-07-14 01:35:21,628 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20503ms
2014-07-14 01:35:22,859 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:35:22,895 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:35:22,931 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:35:24,264 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25008ms
2014-07-14 01:35:24,265 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25253ms
2014-07-14 01:35:24,265 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25520ms
2014-07-14 01:35:24,265 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25403ms
2014-07-14 01:35:24,266 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20238ms
2014-07-14 01:35:24,266 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25220ms
2014-07-14 01:35:24,266 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20209ms
2014-07-14 01:35:24,266 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25190ms
2014-07-14 01:35:24,267 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25157ms
2014-07-14 01:35:24,267 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25119ms
2014-07-14 01:35:24,267 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25084ms
2014-07-14 01:35:24,267 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25046ms
2014-07-14 01:35:24,267 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25333ms
2014-07-14 01:35:24,268 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25767ms
2014-07-14 01:35:24,268 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25822ms
2014-07-14 01:35:24,268 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25839ms
2014-07-14 01:35:24,269 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25831ms
2014-07-14 01:35:24,269 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25480ms
2014-07-14 01:35:24,270 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25376ms
2014-07-14 01:35:24,270 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25446ms
2014-07-14 01:35:24,270 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25297ms
2014-07-14 01:35:24,271 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20297ms
2014-07-14 01:35:24,271 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20287ms
2014-07-14 01:35:24,272 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20274ms
2014-07-14 01:35:24,294 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:35:24,337 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:35:24,373 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:35:24,434 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14741, memsize=648.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/e37a64a5d1dd48d0864cb6e1ab2983fb
2014-07-14 01:35:24,451 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/e37a64a5d1dd48d0864cb6e1ab2983fb as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/e37a64a5d1dd48d0864cb6e1ab2983fb
2014-07-14 01:35:24,476 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/e37a64a5d1dd48d0864cb6e1ab2983fb, entries=2359670, sequenceid=14741, filesize=168.0m
2014-07-14 01:35:24,477 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.3g/1355394480, currentsize=49.8m/52184240 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 28789ms, sequenceid=14741, compaction requested=true
2014-07-14 01:35:24,478 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:35:24,478 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 33 store files, 0 compacting, 33 eligible, 20 blocking
2014-07-14 01:35:24,478 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25106ms
2014-07-14 01:35:24,478 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,478 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 33 files from compaction candidates
2014-07-14 01:35:24,479 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25144ms
2014-07-14 01:35:24,479 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,479 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:35:24,479 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25187ms
2014-07-14 01:35:24,479 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,479 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:35:24,480 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:35:24,480 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20483ms
2014-07-14 01:35:24,480 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,481 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20497ms
2014-07-14 01:35:24,481 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,485 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20511ms
2014-07-14 01:35:24,485 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,486 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25512ms
2014-07-14 01:35:24,486 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,486 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25662ms
2014-07-14 01:35:24,486 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,486 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25592ms
2014-07-14 01:35:24,486 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,497 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25708ms
2014-07-14 01:35:24,497 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,497 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26060ms
2014-07-14 01:35:24,497 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,497 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26068ms
2014-07-14 01:35:24,497 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,497 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26051ms
2014-07-14 01:35:24,497 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,501 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26000ms
2014-07-14 01:35:24,501 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,505 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25571ms
2014-07-14 01:35:24,505 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,505 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25284ms
2014-07-14 01:35:24,505 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,505 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25322ms
2014-07-14 01:35:24,505 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,508 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25360ms
2014-07-14 01:35:24,508 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,508 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25398ms
2014-07-14 01:35:24,508 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,508 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25432ms
2014-07-14 01:35:24,508 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,508 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20451ms
2014-07-14 01:35:24,509 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,517 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25471ms
2014-07-14 01:35:24,517 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,525 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20497ms
2014-07-14 01:35:24,525 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,525 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25663ms
2014-07-14 01:35:24,525 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,525 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25780ms
2014-07-14 01:35:24,525 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,525 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25513ms
2014-07-14 01:35:24,525 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,525 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25269ms
2014-07-14 01:35:24,525 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,525 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21595ms
2014-07-14 01:35:24,525 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,527 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21633ms
2014-07-14 01:35:24,527 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,527 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21670ms
2014-07-14 01:35:24,527 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,527 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23402ms
2014-07-14 01:35:24,527 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,528 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23371ms
2014-07-14 01:35:24,528 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,533 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23678ms
2014-07-14 01:35:24,533 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,533 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23643ms
2014-07-14 01:35:24,533 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,533 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23606ms
2014-07-14 01:35:24,533 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,537 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23574ms
2014-07-14 01:35:24,537 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,537 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23541ms
2014-07-14 01:35:24,537 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,541 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23512ms
2014-07-14 01:35:24,541 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,541 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23477ms
2014-07-14 01:35:24,541 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,541 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23446ms
2014-07-14 01:35:24,541 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,541 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23354ms
2014-07-14 01:35:24,541 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,541 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23958ms
2014-07-14 01:35:24,541 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,542 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24187ms
2014-07-14 01:35:24,542 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,542 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24399ms
2014-07-14 01:35:24,542 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,543 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24650ms
2014-07-14 01:35:24,543 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,544 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24611ms
2014-07-14 01:35:24,544 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,546 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24450ms
2014-07-14 01:35:24,546 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,551 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25122ms
2014-07-14 01:35:24,551 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,554 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24739ms
2014-07-14 01:35:24,554 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,554 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24339ms
2014-07-14 01:35:24,555 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:35:24,574 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26696,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326897877,"queuetimems":0,"class":"HRegionServer","responsesize":16002,"method":"Multi"}
2014-07-14 01:35:24,598 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26664,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326897934,"queuetimems":1,"class":"HRegionServer","responsesize":15473,"method":"Multi"}
2014-07-14 01:35:24,763 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26727,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326898036,"queuetimems":0,"class":"HRegionServer","responsesize":15802,"method":"Multi"}
2014-07-14 01:35:24,764 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26783,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326897980,"queuetimems":0,"class":"HRegionServer","responsesize":15656,"method":"Multi"}
2014-07-14 01:35:24,931 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:35:24,944 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59065 synced till here 59041
2014-07-14 01:35:25,123 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326897502 with entries=110, filesize=91.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326924932
2014-07-14 01:35:25,682 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25602,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326900079,"queuetimems":3,"class":"HRegionServer","responsesize":15861,"method":"Multi"}
2014-07-14 01:35:26,789 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1021ms
GC pool 'ParNew' had collection(s): count=1 time=1055ms
2014-07-14 01:35:27,003 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14758, memsize=659.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/f30c5ee741ee493f97711546582ece27
2014-07-14 01:35:27,018 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/f30c5ee741ee493f97711546582ece27 as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/f30c5ee741ee493f97711546582ece27
2014-07-14 01:35:27,029 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/f30c5ee741ee493f97711546582ece27, entries=2399470, sequenceid=14758, filesize=170.9m
2014-07-14 01:35:27,030 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.3g/1358165120, currentsize=51.3m/53834240 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 30272ms, sequenceid=14758, compaction requested=true
2014-07-14 01:35:27,030 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:35:27,030 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 34 store files, 0 compacting, 34 eligible, 20 blocking
2014-07-14 01:35:27,030 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 34 files from compaction candidates
2014-07-14 01:35:27,030 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:35:27,030 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:35:27,031 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:35:27,111 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:35:27,129 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59179 synced till here 59148
2014-07-14 01:35:27,332 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27905,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326899426,"queuetimems":0,"class":"HRegionServer","responsesize":15643,"method":"Multi"}
2014-07-14 01:35:27,335 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28592,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326898743,"queuetimems":0,"class":"HRegionServer","responsesize":15760,"method":"Multi"}
2014-07-14 01:35:27,335 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27124,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326900211,"queuetimems":1,"class":"HRegionServer","responsesize":15539,"method":"Multi"}
2014-07-14 01:35:27,335 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28228,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326899107,"queuetimems":0,"class":"HRegionServer","responsesize":15642,"method":"Multi"}
2014-07-14 01:35:27,341 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326924932 with entries=114, filesize=91.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326927111
2014-07-14 01:35:27,341 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326786301
2014-07-14 01:35:27,341 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326787115
2014-07-14 01:35:27,341 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326788951
2014-07-14 01:35:27,342 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326811089
2014-07-14 01:35:27,342 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326812824
2014-07-14 01:35:27,342 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326814791
2014-07-14 01:35:27,342 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326816463
2014-07-14 01:35:27,342 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326819590
2014-07-14 01:35:27,342 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326821285
2014-07-14 01:35:27,342 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326822930
2014-07-14 01:35:27,342 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326824233
2014-07-14 01:35:27,342 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326825282
2014-07-14 01:35:27,342 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326826732
2014-07-14 01:35:27,342 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326827998
2014-07-14 01:35:27,342 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326829639
2014-07-14 01:35:27,342 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326831293
2014-07-14 01:35:27,342 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326833114
2014-07-14 01:35:27,342 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326835476
2014-07-14 01:35:27,362 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28470,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326898892,"queuetimems":0,"class":"HRegionServer","responsesize":16172,"method":"Multi"}
2014-07-14 01:35:27,362 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28109,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326899253,"queuetimems":0,"class":"HRegionServer","responsesize":15842,"method":"Multi"}
2014-07-14 01:35:27,655 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23658,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326903996,"queuetimems":0,"class":"HRegionServer","responsesize":15731,"method":"Multi"}
2014-07-14 01:35:27,824 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28535,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326899289,"queuetimems":0,"class":"HRegionServer","responsesize":15811,"method":"Multi"}
2014-07-14 01:35:27,824 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28785,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326899039,"queuetimems":0,"class":"HRegionServer","responsesize":15760,"method":"Multi"}
2014-07-14 01:35:27,824 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28891,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326898933,"queuetimems":1,"class":"HRegionServer","responsesize":15666,"method":"Multi"}
2014-07-14 01:35:27,825 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28679,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326899145,"queuetimems":0,"class":"HRegionServer","responsesize":16076,"method":"Multi"}
2014-07-14 01:35:27,825 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28644,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326899180,"queuetimems":0,"class":"HRegionServer","responsesize":15410,"method":"Multi"}
2014-07-14 01:35:27,825 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28607,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326899218,"queuetimems":0,"class":"HRegionServer","responsesize":15710,"method":"Multi"}
2014-07-14 01:35:27,825 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23853,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326903972,"queuetimems":0,"class":"HRegionServer","responsesize":16082,"method":"Multi"}
2014-07-14 01:35:27,824 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24933,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326902891,"queuetimems":1,"class":"HRegionServer","responsesize":16166,"method":"Multi"}
2014-07-14 01:35:27,826 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28754,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326899071,"queuetimems":0,"class":"HRegionServer","responsesize":15581,"method":"Multi"}
2014-07-14 01:35:27,826 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29038,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326898787,"queuetimems":0,"class":"HRegionServer","responsesize":15617,"method":"Multi"}
2014-07-14 01:35:27,826 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28493,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326899332,"queuetimems":1,"class":"HRegionServer","responsesize":15992,"method":"Multi"}
2014-07-14 01:35:27,830 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26768,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326901061,"queuetimems":0,"class":"HRegionServer","responsesize":16002,"method":"Multi"}
2014-07-14 01:35:27,824 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28817,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326899007,"queuetimems":0,"class":"HRegionServer","responsesize":15630,"method":"Multi"}
2014-07-14 01:35:27,830 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27484,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326900345,"queuetimems":0,"class":"HRegionServer","responsesize":15805,"method":"Multi"}
2014-07-14 01:35:27,830 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23847,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326903982,"queuetimems":0,"class":"HRegionServer","responsesize":15642,"method":"Multi"}
2014-07-14 01:35:27,830 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26946,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326900883,"queuetimems":0,"class":"HRegionServer","responsesize":15473,"method":"Multi"}
2014-07-14 01:35:27,826 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26798,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326901027,"queuetimems":1,"class":"HRegionServer","responsesize":15842,"method":"Multi"}
2014-07-14 01:35:27,842 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27714,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326900127,"queuetimems":0,"class":"HRegionServer","responsesize":15989,"method":"Multi"}
2014-07-14 01:35:27,842 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28981,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326898860,"queuetimems":0,"class":"HRegionServer","responsesize":15949,"method":"Multi"}
2014-07-14 01:35:27,842 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28872,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326898969,"queuetimems":0,"class":"HRegionServer","responsesize":15809,"method":"Multi"}
2014-07-14 01:35:27,842 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27913,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326899929,"queuetimems":0,"class":"HRegionServer","responsesize":15901,"method":"Multi"}
2014-07-14 01:35:27,825 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28458,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326899367,"queuetimems":0,"class":"HRegionServer","responsesize":15847,"method":"Multi"}
2014-07-14 01:35:27,825 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29002,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326898822,"queuetimems":0,"class":"HRegionServer","responsesize":15939,"method":"Multi"}
2014-07-14 01:35:27,825 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26731,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326901093,"queuetimems":0,"class":"HRegionServer","responsesize":15802,"method":"Multi"}
2014-07-14 01:35:27,842 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26920,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326900921,"queuetimems":0,"class":"HRegionServer","responsesize":15656,"method":"Multi"}
2014-07-14 01:35:27,850 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27271,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326900578,"queuetimems":0,"class":"HRegionServer","responsesize":15731,"method":"Multi"}
2014-07-14 01:35:27,850 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26890,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326900959,"queuetimems":0,"class":"HRegionServer","responsesize":15939,"method":"Multi"}
2014-07-14 01:35:27,850 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28038,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326899811,"queuetimems":0,"class":"HRegionServer","responsesize":16082,"method":"Multi"}
2014-07-14 01:35:27,850 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26856,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326900994,"queuetimems":1,"class":"HRegionServer","responsesize":15760,"method":"Multi"}
2014-07-14 01:35:27,850 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23795,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326904055,"queuetimems":0,"class":"HRegionServer","responsesize":15581,"method":"Multi"}
2014-07-14 01:35:27,859 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27980,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326899879,"queuetimems":0,"class":"HRegionServer","responsesize":16166,"method":"Multi"}
2014-07-14 01:35:27,966 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23939,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326904026,"queuetimems":0,"class":"HRegionServer","responsesize":16076,"method":"Multi"}
2014-07-14 01:35:27,966 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25039,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326902927,"queuetimems":1,"class":"HRegionServer","responsesize":15809,"method":"Multi"}
2014-07-14 01:35:27,966 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26781,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326901185,"queuetimems":0,"class":"HRegionServer","responsesize":15811,"method":"Multi"}
2014-07-14 01:35:27,974 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26820,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326901154,"queuetimems":0,"class":"HRegionServer","responsesize":15992,"method":"Multi"}
2014-07-14 01:35:27,975 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25121,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326902854,"queuetimems":0,"class":"HRegionServer","responsesize":15760,"method":"Multi"}
2014-07-14 01:35:27,976 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27123,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326900851,"queuetimems":0,"class":"HRegionServer","responsesize":15617,"method":"Multi"}
2014-07-14 01:35:27,976 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26853,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326901122,"queuetimems":0,"class":"HRegionServer","responsesize":15710,"method":"Multi"}
2014-07-14 01:35:28,737 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:35:28,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59275 synced till here 59254
2014-07-14 01:35:28,935 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326927111 with entries=96, filesize=80.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326928737
2014-07-14 01:35:29,642 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:35:29,672 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59381 synced till here 59352
2014-07-14 01:35:29,925 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326928737 with entries=106, filesize=90.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326929643
2014-07-14 01:35:31,718 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:35:33,010 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59518 synced till here 59470
2014-07-14 01:35:33,319 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326929643 with entries=137, filesize=117.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326931718
2014-07-14 01:35:34,336 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:35:34,353 DEBUG [RpcServer.handler=6,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:35:34,361 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files; delaying flush up to 90000ms
2014-07-14 01:35:34,362 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 34 store files, 0 compacting, 34 eligible, 20 blocking
2014-07-14 01:35:34,362 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 34 files from compaction candidates
2014-07-14 01:35:34,362 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:35:34,362 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:35:34,362 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:35:34,362 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:35:35,235 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59639 synced till here 59636
2014-07-14 01:35:35,264 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:35:35,264 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files; delaying flush up to 90000ms
2014-07-14 01:35:35,264 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 33 store files, 0 compacting, 33 eligible, 20 blocking
2014-07-14 01:35:35,264 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 33 files from compaction candidates
2014-07-14 01:35:35,264 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:35:35,264 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:35:35,264 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:35:35,265 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:35:35,331 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326931718 with entries=121, filesize=102.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326934336
2014-07-14 01:35:36,200 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:35:36,242 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59748 synced till here 59712
2014-07-14 01:35:37,097 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326934336 with entries=109, filesize=92.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326936200
2014-07-14 01:35:38,760 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:35:39,041 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59881 synced till here 59861
2014-07-14 01:35:39,169 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326936200 with entries=133, filesize=113.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326938761
2014-07-14 01:35:43,177 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:35:43,206 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59955 synced till here 59953
2014-07-14 01:35:43,222 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326938761 with entries=74, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326943178
2014-07-14 01:35:44,815 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:35:44,840 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60030 synced till here 60027
2014-07-14 01:35:44,889 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326943178 with entries=75, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326944816
2014-07-14 01:35:45,681 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:35:45,716 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60109 synced till here 60103
2014-07-14 01:35:46,313 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326944816 with entries=79, filesize=67.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326945681
2014-07-14 01:35:47,198 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:35:47,217 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60182 synced till here 60179
2014-07-14 01:35:47,817 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326945681 with entries=73, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326947198
2014-07-14 01:35:50,782 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:35:52,390 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1475ms
GC pool 'ParNew' had collection(s): count=1 time=1596ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=0ms
2014-07-14 01:35:52,465 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326947198 with entries=110, filesize=94.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326950783
2014-07-14 01:35:52,465 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 6e2bf74487f61672cd8bc06d8b34f003
2014-07-14 01:35:53,367 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:35:53,393 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60389 synced till here 60369
2014-07-14 01:35:54,283 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326950783 with entries=97, filesize=82.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326953368
2014-07-14 01:35:54,283 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 6e2bf74487f61672cd8bc06d8b34f003
2014-07-14 01:35:59,495 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10059,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326949434,"queuetimems":0,"class":"HRegionServer","responsesize":15754,"method":"Multi"}
2014-07-14 01:35:59,509 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10121,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326949387,"queuetimems":0,"class":"HRegionServer","responsesize":15674,"method":"Multi"}
2014-07-14 01:36:00,696 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11358,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326949337,"queuetimems":0,"class":"HRegionServer","responsesize":15928,"method":"Multi"}
2014-07-14 01:36:00,695 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11224,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326949470,"queuetimems":1,"class":"HRegionServer","responsesize":15301,"method":"Multi"}
2014-07-14 01:36:00,705 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10992,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326949713,"queuetimems":1,"class":"HRegionServer","responsesize":16025,"method":"Multi"}
2014-07-14 01:36:00,695 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11190,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326949504,"queuetimems":0,"class":"HRegionServer","responsesize":15917,"method":"Multi"}
2014-07-14 01:36:00,705 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11051,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326949654,"queuetimems":0,"class":"HRegionServer","responsesize":15939,"method":"Multi"}
2014-07-14 01:36:00,705 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11140,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326949565,"queuetimems":0,"class":"HRegionServer","responsesize":15505,"method":"Multi"}
2014-07-14 01:36:00,884 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:36:06,973 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90453ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:36:06,973 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90417ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:36:06,974 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 1.2g
2014-07-14 01:36:06,974 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 1.2g
2014-07-14 01:36:15,024 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1348ms
GC pool 'ParNew' had collection(s): count=1 time=1599ms
2014-07-14 01:36:15,039 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60518 synced till here 60473
2014-07-14 01:36:15,417 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22382,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326953035,"queuetimems":93,"class":"HRegionServer","responsesize":15691,"method":"Multi"}
2014-07-14 01:36:15,452 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22416,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326953035,"queuetimems":64,"class":"HRegionServer","responsesize":15939,"method":"Multi"}
2014-07-14 01:36:15,457 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22421,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326953036,"queuetimems":0,"class":"HRegionServer","responsesize":15562,"method":"Multi"}
2014-07-14 01:36:15,457 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22194,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326953263,"queuetimems":0,"class":"HRegionServer","responsesize":15380,"method":"Multi"}
2014-07-14 01:36:15,462 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22281,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326953176,"queuetimems":0,"class":"HRegionServer","responsesize":15542,"method":"Multi"}
2014-07-14 01:36:15,510 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22435,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326953075,"queuetimems":1,"class":"HRegionServer","responsesize":15669,"method":"Multi"}
2014-07-14 01:36:15,512 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22372,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326953139,"queuetimems":0,"class":"HRegionServer","responsesize":15917,"method":"Multi"}
2014-07-14 01:36:15,512 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22292,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326953220,"queuetimems":0,"class":"HRegionServer","responsesize":15505,"method":"Multi"}
2014-07-14 01:36:15,514 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22414,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326953100,"queuetimems":0,"class":"HRegionServer","responsesize":15947,"method":"Multi"}
2014-07-14 01:36:15,522 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22073,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326953449,"queuetimems":0,"class":"HRegionServer","responsesize":15699,"method":"Multi"}
2014-07-14 01:36:15,553 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326953368 with entries=129, filesize=110.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326960884
2014-07-14 01:36:15,653 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22238,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326953415,"queuetimems":1,"class":"HRegionServer","responsesize":15659,"method":"Multi"}
2014-07-14 01:36:15,895 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22415,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326953479,"queuetimems":0,"class":"HRegionServer","responsesize":15928,"method":"Multi"}
2014-07-14 01:36:15,896 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22514,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326953382,"queuetimems":0,"class":"HRegionServer","responsesize":15754,"method":"Multi"}
2014-07-14 01:36:16,255 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22906,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326953348,"queuetimems":0,"class":"HRegionServer","responsesize":15628,"method":"Multi"}
2014-07-14 01:36:16,255 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22946,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326953309,"queuetimems":0,"class":"HRegionServer","responsesize":16025,"method":"Multi"}
2014-07-14 01:36:16,259 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21979,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326954279,"queuetimems":1,"class":"HRegionServer","responsesize":15301,"method":"Multi"}
2014-07-14 01:36:16,259 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21924,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326954334,"queuetimems":0,"class":"HRegionServer","responsesize":15751,"method":"Multi"}
2014-07-14 01:36:16,286 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22016,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326954269,"queuetimems":1,"class":"HRegionServer","responsesize":15674,"method":"Multi"}
2014-07-14 01:36:16,286 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21809,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326954476,"queuetimems":0,"class":"HRegionServer","responsesize":15813,"method":"Multi"}
2014-07-14 01:36:17,476 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:36:17,567 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:36:17,597 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:36:18,876 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60641 synced till here 60591
2014-07-14 01:36:19,122 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24709,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326954413,"queuetimems":0,"class":"HRegionServer","responsesize":15847,"method":"Multi"}
2014-07-14 01:36:19,129 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24617,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326954512,"queuetimems":1,"class":"HRegionServer","responsesize":15855,"method":"Multi"}
2014-07-14 01:36:19,130 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24442,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326954687,"queuetimems":1,"class":"HRegionServer","responsesize":15728,"method":"Multi"}
2014-07-14 01:36:19,134 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24325,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326954809,"queuetimems":0,"class":"HRegionServer","responsesize":15817,"method":"Multi"}
2014-07-14 01:36:19,134 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24557,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326954577,"queuetimems":1,"class":"HRegionServer","responsesize":16025,"method":"Multi"}
2014-07-14 01:36:19,135 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24355,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326954779,"queuetimems":18,"class":"HRegionServer","responsesize":15706,"method":"Multi"}
2014-07-14 01:36:19,135 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19664,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326959471,"queuetimems":2288,"class":"HRegionServer","responsesize":15813,"method":"Multi"}
2014-07-14 01:36:19,135 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24484,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326954651,"queuetimems":1,"class":"HRegionServer","responsesize":15877,"method":"Multi"}
2014-07-14 01:36:19,135 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22048,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326957087,"queuetimems":0,"class":"HRegionServer","responsesize":15939,"method":"Multi"}
2014-07-14 01:36:19,140 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24413,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326954726,"queuetimems":0,"class":"HRegionServer","responsesize":16068,"method":"Multi"}
2014-07-14 01:36:19,134 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24519,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326954615,"queuetimems":1,"class":"HRegionServer","responsesize":15813,"method":"Multi"}
2014-07-14 01:36:19,140 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19661,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326959479,"queuetimems":2158,"class":"HRegionServer","responsesize":15562,"method":"Multi"}
2014-07-14 01:36:19,140 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21995,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326957145,"queuetimems":0,"class":"HRegionServer","responsesize":15301,"method":"Multi"}
2014-07-14 01:36:19,140 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22090,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326957049,"queuetimems":1,"class":"HRegionServer","responsesize":15691,"method":"Multi"}
2014-07-14 01:36:19,140 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18417,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326960722,"queuetimems":2970,"class":"HRegionServer","responsesize":15855,"method":"Multi"}
2014-07-14 01:36:19,140 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23334,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326955805,"queuetimems":1,"class":"HRegionServer","responsesize":15989,"method":"Multi"}
2014-07-14 01:36:19,140 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19661,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326959478,"queuetimems":2189,"class":"HRegionServer","responsesize":15628,"method":"Multi"}
2014-07-14 01:36:19,332 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19858,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326959474,"queuetimems":2219,"class":"HRegionServer","responsesize":15751,"method":"Multi"}
2014-07-14 01:36:19,334 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18637,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326960696,"queuetimems":3146,"class":"HRegionServer","responsesize":15947,"method":"Multi"}
2014-07-14 01:36:19,334 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18637,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326960696,"queuetimems":3178,"class":"HRegionServer","responsesize":16025,"method":"Multi"}
2014-07-14 01:36:19,334 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19832,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326959502,"queuetimems":2019,"class":"HRegionServer","responsesize":15505,"method":"Multi"}
2014-07-14 01:36:19,335 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19838,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326959496,"queuetimems":2075,"class":"HRegionServer","responsesize":15847,"method":"Multi"}
2014-07-14 01:36:19,337 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19864,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326959472,"queuetimems":2252,"class":"HRegionServer","responsesize":15674,"method":"Multi"}
2014-07-14 01:36:19,337 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18630,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326960706,"queuetimems":3052,"class":"HRegionServer","responsesize":15699,"method":"Multi"}
2014-07-14 01:36:19,337 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18632,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326960705,"queuetimems":3118,"class":"HRegionServer","responsesize":15817,"method":"Multi"}
2014-07-14 01:36:19,341 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19856,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326959485,"queuetimems":2131,"class":"HRegionServer","responsesize":15542,"method":"Multi"}
2014-07-14 01:36:19,334 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18627,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326960706,"queuetimems":3019,"class":"HRegionServer","responsesize":15928,"method":"Multi"}
2014-07-14 01:36:19,337 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18630,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326960706,"queuetimems":3083,"class":"HRegionServer","responsesize":15659,"method":"Multi"}
2014-07-14 01:36:19,353 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326960884 with entries=123, filesize=105.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326977477
2014-07-14 01:36:20,594 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21108,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326959486,"queuetimems":2099,"class":"HRegionServer","responsesize":15380,"method":"Multi"}
2014-07-14 01:36:20,594 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19881,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326960713,"queuetimems":2993,"class":"HRegionServer","responsesize":15754,"method":"Multi"}
2014-07-14 01:36:20,597 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21096,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326959501,"queuetimems":2049,"class":"HRegionServer","responsesize":15669,"method":"Multi"}
2014-07-14 01:36:21,789 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:21,790 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:21,791 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:21,797 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:21,807 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:21,814 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:21,817 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:21,817 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:21,826 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:21,827 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:21,830 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:21,830 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:21,830 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:21,830 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:21,830 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:21,830 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:21,832 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:21,864 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:21,921 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,058 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:36:22,064 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,065 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,065 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,066 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,067 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,067 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,067 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,068 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,074 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60730 synced till here 60718
2014-07-14 01:36:22,151 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,152 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,160 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,160 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,160 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,161 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,162 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,162 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,162 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,163 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,163 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,163 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,165 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,165 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326977477 with entries=89, filesize=75.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326982058
2014-07-14 01:36:22,166 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,166 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,167 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,167 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,168 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,169 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,170 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,170 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,170 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:22,220 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:36:23,352 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=10829, hits=3899, hitRatio=36.00%, , cachingAccesses=3903, cachingHits=3898, cachingHitsRatio=99.87%, evictions=0, evicted=3, evictedPerRun=Infinity
2014-07-14 01:36:26,790 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:36:26,791 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:36:26,792 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:36:26,805 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5009ms
2014-07-14 01:36:26,807 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5010ms
2014-07-14 01:36:26,814 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:36:26,817 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5010ms
2014-07-14 01:36:26,818 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:36:26,826 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:36:26,828 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:36:26,831 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:36:26,831 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5029ms
2014-07-14 01:36:26,832 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5029ms
2014-07-14 01:36:26,832 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5025ms
2014-07-14 01:36:26,833 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5027ms
2014-07-14 01:36:26,833 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5003ms
2014-07-14 01:36:26,834 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-14 01:36:26,864 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:36:26,921 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:36:27,065 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:36:27,065 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:36:27,066 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:36:27,067 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:36:27,067 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:36:27,067 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:36:27,068 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:36:27,068 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:36:27,152 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:36:27,152 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:36:27,160 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:36:27,160 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:36:27,161 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:36:27,162 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:36:27,162 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:36:27,162 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:36:27,162 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:36:27,163 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:36:27,164 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:36:27,164 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:36:27,165 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-14 01:36:27,166 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:36:27,166 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:36:27,167 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:36:27,167 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:36:27,168 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:36:27,169 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:36:27,170 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:36:27,170 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:36:27,170 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:36:27,221 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:36:31,791 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:36:31,791 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:36:31,793 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:36:31,806 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10009ms
2014-07-14 01:36:31,808 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10011ms
2014-07-14 01:36:31,814 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:36:31,817 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10010ms
2014-07-14 01:36:31,818 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:36:31,827 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:36:31,828 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:36:31,831 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:36:31,832 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10030ms
2014-07-14 01:36:31,832 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10029ms
2014-07-14 01:36:31,832 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10025ms
2014-07-14 01:36:31,833 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10028ms
2014-07-14 01:36:31,834 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10004ms
2014-07-14 01:36:31,835 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-14 01:36:31,865 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:36:31,922 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:36:32,065 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:36:32,066 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:36:32,066 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:36:32,067 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:36:32,067 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:36:32,068 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:36:32,068 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:36:32,068 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:36:32,152 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:36:32,153 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:36:32,160 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:36:32,160 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:36:32,161 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:36:32,162 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:36:32,162 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:36:32,163 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:36:32,163 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:36:32,164 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:36:32,164 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:36:32,164 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:36:32,165 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:36:32,167 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:36:32,167 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:36:32,168 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:36:32,168 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:36:32,169 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:36:32,170 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:36:32,170 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:36:32,170 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:36:32,171 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:36:32,221 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:36:36,792 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:36:36,792 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:36:36,793 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:36:36,806 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15010ms
2014-07-14 01:36:36,808 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15011ms
2014-07-14 01:36:36,815 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:36,818 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15011ms
2014-07-14 01:36:36,818 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:36,827 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:36,828 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:36,832 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:36:36,832 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15030ms
2014-07-14 01:36:36,832 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15029ms
2014-07-14 01:36:36,833 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15026ms
2014-07-14 01:36:36,834 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15029ms
2014-07-14 01:36:36,834 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15004ms
2014-07-14 01:36:36,835 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15003ms
2014-07-14 01:36:36,865 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:36,922 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:37,066 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:36:37,066 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:37,067 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:36:37,067 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:37,068 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:37,068 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:36:37,068 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:37,069 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:37,153 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:36:37,153 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:37,161 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:36:37,161 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:37,162 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:37,162 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:37,163 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:37,163 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:37,163 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:37,164 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:37,164 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:37,165 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:36:37,165 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:36:37,167 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:36:37,167 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:37,168 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:37,168 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:37,169 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:37,170 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:37,170 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:37,171 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:37,171 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:37,222 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:36:37,272 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=15214, memsize=400.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/e1c667a5c8fa48d1aee312b5e2427699
2014-07-14 01:36:37,291 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/e1c667a5c8fa48d1aee312b5e2427699 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/e1c667a5c8fa48d1aee312b5e2427699
2014-07-14 01:36:37,308 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/e1c667a5c8fa48d1aee312b5e2427699, entries=1459750, sequenceid=15214, filesize=104.0m
2014-07-14 01:36:37,309 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.3g/1343024320, currentsize=83.9m/87975280 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 30335ms, sequenceid=15214, compaction requested=true
2014-07-14 01:36:37,309 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:36:37,309 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 37 store files, 0 compacting, 37 eligible, 20 blocking
2014-07-14 01:36:37,310 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15089ms
2014-07-14 01:36:37,310 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 37 files from compaction candidates
2014-07-14 01:36:37,310 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,310 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:36:37,310 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15140ms
2014-07-14 01:36:37,310 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,310 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:36:37,310 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15140ms
2014-07-14 01:36:37,310 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,310 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:36:37,310 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15141ms
2014-07-14 01:36:37,311 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,311 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15142ms
2014-07-14 01:36:37,311 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,313 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15145ms
2014-07-14 01:36:37,313 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,315 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15148ms
2014-07-14 01:36:37,315 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,315 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15148ms
2014-07-14 01:36:37,315 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,315 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15149ms
2014-07-14 01:36:37,315 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,315 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15150ms
2014-07-14 01:36:37,315 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,325 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15162ms
2014-07-14 01:36:37,325 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,325 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15163ms
2014-07-14 01:36:37,325 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,325 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15162ms
2014-07-14 01:36:37,325 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,325 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15162ms
2014-07-14 01:36:37,325 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,325 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15163ms
2014-07-14 01:36:37,325 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,815 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15653ms
2014-07-14 01:36:37,815 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,815 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15653ms
2014-07-14 01:36:37,815 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,815 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15654ms
2014-07-14 01:36:37,815 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,819 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15659ms
2014-07-14 01:36:37,819 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,824 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15664ms
2014-07-14 01:36:37,824 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,825 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15666ms
2014-07-14 01:36:37,825 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,825 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15673ms
2014-07-14 01:36:37,825 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,837 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15686ms
2014-07-14 01:36:37,837 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,845 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15777ms
2014-07-14 01:36:37,845 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,845 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15778ms
2014-07-14 01:36:37,845 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,846 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15779ms
2014-07-14 01:36:37,846 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,846 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15779ms
2014-07-14 01:36:37,846 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,853 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15787ms
2014-07-14 01:36:37,853 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,861 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15796ms
2014-07-14 01:36:37,861 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,869 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15804ms
2014-07-14 01:36:37,869 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,869 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15805ms
2014-07-14 01:36:37,869 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,873 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15952ms
2014-07-14 01:36:37,873 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,875 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16011ms
2014-07-14 01:36:37,875 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,875 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16043ms
2014-07-14 01:36:37,875 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,885 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16055ms
2014-07-14 01:36:37,885 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,885 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16080ms
2014-07-14 01:36:37,885 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,885 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16078ms
2014-07-14 01:36:37,885 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,885 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16082ms
2014-07-14 01:36:37,885 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,888 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16086ms
2014-07-14 01:36:37,888 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,888 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16058ms
2014-07-14 01:36:37,888 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,897 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16070ms
2014-07-14 01:36:37,897 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,897 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16071ms
2014-07-14 01:36:37,897 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,897 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16080ms
2014-07-14 01:36:37,897 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,905 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16098ms
2014-07-14 01:36:37,905 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,909 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16095ms
2014-07-14 01:36:37,909 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,909 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16112ms
2014-07-14 01:36:37,909 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,910 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16113ms
2014-07-14 01:36:37,910 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,911 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16120ms
2014-07-14 01:36:37,911 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,911 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16121ms
2014-07-14 01:36:37,911 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:37,912 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16123ms
2014-07-14 01:36:37,912 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:36:38,174 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21914,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326976259,"queuetimems":15274,"class":"HRegionServer","responsesize":15928,"method":"Multi"}
2014-07-14 01:36:38,361 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22104,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326976256,"queuetimems":15377,"class":"HRegionServer","responsesize":15699,"method":"Multi"}
2014-07-14 01:36:38,445 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21143,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326977302,"queuetimems":16253,"class":"HRegionServer","responsesize":15542,"method":"Multi"}
2014-07-14 01:36:38,445 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19110,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326979335,"queuetimems":15499,"class":"HRegionServer","responsesize":15813,"method":"Multi"}
2014-07-14 01:36:38,457 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21156,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326977301,"queuetimems":16284,"class":"HRegionServer","responsesize":15947,"method":"Multi"}
2014-07-14 01:36:38,457 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22198,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326976259,"queuetimems":15304,"class":"HRegionServer","responsesize":15917,"method":"Multi"}
2014-07-14 01:36:38,457 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22201,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326976256,"queuetimems":15332,"class":"HRegionServer","responsesize":15751,"method":"Multi"}
2014-07-14 01:36:38,519 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19177,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326979342,"queuetimems":15341,"class":"HRegionServer","responsesize":15699,"method":"Multi"}
2014-07-14 01:36:38,525 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19190,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326979335,"queuetimems":15529,"class":"HRegionServer","responsesize":16025,"method":"Multi"}
2014-07-14 01:36:38,528 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19392,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326979136,"queuetimems":17849,"class":"HRegionServer","responsesize":15628,"method":"Multi"}
2014-07-14 01:36:38,643 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=15172, memsize=442.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/3e877f1b65474ec1a307a278bfaaadb6
2014-07-14 01:36:38,660 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/3e877f1b65474ec1a307a278bfaaadb6 as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/3e877f1b65474ec1a307a278bfaaadb6
2014-07-14 01:36:38,673 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/3e877f1b65474ec1a307a278bfaaadb6, entries=1610740, sequenceid=15172, filesize=114.8m
2014-07-14 01:36:38,674 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.3g/1366446800, currentsize=125.2m/131241200 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 31701ms, sequenceid=15172, compaction requested=true
2014-07-14 01:36:38,674 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:36:38,674 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 38 store files, 0 compacting, 38 eligible, 20 blocking
2014-07-14 01:36:38,675 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 38 files from compaction candidates
2014-07-14 01:36:38,675 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:36:38,675 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:36:38,675 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:36:38,684 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:36:38,709 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60839 synced till here 60807
2014-07-14 01:36:39,021 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19867,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326979153,"queuetimems":17657,"class":"HRegionServer","responsesize":16068,"method":"Multi"}
2014-07-14 01:36:39,025 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19889,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326979135,"queuetimems":17937,"class":"HRegionServer","responsesize":15301,"method":"Multi"}
2014-07-14 01:36:39,028 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19867,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326979161,"queuetimems":15878,"class":"HRegionServer","responsesize":15691,"method":"Multi"}
2014-07-14 01:36:39,028 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19898,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326979130,"queuetimems":17991,"class":"HRegionServer","responsesize":15754,"method":"Multi"}
2014-07-14 01:36:39,030 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19876,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326979153,"queuetimems":17687,"class":"HRegionServer","responsesize":15813,"method":"Multi"}
2014-07-14 01:36:39,032 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19909,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326979123,"queuetimems":18043,"class":"HRegionServer","responsesize":15813,"method":"Multi"}
2014-07-14 01:36:39,036 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18440,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326980595,"queuetimems":16395,"class":"HRegionServer","responsesize":15542,"method":"Multi"}
2014-07-14 01:36:39,036 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19880,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326979156,"queuetimems":17631,"class":"HRegionServer","responsesize":15706,"method":"Multi"}
2014-07-14 01:36:39,036 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19698,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326979338,"queuetimems":15377,"class":"HRegionServer","responsesize":15628,"method":"Multi"}
2014-07-14 01:36:39,036 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19901,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326979130,"queuetimems":18021,"class":"HRegionServer","responsesize":15380,"method":"Multi"}
2014-07-14 01:36:39,047 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326982058 with entries=109, filesize=90.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326998684
2014-07-14 01:36:39,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326836263
2014-07-14 01:36:39,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326837808
2014-07-14 01:36:39,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326867462
2014-07-14 01:36:39,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326869203
2014-07-14 01:36:39,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326870640
2014-07-14 01:36:39,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326872280
2014-07-14 01:36:39,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326874221
2014-07-14 01:36:39,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326875427
2014-07-14 01:36:39,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326876834
2014-07-14 01:36:39,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326878642
2014-07-14 01:36:39,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326880533
2014-07-14 01:36:39,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326881712
2014-07-14 01:36:39,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326883502
2014-07-14 01:36:39,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326885227
2014-07-14 01:36:39,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326887332
2014-07-14 01:36:39,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326889764
2014-07-14 01:36:39,049 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326891190
2014-07-14 01:36:39,049 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326892809
2014-07-14 01:36:39,056 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19722,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326979334,"queuetimems":15558,"class":"HRegionServer","responsesize":15751,"method":"Multi"}
2014-07-14 01:36:39,056 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19907,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326979149,"queuetimems":17771,"class":"HRegionServer","responsesize":16025,"method":"Multi"}
2014-07-14 01:36:39,056 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19718,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326979338,"queuetimems":15414,"class":"HRegionServer","responsesize":15847,"method":"Multi"}
2014-07-14 01:36:39,170 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20028,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326979141,"queuetimems":17794,"class":"HRegionServer","responsesize":15669,"method":"Multi"}
2014-07-14 01:36:40,160 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18421,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326981738,"queuetimems":17386,"class":"HRegionServer","responsesize":15754,"method":"Multi"}
2014-07-14 01:36:40,177 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20833,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326979344,"queuetimems":15301,"class":"HRegionServer","responsesize":15380,"method":"Multi"}
2014-07-14 01:36:40,181 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18459,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326981722,"queuetimems":17407,"class":"HRegionServer","responsesize":15301,"method":"Multi"}
2014-07-14 01:36:40,181 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18018,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326982163,"queuetimems":17238,"class":"HRegionServer","responsesize":15813,"method":"Multi"}
2014-07-14 01:36:40,181 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21032,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326979149,"queuetimems":17712,"class":"HRegionServer","responsesize":15877,"method":"Multi"}
2014-07-14 01:36:40,181 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21041,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326979140,"queuetimems":17823,"class":"HRegionServer","responsesize":15674,"method":"Multi"}
2014-07-14 01:36:40,181 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19583,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326980598,"queuetimems":16362,"class":"HRegionServer","responsesize":15562,"method":"Multi"}
2014-07-14 01:36:40,375 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21042,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326979333,"queuetimems":16020,"class":"HRegionServer","responsesize":15939,"method":"Multi"}
2014-07-14 01:36:40,375 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19777,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326980598,"queuetimems":16324,"class":"HRegionServer","responsesize":15928,"method":"Multi"}
2014-07-14 01:36:40,375 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21239,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326979136,"queuetimems":17879,"class":"HRegionServer","responsesize":15505,"method":"Multi"}
2014-07-14 01:36:40,381 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18235,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326982146,"queuetimems":17362,"class":"HRegionServer","responsesize":15728,"method":"Multi"}
2014-07-14 01:36:40,381 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21037,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326979344,"queuetimems":15260,"class":"HRegionServer","responsesize":15947,"method":"Multi"}
2014-07-14 01:36:40,381 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19787,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326980594,"queuetimems":16475,"class":"HRegionServer","responsesize":15917,"method":"Multi"}
2014-07-14 01:36:40,383 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18239,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326982143,"queuetimems":17687,"class":"HRegionServer","responsesize":15659,"method":"Multi"}
2014-07-14 01:36:40,383 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18219,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326982163,"queuetimems":17201,"class":"HRegionServer","responsesize":16068,"method":"Multi"}
2014-07-14 01:36:40,384 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21046,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326979337,"queuetimems":15442,"class":"HRegionServer","responsesize":15855,"method":"Multi"}
2014-07-14 01:36:40,375 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19780,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326980595,"queuetimems":16436,"class":"HRegionServer","responsesize":15817,"method":"Multi"}
2014-07-14 01:36:40,384 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18241,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326982142,"queuetimems":17718,"class":"HRegionServer","responsesize":15669,"method":"Multi"}
2014-07-14 01:36:40,383 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18222,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326982160,"queuetimems":17308,"class":"HRegionServer","responsesize":15706,"method":"Multi"}
2014-07-14 01:36:40,381 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18229,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326982152,"queuetimems":17335,"class":"HRegionServer","responsesize":16025,"method":"Multi"}
2014-07-14 01:36:40,576 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:36:40,578 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18415,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326982162,"queuetimems":17278,"class":"HRegionServer","responsesize":15877,"method":"Multi"}
2014-07-14 01:36:40,578 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21442,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326979135,"queuetimems":17965,"class":"HRegionServer","responsesize":15817,"method":"Multi"}
2014-07-14 01:36:40,578 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21442,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326979135,"queuetimems":17907,"class":"HRegionServer","responsesize":15562,"method":"Multi"}
2014-07-14 01:36:40,579 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18436,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326982142,"queuetimems":17754,"class":"HRegionServer","responsesize":15505,"method":"Multi"}
2014-07-14 01:36:40,619 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60954 synced till here 60921
2014-07-14 01:36:40,790 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21640,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326979149,"queuetimems":17741,"class":"HRegionServer","responsesize":15728,"method":"Multi"}
2014-07-14 01:36:40,803 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21466,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47143","starttimems":1405326979337,"queuetimems":15472,"class":"HRegionServer","responsesize":15674,"method":"Multi"}
2014-07-14 01:36:40,905 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326998684 with entries=115, filesize=98.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327000576
2014-07-14 01:36:42,514 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:36:42,812 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61077 synced till here 61069
2014-07-14 01:36:42,907 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327000576 with entries=123, filesize=104.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327002514
2014-07-14 01:36:44,504 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:36:44,518 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61171 synced till here 61153
2014-07-14 01:36:44,709 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327002514 with entries=94, filesize=80.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327004504
2014-07-14 01:36:45,137 DEBUG [RpcServer.handler=41,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:36:45,137 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files; delaying flush up to 90000ms
2014-07-14 01:36:45,138 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:36:45,138 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 38 store files, 0 compacting, 38 eligible, 20 blocking
2014-07-14 01:36:45,138 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 38 files from compaction candidates
2014-07-14 01:36:45,138 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:36:45,138 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:36:45,138 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:36:45,476 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:36:45,560 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327004504 with entries=73, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327005476
2014-07-14 01:36:45,889 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:36:45,889 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files; delaying flush up to 90000ms
2014-07-14 01:36:45,889 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:36:45,889 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 37 store files, 0 compacting, 37 eligible, 20 blocking
2014-07-14 01:36:45,890 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 37 files from compaction candidates
2014-07-14 01:36:45,890 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:36:45,890 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:36:45,890 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:36:46,913 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:36:46,937 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61322 synced till here 61316
2014-07-14 01:36:47,079 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327005476 with entries=78, filesize=66.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327006913
2014-07-14 01:36:49,526 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:36:49,566 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61403 synced till here 61395
2014-07-14 01:36:49,694 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327006913 with entries=81, filesize=69.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327009527
2014-07-14 01:36:51,049 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:36:51,079 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327009527 with entries=76, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327011049
2014-07-14 01:36:51,883 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:36:52,631 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61582 synced till here 61575
2014-07-14 01:36:52,746 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327011049 with entries=103, filesize=88.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327011883
2014-07-14 01:36:54,010 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:36:54,296 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61673 synced till here 61672
2014-07-14 01:36:54,311 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327011883 with entries=91, filesize=78.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327014011
2014-07-14 01:36:56,380 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:36:56,603 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327014011 with entries=80, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327016381
2014-07-14 01:36:59,229 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:36:59,256 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61826 synced till here 61825
2014-07-14 01:36:59,277 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327016381 with entries=73, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327019229
2014-07-14 01:37:04,579 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90226ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:37:04,580 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 1.1g
2014-07-14 01:37:05,480 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90216ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:37:05,481 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 1.1g
2014-07-14 01:37:05,484 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:37:06,332 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:37:19,052 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:37:19,076 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61900 synced till here 61899
2014-07-14 01:37:19,096 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327019229 with entries=74, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327039052
2014-07-14 01:37:19,446 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=15494, memsize=343.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/de3c57c6bdf44cc5ba661915e1cb3d7f
2014-07-14 01:37:19,460 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/de3c57c6bdf44cc5ba661915e1cb3d7f as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/de3c57c6bdf44cc5ba661915e1cb3d7f
2014-07-14 01:37:19,471 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/de3c57c6bdf44cc5ba661915e1cb3d7f, entries=1250520, sequenceid=15494, filesize=89.1m
2014-07-14 01:37:19,472 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.1g/1188333040, currentsize=25.1m/26282080 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 14892ms, sequenceid=15494, compaction requested=true
2014-07-14 01:37:19,473 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:37:19,473 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 35 store files, 0 compacting, 35 eligible, 20 blocking
2014-07-14 01:37:19,474 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 35 files from compaction candidates
2014-07-14 01:37:19,474 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:37:19,474 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:37:19,474 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:37:20,904 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=15484, memsize=358.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/a055dcc902264ecb8d15cf410c0cd145
2014-07-14 01:37:20,921 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/a055dcc902264ecb8d15cf410c0cd145 as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/a055dcc902264ecb8d15cf410c0cd145
2014-07-14 01:37:20,931 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/a055dcc902264ecb8d15cf410c0cd145, entries=1304910, sequenceid=15484, filesize=93.0m
2014-07-14 01:37:20,932 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.1g/1205029760, currentsize=38.7m/40617920 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 15451ms, sequenceid=15484, compaction requested=true
2014-07-14 01:37:20,933 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 34 store files, 0 compacting, 34 eligible, 20 blocking
2014-07-14 01:37:20,933 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 34 files from compaction candidates
2014-07-14 01:37:20,933 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:37:20,933 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:37:20,933 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:37:20,934 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:37:22,558 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:37:22,593 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61975 synced till here 61974
2014-07-14 01:37:22,620 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327039052 with entries=75, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327042559
2014-07-14 01:37:22,620 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326893956
2014-07-14 01:37:22,620 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326895804
2014-07-14 01:37:22,620 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326897502
2014-07-14 01:37:22,620 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326924932
2014-07-14 01:37:22,620 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326927111
2014-07-14 01:37:22,620 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326928737
2014-07-14 01:37:22,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326929643
2014-07-14 01:37:22,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326931718
2014-07-14 01:37:22,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326934336
2014-07-14 01:37:22,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326936200
2014-07-14 01:37:22,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326938761
2014-07-14 01:37:22,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326943178
2014-07-14 01:37:22,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326944816
2014-07-14 01:37:22,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326945681
2014-07-14 01:37:22,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326947198
2014-07-14 01:37:22,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326950783
2014-07-14 01:37:22,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326953368
2014-07-14 01:37:23,601 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:37:23,905 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327042559 with entries=79, filesize=67.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327043602
2014-07-14 01:37:27,631 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:37:27,870 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 62135 synced till here 62128
2014-07-14 01:37:27,920 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327043602 with entries=81, filesize=69.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327047631
2014-07-14 01:37:31,939 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:37:31,960 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 62210 synced till here 62207
2014-07-14 01:37:31,985 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327047631 with entries=75, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327051940
2014-07-14 01:37:34,248 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:37:34,363 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 62292 synced till here 62284
2014-07-14 01:37:34,439 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327051940 with entries=82, filesize=70.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327054248
2014-07-14 01:37:35,800 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:37:36,089 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 62364 synced till here 62363
2014-07-14 01:37:36,109 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327054248 with entries=72, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327055801
2014-07-14 01:37:39,774 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:37:39,792 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 62439 synced till here 62437
2014-07-14 01:37:39,830 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327055801 with entries=75, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327059774
2014-07-14 01:37:41,136 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:37:41,340 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 62522 synced till here 62521
2014-07-14 01:37:41,765 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327059774 with entries=83, filesize=71.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327061136
2014-07-14 01:37:41,853 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:37:41,853 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files; delaying flush up to 90000ms
2014-07-14 01:37:41,854 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 35 store files, 0 compacting, 35 eligible, 20 blocking
2014-07-14 01:37:41,854 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 35 files from compaction candidates
2014-07-14 01:37:41,854 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:37:41,854 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:37:41,854 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:37:41,854 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:37:41,912 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:37:41,912 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files; delaying flush up to 90000ms
2014-07-14 01:37:41,913 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 34 store files, 0 compacting, 34 eligible, 20 blocking
2014-07-14 01:37:41,913 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 34 files from compaction candidates
2014-07-14 01:37:41,913 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:37:41,913 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:37:41,913 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:37:41,913 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:37:43,205 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:37:43,228 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 62595 synced till here 62593
2014-07-14 01:37:43,267 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327061136 with entries=73, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327063205
2014-07-14 01:37:48,385 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:37:48,408 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 62670 synced till here 62669
2014-07-14 01:37:48,677 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327063205 with entries=75, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327068385
2014-07-14 01:37:50,349 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:37:50,369 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 62742 synced till here 62741
2014-07-14 01:37:50,383 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327068385 with entries=72, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327070350
2014-07-14 01:37:51,597 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:37:51,957 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327070350 with entries=87, filesize=74.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327071598
2014-07-14 01:37:53,107 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:37:53,123 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 62904 synced till here 62902
2014-07-14 01:37:53,173 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327071598 with entries=75, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327073107
2014-07-14 01:37:58,960 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:37:58,973 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 62978 synced till here 62977
2014-07-14 01:37:58,985 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327073107 with entries=74, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327078960
2014-07-14 01:38:02,314 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:38:02,371 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327078960 with entries=81, filesize=69.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327082314
2014-07-14 01:38:07,368 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:38:07,441 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 63139 synced till here 63130
2014-07-14 01:38:08,260 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327082314 with entries=80, filesize=68.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327087368
2014-07-14 01:38:09,314 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:38:09,338 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327087368 with entries=72, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327089315
2014-07-14 01:38:11,271 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:38:11,293 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 63284 synced till here 63283
2014-07-14 01:38:11,322 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327089315 with entries=73, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327091272
2014-07-14 01:38:11,323 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 2 regions(s): 6e2bf74487f61672cd8bc06d8b34f003, 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:38:15,912 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90775ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:38:15,912 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90023ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:38:15,913 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 1.1g
2014-07-14 01:38:15,915 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 1.0g
2014-07-14 01:38:16,607 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:38:16,648 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:38:17,585 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:38:17,608 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 63357 synced till here 63356
2014-07-14 01:38:17,627 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327091272 with entries=73, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327097585
2014-07-14 01:38:22,385 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:38:22,614 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 63450 synced till here 63448
2014-07-14 01:38:22,638 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327097585 with entries=93, filesize=79.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327102386
2014-07-14 01:38:26,827 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:38:26,847 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 63524 synced till here 63522
2014-07-14 01:38:26,871 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327102386 with entries=74, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327106828
2014-07-14 01:38:28,645 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:38:28,659 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 63596 synced till here 63595
2014-07-14 01:38:28,668 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327106828 with entries=72, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327108645
2014-07-14 01:38:31,353 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:38:31,385 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327108645 with entries=71, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327111354
2014-07-14 01:38:35,268 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:38:35,298 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327111354 with entries=73, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327115269
2014-07-14 01:38:36,811 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:38:36,871 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327115269 with entries=74, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327116812
2014-07-14 01:38:37,145 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:38:37,190 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:38:37,364 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:38:37,884 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:38:37,928 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:38:39,310 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:38:41,316 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:38:41,356 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:38:41,394 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:38:41,433 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:38:42,146 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:38:42,190 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:38:42,365 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:38:42,885 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:38:42,928 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:38:43,418 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:38:44,310 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:38:44,910 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=15872, memsize=780.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/8ae16bd9b4024cbabc2a1b5e673fe775
2014-07-14 01:38:44,914 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=15894, memsize=780.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/05fdc72dec054a868175ad94fc666043
2014-07-14 01:38:44,950 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/8ae16bd9b4024cbabc2a1b5e673fe775 as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/8ae16bd9b4024cbabc2a1b5e673fe775
2014-07-14 01:38:44,962 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/05fdc72dec054a868175ad94fc666043 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/05fdc72dec054a868175ad94fc666043
2014-07-14 01:38:44,992 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/8ae16bd9b4024cbabc2a1b5e673fe775, entries=2840770, sequenceid=15872, filesize=202.2m
2014-07-14 01:38:44,992 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.1g/1137965680, currentsize=196.5m/206085200 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 29079ms, sequenceid=15872, compaction requested=true
2014-07-14 01:38:44,993 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:38:44,993 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 39 store files, 0 compacting, 39 eligible, 20 blocking
2014-07-14 01:38:44,993 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5683ms
2014-07-14 01:38:44,994 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:38:44,994 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 39 files from compaction candidates
2014-07-14 01:38:44,994 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:38:44,994 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:38:44,994 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:38:44,997 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1579ms
2014-07-14 01:38:44,997 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:38:44,997 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7069ms
2014-07-14 01:38:44,997 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:38:44,997 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7113ms
2014-07-14 01:38:44,997 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:38:44,998 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7633ms
2014-07-14 01:38:44,998 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:38:44,998 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7808ms
2014-07-14 01:38:44,998 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:38:44,998 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/05fdc72dec054a868175ad94fc666043, entries=2840260, sequenceid=15894, filesize=202.2m
2014-07-14 01:38:44,999 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.0g/1106184080, currentsize=195.4m/204916960 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 29085ms, sequenceid=15894, compaction requested=true
2014-07-14 01:38:45,000 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:38:45,000 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 38 store files, 0 compacting, 38 eligible, 20 blocking
2014-07-14 01:38:45,000 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 38 files from compaction candidates
2014-07-14 01:38:45,000 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:38:45,000 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:38:45,001 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:38:45,010 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7865ms
2014-07-14 01:38:45,011 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:38:45,011 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3578ms
2014-07-14 01:38:45,011 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:38:45,011 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3617ms
2014-07-14 01:38:45,012 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:38:45,012 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3656ms
2014-07-14 01:38:45,012 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:38:45,012 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3696ms
2014-07-14 01:38:45,012 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:38:45,670 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:38:45,710 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327116812 with entries=71, filesize=61.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327125671
2014-07-14 01:38:45,710 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326960884
2014-07-14 01:38:45,710 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326977477
2014-07-14 01:38:45,711 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326982058
2014-07-14 01:38:45,711 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405326998684
2014-07-14 01:38:45,711 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327000576
2014-07-14 01:38:45,711 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327002514
2014-07-14 01:38:45,711 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327004504
2014-07-14 01:38:45,711 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327005476
2014-07-14 01:38:45,711 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327006913
2014-07-14 01:38:45,711 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327009527
2014-07-14 01:38:45,711 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327011049
2014-07-14 01:38:45,711 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327011883
2014-07-14 01:38:45,711 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327014011
2014-07-14 01:38:45,711 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327016381
2014-07-14 01:39:11,995 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90142ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:39:11,995 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 797.4m
2014-07-14 01:39:11,998 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90086ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:39:11,998 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 797.2m
2014-07-14 01:39:12,539 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:39:12,564 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:39:14,086 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:39:14,110 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 63959 synced till here 63957
2014-07-14 01:39:14,159 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327125671 with entries=74, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327154087
2014-07-14 01:39:14,384 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:39:14,486 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:39:15,366 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:39:15,503 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64041 synced till here 64032
2014-07-14 01:39:15,556 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327154087 with entries=82, filesize=70.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327155488
2014-07-14 01:39:16,853 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:39:17,171 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327155488 with entries=83, filesize=71.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327156853
2014-07-14 01:39:19,953 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:39:20,116 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64213 synced till here 64203
2014-07-14 01:39:20,315 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327156853 with entries=89, filesize=76.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327160014
2014-07-14 01:39:22,387 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:39:22,420 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64298 synced till here 64287
2014-07-14 01:39:22,575 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327160014 with entries=85, filesize=72.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327162388
2014-07-14 01:39:24,626 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:39:24,694 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64402 synced till here 64381
2014-07-14 01:39:26,080 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327162388 with entries=104, filesize=89.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327164627
2014-07-14 01:39:28,391 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:39:28,480 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64516 synced till here 64496
2014-07-14 01:39:29,126 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327164627 with entries=114, filesize=97.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327168392
2014-07-14 01:39:30,997 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1299ms
GC pool 'ParNew' had collection(s): count=1 time=1443ms
2014-07-14 01:39:31,595 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:39:31,626 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64626 synced till here 64593
2014-07-14 01:39:31,961 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327168392 with entries=110, filesize=94.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327171595
2014-07-14 01:39:46,671 WARN  [regionserver60020] util.Sleeper: We slept 15465ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-14 01:39:46,674 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 14175ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=14524ms
2014-07-14 01:39:46,750 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18091,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327168658,"queuetimems":286,"class":"HRegionServer","responsesize":15737,"method":"Multi"}
2014-07-14 01:39:46,750 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27570 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:46,751 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:46,974 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18333,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327168641,"queuetimems":298,"class":"HRegionServer","responsesize":15640,"method":"Multi"}
2014-07-14 01:39:46,975 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27571 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:46,975 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:46,979 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18585,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327168393,"queuetimems":1507,"class":"HRegionServer","responsesize":15656,"method":"Multi"}
2014-07-14 01:39:46,979 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18585,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327168393,"queuetimems":1449,"class":"HRegionServer","responsesize":15992,"method":"Multi"}
2014-07-14 01:39:46,979 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27534 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:46,979 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:46,979 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27585 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:46,979 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:47,159 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18518,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327168641,"queuetimems":327,"class":"HRegionServer","responsesize":15727,"method":"Multi"}
2014-07-14 01:39:47,160 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27574 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:47,160 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:47,160 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18766,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327168394,"queuetimems":1404,"class":"HRegionServer","responsesize":16199,"method":"Multi"}
2014-07-14 01:39:47,161 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27582 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:47,161 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:47,161 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18758,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327168402,"queuetimems":196,"class":"HRegionServer","responsesize":15713,"method":"Multi"}
2014-07-14 01:39:47,161 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27578 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:47,161 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:47,162 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18501,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327168658,"queuetimems":239,"class":"HRegionServer","responsesize":16112,"method":"Multi"}
2014-07-14 01:39:47,161 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18527,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327168633,"queuetimems":349,"class":"HRegionServer","responsesize":15795,"method":"Multi"}
2014-07-14 01:39:47,162 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27569 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:47,162 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:47,161 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18748,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327168413,"queuetimems":158,"class":"HRegionServer","responsesize":15525,"method":"Multi"}
2014-07-14 01:39:47,161 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18762,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327168399,"queuetimems":1341,"class":"HRegionServer","responsesize":15823,"method":"Multi"}
2014-07-14 01:39:47,162 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27575 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:47,163 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:47,163 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27580 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:47,163 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:47,163 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27576 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:47,163 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:47,292 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:39:47,382 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64730 synced till here 64699
2014-07-14 01:39:47,621 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16412,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171209,"queuetimems":2671,"class":"HRegionServer","responsesize":15987,"method":"Multi"}
2014-07-14 01:39:47,622 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19225,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327168396,"queuetimems":1374,"class":"HRegionServer","responsesize":15548,"method":"Multi"}
2014-07-14 01:39:47,621 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16404,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171217,"queuetimems":2629,"class":"HRegionServer","responsesize":15940,"method":"Multi"}
2014-07-14 01:39:47,621 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19219,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327168402,"queuetimems":206,"class":"HRegionServer","responsesize":15718,"method":"Multi"}
2014-07-14 01:39:47,621 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18078,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327169543,"queuetimems":1089,"class":"HRegionServer","responsesize":15720,"method":"Multi"}
2014-07-14 01:39:47,621 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19212,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327168409,"queuetimems":183,"class":"HRegionServer","responsesize":15474,"method":"Multi"}
2014-07-14 01:39:47,622 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27564 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:47,622 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:47,622 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27568 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:47,622 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:47,622 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27579 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:47,622 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:47,623 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27581 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:47,623 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:47,623 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27563 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:47,623 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:47,623 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27577 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:47,623 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:47,729 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327171595 with entries=104, filesize=88.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327187293
2014-07-14 01:39:47,827 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16601,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171225,"queuetimems":2575,"class":"HRegionServer","responsesize":15610,"method":"Multi"}
2014-07-14 01:39:47,827 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16601,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171225,"queuetimems":2521,"class":"HRegionServer","responsesize":15756,"method":"Multi"}
2014-07-14 01:39:47,827 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16617,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171209,"queuetimems":2709,"class":"HRegionServer","responsesize":16138,"method":"Multi"}
2014-07-14 01:39:47,827 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27561 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:47,827 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:47,827 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27567 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:47,827 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:47,828 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27601 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:47,828 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:47,953 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16154,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171798,"queuetimems":2880,"class":"HRegionServer","responsesize":15823,"method":"Multi"}
2014-07-14 01:39:47,953 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16722,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171230,"queuetimems":2493,"class":"HRegionServer","responsesize":15740,"method":"Multi"}
2014-07-14 01:39:47,953 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16715,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171237,"queuetimems":2379,"class":"HRegionServer","responsesize":15912,"method":"Multi"}
2014-07-14 01:39:47,953 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16155,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171798,"queuetimems":2849,"class":"HRegionServer","responsesize":16079,"method":"Multi"}
2014-07-14 01:39:47,954 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27594 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:47,954 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16720,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171233,"queuetimems":2466,"class":"HRegionServer","responsesize":15748,"method":"Multi"}
2014-07-14 01:39:47,954 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:47,954 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27596 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:47,954 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:47,954 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27599 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:47,954 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:47,954 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27600 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:47,954 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:47,954 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27593 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:47,954 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:47,955 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16717,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171237,"queuetimems":2409,"class":"HRegionServer","responsesize":15782,"method":"Multi"}
2014-07-14 01:39:47,955 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16713,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171241,"queuetimems":2353,"class":"HRegionServer","responsesize":15811,"method":"Multi"}
2014-07-14 01:39:47,955 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27597 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:47,955 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:47,955 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27595 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:47,955 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:47,955 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16720,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171234,"queuetimems":2437,"class":"HRegionServer","responsesize":15656,"method":"Multi"}
2014-07-14 01:39:47,955 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27598 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:47,956 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:47,998 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16152,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171846,"queuetimems":2806,"class":"HRegionServer","responsesize":15561,"method":"Multi"}
2014-07-14 01:39:47,999 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27590 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:47,999 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:47,999 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16199,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171799,"queuetimems":2789,"class":"HRegionServer","responsesize":15883,"method":"Multi"}
2014-07-14 01:39:48,001 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27591 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:48,001 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:48,002 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16779,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171221,"queuetimems":2602,"class":"HRegionServer","responsesize":15621,"method":"Multi"}
2014-07-14 01:39:48,002 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27562 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:48,002 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:48,035 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16180,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171854,"queuetimems":2785,"class":"HRegionServer","responsesize":15655,"method":"Multi"}
2014-07-14 01:39:48,036 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27589 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:48,036 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:48,057 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16144,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171913,"queuetimems":2536,"class":"HRegionServer","responsesize":15987,"method":"Multi"}
2014-07-14 01:39:48,058 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27608 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:48,058 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16200,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171857,"queuetimems":2758,"class":"HRegionServer","responsesize":15539,"method":"Multi"}
2014-07-14 01:39:48,058 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:48,058 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27588 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:48,058 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:48,083 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16176,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171906,"queuetimems":2622,"class":"HRegionServer","responsesize":15621,"method":"Multi"}
2014-07-14 01:39:48,084 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27611 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:48,084 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:48,117 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16179,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171937,"queuetimems":2434,"class":"HRegionServer","responsesize":15558,"method":"Multi"}
2014-07-14 01:39:48,119 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27604 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:48,119 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:48,592 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16678,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171913,"queuetimems":2565,"class":"HRegionServer","responsesize":15610,"method":"Multi"}
2014-07-14 01:39:48,593 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27609 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:48,593 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:48,611 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16712,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171898,"queuetimems":2701,"class":"HRegionServer","responsesize":16112,"method":"Multi"}
2014-07-14 01:39:48,611 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16712,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171898,"queuetimems":2672,"class":"HRegionServer","responsesize":15640,"method":"Multi"}
2014-07-14 01:39:48,611 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16696,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171914,"queuetimems":2503,"class":"HRegionServer","responsesize":15663,"method":"Multi"}
2014-07-14 01:39:48,611 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16673,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171937,"queuetimems":2462,"class":"HRegionServer","responsesize":15924,"method":"Multi"}
2014-07-14 01:39:48,611 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16674,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171937,"queuetimems":2494,"class":"HRegionServer","responsesize":15676,"method":"Multi"}
2014-07-14 01:39:48,611 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16720,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171891,"queuetimems":2725,"class":"HRegionServer","responsesize":15737,"method":"Multi"}
2014-07-14 01:39:48,611 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16606,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327172005,"queuetimems":760,"class":"HRegionServer","responsesize":15655,"method":"Multi"}
2014-07-14 01:39:48,611 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16700,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171911,"queuetimems":2592,"class":"HRegionServer","responsesize":15940,"method":"Multi"}
2014-07-14 01:39:48,611 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16812,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171799,"queuetimems":2820,"class":"HRegionServer","responsesize":15790,"method":"Multi"}
2014-07-14 01:39:48,611 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16752,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171858,"queuetimems":2727,"class":"HRegionServer","responsesize":16138,"method":"Multi"}
2014-07-14 01:39:48,612 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16673,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171938,"queuetimems":761,"class":"HRegionServer","responsesize":15656,"method":"Multi"}
2014-07-14 01:39:48,611 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16613,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171998,"queuetimems":810,"class":"HRegionServer","responsesize":15811,"method":"Multi"}
2014-07-14 01:39:48,611 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16705,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47146","starttimems":1405327171906,"queuetimems":2651,"class":"HRegionServer","responsesize":15720,"method":"Multi"}
2014-07-14 01:39:48,611 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27614 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:48,612 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:48,612 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27624 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:48,612 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:48,612 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27619 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:48,612 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:48,612 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27616 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:48,612 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:48,613 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27592 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:48,613 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:48,613 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27623 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:48,613 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:48,613 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27610 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:48,613 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:48,613 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27615 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:48,613 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:48,613 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27606 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:48,613 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:48,613 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27605 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:48,613 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:48,613 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27607 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:48,614 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:48,614 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27613 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:48,614 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:48,614 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27612 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:47146: output error
2014-07-14 01:39:48,614 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-14 01:39:52,980 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:39:53,131 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64816 synced till here 64809
2014-07-14 01:39:53,270 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327187293 with entries=86, filesize=73.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327192980
2014-07-14 01:39:54,564 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:39:54,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64893 synced till here 64889
2014-07-14 01:39:54,637 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327192980 with entries=77, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327194564
2014-07-14 01:39:55,952 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:39:55,974 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64971 synced till here 64963
2014-07-14 01:39:56,255 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327194564 with entries=78, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327195953
2014-07-14 01:39:57,564 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:39:57,830 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65051 synced till here 65045
2014-07-14 01:39:57,890 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327195953 with entries=80, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327197806
2014-07-14 01:39:59,420 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:39:59,446 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65126 synced till here 65125
2014-07-14 01:39:59,484 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327197806 with entries=75, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327199421
2014-07-14 01:40:01,136 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:40:01,154 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65202 synced till here 65196
2014-07-14 01:40:01,217 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327199421 with entries=76, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327201136
2014-07-14 01:40:01,278 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:01,336 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:01,336 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:01,336 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:01,337 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:01,337 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:01,337 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:01,373 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:01,374 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:01,374 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:01,720 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:01,796 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:01,841 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:02,042 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:02,176 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:02,384 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:03,159 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16008, memsize=772.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/76bf6857caea42e8a69aae8399f26b89
2014-07-14 01:40:03,175 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/76bf6857caea42e8a69aae8399f26b89 as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/76bf6857caea42e8a69aae8399f26b89
2014-07-14 01:40:03,189 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/76bf6857caea42e8a69aae8399f26b89, entries=2811480, sequenceid=16008, filesize=200.1m
2014-07-14 01:40:03,189 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~797.4m/836149600, currentsize=503.3m/527772320 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 51194ms, sequenceid=16008, compaction requested=true
2014-07-14 01:40:03,190 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:40:03,190 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 807ms
2014-07-14 01:40:03,190 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 36 store files, 0 compacting, 36 eligible, 20 blocking
2014-07-14 01:40:03,190 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files; delaying flush up to 90000ms
2014-07-14 01:40:03,190 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 36 files from compaction candidates
2014-07-14 01:40:03,190 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:03,190 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-14 01:40:03,190 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:40:03,191 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files; delaying flush up to 90000ms
2014-07-14 01:40:03,191 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1014ms
2014-07-14 01:40:03,191 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:2), split_queue=0, merge_queue=0
2014-07-14 01:40:03,191 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:03,191 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:40:03,191 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1149ms
2014-07-14 01:40:03,191 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:03,191 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:40:03,191 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1350ms
2014-07-14 01:40:03,191 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 39 store files, 0 compacting, 39 eligible, 20 blocking
2014-07-14 01:40:03,192 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:03,192 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 39 files from compaction candidates
2014-07-14 01:40:03,192 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:40:03,192 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:40:03,192 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:40:03,192 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 38 store files, 0 compacting, 38 eligible, 20 blocking
2014-07-14 01:40:03,192 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 38 files from compaction candidates
2014-07-14 01:40:03,192 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:40:03,192 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:40:03,192 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:40:03,193 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1397ms
2014-07-14 01:40:03,193 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:03,193 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1473ms
2014-07-14 01:40:03,193 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:03,193 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1819ms
2014-07-14 01:40:03,193 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:03,193 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1820ms
2014-07-14 01:40:03,193 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:03,205 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1832ms
2014-07-14 01:40:03,205 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:03,205 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1868ms
2014-07-14 01:40:03,205 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:03,205 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1868ms
2014-07-14 01:40:03,205 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:03,205 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1869ms
2014-07-14 01:40:03,205 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:03,205 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1869ms
2014-07-14 01:40:03,206 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:03,221 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1885ms
2014-07-14 01:40:03,221 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:03,221 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1885ms
2014-07-14 01:40:03,221 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:03,225 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1947ms
2014-07-14 01:40:03,225 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:03,371 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=15998, memsize=772.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/c8cd0c8f5bbf4fcda42f695b2a5ae8f8
2014-07-14 01:40:03,383 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/c8cd0c8f5bbf4fcda42f695b2a5ae8f8 as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/c8cd0c8f5bbf4fcda42f695b2a5ae8f8
2014-07-14 01:40:03,396 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/c8cd0c8f5bbf4fcda42f695b2a5ae8f8, entries=2811720, sequenceid=15998, filesize=200.1m
2014-07-14 01:40:03,396 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~797.2m/835933440, currentsize=499.1m/523322240 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 51398ms, sequenceid=15998, compaction requested=true
2014-07-14 01:40:03,397 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:40:03,397 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 35 store files, 0 compacting, 35 eligible, 20 blocking
2014-07-14 01:40:03,397 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 35 files from compaction candidates
2014-07-14 01:40:03,397 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:40:03,397 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:40:03,397 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:40:03,408 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:40:03,409 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files; delaying flush up to 90000ms
2014-07-14 01:40:03,409 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:40:03,409 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:40:03,409 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files; delaying flush up to 90000ms
2014-07-14 01:40:03,409 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 36 store files, 0 compacting, 36 eligible, 20 blocking
2014-07-14 01:40:03,409 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-14 01:40:03,409 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 36 files from compaction candidates
2014-07-14 01:40:03,409 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:40:03,409 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:40:03,409 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:40:03,409 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 35 store files, 0 compacting, 35 eligible, 20 blocking
2014-07-14 01:40:03,410 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 35 files from compaction candidates
2014-07-14 01:40:03,410 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:40:03,410 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:40:03,410 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:40:04,513 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:40:04,528 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65278 synced till here 65275
2014-07-14 01:40:04,575 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327201136 with entries=76, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327204514
2014-07-14 01:40:04,575 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327019229
2014-07-14 01:40:04,575 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327039052
2014-07-14 01:40:04,576 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327042559
2014-07-14 01:40:04,576 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327043602
2014-07-14 01:40:04,576 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327047631
2014-07-14 01:40:04,576 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327051940
2014-07-14 01:40:04,576 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327054248
2014-07-14 01:40:04,576 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327055801
2014-07-14 01:40:04,576 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327059774
2014-07-14 01:40:04,576 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327061136
2014-07-14 01:40:04,576 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327063205
2014-07-14 01:40:04,576 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327068385
2014-07-14 01:40:04,576 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327070350
2014-07-14 01:40:04,576 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327071598
2014-07-14 01:40:04,576 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327073107
2014-07-14 01:40:04,576 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327078960
2014-07-14 01:40:04,576 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327082314
2014-07-14 01:40:04,576 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327087368
2014-07-14 01:40:04,576 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327089315
2014-07-14 01:40:06,044 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:40:06,075 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65354 synced till here 65353
2014-07-14 01:40:06,152 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327204514 with entries=76, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327206044
2014-07-14 01:40:06,943 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:40:07,829 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65454 synced till here 65448
2014-07-14 01:40:07,891 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327206044 with entries=100, filesize=86.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327206943
2014-07-14 01:40:08,874 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:40:08,899 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65536 synced till here 65528
2014-07-14 01:40:08,978 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327206943 with entries=82, filesize=70.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327208874
2014-07-14 01:40:10,476 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:40:10,508 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65621 synced till here 65610
2014-07-14 01:40:10,829 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327208874 with entries=85, filesize=73.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327210476
2014-07-14 01:40:12,229 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:40:12,246 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65696 synced till here 65695
2014-07-14 01:40:12,257 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327210476 with entries=75, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327212229
2014-07-14 01:40:13,826 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:40:13,854 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65772 synced till here 65768
2014-07-14 01:40:13,895 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327212229 with entries=76, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327213827
2014-07-14 01:40:15,338 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:40:15,800 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65847 synced till here 65845
2014-07-14 01:40:15,845 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327213827 with entries=75, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327215339
2014-07-14 01:40:16,866 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:40:17,240 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65946 synced till here 65943
2014-07-14 01:40:17,268 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327215339 with entries=99, filesize=84.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327216867
2014-07-14 01:40:18,639 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:40:18,666 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 66023 synced till here 66018
2014-07-14 01:40:18,704 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327216867 with entries=77, filesize=66.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327218639
2014-07-14 01:40:18,704 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 2 regions(s): 6e2bf74487f61672cd8bc06d8b34f003, 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:40:20,252 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:40:20,291 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 66105 synced till here 66096
2014-07-14 01:40:20,379 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:40:20,379 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files, but is 1.1g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:40:20,379 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. due to global heap pressure
2014-07-14 01:40:20,379 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 1.1g
2014-07-14 01:40:20,388 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327218639 with entries=82, filesize=70.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327220253
2014-07-14 01:40:20,388 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 2 regions(s): 6e2bf74487f61672cd8bc06d8b34f003, 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:40:20,388 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files; delaying flush up to 90000ms
2014-07-14 01:40:20,389 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:40:20,389 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 38 store files, 0 compacting, 38 eligible, 20 blocking
2014-07-14 01:40:20,389 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:40:20,389 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files, but is 1.1g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:40:20,389 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. due to global heap pressure
2014-07-14 01:40:20,389 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 38 files from compaction candidates
2014-07-14 01:40:20,389 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:40:20,389 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 1.1g
2014-07-14 01:40:20,389 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:40:20,390 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:40:22,108 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:40:22,245 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:40:22,476 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:40:22,512 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 66198 synced till here 66180
2014-07-14 01:40:22,652 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327220253 with entries=93, filesize=79.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327222476
2014-07-14 01:40:23,891 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:23,891 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:23,892 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:23,892 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:23,893 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:23,895 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:23,904 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:23,914 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:23,921 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:23,959 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:23,969 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,032 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,066 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,078 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,108 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,123 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,124 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,124 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,124 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,124 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,124 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,125 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,158 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,201 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,237 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,269 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,309 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,356 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,392 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,431 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,484 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,509 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,556 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,596 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,645 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,684 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,722 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,760 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,803 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,836 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,880 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,941 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:24,986 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:25,024 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:25,069 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:25,103 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:25,133 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:25,168 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:25,420 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:25,466 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:40:28,892 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:28,892 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:40:28,892 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:28,893 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:40:28,894 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:40:28,896 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:40:28,905 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:40:28,914 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:28,921 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:28,959 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:40:28,969 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:29,032 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:29,067 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:40:29,079 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:29,109 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:40:29,124 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:40:29,124 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:40:29,124 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:29,124 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:29,125 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:40:29,125 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:40:29,126 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:29,159 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:40:29,201 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:29,237 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:29,269 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:29,309 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:29,357 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:40:29,392 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:29,431 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:29,485 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:29,510 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:29,556 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:29,597 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:29,645 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:29,685 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:29,723 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:40:29,760 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:29,853 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5017ms
2014-07-14 01:40:29,853 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5050ms
2014-07-14 01:40:29,880 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:29,941 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:29,986 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:40:30,024 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:30,069 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:30,104 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:40:30,133 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:30,168 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:40:30,420 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:40:30,467 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:40:33,892 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:33,892 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:40:33,893 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:33,893 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:33,894 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:33,896 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:33,905 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:33,914 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:40:33,922 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:33,959 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:33,970 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:34,033 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:34,067 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:34,079 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:34,109 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:34,124 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:34,124 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:34,125 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:34,125 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:34,125 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:34,126 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:34,126 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:34,159 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:34,202 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:40:34,237 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:40:34,270 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:34,310 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:34,357 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:34,392 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:40:34,432 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:34,485 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:34,510 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:34,556 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:40:34,597 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:34,645 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:40:34,685 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:34,723 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:34,760 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:40:34,854 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10017ms
2014-07-14 01:40:34,854 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10051ms
2014-07-14 01:40:34,880 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:40:34,941 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:40:34,986 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:35,025 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:35,069 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:40:35,104 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:35,134 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:35,169 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:40:35,421 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:40:35,467 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:40:38,892 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:38,893 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:38,893 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:40:38,894 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:38,895 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:38,897 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:40:38,906 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:40:38,915 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:38,922 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:38,960 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:40:38,970 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:39,033 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:39,067 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:39,079 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:39,109 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:39,125 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:39,125 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:40:39,126 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:39,127 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:40:39,127 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15003ms
2014-07-14 01:40:39,127 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15003ms
2014-07-14 01:40:39,127 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15003ms
2014-07-14 01:40:39,160 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:40:39,202 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:39,238 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:39,270 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:39,310 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:39,358 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:39,393 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:39,432 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:39,486 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:39,511 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:39,557 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:39,598 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:39,647 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:39,685 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:39,724 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:39,761 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:39,854 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15018ms
2014-07-14 01:40:39,854 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15051ms
2014-07-14 01:40:39,881 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:39,942 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:39,987 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:40:40,026 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:40,070 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:40,105 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:40:40,135 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:40,169 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:40,421 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:40:40,467 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:40:43,885 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1024ms
GC pool 'ParNew' had collection(s): count=1 time=1026ms
2014-07-14 01:40:43,893 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:40:43,893 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:40:43,893 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:40:43,894 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:40:43,895 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:40:43,897 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:40:43,906 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:40:43,915 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:40:43,922 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:40:43,960 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:40:43,970 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:40:44,033 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:40:44,068 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:40:44,079 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:40:44,109 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:40:44,125 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:40:44,126 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:40:44,127 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20003ms
2014-07-14 01:40:44,128 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:40:44,128 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20004ms
2014-07-14 01:40:44,129 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20004ms
2014-07-14 01:40:44,129 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20005ms
2014-07-14 01:40:44,160 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:40:44,202 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:40:44,238 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:40:44,271 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:40:44,310 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:40:44,358 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:40:44,393 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:40:44,432 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:40:44,486 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:40:44,511 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:40:44,557 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:40:44,599 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:40:44,647 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:40:44,686 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:40:44,724 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:40:44,761 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:40:44,855 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20018ms
2014-07-14 01:40:44,855 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20052ms
2014-07-14 01:40:44,881 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:40:44,942 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:40:44,987 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:40:45,026 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:40:45,071 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:40:45,105 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:40:45,135 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:40:45,169 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:40:45,422 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20003ms
2014-07-14 01:40:45,468 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:40:49,733 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25011ms
2014-07-14 01:40:49,734 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25424ms
2014-07-14 01:40:49,735 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25576ms
2014-07-14 01:40:49,735 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25379ms
2014-07-14 01:40:49,735 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25343ms
2014-07-14 01:40:49,736 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25304ms
2014-07-14 01:40:49,736 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25252ms
2014-07-14 01:40:49,736 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25227ms
2014-07-14 01:40:49,736 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25180ms
2014-07-14 01:40:49,737 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25141ms
2014-07-14 01:40:49,737 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25092ms
2014-07-14 01:40:49,737 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25053ms
2014-07-14 01:40:49,738 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25847ms
2014-07-14 01:40:49,738 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25846ms
2014-07-14 01:40:49,739 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25848ms
2014-07-14 01:40:49,739 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25847ms
2014-07-14 01:40:49,739 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25846ms
2014-07-14 01:40:49,739 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25844ms
2014-07-14 01:40:49,739 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25835ms
2014-07-14 01:40:49,740 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25826ms
2014-07-14 01:40:49,740 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25819ms
2014-07-14 01:40:49,740 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25782ms
2014-07-14 01:40:49,740 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25771ms
2014-07-14 01:40:49,741 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25709ms
2014-07-14 01:40:49,741 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25675ms
2014-07-14 01:40:49,741 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25663ms
2014-07-14 01:40:49,741 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25633ms
2014-07-14 01:40:49,741 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25618ms
2014-07-14 01:40:49,742 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25618ms
2014-07-14 01:40:49,742 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25618ms
2014-07-14 01:40:49,742 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25617ms
2014-07-14 01:40:49,742 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25618ms
2014-07-14 01:40:49,742 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25618ms
2014-07-14 01:40:49,742 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25618ms
2014-07-14 01:40:49,742 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25541ms
2014-07-14 01:40:49,742 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25505ms
2014-07-14 01:40:49,743 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25474ms
2014-07-14 01:40:49,762 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:40:49,855 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25019ms
2014-07-14 01:40:49,855 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25052ms
2014-07-14 01:40:49,881 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:40:49,943 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:40:49,987 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:40:50,026 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:40:50,071 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:40:50,105 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:40:50,135 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:40:50,169 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:40:50,422 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25003ms
2014-07-14 01:40:50,468 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:40:52,350 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16571, memsize=722.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/e4c10dfedd7c494493a30f06c110f8e9
2014-07-14 01:40:52,351 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16597, memsize=728.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/847b35688c35463b88b4103733f6c7fe
2014-07-14 01:40:52,369 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/847b35688c35463b88b4103733f6c7fe as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/847b35688c35463b88b4103733f6c7fe
2014-07-14 01:40:52,369 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/e4c10dfedd7c494493a30f06c110f8e9 as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/e4c10dfedd7c494493a30f06c110f8e9
2014-07-14 01:40:52,378 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/847b35688c35463b88b4103733f6c7fe, entries=2653330, sequenceid=16597, filesize=188.9m
2014-07-14 01:40:52,379 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.1g/1143152080, currentsize=53.2m/55804640 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 32000ms, sequenceid=16597, compaction requested=true
2014-07-14 01:40:52,379 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/e4c10dfedd7c494493a30f06c110f8e9, entries=2630550, sequenceid=16571, filesize=187.3m
2014-07-14 01:40:52,379 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:40:52,379 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 39 store files, 0 compacting, 39 eligible, 20 blocking
2014-07-14 01:40:52,380 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.1g/1136096080, currentsize=55.7m/58392800 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 31990ms, sequenceid=16571, compaction requested=true
2014-07-14 01:40:52,380 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 97894ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:40:52,380 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26914ms
2014-07-14 01:40:52,380 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 39 files from compaction candidates
2014-07-14 01:40:52,380 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-14 01:40:52,380 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 55.7m
2014-07-14 01:40:52,380 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,380 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:40:52,380 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:40:52,380 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:40:52,380 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26961ms
2014-07-14 01:40:52,381 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 40 store files, 0 compacting, 40 eligible, 20 blocking
2014-07-14 01:40:52,381 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,381 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 40 files from compaction candidates
2014-07-14 01:40:52,381 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:40:52,381 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:40:52,381 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27213ms
2014-07-14 01:40:52,381 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:40:52,381 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,382 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27249ms
2014-07-14 01:40:52,382 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,383 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27279ms
2014-07-14 01:40:52,383 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,383 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27314ms
2014-07-14 01:40:52,383 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,385 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27361ms
2014-07-14 01:40:52,385 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,386 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27400ms
2014-07-14 01:40:52,386 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,393 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27452ms
2014-07-14 01:40:52,393 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,401 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27521ms
2014-07-14 01:40:52,401 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,403 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27600ms
2014-07-14 01:40:52,403 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,403 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27567ms
2014-07-14 01:40:52,403 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,409 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27649ms
2014-07-14 01:40:52,409 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,410 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28140ms
2014-07-14 01:40:52,410 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,410 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28173ms
2014-07-14 01:40:52,410 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,411 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28209ms
2014-07-14 01:40:52,411 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,411 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28287ms
2014-07-14 01:40:52,411 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,411 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28287ms
2014-07-14 01:40:52,411 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,412 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28288ms
2014-07-14 01:40:52,412 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,420 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:40:52,420 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28294ms
2014-07-14 01:40:52,420 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,421 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28296ms
2014-07-14 01:40:52,421 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,423 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28300ms
2014-07-14 01:40:52,423 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,425 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28302ms
2014-07-14 01:40:52,425 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,438 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28329ms
2014-07-14 01:40:52,438 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,439 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28360ms
2014-07-14 01:40:52,439 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,441 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28374ms
2014-07-14 01:40:52,441 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,441 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28409ms
2014-07-14 01:40:52,441 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,441 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28472ms
2014-07-14 01:40:52,441 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,441 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28483ms
2014-07-14 01:40:52,441 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,442 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28520ms
2014-07-14 01:40:52,442 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,445 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28531ms
2014-07-14 01:40:52,445 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,448 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28544ms
2014-07-14 01:40:52,448 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,456 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28561ms
2014-07-14 01:40:52,456 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,458 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28565ms
2014-07-14 01:40:52,458 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,463 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28566ms
2014-07-14 01:40:52,463 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,464 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28572ms
2014-07-14 01:40:52,464 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,464 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28572ms
2014-07-14 01:40:52,464 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,464 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28573ms
2014-07-14 01:40:52,464 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,480 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27796ms
2014-07-14 01:40:52,480 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,489 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27844ms
2014-07-14 01:40:52,489 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,497 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27901ms
2014-07-14 01:40:52,497 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,498 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27941ms
2014-07-14 01:40:52,498 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,498 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27989ms
2014-07-14 01:40:52,498 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,498 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28014ms
2014-07-14 01:40:52,498 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,501 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28070ms
2014-07-14 01:40:52,501 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,501 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28109ms
2014-07-14 01:40:52,501 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,502 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28145ms
2014-07-14 01:40:52,502 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,505 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28347ms
2014-07-14 01:40:52,505 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,506 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28197ms
2014-07-14 01:40:52,506 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,506 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27784ms
2014-07-14 01:40:52,506 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:40:52,507 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 98122ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:40:52,507 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 53.2m
2014-07-14 01:40:52,592 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30320,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327222271,"queuetimems":0,"class":"HRegionServer","responsesize":15945,"method":"Multi"}
2014-07-14 01:40:52,757 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:40:52,759 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30519,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327222240,"queuetimems":0,"class":"HRegionServer","responsesize":16087,"method":"Multi"}
2014-07-14 01:40:52,760 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30420,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327222339,"queuetimems":0,"class":"HRegionServer","responsesize":16192,"method":"Multi"}
2014-07-14 01:40:52,760 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30581,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327222178,"queuetimems":0,"class":"HRegionServer","responsesize":15798,"method":"Multi"}
2014-07-14 01:40:52,759 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30457,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327222302,"queuetimems":0,"class":"HRegionServer","responsesize":15837,"method":"Multi"}
2014-07-14 01:40:52,759 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30640,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327222119,"queuetimems":0,"class":"HRegionServer","responsesize":15839,"method":"Multi"}
2014-07-14 01:40:52,759 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30611,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327222148,"queuetimems":0,"class":"HRegionServer","responsesize":15973,"method":"Multi"}
2014-07-14 01:40:52,759 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30551,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327222208,"queuetimems":0,"class":"HRegionServer","responsesize":15598,"method":"Multi"}
2014-07-14 01:40:52,759 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30670,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327222089,"queuetimems":0,"class":"HRegionServer","responsesize":15727,"method":"Multi"}
2014-07-14 01:40:52,783 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 66297 synced till here 66269
2014-07-14 01:40:53,009 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327222476 with entries=99, filesize=85.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327252758
2014-07-14 01:40:53,010 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327091272
2014-07-14 01:40:53,010 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327097585
2014-07-14 01:40:53,010 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327102386
2014-07-14 01:40:53,010 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327106828
2014-07-14 01:40:53,010 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327108645
2014-07-14 01:40:53,010 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327111354
2014-07-14 01:40:53,010 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327115269
2014-07-14 01:40:53,010 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327116812
2014-07-14 01:40:54,142 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:40:54,415 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31756,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327222659,"queuetimems":1,"class":"HRegionServer","responsesize":15859,"method":"Multi"}
2014-07-14 01:40:54,421 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31627,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327222793,"queuetimems":0,"class":"HRegionServer","responsesize":15390,"method":"Multi"}
2014-07-14 01:40:54,426 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31532,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327222893,"queuetimems":0,"class":"HRegionServer","responsesize":15336,"method":"Multi"}
2014-07-14 01:40:54,453 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32085,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327222368,"queuetimems":0,"class":"HRegionServer","responsesize":15552,"method":"Multi"}
2014-07-14 01:40:54,473 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32076,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327222397,"queuetimems":0,"class":"HRegionServer","responsesize":15775,"method":"Multi"}
2014-07-14 01:40:54,473 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32007,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327222466,"queuetimems":3,"class":"HRegionServer","responsesize":15627,"method":"Multi"}
2014-07-14 01:40:54,484 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32054,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327222429,"queuetimems":0,"class":"HRegionServer","responsesize":15787,"method":"Multi"}
2014-07-14 01:40:54,889 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:40:54,972 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 66413 synced till here 66381
2014-07-14 01:40:55,069 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16608, memsize=49.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/bd3d00eec5c24a53a107f99c2daaab67
2014-07-14 01:40:55,079 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/bd3d00eec5c24a53a107f99c2daaab67 as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/bd3d00eec5c24a53a107f99c2daaab67
2014-07-14 01:40:55,088 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/bd3d00eec5c24a53a107f99c2daaab67, entries=180940, sequenceid=16608, filesize=12.9m
2014-07-14 01:40:55,088 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~55.7m/58392800, currentsize=69.9m/73325200 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 2708ms, sequenceid=16608, compaction requested=true
2014-07-14 01:40:55,089 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:40:55,089 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 41 store files, 0 compacting, 41 eligible, 20 blocking
2014-07-14 01:40:55,089 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 41 files from compaction candidates
2014-07-14 01:40:55,089 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:40:55,089 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:40:55,089 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:40:55,168 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29704,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327225464,"queuetimems":0,"class":"HRegionServer","responsesize":16143,"method":"Multi"}
2014-07-14 01:40:55,168 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30067,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327225101,"queuetimems":0,"class":"HRegionServer","responsesize":15838,"method":"Multi"}
2014-07-14 01:40:55,717 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327252758 with entries=116, filesize=99.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327254889
2014-07-14 01:40:55,912 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31230,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327224681,"queuetimems":0,"class":"HRegionServer","responsesize":15727,"method":"Multi"}
2014-07-14 01:40:55,912 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31439,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327224473,"queuetimems":1,"class":"HRegionServer","responsesize":15627,"method":"Multi"}
2014-07-14 01:40:55,917 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31364,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327224553,"queuetimems":1,"class":"HRegionServer","responsesize":15775,"method":"Multi"}
2014-07-14 01:40:55,917 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31762,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327224155,"queuetimems":0,"class":"HRegionServer","responsesize":15803,"method":"Multi"}
2014-07-14 01:40:55,918 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31888,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327224030,"queuetimems":0,"class":"HRegionServer","responsesize":15345,"method":"Multi"}
2014-07-14 01:40:55,923 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31045,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327224877,"queuetimems":1,"class":"HRegionServer","responsesize":15839,"method":"Multi"}
2014-07-14 01:40:55,923 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31533,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327224390,"queuetimems":1,"class":"HRegionServer","responsesize":15552,"method":"Multi"}
2014-07-14 01:40:55,929 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30512,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327225417,"queuetimems":1,"class":"HRegionServer","responsesize":15791,"method":"Multi"}
2014-07-14 01:40:55,931 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31287,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327224643,"queuetimems":0,"class":"HRegionServer","responsesize":15945,"method":"Multi"}
2014-07-14 01:40:55,931 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30800,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327225131,"queuetimems":0,"class":"HRegionServer","responsesize":15836,"method":"Multi"}
2014-07-14 01:40:55,932 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31967,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327223965,"queuetimems":0,"class":"HRegionServer","responsesize":15935,"method":"Multi"}
2014-07-14 01:40:55,917 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32024,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327223893,"queuetimems":0,"class":"HRegionServer","responsesize":16186,"method":"Multi"}
2014-07-14 01:40:55,931 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31504,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327224426,"queuetimems":0,"class":"HRegionServer","responsesize":15837,"method":"Multi"}
2014-07-14 01:40:55,923 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31858,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327224064,"queuetimems":0,"class":"HRegionServer","responsesize":15445,"method":"Multi"}
2014-07-14 01:40:56,107 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31124,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327224983,"queuetimems":0,"class":"HRegionServer","responsesize":15308,"method":"Multi"}
2014-07-14 01:40:56,108 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32002,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327224105,"queuetimems":0,"class":"HRegionServer","responsesize":16252,"method":"Multi"}
2014-07-14 01:40:56,108 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31518,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327224590,"queuetimems":0,"class":"HRegionServer","responsesize":16192,"method":"Multi"}
2014-07-14 01:40:56,108 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31310,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327224798,"queuetimems":0,"class":"HRegionServer","responsesize":15598,"method":"Multi"}
2014-07-14 01:40:56,108 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31914,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327224194,"queuetimems":0,"class":"HRegionServer","responsesize":15584,"method":"Multi"}
2014-07-14 01:40:56,108 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32198,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327223910,"queuetimems":2,"class":"HRegionServer","responsesize":15485,"method":"Multi"}
2014-07-14 01:40:56,109 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31171,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327224938,"queuetimems":1,"class":"HRegionServer","responsesize":16087,"method":"Multi"}
2014-07-14 01:40:56,109 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31875,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327224234,"queuetimems":0,"class":"HRegionServer","responsesize":16354,"method":"Multi"}
2014-07-14 01:40:56,109 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31389,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327224720,"queuetimems":0,"class":"HRegionServer","responsesize":15973,"method":"Multi"}
2014-07-14 01:40:56,110 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31604,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327224506,"queuetimems":0,"class":"HRegionServer","responsesize":15787,"method":"Multi"}
2014-07-14 01:40:56,111 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31044,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327225066,"queuetimems":0,"class":"HRegionServer","responsesize":15967,"method":"Multi"}
2014-07-14 01:40:56,112 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31759,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327224352,"queuetimems":0,"class":"HRegionServer","responsesize":15823,"method":"Multi"}
2014-07-14 01:40:56,117 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31097,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327225020,"queuetimems":0,"class":"HRegionServer","responsesize":15770,"method":"Multi"}
2014-07-14 01:40:56,109 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31802,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327224307,"queuetimems":0,"class":"HRegionServer","responsesize":16033,"method":"Multi"}
2014-07-14 01:40:56,111 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31353,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327224757,"queuetimems":0,"class":"HRegionServer","responsesize":15736,"method":"Multi"}
2014-07-14 01:40:56,133 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31300,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327224833,"queuetimems":1,"class":"HRegionServer","responsesize":15798,"method":"Multi"}
2014-07-14 01:40:56,144 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30980,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327225164,"queuetimems":0,"class":"HRegionServer","responsesize":16009,"method":"Multi"}
2014-07-14 01:40:56,329 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32061,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327224267,"queuetimems":0,"class":"HRegionServer","responsesize":15682,"method":"Multi"}
2014-07-14 01:40:56,575 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:40:56,609 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 66533 synced till here 66511
2014-07-14 01:40:56,641 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16664, memsize=59.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/87fea6592a064161afa7ca28a47835c7
2014-07-14 01:40:57,439 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/87fea6592a064161afa7ca28a47835c7 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/87fea6592a064161afa7ca28a47835c7
2014-07-14 01:40:57,450 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/87fea6592a064161afa7ca28a47835c7, entries=216090, sequenceid=16664, filesize=15.4m
2014-07-14 01:40:57,451 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~103.1m/108070000, currentsize=91.9m/96354480 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 4944ms, sequenceid=16664, compaction requested=true
2014-07-14 01:40:57,451 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:40:57,451 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 40 store files, 0 compacting, 40 eligible, 20 blocking
2014-07-14 01:40:57,451 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 40 files from compaction candidates
2014-07-14 01:40:57,451 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:40:57,451 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:40:57,452 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:40:57,637 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327254889 with entries=120, filesize=103.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327256575
2014-07-14 01:40:58,500 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:40:58,537 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 66644 synced till here 66609
2014-07-14 01:40:58,921 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327256575 with entries=111, filesize=94.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327258500
2014-07-14 01:41:00,205 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:41:00,292 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 66746 synced till here 66714
2014-07-14 01:41:00,484 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327258500 with entries=102, filesize=87.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327260205
2014-07-14 01:41:02,058 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:41:02,103 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 66866 synced till here 66834
2014-07-14 01:41:02,995 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327260205 with entries=120, filesize=102.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327262058
2014-07-14 01:41:02,996 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 2 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72, b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:41:03,524 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:41:03,525 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files; delaying flush up to 90000ms
2014-07-14 01:41:04,385 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 41 store files, 0 compacting, 41 eligible, 20 blocking
2014-07-14 01:41:04,386 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 41 files from compaction candidates
2014-07-14 01:41:04,386 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:41:04,386 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:41:04,386 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:41:04,393 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:41:04,641 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:41:04,683 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 66966 synced till here 66938
2014-07-14 01:41:04,960 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327262058 with entries=100, filesize=85.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327264641
2014-07-14 01:41:04,960 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 2 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72, b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:41:05,321 DEBUG [RpcServer.handler=7,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:41:05,333 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files; delaying flush up to 90000ms
2014-07-14 01:41:05,337 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 40 store files, 0 compacting, 40 eligible, 20 blocking
2014-07-14 01:41:05,337 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 40 files from compaction candidates
2014-07-14 01:41:05,337 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:41:05,337 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:41:05,337 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:41:05,337 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:41:07,094 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:41:07,213 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 67064 synced till here 67051
2014-07-14 01:41:07,481 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327264641 with entries=98, filesize=84.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327267096
2014-07-14 01:41:07,482 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 2 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72, b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:41:08,376 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:41:09,204 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 67166 synced till here 67138
2014-07-14 01:41:09,584 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327267096 with entries=102, filesize=87.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327268376
2014-07-14 01:41:09,657 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 2 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72, b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:41:10,401 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:41:10,467 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 67258 synced till here 67252
2014-07-14 01:41:10,508 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327268376 with entries=92, filesize=79.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327270401
2014-07-14 01:41:10,508 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 2 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72, b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:41:11,973 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:41:12,049 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 67360 synced till here 67330
2014-07-14 01:41:12,264 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327270401 with entries=102, filesize=86.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327271973
2014-07-14 01:41:12,264 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 2 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72, b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:41:13,863 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:41:14,389 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 67530 synced till here 67480
2014-07-14 01:41:14,732 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327271973 with entries=170, filesize=144.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327273863
2014-07-14 01:41:14,732 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 2 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72, b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:41:16,804 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:41:16,837 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:41:16,837 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files, but is 1.4g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:41:16,837 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. due to global heap pressure
2014-07-14 01:41:16,838 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 1.4g
2014-07-14 01:41:16,929 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 67634 synced till here 67633
2014-07-14 01:41:16,957 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327273863 with entries=104, filesize=89.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327276806
2014-07-14 01:41:16,958 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 2 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72, b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:41:16,958 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files; delaying flush up to 90000ms
2014-07-14 01:41:16,959 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 36 store files, 0 compacting, 36 eligible, 20 blocking
2014-07-14 01:41:16,959 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 36 files from compaction candidates
2014-07-14 01:41:16,960 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:41:16,960 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:41:16,960 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:41:16,960 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:41:16,987 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:41:16,987 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files, but is 1.4g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:41:16,987 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. due to global heap pressure
2014-07-14 01:41:16,987 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 1.4g
2014-07-14 01:41:18,666 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:41:18,731 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 67734 synced till here 67713
2014-07-14 01:41:18,871 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:18,871 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:18,871 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:18,915 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327276806 with entries=100, filesize=85.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327278667
2014-07-14 01:41:19,163 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,163 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,163 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,164 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,164 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,166 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,166 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,166 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,166 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,166 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,167 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,168 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,168 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,169 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,170 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,170 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,171 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,171 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,171 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,171 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,172 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,172 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,173 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,173 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,173 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,174 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,174 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,199 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:41:19,420 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:41:19,544 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,574 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,605 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,633 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,662 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,692 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:19,723 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:21,589 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:21,618 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:21,647 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:21,675 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:21,705 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:21,737 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:23,354 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=10829, hits=3899, hitRatio=36.00%, , cachingAccesses=3903, cachingHits=3898, cachingHitsRatio=99.87%, evictions=0, evicted=3, evictedPerRun=Infinity
2014-07-14 01:41:23,871 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:41:23,871 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:41:23,871 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:41:24,164 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:41:24,164 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:41:24,165 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:41:24,165 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-14 01:41:24,165 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-14 01:41:24,166 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:41:24,166 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5003ms
2014-07-14 01:41:24,167 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5004ms
2014-07-14 01:41:24,167 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:41:24,168 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-14 01:41:24,168 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5004ms
2014-07-14 01:41:24,169 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:41:24,169 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5003ms
2014-07-14 01:41:24,170 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:41:24,170 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:41:24,171 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:41:24,171 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5004ms
2014-07-14 01:41:24,171 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:41:24,171 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:41:24,172 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:41:24,172 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:41:24,173 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:41:24,173 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:41:24,173 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:41:24,174 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:41:24,174 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:41:24,174 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:41:24,544 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:41:24,574 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:41:24,605 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:41:24,634 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:41:24,663 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:41:24,692 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:41:24,724 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:41:25,954 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:26,590 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:41:26,618 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:41:26,648 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:41:26,676 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:41:26,706 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:41:26,737 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:41:26,876 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:26,913 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:26,948 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:26,982 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:27,018 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:27,053 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:41:28,872 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:41:28,872 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:41:28,872 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:41:29,164 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:41:29,165 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:41:29,165 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:41:29,166 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:41:29,166 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-14 01:41:29,167 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:41:29,167 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10004ms
2014-07-14 01:41:29,168 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10004ms
2014-07-14 01:41:29,168 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:41:29,168 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:41:29,169 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10005ms
2014-07-14 01:41:29,169 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-14 01:41:29,170 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:41:29,170 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:41:29,171 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:41:29,171 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:41:29,172 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:41:29,173 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:41:29,173 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10006ms
2014-07-14 01:41:29,174 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:41:29,174 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-14 01:41:29,174 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-14 01:41:29,174 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:41:29,175 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:41:29,175 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:41:29,175 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:41:29,176 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:41:29,847 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10124ms
2014-07-14 01:41:29,847 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10273ms
2014-07-14 01:41:29,847 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10242ms
2014-07-14 01:41:29,848 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10215ms
2014-07-14 01:41:29,849 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10186ms
2014-07-14 01:41:29,850 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10158ms
2014-07-14 01:41:29,850 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10307ms
2014-07-14 01:41:30,954 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:41:31,590 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:41:31,618 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:41:31,648 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:41:31,676 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:41:31,706 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:41:31,738 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:41:31,877 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:41:31,913 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:41:31,949 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:41:31,983 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:41:32,018 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:41:32,053 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:41:33,872 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:41:33,873 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:41:33,873 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15003ms
2014-07-14 01:41:34,165 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:41:34,165 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:41:34,165 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:41:34,166 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15003ms
2014-07-14 01:41:34,166 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15003ms
2014-07-14 01:41:34,167 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:41:34,168 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15005ms
2014-07-14 01:41:34,168 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15005ms
2014-07-14 01:41:34,168 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:41:34,169 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15003ms
2014-07-14 01:41:34,169 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15005ms
2014-07-14 01:41:34,170 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15004ms
2014-07-14 01:41:34,170 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:41:34,171 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:41:34,171 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:41:34,172 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:41:34,172 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:41:34,173 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:41:34,173 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15006ms
2014-07-14 01:41:34,174 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:41:34,174 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15003ms
2014-07-14 01:41:34,174 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15003ms
2014-07-14 01:41:34,175 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:41:34,175 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:41:34,175 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:41:34,176 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15003ms
2014-07-14 01:41:34,176 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:41:34,847 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15124ms
2014-07-14 01:41:34,847 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15273ms
2014-07-14 01:41:34,848 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15243ms
2014-07-14 01:41:34,848 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15215ms
2014-07-14 01:41:34,850 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15188ms
2014-07-14 01:41:34,850 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15158ms
2014-07-14 01:41:34,851 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15308ms
2014-07-14 01:41:35,955 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:41:36,590 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:41:36,619 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:41:36,648 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:41:36,677 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:41:36,707 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:41:36,738 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:41:36,877 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:41:36,914 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:41:36,949 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:41:36,983 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:41:37,019 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:41:37,054 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:41:38,873 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20003ms
2014-07-14 01:41:38,874 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20003ms
2014-07-14 01:41:38,874 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20003ms
2014-07-14 01:41:39,373 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20199ms
2014-07-14 01:41:39,373 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20206ms
2014-07-14 01:41:39,373 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20204ms
2014-07-14 01:41:39,374 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20211ms
2014-07-14 01:41:39,374 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20211ms
2014-07-14 01:41:39,374 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20202ms
2014-07-14 01:41:39,374 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20203ms
2014-07-14 01:41:39,374 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20203ms
2014-07-14 01:41:39,374 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20200ms
2014-07-14 01:41:39,375 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20202ms
2014-07-14 01:41:39,375 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20202ms
2014-07-14 01:41:39,375 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20202ms
2014-07-14 01:41:39,375 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20211ms
2014-07-14 01:41:39,376 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20213ms
2014-07-14 01:41:39,376 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20213ms
2014-07-14 01:41:39,376 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20210ms
2014-07-14 01:41:39,377 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20213ms
2014-07-14 01:41:39,377 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20214ms
2014-07-14 01:41:39,377 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20211ms
2014-07-14 01:41:39,378 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20211ms
2014-07-14 01:41:39,378 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20214ms
2014-07-14 01:41:39,378 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20212ms
2014-07-14 01:41:39,378 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20210ms
2014-07-14 01:41:39,378 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20208ms
2014-07-14 01:41:39,378 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20208ms
2014-07-14 01:41:39,378 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20207ms
2014-07-14 01:41:39,379 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20208ms
2014-07-14 01:41:39,848 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20125ms
2014-07-14 01:41:39,848 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20274ms
2014-07-14 01:41:39,848 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20243ms
2014-07-14 01:41:39,849 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20216ms
2014-07-14 01:41:39,850 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20188ms
2014-07-14 01:41:39,851 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20159ms
2014-07-14 01:41:39,851 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20308ms
2014-07-14 01:41:40,955 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:41:41,591 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:41:41,619 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:41:41,648 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:41:41,677 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:41:41,707 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:41:41,739 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:41:41,878 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:41:41,914 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:41:41,949 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:41:41,984 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:41:42,019 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:41:42,054 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:41:42,748 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16936, memsize=570.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/c3597bada58a44acb22110deb87f81e7
2014-07-14 01:41:42,771 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/c3597bada58a44acb22110deb87f81e7 as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/c3597bada58a44acb22110deb87f81e7
2014-07-14 01:41:42,823 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/c3597bada58a44acb22110deb87f81e7, entries=2075540, sequenceid=16936, filesize=147.8m
2014-07-14 01:41:42,824 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.4g/1508221040, currentsize=52.7m/55260400 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 25985ms, sequenceid=16936, compaction requested=true
2014-07-14 01:41:42,825 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:41:42,825 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 37 store files, 0 compacting, 37 eligible, 20 blocking
2014-07-14 01:41:42,825 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15772ms
2014-07-14 01:41:42,825 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,825 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 37 files from compaction candidates
2014-07-14 01:41:42,825 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15807ms
2014-07-14 01:41:42,826 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,826 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:41:42,826 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15844ms
2014-07-14 01:41:42,826 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,826 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:41:42,826 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15878ms
2014-07-14 01:41:42,826 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,826 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:41:42,827 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15914ms
2014-07-14 01:41:42,827 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,829 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15953ms
2014-07-14 01:41:42,829 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,829 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21092ms
2014-07-14 01:41:42,829 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,829 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21124ms
2014-07-14 01:41:42,829 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,830 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21155ms
2014-07-14 01:41:42,830 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,830 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21183ms
2014-07-14 01:41:42,830 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,833 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21215ms
2014-07-14 01:41:42,833 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,842 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21253ms
2014-07-14 01:41:42,842 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,842 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16888ms
2014-07-14 01:41:42,842 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,849 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23306ms
2014-07-14 01:41:42,849 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,849 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23157ms
2014-07-14 01:41:42,849 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,849 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23187ms
2014-07-14 01:41:42,849 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,849 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23216ms
2014-07-14 01:41:42,849 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,850 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23245ms
2014-07-14 01:41:42,850 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,850 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23276ms
2014-07-14 01:41:42,850 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,850 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23127ms
2014-07-14 01:41:42,850 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,857 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23686ms
2014-07-14 01:41:42,857 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,857 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23686ms
2014-07-14 01:41:42,857 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,857 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23687ms
2014-07-14 01:41:42,857 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,857 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23687ms
2014-07-14 01:41:42,858 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,858 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23690ms
2014-07-14 01:41:42,858 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,865 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23699ms
2014-07-14 01:41:42,865 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,866 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23701ms
2014-07-14 01:41:42,866 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,866 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23700ms
2014-07-14 01:41:42,866 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,866 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23700ms
2014-07-14 01:41:42,866 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,867 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23704ms
2014-07-14 01:41:42,867 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,867 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23704ms
2014-07-14 01:41:42,867 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,873 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23707ms
2014-07-14 01:41:42,873 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,874 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23711ms
2014-07-14 01:41:42,874 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,874 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23711ms
2014-07-14 01:41:42,874 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,881 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23717ms
2014-07-14 01:41:42,881 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,881 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23708ms
2014-07-14 01:41:42,881 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,881 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23708ms
2014-07-14 01:41:42,881 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,882 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23709ms
2014-07-14 01:41:42,882 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,883 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23709ms
2014-07-14 01:41:42,883 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,886 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23715ms
2014-07-14 01:41:42,886 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,886 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23715ms
2014-07-14 01:41:42,886 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,887 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23714ms
2014-07-14 01:41:42,887 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,887 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23724ms
2014-07-14 01:41:42,887 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,888 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23725ms
2014-07-14 01:41:42,888 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,888 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23719ms
2014-07-14 01:41:42,888 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,890 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23723ms
2014-07-14 01:41:42,890 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,890 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23716ms
2014-07-14 01:41:42,890 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,891 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24019ms
2014-07-14 01:41:42,891 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,892 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24022ms
2014-07-14 01:41:42,892 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,896 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24026ms
2014-07-14 01:41:42,896 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:41:42,982 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26145,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327276836,"queuetimems":1170,"class":"HRegionServer","responsesize":15627,"method":"Multi"}
2014-07-14 01:41:43,196 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26384,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327276811,"queuetimems":2265,"class":"HRegionServer","responsesize":15336,"method":"Multi"}
2014-07-14 01:41:43,275 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16923, memsize=570.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/f580e3f46a264ae9a36a939decc7d01c
2014-07-14 01:41:43,290 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/f580e3f46a264ae9a36a939decc7d01c as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/f580e3f46a264ae9a36a939decc7d01c
2014-07-14 01:41:43,312 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/f580e3f46a264ae9a36a939decc7d01c, entries=2076190, sequenceid=16923, filesize=147.8m
2014-07-14 01:41:43,312 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.4g/1504076480, currentsize=34.0m/35702640 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 26325ms, sequenceid=16923, compaction requested=true
2014-07-14 01:41:43,313 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:41:43,313 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 36 store files, 0 compacting, 36 eligible, 20 blocking
2014-07-14 01:41:43,313 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 36 files from compaction candidates
2014-07-14 01:41:43,313 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:41:43,313 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:41:43,313 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:41:43,407 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26383,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327277023,"queuetimems":0,"class":"HRegionServer","responsesize":15485,"method":"Multi"}
2014-07-14 01:41:43,407 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26564,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327276842,"queuetimems":1149,"class":"HRegionServer","responsesize":15837,"method":"Multi"}
2014-07-14 01:41:43,407 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26240,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327277166,"queuetimems":0,"class":"HRegionServer","responsesize":16087,"method":"Multi"}
2014-07-14 01:41:43,407 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26592,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327276814,"queuetimems":2181,"class":"HRegionServer","responsesize":16033,"method":"Multi"}
2014-07-14 01:41:43,407 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26564,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327276842,"queuetimems":1116,"class":"HRegionServer","responsesize":16192,"method":"Multi"}
2014-07-14 01:41:43,407 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26193,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327277213,"queuetimems":0,"class":"HRegionServer","responsesize":15445,"method":"Multi"}
2014-07-14 01:41:43,407 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26580,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327276826,"queuetimems":2164,"class":"HRegionServer","responsesize":15727,"method":"Multi"}
2014-07-14 01:41:43,407 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26580,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327276826,"queuetimems":1201,"class":"HRegionServer","responsesize":15552,"method":"Multi"}
2014-07-14 01:41:43,407 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26581,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327276826,"queuetimems":2105,"class":"HRegionServer","responsesize":15803,"method":"Multi"}
2014-07-14 01:41:43,408 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26581,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327276826,"queuetimems":2134,"class":"HRegionServer","responsesize":15973,"method":"Multi"}
2014-07-14 01:41:43,408 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26436,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327276971,"queuetimems":0,"class":"HRegionServer","responsesize":16129,"method":"Multi"}
2014-07-14 01:41:43,411 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:41:43,413 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26343,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327277069,"queuetimems":0,"class":"HRegionServer","responsesize":15308,"method":"Multi"}
2014-07-14 01:41:43,413 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26598,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327276814,"queuetimems":2220,"class":"HRegionServer","responsesize":16354,"method":"Multi"}
2014-07-14 01:41:43,413 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26286,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327277126,"queuetimems":1,"class":"HRegionServer","responsesize":15584,"method":"Multi"}
2014-07-14 01:41:43,413 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25013,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327278399,"queuetimems":0,"class":"HRegionServer","responsesize":15798,"method":"Multi"}
2014-07-14 01:41:43,413 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26571,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327276842,"queuetimems":1087,"class":"HRegionServer","responsesize":15775,"method":"Multi"}
2014-07-14 01:41:43,413 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26577,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327276836,"queuetimems":1200,"class":"HRegionServer","responsesize":15787,"method":"Multi"}
2014-07-14 01:41:43,422 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25187,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327278234,"queuetimems":9,"class":"HRegionServer","responsesize":15736,"method":"Multi"}
2014-07-14 01:41:43,430 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24881,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327278548,"queuetimems":0,"class":"HRegionServer","responsesize":15935,"method":"Multi"}
2014-07-14 01:41:43,430 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24912,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327278517,"queuetimems":0,"class":"HRegionServer","responsesize":16186,"method":"Multi"}
2014-07-14 01:41:43,480 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 67827 synced till here 67813
2014-07-14 01:41:44,436 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25997,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327278438,"queuetimems":0,"class":"HRegionServer","responsesize":15839,"method":"Multi"}
2014-07-14 01:41:44,436 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26258,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327278177,"queuetimems":0,"class":"HRegionServer","responsesize":16252,"method":"Multi"}
2014-07-14 01:41:44,436 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27178,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327277257,"queuetimems":1,"class":"HRegionServer","responsesize":15770,"method":"Multi"}
2014-07-14 01:41:44,441 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26072,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327278369,"queuetimems":1,"class":"HRegionServer","responsesize":15967,"method":"Multi"}
2014-07-14 01:41:44,445 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26116,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327278329,"queuetimems":1,"class":"HRegionServer","responsesize":15345,"method":"Multi"}
2014-07-14 01:41:44,441 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25962,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327278479,"queuetimems":1,"class":"HRegionServer","responsesize":16009,"method":"Multi"}
2014-07-14 01:41:44,441 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26278,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327278163,"queuetimems":0,"class":"HRegionServer","responsesize":15598,"method":"Multi"}
2014-07-14 01:41:44,475 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327278667 with entries=93, filesize=79.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327303411
2014-07-14 01:41:44,475 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327125671
2014-07-14 01:41:44,475 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327154087
2014-07-14 01:41:44,475 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327155488
2014-07-14 01:41:44,475 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327156853
2014-07-14 01:41:44,476 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327160014
2014-07-14 01:41:44,476 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327162388
2014-07-14 01:41:44,476 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327164627
2014-07-14 01:41:44,476 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327168392
2014-07-14 01:41:44,476 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327171595
2014-07-14 01:41:44,476 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327187293
2014-07-14 01:41:44,476 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327192980
2014-07-14 01:41:44,476 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327194564
2014-07-14 01:41:44,476 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327195953
2014-07-14 01:41:44,476 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327197806
2014-07-14 01:41:44,476 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327199421
2014-07-14 01:41:44,476 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327201136
2014-07-14 01:41:44,476 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327204514
2014-07-14 01:41:44,476 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327206044
2014-07-14 01:41:44,476 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327206943
2014-07-14 01:41:44,476 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327208874
2014-07-14 01:41:44,476 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327210476
2014-07-14 01:41:44,476 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327212229
2014-07-14 01:41:44,477 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327213827
2014-07-14 01:41:44,477 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327215339
2014-07-14 01:41:44,477 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327216867
2014-07-14 01:41:44,477 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327218639
2014-07-14 01:41:44,477 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327220253
2014-07-14 01:41:44,722 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26143,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327278579,"queuetimems":0,"class":"HRegionServer","responsesize":15390,"method":"Multi"}
2014-07-14 01:41:45,299 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:41:45,364 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 67933 synced till here 67903
2014-07-14 01:41:45,519 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18539,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327286980,"queuetimems":1,"class":"HRegionServer","responsesize":15727,"method":"Multi"}
2014-07-14 01:41:45,519 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18505,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327287014,"queuetimems":0,"class":"HRegionServer","responsesize":15775,"method":"Multi"}
2014-07-14 01:41:45,519 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18611,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327286908,"queuetimems":0,"class":"HRegionServer","responsesize":15336,"method":"Multi"}
2014-07-14 01:41:45,520 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18575,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327286945,"queuetimems":0,"class":"HRegionServer","responsesize":15973,"method":"Multi"}
2014-07-14 01:41:45,519 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18470,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327287049,"queuetimems":0,"class":"HRegionServer","responsesize":15552,"method":"Multi"}
2014-07-14 01:41:45,535 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327303411 with entries=106, filesize=90.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327305299
2014-07-14 01:41:46,456 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24753,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327281703,"queuetimems":0,"class":"HRegionServer","responsesize":15859,"method":"Multi"}
2014-07-14 01:41:46,456 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26736,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327279720,"queuetimems":0,"class":"HRegionServer","responsesize":15663,"method":"Multi"}
2014-07-14 01:41:46,456 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26853,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327279603,"queuetimems":0,"class":"HRegionServer","responsesize":15795,"method":"Multi"}
2014-07-14 01:41:46,456 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24782,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327281674,"queuetimems":1,"class":"HRegionServer","responsesize":15570,"method":"Multi"}
2014-07-14 01:41:46,456 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26766,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327279690,"queuetimems":0,"class":"HRegionServer","responsesize":15902,"method":"Multi"}
2014-07-14 01:41:46,465 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20517,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327285948,"queuetimems":1,"class":"HRegionServer","responsesize":15945,"method":"Multi"}
2014-07-14 01:41:46,465 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26833,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327279632,"queuetimems":1,"class":"HRegionServer","responsesize":15531,"method":"Multi"}
2014-07-14 01:41:46,466 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26924,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327279542,"queuetimems":0,"class":"HRegionServer","responsesize":15496,"method":"Multi"}
2014-07-14 01:41:46,466 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24731,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327281735,"queuetimems":0,"class":"HRegionServer","responsesize":16143,"method":"Multi"}
2014-07-14 01:41:46,465 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24820,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327281645,"queuetimems":0,"class":"HRegionServer","responsesize":15682,"method":"Multi"}
2014-07-14 01:41:46,465 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24849,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327281616,"queuetimems":0,"class":"HRegionServer","responsesize":15838,"method":"Multi"}
2014-07-14 01:41:46,466 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19594,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327286872,"queuetimems":0,"class":"HRegionServer","responsesize":16354,"method":"Multi"}
2014-07-14 01:41:46,465 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24878,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327281587,"queuetimems":0,"class":"HRegionServer","responsesize":15791,"method":"Multi"}
2014-07-14 01:41:46,465 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26804,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327279661,"queuetimems":0,"class":"HRegionServer","responsesize":15836,"method":"Multi"}
2014-07-14 01:41:46,474 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26900,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47149","starttimems":1405327279573,"queuetimems":1,"class":"HRegionServer","responsesize":15623,"method":"Multi"}
2014-07-14 01:41:47,225 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:41:47,307 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327305299 with entries=101, filesize=86.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327307226
2014-07-14 01:41:49,019 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:41:49,224 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 68147 synced till here 68146
2014-07-14 01:41:49,259 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327307226 with entries=113, filesize=97.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327309019
2014-07-14 01:41:50,058 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:41:50,105 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327309019 with entries=72, filesize=61.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327310059
2014-07-14 01:41:51,118 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90730ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:41:51,119 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 708.7m
2014-07-14 01:41:51,553 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:41:55,090 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=17121, memsize=73.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/c9aad76f2bad4763b29eaff58391bd80
2014-07-14 01:41:55,119 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/c9aad76f2bad4763b29eaff58391bd80 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/c9aad76f2bad4763b29eaff58391bd80
2014-07-14 01:41:55,133 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/c9aad76f2bad4763b29eaff58391bd80, entries=265860, sequenceid=17121, filesize=18.9m
2014-07-14 01:41:55,134 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~708.7m/743104240, currentsize=3.2m/3337840 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 4015ms, sequenceid=17121, compaction requested=true
2014-07-14 01:41:55,134 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:41:55,134 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 41 store files, 0 compacting, 41 eligible, 20 blocking
2014-07-14 01:41:55,134 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 41 files from compaction candidates
2014-07-14 01:41:55,135 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:41:55,135 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:41:55,135 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:41:58,973 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:41:58,973 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files; delaying flush up to 90000ms
2014-07-14 01:41:58,973 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:41:58,974 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 36 store files, 0 compacting, 36 eligible, 20 blocking
2014-07-14 01:41:58,974 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 36 files from compaction candidates
2014-07-14 01:41:58,974 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:41:58,974 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:41:58,974 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:41:59,493 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:41:59,966 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:42:00,035 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 68292 synced till here 68291
2014-07-14 01:42:00,045 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327310059 with entries=73, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327319967
2014-07-14 01:42:04,430 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:42:04,566 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 68368 synced till here 68366
2014-07-14 01:42:04,675 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327319967 with entries=76, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327324502
2014-07-14 01:42:05,767 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:42:06,104 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 68461 synced till here 68457
2014-07-14 01:42:06,138 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327324502 with entries=93, filesize=79.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327325768
2014-07-14 01:42:08,102 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:42:08,145 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327325768 with entries=72, filesize=61.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327328102
2014-07-14 01:42:33,570 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90045ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:42:33,570 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 890.3m
2014-07-14 01:42:34,133 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:42:35,370 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90048ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:42:35,370 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 124.1m
2014-07-14 01:42:35,421 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:42:38,854 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=17202, memsize=124.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/8bd85c4c282a4f618cd3853785f325c9
2014-07-14 01:42:38,879 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/8bd85c4c282a4f618cd3853785f325c9 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/8bd85c4c282a4f618cd3853785f325c9
2014-07-14 01:42:38,898 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/8bd85c4c282a4f618cd3853785f325c9, entries=451910, sequenceid=17202, filesize=32.2m
2014-07-14 01:42:38,899 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~124.1m/130146400, currentsize=0.0/0 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 3529ms, sequenceid=17202, compaction requested=true
2014-07-14 01:42:38,900 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:42:38,900 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 42 store files, 0 compacting, 42 eligible, 20 blocking
2014-07-14 01:42:38,900 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 42 files from compaction candidates
2014-07-14 01:42:38,900 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:42:38,900 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:42:38,900 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:42:40,654 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=17180, memsize=137.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/376ee48c07b249f7a54f712625ae2303
2014-07-14 01:42:40,672 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/376ee48c07b249f7a54f712625ae2303 as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/376ee48c07b249f7a54f712625ae2303
2014-07-14 01:42:40,884 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:42:40,969 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/376ee48c07b249f7a54f712625ae2303, entries=500680, sequenceid=17180, filesize=35.6m
2014-07-14 01:42:40,970 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~890.3m/933586000, currentsize=26.0m/27244320 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 7400ms, sequenceid=17180, compaction requested=true
2014-07-14 01:42:40,970 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:42:40,970 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 42 store files, 0 compacting, 42 eligible, 20 blocking
2014-07-14 01:42:40,970 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 42 files from compaction candidates
2014-07-14 01:42:40,970 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:42:40,970 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:42:40,971 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:42:40,977 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 68609 synced till here 68607
2014-07-14 01:42:41,040 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327328102 with entries=76, filesize=65.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327360884
2014-07-14 01:42:41,040 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327222476
2014-07-14 01:42:41,040 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327252758
2014-07-14 01:42:41,040 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327254889
2014-07-14 01:42:41,040 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327256575
2014-07-14 01:42:41,040 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327258500
2014-07-14 01:42:41,040 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327260205
2014-07-14 01:42:41,040 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327262058
2014-07-14 01:42:41,040 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327264641
2014-07-14 01:42:41,040 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327267096
2014-07-14 01:42:41,040 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327268376
2014-07-14 01:42:41,041 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327270401
2014-07-14 01:42:41,041 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327271973
2014-07-14 01:42:41,041 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327273863
2014-07-14 01:42:43,162 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:42:43,216 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327360884 with entries=72, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327363162
2014-07-14 01:42:47,003 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90044ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:42:47,004 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 431.7m
2014-07-14 01:42:47,282 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:42:49,164 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:42:49,216 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 68759 synced till here 68755
2014-07-14 01:42:49,279 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327363162 with entries=78, filesize=66.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327369164
2014-07-14 01:42:50,099 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:42:50,118 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 68835 synced till here 68832
2014-07-14 01:42:50,185 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327369164 with entries=76, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327370099
2014-07-14 01:42:54,631 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=17216, memsize=211.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/c063feb32168422cb65320334b6c2ef5
2014-07-14 01:42:54,653 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/c063feb32168422cb65320334b6c2ef5 as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/c063feb32168422cb65320334b6c2ef5
2014-07-14 01:42:54,674 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/c063feb32168422cb65320334b6c2ef5, entries=768600, sequenceid=17216, filesize=54.7m
2014-07-14 01:42:54,675 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~431.7m/452624240, currentsize=57.8m/60593600 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 7672ms, sequenceid=17216, compaction requested=true
2014-07-14 01:42:54,676 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:42:54,676 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 38 store files, 0 compacting, 38 eligible, 20 blocking
2014-07-14 01:42:54,676 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 38 files from compaction candidates
2014-07-14 01:42:54,676 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:42:54,676 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:42:54,676 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:43:24,244 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:43:24,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 68908 synced till here 68906
2014-07-14 01:43:24,280 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327370099 with entries=73, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327404244
2014-07-14 01:43:25,715 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:43:25,743 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 68983 synced till here 68981
2014-07-14 01:43:25,803 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327404244 with entries=75, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327405716
2014-07-14 01:43:27,086 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:43:27,183 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 69063 synced till here 69055
2014-07-14 01:43:27,346 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327405716 with entries=80, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327407086
2014-07-14 01:43:29,311 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90339ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:43:29,312 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 582.4m
2014-07-14 01:43:29,655 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:43:36,813 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:43:36,851 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327407086 with entries=73, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327416814
2014-07-14 01:43:40,374 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=17299, memsize=351.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/2c59de86ab9943adada3742b5ab9854a
2014-07-14 01:43:40,387 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/2c59de86ab9943adada3742b5ab9854a as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/2c59de86ab9943adada3742b5ab9854a
2014-07-14 01:43:40,403 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/2c59de86ab9943adada3742b5ab9854a, entries=1278860, sequenceid=17299, filesize=91.1m
2014-07-14 01:43:40,404 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~582.4m/610700640, currentsize=23.5m/24626080 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 11092ms, sequenceid=17299, compaction requested=true
2014-07-14 01:43:40,405 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:43:40,405 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 37 store files, 0 compacting, 37 eligible, 20 blocking
2014-07-14 01:43:40,405 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 37 files from compaction candidates
2014-07-14 01:43:40,405 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:43:40,405 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:43:40,405 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:43:44,693 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:43:44,693 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files; delaying flush up to 90000ms
2014-07-14 01:43:44,693 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:43:44,694 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 42 store files, 0 compacting, 42 eligible, 20 blocking
2014-07-14 01:43:44,694 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 42 files from compaction candidates
2014-07-14 01:43:44,694 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:43:44,694 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:43:44,694 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:43:44,786 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:43:44,787 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files; delaying flush up to 90000ms
2014-07-14 01:43:44,787 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:43:44,787 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 42 store files, 0 compacting, 42 eligible, 20 blocking
2014-07-14 01:43:44,787 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 42 files from compaction candidates
2014-07-14 01:43:44,787 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:43:44,788 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:43:44,788 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:43:44,798 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:43:44,834 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 69210 synced till here 69208
2014-07-14 01:43:44,863 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327416814 with entries=74, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327424798
2014-07-14 01:43:44,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327276806
2014-07-14 01:43:44,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327278667
2014-07-14 01:43:44,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327303411
2014-07-14 01:43:44,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327305299
2014-07-14 01:43:44,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327307226
2014-07-14 01:43:44,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327309019
2014-07-14 01:43:44,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327310059
2014-07-14 01:43:44,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327319967
2014-07-14 01:43:44,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327324502
2014-07-14 01:43:44,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327325768
2014-07-14 01:43:47,154 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:43:47,181 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 69285 synced till here 69283
2014-07-14 01:43:47,468 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327424798 with entries=75, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327427154
2014-07-14 01:43:48,510 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:43:48,738 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327427154 with entries=78, filesize=67.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327428511
2014-07-14 01:43:49,459 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:43:49,460 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files; delaying flush up to 90000ms
2014-07-14 01:43:49,460 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:43:49,461 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 38 store files, 0 compacting, 38 eligible, 20 blocking
2014-07-14 01:43:49,461 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 38 files from compaction candidates
2014-07-14 01:43:49,462 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:43:49,462 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:43:49,462 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:43:49,935 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:43:49,957 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 69436 synced till here 69435
2014-07-14 01:43:49,978 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327428511 with entries=73, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327429935
2014-07-14 01:43:51,291 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:43:51,733 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 69513 synced till here 69510
2014-07-14 01:43:51,765 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327429935 with entries=77, filesize=65.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327431291
2014-07-14 01:43:55,285 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:43:55,609 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 69608 synced till here 69606
2014-07-14 01:43:55,634 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327431291 with entries=95, filesize=81.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327435285
2014-07-14 01:43:57,680 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:43:58,101 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 69690 synced till here 69681
2014-07-14 01:43:58,222 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327435285 with entries=82, filesize=70.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327437681
2014-07-14 01:43:59,764 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:43:59,782 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 69765 synced till here 69762
2014-07-14 01:44:00,256 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327437681 with entries=75, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327439765
2014-07-14 01:44:00,660 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:44:00,660 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files; delaying flush up to 90000ms
2014-07-14 01:44:00,661 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:44:00,661 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 37 store files, 0 compacting, 37 eligible, 20 blocking
2014-07-14 01:44:00,661 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 37 files from compaction candidates
2014-07-14 01:44:00,661 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:44:00,661 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:44:00,661 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:44:01,439 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:44:01,464 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 69838 synced till here 69837
2014-07-14 01:44:01,474 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327439765 with entries=73, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327441440
2014-07-14 01:44:02,941 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:44:03,087 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327441440 with entries=77, filesize=66.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327442942
2014-07-14 01:44:04,349 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:44:04,394 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 69991 synced till here 69989
2014-07-14 01:44:04,492 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327442942 with entries=76, filesize=65.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327444350
2014-07-14 01:44:06,601 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:44:06,654 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327444350 with entries=71, filesize=60.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327446602
2014-07-14 01:44:08,865 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:44:09,009 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 70142 synced till here 70138
2014-07-14 01:44:09,069 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327446602 with entries=80, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327448866
2014-07-14 01:44:10,486 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:44:10,524 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 70214 synced till here 70213
2014-07-14 01:44:10,587 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327448866 with entries=72, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327450487
2014-07-14 01:44:33,881 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:44:33,912 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 70288 synced till here 70287
2014-07-14 01:44:33,932 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327450487 with entries=74, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327473882
2014-07-14 01:44:35,111 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:44:35,160 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 70365 synced till here 70362
2014-07-14 01:44:35,188 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327473882 with entries=77, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327475112
2014-07-14 01:44:36,164 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:44:36,191 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 70438 synced till here 70437
2014-07-14 01:44:36,210 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327475112 with entries=73, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327476164
2014-07-14 01:44:37,506 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:44:37,662 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 70517 synced till here 70515
2014-07-14 01:44:37,684 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327476164 with entries=79, filesize=67.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327477549
2014-07-14 01:44:38,895 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:44:38,916 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 70593 synced till here 70591
2014-07-14 01:44:38,942 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327477549 with entries=76, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327478895
2014-07-14 01:44:40,224 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:44:40,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 70669 synced till here 70666
2014-07-14 01:44:40,326 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327478895 with entries=76, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327480224
2014-07-14 01:44:41,193 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:44:41,673 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 70747 synced till here 70745
2014-07-14 01:44:41,698 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327480224 with entries=78, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327481194
2014-07-14 01:44:42,683 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:44:42,769 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327481194 with entries=71, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327482684
2014-07-14 01:44:45,111 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:44:45,163 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327482684 with entries=73, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327485111
2014-07-14 01:44:49,669 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:44:49,965 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 70985 synced till here 70982
2014-07-14 01:44:50,002 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327485111 with entries=94, filesize=80.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327489670
2014-07-14 01:44:51,377 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:44:51,584 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 71072 synced till here 71070
2014-07-14 01:44:51,608 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327489670 with entries=87, filesize=74.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327491378
2014-07-14 01:44:51,608 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 2 regions(s): 6e2bf74487f61672cd8bc06d8b34f003, 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:44:53,333 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:44:53,387 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 71148 synced till here 71144
2014-07-14 01:44:53,434 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327491378 with entries=76, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327493334
2014-07-14 01:44:53,435 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 2 regions(s): 6e2bf74487f61672cd8bc06d8b34f003, 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:44:54,378 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:44:54,378 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files, but is 1.0g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:44:54,378 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. due to global heap pressure
2014-07-14 01:44:54,379 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 1.0g
2014-07-14 01:44:54,449 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:44:54,470 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 71224 synced till here 71222
2014-07-14 01:44:54,506 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327493334 with entries=76, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327494450
2014-07-14 01:44:54,507 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 6e2bf74487f61672cd8bc06d8b34f003
2014-07-14 01:44:55,152 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:44:55,153 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files, but is 1.0g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:44:55,154 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. due to global heap pressure
2014-07-14 01:44:55,154 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 1.0g
2014-07-14 01:44:55,640 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:44:55,941 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:44:56,108 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:44:56,135 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 71297 synced till here 71296
2014-07-14 01:44:56,154 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327494450 with entries=73, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327496108
2014-07-14 01:44:57,881 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:44:57,885 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:44:57,901 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:44:57,934 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:44:58,075 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:44:58,123 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:44:58,182 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:44:58,347 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:44:58,389 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:44:58,435 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:44:58,465 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:44:58,498 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:44:58,744 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:44:58,944 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:44:58,996 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:44:59,094 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:44:59,125 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:44:59,158 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:00,301 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:00,348 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:00,392 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:00,438 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:00,486 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:00,537 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:00,590 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:00,650 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:00,712 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:00,768 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:00,827 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:00,874 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:00,913 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:00,950 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:00,985 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:02,253 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:02,284 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:02,313 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:02,342 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:02,373 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:02,405 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:02,435 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:02,469 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:02,498 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:02,535 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:02,562 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:02,594 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:02,628 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:02,660 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:02,881 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:45:02,885 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:45:02,902 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:45:02,934 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:45:03,076 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:45:03,123 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:45:03,183 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:45:03,347 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:45:03,389 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:45:03,436 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:45:03,465 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:45:03,499 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:45:03,745 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:45:03,945 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:45:03,996 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:45:04,094 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:45:04,125 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:04,126 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:45:04,158 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:45:04,160 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:04,193 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:05,301 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:45:05,348 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:45:05,393 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:45:05,438 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:45:05,486 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:45:05,538 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:45:05,591 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:45:05,651 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:45:05,713 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:45:05,769 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:45:05,828 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:45:05,875 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:45:05,914 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:45:05,951 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:45:05,986 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:45:07,975 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10041ms
2014-07-14 01:45:07,975 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5413ms
2014-07-14 01:45:07,976 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5603ms
2014-07-14 01:45:07,976 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5723ms
2014-07-14 01:45:07,977 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5316ms
2014-07-14 01:45:07,977 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5350ms
2014-07-14 01:45:07,977 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5383ms
2014-07-14 01:45:07,977 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10096ms
2014-07-14 01:45:07,978 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10092ms
2014-07-14 01:45:07,978 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10077ms
2014-07-14 01:45:07,978 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5694ms
2014-07-14 01:45:07,978 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5665ms
2014-07-14 01:45:07,979 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5636ms
2014-07-14 01:45:07,979 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5574ms
2014-07-14 01:45:07,979 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5544ms
2014-07-14 01:45:07,979 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5481ms
2014-07-14 01:45:07,979 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5444ms
2014-07-14 01:45:07,980 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5511ms
2014-07-14 01:45:08,076 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:45:08,123 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:45:08,183 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:45:08,348 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:45:08,389 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:45:08,436 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:45:08,465 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:45:08,499 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:45:08,745 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:45:08,945 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:45:08,997 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:45:09,095 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:45:09,125 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:45:09,126 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:45:09,159 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:45:09,160 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:45:09,193 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:45:10,302 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:45:10,349 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:45:10,394 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:45:10,439 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:45:10,487 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:45:10,539 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:45:10,591 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:45:10,651 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:45:10,713 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:45:10,769 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:45:10,828 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:45:10,875 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:45:10,914 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:45:10,951 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:45:10,986 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:45:12,976 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15042ms
2014-07-14 01:45:12,976 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10414ms
2014-07-14 01:45:12,976 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10603ms
2014-07-14 01:45:12,977 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10724ms
2014-07-14 01:45:12,977 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10317ms
2014-07-14 01:45:12,977 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10350ms
2014-07-14 01:45:12,977 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15096ms
2014-07-14 01:45:12,977 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10383ms
2014-07-14 01:45:12,978 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15093ms
2014-07-14 01:45:12,978 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15077ms
2014-07-14 01:45:12,978 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10694ms
2014-07-14 01:45:12,979 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10666ms
2014-07-14 01:45:12,979 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10637ms
2014-07-14 01:45:12,979 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10574ms
2014-07-14 01:45:12,979 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10544ms
2014-07-14 01:45:12,980 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10482ms
2014-07-14 01:45:12,980 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10445ms
2014-07-14 01:45:12,980 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10511ms
2014-07-14 01:45:13,077 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:45:13,124 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:45:13,183 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:45:14,161 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:45:14,161 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15165ms
2014-07-14 01:45:14,162 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15418ms
2014-07-14 01:45:14,162 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15773ms
2014-07-14 01:45:14,162 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15068ms
2014-07-14 01:45:14,162 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10037ms
2014-07-14 01:45:14,162 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15004ms
2014-07-14 01:45:14,163 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15037ms
2014-07-14 01:45:14,163 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15816ms
2014-07-14 01:45:14,163 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15728ms
2014-07-14 01:45:14,163 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15699ms
2014-07-14 01:45:14,164 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15665ms
2014-07-14 01:45:14,164 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15220ms
2014-07-14 01:45:14,193 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:45:15,303 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:45:15,349 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:45:15,394 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:45:15,439 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:45:15,487 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:45:15,539 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:45:15,592 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:45:15,652 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:45:15,714 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:45:15,770 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:45:15,828 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:45:15,875 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:45:15,914 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:45:15,952 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:45:15,986 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:45:17,976 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20042ms
2014-07-14 01:45:17,976 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15603ms
2014-07-14 01:45:17,977 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15415ms
2014-07-14 01:45:17,977 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15724ms
2014-07-14 01:45:17,978 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15384ms
2014-07-14 01:45:17,978 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20097ms
2014-07-14 01:45:17,978 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15351ms
2014-07-14 01:45:17,979 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15318ms
2014-07-14 01:45:17,979 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15666ms
2014-07-14 01:45:17,979 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15637ms
2014-07-14 01:45:17,980 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15695ms
2014-07-14 01:45:17,980 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20079ms
2014-07-14 01:45:17,980 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15482ms
2014-07-14 01:45:17,980 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20095ms
2014-07-14 01:45:17,981 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15511ms
2014-07-14 01:45:17,981 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15446ms
2014-07-14 01:45:17,981 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15546ms
2014-07-14 01:45:17,981 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15576ms
2014-07-14 01:45:18,077 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:45:18,124 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:45:18,183 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:45:19,162 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:45:19,162 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20418ms
2014-07-14 01:45:19,162 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20166ms
2014-07-14 01:45:19,162 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20068ms
2014-07-14 01:45:19,163 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20773ms
2014-07-14 01:45:19,163 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20005ms
2014-07-14 01:45:19,163 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15038ms
2014-07-14 01:45:19,163 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20038ms
2014-07-14 01:45:19,163 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20816ms
2014-07-14 01:45:19,164 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20728ms
2014-07-14 01:45:19,164 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20700ms
2014-07-14 01:45:19,165 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20220ms
2014-07-14 01:45:19,165 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20667ms
2014-07-14 01:45:19,194 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:45:20,433 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20041ms
2014-07-14 01:45:20,434 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20133ms
2014-07-14 01:45:20,434 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20086ms
2014-07-14 01:45:20,439 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:45:20,487 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:45:20,539 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:45:20,592 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:45:20,652 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:45:20,714 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:45:20,770 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:45:20,828 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:45:20,876 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:45:20,915 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:45:20,952 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:45:20,986 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:45:22,977 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25042ms
2014-07-14 01:45:22,977 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20415ms
2014-07-14 01:45:22,978 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20605ms
2014-07-14 01:45:22,978 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20725ms
2014-07-14 01:45:22,978 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20384ms
2014-07-14 01:45:22,978 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25097ms
2014-07-14 01:45:22,979 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20352ms
2014-07-14 01:45:22,979 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20319ms
2014-07-14 01:45:22,980 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20667ms
2014-07-14 01:45:22,980 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20638ms
2014-07-14 01:45:22,980 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20696ms
2014-07-14 01:45:22,980 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25079ms
2014-07-14 01:45:22,980 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20482ms
2014-07-14 01:45:22,981 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25096ms
2014-07-14 01:45:22,981 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20512ms
2014-07-14 01:45:22,981 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20446ms
2014-07-14 01:45:22,982 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20577ms
2014-07-14 01:45:22,982 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20547ms
2014-07-14 01:45:23,078 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25003ms
2014-07-14 01:45:23,124 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:45:23,184 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:45:24,162 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:45:24,163 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25166ms
2014-07-14 01:45:24,163 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25419ms
2014-07-14 01:45:24,163 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25774ms
2014-07-14 01:45:24,163 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25005ms
2014-07-14 01:45:24,163 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25069ms
2014-07-14 01:45:24,164 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20038ms
2014-07-14 01:45:24,164 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25817ms
2014-07-14 01:45:24,164 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25039ms
2014-07-14 01:45:24,164 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25729ms
2014-07-14 01:45:24,164 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25700ms
2014-07-14 01:45:24,165 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25221ms
2014-07-14 01:45:24,165 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25667ms
2014-07-14 01:45:24,194 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:45:25,434 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25042ms
2014-07-14 01:45:25,434 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25133ms
2014-07-14 01:45:25,435 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25087ms
2014-07-14 01:45:25,440 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:45:25,487 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:45:25,539 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:45:25,593 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25003ms
2014-07-14 01:45:25,652 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:45:25,714 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:45:25,770 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:45:25,828 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:45:25,876 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:45:25,915 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:45:25,952 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:45:25,987 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:45:27,977 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30043ms
2014-07-14 01:45:27,978 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25416ms
2014-07-14 01:45:27,978 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25605ms
2014-07-14 01:45:27,978 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30097ms
2014-07-14 01:45:27,979 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25385ms
2014-07-14 01:45:27,979 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25352ms
2014-07-14 01:45:27,979 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25726ms
2014-07-14 01:45:27,979 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25319ms
2014-07-14 01:45:27,980 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25667ms
2014-07-14 01:45:27,980 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25638ms
2014-07-14 01:45:27,980 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25696ms
2014-07-14 01:45:27,981 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30080ms
2014-07-14 01:45:27,981 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25483ms
2014-07-14 01:45:27,981 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30096ms
2014-07-14 01:45:27,981 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25512ms
2014-07-14 01:45:27,982 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25447ms
2014-07-14 01:45:27,982 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25577ms
2014-07-14 01:45:27,982 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25547ms
2014-07-14 01:45:28,078 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30003ms
2014-07-14 01:45:28,124 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30001ms
2014-07-14 01:45:28,184 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:45:29,163 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25003ms
2014-07-14 01:45:29,163 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30167ms
2014-07-14 01:45:29,163 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30774ms
2014-07-14 01:45:29,163 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30419ms
2014-07-14 01:45:29,163 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30005ms
2014-07-14 01:45:29,163 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30069ms
2014-07-14 01:45:29,164 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25039ms
2014-07-14 01:45:29,164 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30817ms
2014-07-14 01:45:29,164 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30039ms
2014-07-14 01:45:29,164 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30729ms
2014-07-14 01:45:29,165 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30701ms
2014-07-14 01:45:29,165 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30221ms
2014-07-14 01:45:29,165 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30667ms
2014-07-14 01:45:29,194 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:45:30,434 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30042ms
2014-07-14 01:45:30,435 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30134ms
2014-07-14 01:45:30,435 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30087ms
2014-07-14 01:45:30,440 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:45:30,488 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:45:30,540 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30003ms
2014-07-14 01:45:30,593 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30003ms
2014-07-14 01:45:30,652 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:45:30,715 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30003ms
2014-07-14 01:45:30,771 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:45:30,829 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:45:30,876 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:45:30,915 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:45:30,952 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:45:30,987 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:45:31,161 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=17873, memsize=1022.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/ebd0553474b041ea8fcfd7b1fd2f1e19
2014-07-14 01:45:31,181 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/ebd0553474b041ea8fcfd7b1fd2f1e19 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/ebd0553474b041ea8fcfd7b1fd2f1e19
2014-07-14 01:45:31,199 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/ebd0553474b041ea8fcfd7b1fd2f1e19, entries=3723750, sequenceid=17873, filesize=265.2m
2014-07-14 01:45:31,199 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.0g/1090241840, currentsize=49.8m/52250560 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 36820ms, sequenceid=17873, compaction requested=true
2014-07-14 01:45:31,200 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:45:31,200 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 43 store files, 0 compacting, 43 eligible, 20 blocking
2014-07-14 01:45:31,200 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 43 files from compaction candidates
2014-07-14 01:45:31,200 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30215ms
2014-07-14 01:45:31,200 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:45:31,200 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 106508ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:45:31,200 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,200 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:45:31,200 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30250ms
2014-07-14 01:45:31,200 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,200 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:45:31,200 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30287ms
2014-07-14 01:45:31,201 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,200 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 49.8m
2014-07-14 01:45:31,202 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30328ms
2014-07-14 01:45:31,203 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,205 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30378ms
2014-07-14 01:45:31,205 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,205 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30437ms
2014-07-14 01:45:31,205 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,205 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30493ms
2014-07-14 01:45:31,205 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,205 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30555ms
2014-07-14 01:45:31,205 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,208 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30618ms
2014-07-14 01:45:31,208 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,210 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30673ms
2014-07-14 01:45:31,210 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,210 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30724ms
2014-07-14 01:45:31,210 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,210 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30772ms
2014-07-14 01:45:31,210 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,211 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30863ms
2014-07-14 01:45:31,211 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,212 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30910ms
2014-07-14 01:45:31,212 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,212 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30820ms
2014-07-14 01:45:31,213 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,213 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27020ms
2014-07-14 01:45:31,214 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,214 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32716ms
2014-07-14 01:45:31,214 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,214 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32270ms
2014-07-14 01:45:31,214 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,215 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32751ms
2014-07-14 01:45:31,215 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,215 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32780ms
2014-07-14 01:45:31,216 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,216 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32091ms
2014-07-14 01:45:31,216 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,217 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32870ms
2014-07-14 01:45:31,217 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,218 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27092ms
2014-07-14 01:45:31,218 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,218 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32124ms
2014-07-14 01:45:31,218 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,219 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32061ms
2014-07-14 01:45:31,219 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,220 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32475ms
2014-07-14 01:45:31,220 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,220 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32831ms
2014-07-14 01:45:31,220 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,220 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 32224ms
2014-07-14 01:45:31,220 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,222 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27062ms
2014-07-14 01:45:31,222 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,222 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33040ms
2014-07-14 01:45:31,222 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,222 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33099ms
2014-07-14 01:45:31,222 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,223 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33147ms
2014-07-14 01:45:31,223 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,223 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28788ms
2014-07-14 01:45:31,223 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,223 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28818ms
2014-07-14 01:45:31,223 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,224 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28689ms
2014-07-14 01:45:31,224 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,225 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28755ms
2014-07-14 01:45:31,225 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,225 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33340ms
2014-07-14 01:45:31,225 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,225 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28727ms
2014-07-14 01:45:31,225 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,226 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33324ms
2014-07-14 01:45:31,226 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,226 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28942ms
2014-07-14 01:45:31,226 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,226 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28884ms
2014-07-14 01:45:31,226 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,226 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28913ms
2014-07-14 01:45:31,227 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,227 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28567ms
2014-07-14 01:45:31,227 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,227 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28974ms
2014-07-14 01:45:31,227 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,227 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28600ms
2014-07-14 01:45:31,227 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,227 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28633ms
2014-07-14 01:45:31,228 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,228 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33347ms
2014-07-14 01:45:31,228 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,231 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:45:31,231 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28858ms
2014-07-14 01:45:31,231 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,231 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28669ms
2014-07-14 01:45:31,232 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,232 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33298ms
2014-07-14 01:45:31,232 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:45:31,330 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33633,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327497697,"queuetimems":0,"class":"HRegionServer","responsesize":15651,"method":"Multi"}
2014-07-14 01:45:31,386 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33724,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327497661,"queuetimems":1,"class":"HRegionServer","responsesize":15754,"method":"Multi"}
2014-07-14 01:45:31,388 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33647,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327497740,"queuetimems":0,"class":"HRegionServer","responsesize":16053,"method":"Multi"}
2014-07-14 01:45:31,731 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:45:31,785 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 71404 synced till here 71388
2014-07-14 01:45:31,899 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327496108 with entries=107, filesize=89.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327531731
2014-07-14 01:45:32,914 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=17858, memsize=1.0g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/2ea5bf583b2847489e428a45371c655e
2014-07-14 01:45:32,946 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/2ea5bf583b2847489e428a45371c655e as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/2ea5bf583b2847489e428a45371c655e
2014-07-14 01:45:32,976 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/2ea5bf583b2847489e428a45371c655e, entries=3761490, sequenceid=17858, filesize=267.8m
2014-07-14 01:45:32,977 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.0g/1100972640, currentsize=95.6m/100214640 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 37823ms, sequenceid=17858, compaction requested=true
2014-07-14 01:45:32,977 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 43 store files, 0 compacting, 43 eligible, 20 blocking
2014-07-14 01:45:32,977 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 43 files from compaction candidates
2014-07-14 01:45:32,977 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:45:32,977 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:45:32,977 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:45:32,978 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:45:32,978 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 108192ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:45:32,978 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 95.6m
2014-07-14 01:45:33,125 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:45:33,391 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=17906, memsize=49.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/8abecb5cdc9c4460be7aaaa9923f1031
2014-07-14 01:45:33,412 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/8abecb5cdc9c4460be7aaaa9923f1031 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/8abecb5cdc9c4460be7aaaa9923f1031
2014-07-14 01:45:33,434 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/8abecb5cdc9c4460be7aaaa9923f1031, entries=181430, sequenceid=17906, filesize=12.9m
2014-07-14 01:45:33,434 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~49.8m/52250560, currentsize=77.5m/81260000 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 2234ms, sequenceid=17906, compaction requested=true
2014-07-14 01:45:33,435 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:45:33,435 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 44 store files, 0 compacting, 44 eligible, 20 blocking
2014-07-14 01:45:33,435 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 44 files from compaction candidates
2014-07-14 01:45:33,435 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:45:33,435 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:45:33,435 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:45:33,444 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:45:33,446 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32678,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327500767,"queuetimems":1,"class":"HRegionServer","responsesize":15633,"method":"Multi"}
2014-07-14 01:45:33,463 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 71508 synced till here 71488
2014-07-14 01:45:33,838 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327531731 with entries=104, filesize=85.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327533444
2014-07-14 01:45:33,838 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327328102
2014-07-14 01:45:33,838 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327360884
2014-07-14 01:45:34,591 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:45:34,946 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":35789,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327499156,"queuetimems":1,"class":"HRegionServer","responsesize":16032,"method":"Multi"}
2014-07-14 01:45:34,946 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30755,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327504191,"queuetimems":1,"class":"HRegionServer","responsesize":15778,"method":"Multi"}
2014-07-14 01:45:34,946 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36601,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327498345,"queuetimems":0,"class":"HRegionServer","responsesize":15699,"method":"Multi"}
2014-07-14 01:45:34,949 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":35955,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327498994,"queuetimems":0,"class":"HRegionServer","responsesize":15612,"method":"Multi"}
2014-07-14 01:45:34,949 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32421,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327502528,"queuetimems":0,"class":"HRegionServer","responsesize":15844,"method":"Multi"}
2014-07-14 01:45:34,953 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36566,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327498387,"queuetimems":0,"class":"HRegionServer","responsesize":16006,"method":"Multi"}
2014-07-14 01:45:34,946 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32512,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327502433,"queuetimems":0,"class":"HRegionServer","responsesize":15966,"method":"Multi"}
2014-07-14 01:45:34,962 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36019,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327498942,"queuetimems":0,"class":"HRegionServer","responsesize":15571,"method":"Multi"}
2014-07-14 01:45:34,970 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34083,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327500870,"queuetimems":0,"class":"HRegionServer","responsesize":15679,"method":"Multi"}
2014-07-14 01:45:34,962 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36841,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327498120,"queuetimems":0,"class":"HRegionServer","responsesize":15774,"method":"Multi"}
2014-07-14 01:45:34,962 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36498,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327498463,"queuetimems":0,"class":"HRegionServer","responsesize":15830,"method":"Multi"}
2014-07-14 01:45:34,962 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34477,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327500484,"queuetimems":0,"class":"HRegionServer","responsesize":15651,"method":"Multi"}
2014-07-14 01:45:34,962 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36781,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327498180,"queuetimems":1,"class":"HRegionServer","responsesize":15610,"method":"Multi"}
2014-07-14 01:45:34,961 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36528,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327498433,"queuetimems":0,"class":"HRegionServer","responsesize":16246,"method":"Multi"}
2014-07-14 01:45:34,961 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36889,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327498072,"queuetimems":0,"class":"HRegionServer","responsesize":15966,"method":"Multi"}
2014-07-14 01:45:34,961 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33979,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327500981,"queuetimems":0,"class":"HRegionServer","responsesize":15802,"method":"Multi"}
2014-07-14 01:45:34,961 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34253,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327500707,"queuetimems":0,"class":"HRegionServer","responsesize":15652,"method":"Multi"}
2014-07-14 01:45:34,960 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32708,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327502252,"queuetimems":0,"class":"HRegionServer","responsesize":15699,"method":"Multi"}
2014-07-14 01:45:34,960 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32400,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327502559,"queuetimems":0,"class":"HRegionServer","responsesize":15571,"method":"Multi"}
2014-07-14 01:45:34,959 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34569,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327500390,"queuetimems":0,"class":"HRegionServer","responsesize":15754,"method":"Multi"}
2014-07-14 01:45:34,959 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32617,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327502341,"queuetimems":1,"class":"HRegionServer","responsesize":15774,"method":"Multi"}
2014-07-14 01:45:34,998 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":35874,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327499123,"queuetimems":0,"class":"HRegionServer","responsesize":15706,"method":"Multi"}
2014-07-14 01:45:34,998 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30843,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327504155,"queuetimems":0,"class":"HRegionServer","responsesize":15679,"method":"Multi"}
2014-07-14 01:45:35,002 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34349,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327500648,"queuetimems":1,"class":"HRegionServer","responsesize":15987,"method":"Multi"}
2014-07-14 01:45:35,002 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34467,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327500535,"queuetimems":0,"class":"HRegionServer","responsesize":16061,"method":"Multi"}
2014-07-14 01:45:35,002 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32411,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327502591,"queuetimems":1,"class":"HRegionServer","responsesize":16032,"method":"Multi"}
2014-07-14 01:45:34,958 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32495,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327502463,"queuetimems":0,"class":"HRegionServer","responsesize":15830,"method":"Multi"}
2014-07-14 01:45:34,958 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34049,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327500909,"queuetimems":0,"class":"HRegionServer","responsesize":15211,"method":"Multi"}
2014-07-14 01:45:34,958 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36461,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327498497,"queuetimems":0,"class":"HRegionServer","responsesize":15844,"method":"Multi"}
2014-07-14 01:45:34,957 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36214,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327498743,"queuetimems":0,"class":"HRegionServer","responsesize":15773,"method":"Multi"}
2014-07-14 01:45:34,957 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":35865,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327499092,"queuetimems":0,"class":"HRegionServer","responsesize":15849,"method":"Multi"}
2014-07-14 01:45:34,957 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34521,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327500436,"queuetimems":0,"class":"HRegionServer","responsesize":16053,"method":"Multi"}
2014-07-14 01:45:34,957 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":37059,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327497898,"queuetimems":0,"class":"HRegionServer","responsesize":16149,"method":"Multi"}
2014-07-14 01:45:34,957 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30837,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327504119,"queuetimems":1,"class":"HRegionServer","responsesize":15987,"method":"Multi"}
2014-07-14 01:45:34,956 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34011,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327500945,"queuetimems":0,"class":"HRegionServer","responsesize":15711,"method":"Multi"}
2014-07-14 01:45:34,956 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34610,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327500346,"queuetimems":0,"class":"HRegionServer","responsesize":15485,"method":"Multi"}
2014-07-14 01:45:34,953 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34365,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327500587,"queuetimems":0,"class":"HRegionServer","responsesize":15783,"method":"Multi"}
2014-07-14 01:45:34,949 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32454,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327502495,"queuetimems":0,"class":"HRegionServer","responsesize":15773,"method":"Multi"}
2014-07-14 01:45:34,949 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34650,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327500299,"queuetimems":0,"class":"HRegionServer","responsesize":16149,"method":"Multi"}
2014-07-14 01:45:35,002 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32344,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327502658,"queuetimems":1,"class":"HRegionServer","responsesize":15612,"method":"Multi"}
2014-07-14 01:45:35,002 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32594,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327502403,"queuetimems":0,"class":"HRegionServer","responsesize":16006,"method":"Multi"}
2014-07-14 01:45:34,970 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32642,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327502311,"queuetimems":0,"class":"HRegionServer","responsesize":15706,"method":"Multi"}
2014-07-14 01:45:35,038 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32413,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327502624,"queuetimems":0,"class":"HRegionServer","responsesize":15610,"method":"Multi"}
2014-07-14 01:45:35,070 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32700,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327502370,"queuetimems":0,"class":"HRegionServer","responsesize":15849,"method":"Multi"}
2014-07-14 01:45:35,084 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32801,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327502282,"queuetimems":0,"class":"HRegionServer","responsesize":16246,"method":"Multi"}
2014-07-14 01:45:35,096 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34275,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327500821,"queuetimems":1,"class":"HRegionServer","responsesize":15778,"method":"Multi"}
2014-07-14 01:45:35,240 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:45:35,270 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 71614 synced till here 71582
2014-07-14 01:45:35,482 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327533444 with entries=106, filesize=89.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327535241
2014-07-14 01:45:35,482 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:45:36,896 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=17934, memsize=79.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/f9dfc8f422494e8a8e13a0bf1a48c963
2014-07-14 01:45:36,904 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:45:36,921 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/f9dfc8f422494e8a8e13a0bf1a48c963 as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/f9dfc8f422494e8a8e13a0bf1a48c963
2014-07-14 01:45:36,931 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 71720 synced till here 71698
2014-07-14 01:45:36,947 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/f9dfc8f422494e8a8e13a0bf1a48c963, entries=289960, sequenceid=17934, filesize=20.7m
2014-07-14 01:45:36,947 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~116.5m/122110480, currentsize=79.1m/82929040 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 3969ms, sequenceid=17934, compaction requested=true
2014-07-14 01:45:36,948 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 44 store files, 0 compacting, 44 eligible, 20 blocking
2014-07-14 01:45:36,948 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 44 files from compaction candidates
2014-07-14 01:45:36,948 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:45:36,948 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:45:36,948 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:45:36,948 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:45:37,131 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327535241 with entries=106, filesize=87.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327536904
2014-07-14 01:45:37,132 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:45:38,365 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:45:38,395 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 71826 synced till here 71798
2014-07-14 01:45:38,621 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327536904 with entries=106, filesize=85.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327538366
2014-07-14 01:45:38,622 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:45:39,430 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:45:40,091 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 71930 synced till here 71898
2014-07-14 01:45:40,694 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327538366 with entries=104, filesize=88.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327539431
2014-07-14 01:45:40,695 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:45:41,419 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:45:41,439 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 72035 synced till here 72013
2014-07-14 01:45:41,545 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:45:41,545 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files; delaying flush up to 90000ms
2014-07-14 01:45:41,545 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:45:41,546 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 44 store files, 0 compacting, 44 eligible, 20 blocking
2014-07-14 01:45:41,546 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 44 files from compaction candidates
2014-07-14 01:45:41,546 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:45:41,546 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:45:41,546 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:45:42,185 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327539431 with entries=105, filesize=89.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327541419
2014-07-14 01:45:42,186 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:45:43,074 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:45:43,401 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 72148 synced till here 72109
2014-07-14 01:45:44,653 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327541419 with entries=113, filesize=97.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327543074
2014-07-14 01:45:44,654 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:45:45,783 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:45:46,860 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 72257 synced till here 72226
2014-07-14 01:45:47,117 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:45:47,117 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files; delaying flush up to 90000ms
2014-07-14 01:45:47,121 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 44 store files, 0 compacting, 44 eligible, 20 blocking
2014-07-14 01:45:47,122 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 44 files from compaction candidates
2014-07-14 01:45:47,122 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:45:47,122 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:45:47,122 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:45:47,130 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:45:47,213 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327543074 with entries=109, filesize=90.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327545783
2014-07-14 01:45:47,213 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:45:48,023 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:45:48,091 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 72358 synced till here 72333
2014-07-14 01:45:49,266 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327545783 with entries=101, filesize=84.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327548024
2014-07-14 01:45:49,266 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=44, maxlogs=32; forcing flush of 1 regions(s): b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:45:50,030 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:45:50,106 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 72470 synced till here 72441
2014-07-14 01:45:51,146 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327548024 with entries=112, filesize=92.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327550030
2014-07-14 01:45:51,146 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=45, maxlogs=32; forcing flush of 1 regions(s): b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:45:51,913 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:45:52,032 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 72568 synced till here 72551
2014-07-14 01:45:52,237 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327550030 with entries=98, filesize=84.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327551914
2014-07-14 01:45:52,237 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=46, maxlogs=32; forcing flush of 1 regions(s): b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:45:53,754 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:45:53,794 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 72683 synced till here 72651
2014-07-14 01:45:53,988 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327551914 with entries=115, filesize=98.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327553754
2014-07-14 01:45:53,989 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=47, maxlogs=32; forcing flush of 1 regions(s): b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:45:54,269 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:45:54,269 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files, but is 1.5g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:45:54,269 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. due to global heap pressure
2014-07-14 01:45:54,269 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 1.5g
2014-07-14 01:45:55,136 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:45:55,136 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files, but is 1.3g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:45:55,136 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. due to global heap pressure
2014-07-14 01:45:55,136 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 1.3g
2014-07-14 01:45:55,430 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:45:55,483 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 72776 synced till here 72771
2014-07-14 01:45:55,730 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327553754 with entries=93, filesize=79.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327555431
2014-07-14 01:45:56,229 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:45:56,342 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:45:57,457 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:57,458 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:57,887 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:57,894 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:57,902 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:57,923 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:57,931 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:57,931 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:57,966 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:57,968 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:57,995 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:58,027 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:58,060 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:58,110 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:58,164 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:58,195 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:58,226 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:58,267 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:58,296 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:58,331 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:58,370 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:58,404 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:58,436 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:58,467 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:58,501 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:58,531 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:58,561 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:58,610 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:58,648 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:58,677 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:58,708 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:58,738 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:58,768 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:58,797 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:58,969 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:59,218 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:59,252 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:59,278 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:59,306 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:59,335 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:59,366 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:59,395 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:59,423 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:59,453 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:59,483 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:59,512 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:45:59,645 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:46:01,652 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:46:01,692 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:46:01,724 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:46:02,790 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5331ms
2014-07-14 01:46:02,790 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5333ms
2014-07-14 01:46:02,888 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:46:02,895 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:46:02,902 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:02,923 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:02,931 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:02,932 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:46:02,967 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:46:02,969 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:46:02,995 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:03,027 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:03,061 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:03,111 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:46:03,165 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:46:03,197 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:03,227 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:03,267 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:03,296 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:03,331 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:03,370 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:03,404 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:03,436 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:03,467 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:03,501 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:03,531 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:03,562 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:03,610 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:03,649 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:03,677 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:46:03,709 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:46:03,739 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:03,769 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:46:03,798 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:46:03,970 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:46:04,219 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:46:04,253 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:04,279 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:46:04,307 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:46:04,336 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:46:04,366 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:04,395 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:04,424 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:46:04,453 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:04,483 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:04,512 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:04,645 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:06,653 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:46:06,692 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:46:06,725 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:46:07,790 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10332ms
2014-07-14 01:46:07,791 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10333ms
2014-07-14 01:46:07,888 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:07,895 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:07,902 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:46:07,923 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:46:07,932 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:46:07,932 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:07,967 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:07,969 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:07,995 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:46:08,027 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:46:08,061 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:08,111 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:08,165 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:08,197 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:46:08,227 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:08,268 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:08,296 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:46:08,331 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:46:08,370 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:46:08,405 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:08,436 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:46:08,467 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:46:08,502 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:08,532 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:08,562 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:08,610 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:46:08,649 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:08,678 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:46:08,709 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:08,739 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:08,769 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:08,798 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:08,970 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:09,219 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:09,253 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:09,279 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:09,307 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:09,336 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:09,367 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:09,396 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:09,424 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:09,453 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:46:09,484 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:09,513 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:46:09,645 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:46:11,653 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:11,692 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:46:11,725 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:46:12,791 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15333ms
2014-07-14 01:46:12,791 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15334ms
2014-07-14 01:46:12,888 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:12,895 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:12,903 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:12,924 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:12,932 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:12,932 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:12,968 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:46:12,969 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:12,995 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 01:46:13,028 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:13,061 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:13,111 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:13,165 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:13,197 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:46:13,227 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:13,268 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:13,296 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 01:46:13,332 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:13,371 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:13,405 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:13,437 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 01:46:13,467 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 01:46:13,503 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:46:13,532 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:13,562 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:13,611 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:13,649 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:13,678 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:46:13,709 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:13,739 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:13,769 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:13,798 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:13,970 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:14,220 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:46:14,253 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:14,279 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:14,307 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:14,337 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:14,367 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:14,396 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:14,424 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:14,453 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 01:46:14,484 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:14,513 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:14,646 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:16,654 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:46:16,693 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:46:16,726 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:46:17,999 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20004ms
2014-07-14 01:46:17,999 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20112ms
2014-07-14 01:46:17,999 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20541ms
2014-07-14 01:46:17,999 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20105ms
2014-07-14 01:46:18,000 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20098ms
2014-07-14 01:46:18,000 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20069ms
2014-07-14 01:46:18,000 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20077ms
2014-07-14 01:46:18,000 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20069ms
2014-07-14 01:46:18,000 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20032ms
2014-07-14 01:46:18,000 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20034ms
2014-07-14 01:46:18,001 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20544ms
2014-07-14 01:46:18,028 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:46:18,061 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:46:18,111 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:46:18,165 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:46:18,197 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:46:18,227 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:46:18,268 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:46:18,297 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20000ms
2014-07-14 01:46:18,332 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:46:18,371 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:46:18,405 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:46:18,437 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:46:18,468 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:46:18,503 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:46:18,532 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:46:18,562 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:46:18,611 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:46:18,649 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:46:18,678 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:46:18,709 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:46:18,740 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:46:18,770 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:46:18,798 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:46:18,971 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:46:19,220 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:46:19,253 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:46:19,279 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:46:19,308 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:46:19,337 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:46:19,367 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:46:19,396 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:46:19,424 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:46:19,454 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:46:19,485 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:46:19,513 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:46:19,646 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:46:21,654 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:46:21,693 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:46:21,726 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:46:22,999 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25004ms
2014-07-14 01:46:22,999 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25112ms
2014-07-14 01:46:23,000 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25541ms
2014-07-14 01:46:23,000 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25098ms
2014-07-14 01:46:23,000 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25106ms
2014-07-14 01:46:23,000 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25077ms
2014-07-14 01:46:23,000 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25069ms
2014-07-14 01:46:23,001 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25033ms
2014-07-14 01:46:23,001 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25070ms
2014-07-14 01:46:23,001 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25035ms
2014-07-14 01:46:23,001 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25544ms
2014-07-14 01:46:23,028 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:46:23,062 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:46:23,112 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:46:23,166 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:46:23,198 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25003ms
2014-07-14 01:46:23,228 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:46:23,268 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:46:23,297 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:46:23,332 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:46:23,352 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=10829, hits=3899, hitRatio=36.00%, , cachingAccesses=3903, cachingHits=3898, cachingHitsRatio=99.87%, evictions=0, evicted=3, evictedPerRun=Infinity
2014-07-14 01:46:23,371 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:46:23,405 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:46:23,437 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:46:23,468 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:46:23,504 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:46:23,533 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:46:23,562 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:46:23,611 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:46:23,649 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:46:23,678 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:46:23,710 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:46:23,740 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:46:23,770 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:46:23,799 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:46:23,971 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:46:24,220 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:46:24,254 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:46:24,280 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:46:24,308 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:46:24,337 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:46:24,368 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:46:24,396 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:46:25,158 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25513ms
2014-07-14 01:46:25,159 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25735ms
2014-07-14 01:46:25,159 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25676ms
2014-07-14 01:46:25,159 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25706ms
2014-07-14 01:46:25,159 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25647ms
2014-07-14 01:46:26,655 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25003ms
2014-07-14 01:46:26,693 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:46:26,727 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25003ms
2014-07-14 01:46:27,999 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30004ms
2014-07-14 01:46:28,000 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30113ms
2014-07-14 01:46:28,000 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30542ms
2014-07-14 01:46:28,000 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30098ms
2014-07-14 01:46:28,001 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30106ms
2014-07-14 01:46:28,001 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30078ms
2014-07-14 01:46:28,001 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30033ms
2014-07-14 01:46:28,001 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30070ms
2014-07-14 01:46:28,001 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30070ms
2014-07-14 01:46:28,001 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30544ms
2014-07-14 01:46:28,001 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30035ms
2014-07-14 01:46:28,028 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30001ms
2014-07-14 01:46:28,062 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:46:28,112 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:46:28,166 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:46:28,198 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30003ms
2014-07-14 01:46:28,228 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:46:28,269 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:46:28,297 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30001ms
2014-07-14 01:46:28,332 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30001ms
2014-07-14 01:46:28,371 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30001ms
2014-07-14 01:46:28,405 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30001ms
2014-07-14 01:46:28,437 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30001ms
2014-07-14 01:46:28,468 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30001ms
2014-07-14 01:46:28,504 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30003ms
2014-07-14 01:46:28,533 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:46:28,563 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:46:28,612 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30001ms
2014-07-14 01:46:28,650 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:46:28,679 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30003ms
2014-07-14 01:46:28,710 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:46:28,740 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:46:28,770 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:46:28,799 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30002ms
2014-07-14 01:46:28,842 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=18199, memsize=921.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/7b12132762084ca1ac86826beb4ff7b0
2014-07-14 01:46:28,861 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/7b12132762084ca1ac86826beb4ff7b0 as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/7b12132762084ca1ac86826beb4ff7b0
2014-07-14 01:46:28,871 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/7b12132762084ca1ac86826beb4ff7b0, entries=3356330, sequenceid=18199, filesize=239.0m
2014-07-14 01:46:28,872 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.4g/1451079600, currentsize=45.1m/47268400 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 33736ms, sequenceid=18199, compaction requested=true
2014-07-14 01:46:28,872 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:46:28,872 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 38 store files, 0 compacting, 38 eligible, 20 blocking
2014-07-14 01:46:28,872 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30075ms
2014-07-14 01:46:28,872 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,872 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 38 files from compaction candidates
2014-07-14 01:46:28,873 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30105ms
2014-07-14 01:46:28,873 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,873 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:46:28,873 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:46:28,873 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:46:28,877 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30139ms
2014-07-14 01:46:28,877 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,877 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30169ms
2014-07-14 01:46:28,877 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,877 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30201ms
2014-07-14 01:46:28,877 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,877 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30229ms
2014-07-14 01:46:28,877 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,877 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30267ms
2014-07-14 01:46:28,877 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,877 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30316ms
2014-07-14 01:46:28,877 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,878 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30347ms
2014-07-14 01:46:28,878 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,878 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30377ms
2014-07-14 01:46:28,878 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,878 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30411ms
2014-07-14 01:46:28,878 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,878 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30442ms
2014-07-14 01:46:28,878 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,881 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30477ms
2014-07-14 01:46:28,881 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,892 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30522ms
2014-07-14 01:46:28,892 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,892 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30561ms
2014-07-14 01:46:28,892 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,894 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30598ms
2014-07-14 01:46:28,894 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,894 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30627ms
2014-07-14 01:46:28,894 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,894 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30668ms
2014-07-14 01:46:28,894 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,894 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30699ms
2014-07-14 01:46:28,894 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,905 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30741ms
2014-07-14 01:46:28,905 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,905 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30795ms
2014-07-14 01:46:28,905 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,912 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30852ms
2014-07-14 01:46:28,912 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,921 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30894ms
2014-07-14 01:46:28,921 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,921 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30955ms
2014-07-14 01:46:28,921 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,921 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31464ms
2014-07-14 01:46:28,921 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,921 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30990ms
2014-07-14 01:46:28,921 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,921 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30990ms
2014-07-14 01:46:28,921 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,929 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30961ms
2014-07-14 01:46:28,929 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,929 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31006ms
2014-07-14 01:46:28,929 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,933 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31039ms
2014-07-14 01:46:28,933 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,933 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31031ms
2014-07-14 01:46:28,933 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,933 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31475ms
2014-07-14 01:46:28,933 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,933 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31046ms
2014-07-14 01:46:28,933 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,933 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30938ms
2014-07-14 01:46:28,933 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,936 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27212ms
2014-07-14 01:46:28,936 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,936 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27244ms
2014-07-14 01:46:28,936 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,936 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27284ms
2014-07-14 01:46:28,936 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,937 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 29425ms
2014-07-14 01:46:28,937 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,941 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 29488ms
2014-07-14 01:46:28,941 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,942 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 29458ms
2014-07-14 01:46:28,942 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,942 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 29519ms
2014-07-14 01:46:28,942 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,943 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 29298ms
2014-07-14 01:46:28,943 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,943 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 29548ms
2014-07-14 01:46:28,943 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,943 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 29577ms
2014-07-14 01:46:28,943 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,943 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 29608ms
2014-07-14 01:46:28,943 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,947 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 29641ms
2014-07-14 01:46:28,947 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,950 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 29671ms
2014-07-14 01:46:28,950 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,951 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 29699ms
2014-07-14 01:46:28,952 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,952 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 29734ms
2014-07-14 01:46:28,952 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:28,953 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 29984ms
2014-07-14 01:46:28,954 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:46:29,202 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:46:29,351 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 72893 synced till here 72849
2014-07-14 01:46:29,623 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32322,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327557301,"queuetimems":1,"class":"HRegionServer","responsesize":15779,"method":"Multi"}
2014-07-14 01:46:29,623 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32369,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327557254,"queuetimems":0,"class":"HRegionServer","responsesize":16006,"method":"Multi"}
2014-07-14 01:46:29,624 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32420,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327557204,"queuetimems":1,"class":"HRegionServer","responsesize":16246,"method":"Multi"}
2014-07-14 01:46:29,647 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327555431 with entries=117, filesize=99.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327589202
2014-07-14 01:46:30,477 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33138,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327557339,"queuetimems":1,"class":"HRegionServer","responsesize":15641,"method":"Multi"}
2014-07-14 01:46:30,485 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33043,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327557442,"queuetimems":1,"class":"HRegionServer","responsesize":15706,"method":"Multi"}
2014-07-14 01:46:30,724 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31928,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327558795,"queuetimems":0,"class":"HRegionServer","responsesize":15598,"method":"Multi"}
2014-07-14 01:46:30,724 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33341,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327557382,"queuetimems":1,"class":"HRegionServer","responsesize":15571,"method":"Multi"}
2014-07-14 01:46:30,724 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32048,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327558675,"queuetimems":0,"class":"HRegionServer","responsesize":15612,"method":"Multi"}
2014-07-14 01:46:30,725 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31988,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327558736,"queuetimems":0,"class":"HRegionServer","responsesize":15844,"method":"Multi"}
2014-07-14 01:46:30,725 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31957,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327558767,"queuetimems":1,"class":"HRegionServer","responsesize":15773,"method":"Multi"}
2014-07-14 01:46:30,725 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32019,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327558706,"queuetimems":1,"class":"HRegionServer","responsesize":15849,"method":"Multi"}
2014-07-14 01:46:31,030 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:46:31,400 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73015 synced till here 72965
2014-07-14 01:46:31,754 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33795,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327557958,"queuetimems":0,"class":"HRegionServer","responsesize":15421,"method":"Multi"}
2014-07-14 01:46:31,754 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30032,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327561721,"queuetimems":0,"class":"HRegionServer","responsesize":15708,"method":"Multi"}
2014-07-14 01:46:31,755 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327589202 with entries=122, filesize=104.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327591031
2014-07-14 01:46:31,754 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33560,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327558193,"queuetimems":0,"class":"HRegionServer","responsesize":15699,"method":"Multi"}
2014-07-14 01:46:31,754 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33194,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327558559,"queuetimems":0,"class":"HRegionServer","responsesize":15610,"method":"Multi"}
2014-07-14 01:46:32,509 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33204,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327559305,"queuetimems":0,"class":"HRegionServer","responsesize":15211,"method":"Multi"}
2014-07-14 01:46:32,517 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33240,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327559277,"queuetimems":0,"class":"HRegionServer","responsesize":16061,"method":"Multi"}
2014-07-14 01:46:32,767 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34239,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327558528,"queuetimems":0,"class":"HRegionServer","responsesize":15567,"method":"Multi"}
2014-07-14 01:46:32,904 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34845,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327558058,"queuetimems":0,"class":"HRegionServer","responsesize":15856,"method":"Multi"}
2014-07-14 01:46:32,904 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34910,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327557993,"queuetimems":1,"class":"HRegionServer","responsesize":15733,"method":"Multi"}
2014-07-14 01:46:32,904 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34257,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327558646,"queuetimems":1,"class":"HRegionServer","responsesize":15774,"method":"Multi"}
2014-07-14 01:46:32,904 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31254,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327561649,"queuetimems":1,"class":"HRegionServer","responsesize":15490,"method":"Multi"}
2014-07-14 01:46:32,913 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34689,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327558224,"queuetimems":0,"class":"HRegionServer","responsesize":16032,"method":"Multi"}
2014-07-14 01:46:32,913 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34751,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327558162,"queuetimems":0,"class":"HRegionServer","responsesize":16036,"method":"Multi"}
2014-07-14 01:46:32,914 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34448,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327558465,"queuetimems":0,"class":"HRegionServer","responsesize":15730,"method":"Multi"}
2014-07-14 01:46:32,913 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34549,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327558364,"queuetimems":0,"class":"HRegionServer","responsesize":15794,"method":"Multi"}
2014-07-14 01:46:32,913 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33696,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327559217,"queuetimems":1,"class":"HRegionServer","responsesize":15802,"method":"Multi"}
2014-07-14 01:46:32,914 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33271,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327559643,"queuetimems":0,"class":"HRegionServer","responsesize":15794,"method":"Multi"}
2014-07-14 01:46:32,921 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":35021,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327557900,"queuetimems":1,"class":"HRegionServer","responsesize":15689,"method":"Multi"}
2014-07-14 01:46:32,925 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33679,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327559246,"queuetimems":0,"class":"HRegionServer","responsesize":15633,"method":"Multi"}
2014-07-14 01:46:32,925 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33444,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327559481,"queuetimems":0,"class":"HRegionServer","responsesize":15987,"method":"Multi"}
2014-07-14 01:46:32,926 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34901,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327558025,"queuetimems":1,"class":"HRegionServer","responsesize":16123,"method":"Multi"}
2014-07-14 01:46:32,933 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33599,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327559334,"queuetimems":0,"class":"HRegionServer","responsesize":15778,"method":"Multi"}
2014-07-14 01:46:32,933 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31247,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327561686,"queuetimems":0,"class":"HRegionServer","responsesize":15598,"method":"Multi"}
2014-07-14 01:46:32,937 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":35007,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327557930,"queuetimems":1,"class":"HRegionServer","responsesize":15966,"method":"Multi"}
2014-07-14 01:46:32,913 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34805,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327558108,"queuetimems":0,"class":"HRegionServer","responsesize":15625,"method":"Multi"}
2014-07-14 01:46:32,925 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":35040,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327557885,"queuetimems":0,"class":"HRegionServer","responsesize":15930,"method":"Multi"}
2014-07-14 01:46:32,942 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34647,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327558294,"queuetimems":0,"class":"HRegionServer","responsesize":15708,"method":"Multi"}
2014-07-14 01:46:32,942 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33548,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327559393,"queuetimems":0,"class":"HRegionServer","responsesize":15485,"method":"Multi"}
2014-07-14 01:46:32,946 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33494,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327559451,"queuetimems":0,"class":"HRegionServer","responsesize":15783,"method":"Multi"}
2014-07-14 01:46:32,949 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34516,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327558433,"queuetimems":0,"class":"HRegionServer","responsesize":15617,"method":"Multi"}
2014-07-14 01:46:32,949 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34450,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327558499,"queuetimems":0,"class":"HRegionServer","responsesize":15697,"method":"Multi"}
2014-07-14 01:46:32,950 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33440,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327559510,"queuetimems":0,"class":"HRegionServer","responsesize":15711,"method":"Multi"}
2014-07-14 01:46:32,949 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33585,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327559364,"queuetimems":0,"class":"HRegionServer","responsesize":15679,"method":"Multi"}
2014-07-14 01:46:32,958 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33535,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327559422,"queuetimems":0,"class":"HRegionServer","responsesize":15652,"method":"Multi"}
2014-07-14 01:46:32,967 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34704,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327558262,"queuetimems":1,"class":"HRegionServer","responsesize":16172,"method":"Multi"}
2014-07-14 01:46:32,967 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34568,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327558399,"queuetimems":1,"class":"HRegionServer","responsesize":15853,"method":"Multi"}
2014-07-14 01:46:32,972 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34364,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327558608,"queuetimems":1,"class":"HRegionServer","responsesize":15830,"method":"Multi"}
2014-07-14 01:46:32,984 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34656,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327558328,"queuetimems":1,"class":"HRegionServer","responsesize":16022,"method":"Multi"}
2014-07-14 01:46:33,065 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:46:33,069 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34100,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327558968,"queuetimems":1,"class":"HRegionServer","responsesize":15490,"method":"Multi"}
2014-07-14 01:46:33,182 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73102 synced till here 73094
2014-07-14 01:46:33,249 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327591031 with entries=87, filesize=71.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327593065
2014-07-14 01:46:34,657 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:46:34,802 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73235 synced till here 73217
2014-07-14 01:46:34,957 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327593065 with entries=133, filesize=113.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327594658
2014-07-14 01:46:36,416 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:46:36,508 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73346 synced till here 73331
2014-07-14 01:46:36,621 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327594658 with entries=111, filesize=94.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327596417
2014-07-14 01:46:37,775 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=18219, memsize=1.0g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/5f9df6c9e3314a749fc08c203b99320c
2014-07-14 01:46:37,877 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:46:37,890 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/5f9df6c9e3314a749fc08c203b99320c as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/5f9df6c9e3314a749fc08c203b99320c
2014-07-14 01:46:37,930 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/5f9df6c9e3314a749fc08c203b99320c, entries=3863150, sequenceid=18219, filesize=275.1m
2014-07-14 01:46:37,945 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.5g/1616036240, currentsize=245.3m/257236720 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 43676ms, sequenceid=18219, compaction requested=true
2014-07-14 01:46:37,948 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 39 store files, 0 compacting, 39 eligible, 20 blocking
2014-07-14 01:46:37,948 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 39 files from compaction candidates
2014-07-14 01:46:37,948 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:46:37,948 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:46:37,948 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:46:37,949 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:46:37,978 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73437 synced till here 73422
2014-07-14 01:46:38,102 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327596417 with entries=91, filesize=78.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327597877
2014-07-14 01:46:38,102 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327363162
2014-07-14 01:46:38,102 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327369164
2014-07-14 01:46:38,102 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327370099
2014-07-14 01:46:38,102 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327404244
2014-07-14 01:46:38,102 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327405716
2014-07-14 01:46:38,102 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327407086
2014-07-14 01:46:38,102 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327416814
2014-07-14 01:46:38,103 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327424798
2014-07-14 01:46:38,103 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327427154
2014-07-14 01:46:38,103 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327428511
2014-07-14 01:46:38,103 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327429935
2014-07-14 01:46:38,103 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327431291
2014-07-14 01:46:38,103 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327435285
2014-07-14 01:46:38,103 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327437681
2014-07-14 01:46:38,103 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327439765
2014-07-14 01:46:38,103 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327441440
2014-07-14 01:46:38,103 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327442942
2014-07-14 01:46:38,103 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327444350
2014-07-14 01:46:38,103 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327446602
2014-07-14 01:46:38,103 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327448866
2014-07-14 01:46:38,103 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327450487
2014-07-14 01:46:38,103 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327473882
2014-07-14 01:46:38,103 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327475112
2014-07-14 01:46:38,104 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327476164
2014-07-14 01:46:38,104 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327477549
2014-07-14 01:46:38,104 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327478895
2014-07-14 01:46:38,104 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327480224
2014-07-14 01:46:38,104 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327481194
2014-07-14 01:46:38,104 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327482684
2014-07-14 01:46:38,104 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327485111
2014-07-14 01:46:38,104 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327489670
2014-07-14 01:46:38,157 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327491378
2014-07-14 01:46:38,157 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327493334
2014-07-14 01:46:38,157 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327494450
2014-07-14 01:46:38,526 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:46:38,527 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files; delaying flush up to 90000ms
2014-07-14 01:46:38,528 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 38 store files, 0 compacting, 38 eligible, 20 blocking
2014-07-14 01:46:38,528 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:46:38,528 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 38 files from compaction candidates
2014-07-14 01:46:38,528 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:46:38,528 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:46:38,528 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:46:38,529 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files; delaying flush up to 90000ms
2014-07-14 01:46:38,536 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:46:38,537 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 39 store files, 0 compacting, 39 eligible, 20 blocking
2014-07-14 01:46:38,537 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 39 files from compaction candidates
2014-07-14 01:46:38,537 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:46:38,537 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:46:38,537 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:46:38,537 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:46:38,998 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:46:39,027 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73536 synced till here 73521
2014-07-14 01:46:39,290 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327597877 with entries=99, filesize=84.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327598998
2014-07-14 01:46:42,478 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:46:42,499 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327598998 with entries=71, filesize=60.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327602479
2014-07-14 01:46:45,860 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:46:45,893 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327602479 with entries=73, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327605861
2014-07-14 01:46:46,873 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:46:46,899 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73752 synced till here 73751
2014-07-14 01:46:46,919 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327605861 with entries=72, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327606873
2014-07-14 01:46:48,416 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:46:48,477 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73827 synced till here 73825
2014-07-14 01:46:48,512 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327606873 with entries=75, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327608417
2014-07-14 01:46:49,715 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:46:49,749 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73905 synced till here 73900
2014-07-14 01:46:50,113 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327608417 with entries=78, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327609715
2014-07-14 01:46:51,467 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:46:51,749 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73998 synced till here 73992
2014-07-14 01:46:51,782 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327609715 with entries=93, filesize=79.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327611468
2014-07-14 01:46:53,275 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:46:53,305 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 74077 synced till here 74071
2014-07-14 01:46:53,365 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327611468 with entries=79, filesize=67.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327613275
2014-07-14 01:46:54,229 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:46:54,255 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 74156 synced till here 74153
2014-07-14 01:46:54,620 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327613275 with entries=79, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327614229
2014-07-14 01:46:55,337 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:46:55,619 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 74245 synced till here 74242
2014-07-14 01:46:55,667 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327614229 with entries=89, filesize=75.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327615337
2014-07-14 01:46:57,569 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:46:57,637 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 74320 synced till here 74319
2014-07-14 01:46:58,113 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327615337 with entries=75, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327617570
2014-07-14 01:46:59,831 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:46:59,861 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327617570 with entries=71, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327619831
2014-07-14 01:47:01,346 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:47:01,373 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 74467 synced till here 74465
2014-07-14 01:47:01,414 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327619831 with entries=76, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327621346
2014-07-14 01:47:01,415 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:47:02,430 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:47:02,465 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327621346 with entries=72, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327622431
2014-07-14 01:47:02,465 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:47:03,537 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:47:03,537 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files, but is 1.2g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:47:03,538 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. due to global heap pressure
2014-07-14 01:47:03,538 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 1.2g
2014-07-14 01:47:03,683 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:47:03,909 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:47:03,910 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files, but is 1.2g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:47:03,910 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. due to global heap pressure
2014-07-14 01:47:03,910 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 1.2g
2014-07-14 01:47:03,997 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 74629 synced till here 74627
2014-07-14 01:47:04,057 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327622431 with entries=90, filesize=77.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327623683
2014-07-14 01:47:05,109 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:47:05,312 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:47:05,521 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:47:05,543 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 74704 synced till here 74703
2014-07-14 01:47:05,556 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327623683 with entries=75, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327625521
2014-07-14 01:47:05,873 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:05,884 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:05,896 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:05,912 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:05,912 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:05,915 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:06,046 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:06,097 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:06,139 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:06,184 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:06,215 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:06,251 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:06,282 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:06,759 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:06,767 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:06,781 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:06,810 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:06,842 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:07,224 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:07,283 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:07,341 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:07,400 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:07,460 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:07,622 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:07,703 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:08,031 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:08,058 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:08,097 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:08,152 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:08,187 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:08,219 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:08,262 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:08,295 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:08,332 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:08,368 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:08,404 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:08,444 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:08,488 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:08,525 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:08,557 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:10,151 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:10,184 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:10,213 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:10,246 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:10,280 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:10,311 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:10,341 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:10,374 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:10,409 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:10,440 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:10,874 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:47:10,884 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:47:10,897 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:47:10,912 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:47:10,913 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:47:10,916 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:47:11,047 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:47:11,098 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:47:11,139 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:47:11,185 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:47:11,216 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:47:11,251 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:47:11,282 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:47:11,760 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:47:11,768 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:47:11,781 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:47:11,810 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:47:11,843 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:47:12,224 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:47:13,332 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5037ms
2014-07-14 01:47:13,332 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5301ms
2014-07-14 01:47:13,333 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6050ms
2014-07-14 01:47:13,333 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5992ms
2014-07-14 01:47:13,334 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5933ms
2014-07-14 01:47:13,334 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5874ms
2014-07-14 01:47:13,334 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5712ms
2014-07-14 01:47:13,335 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5631ms
2014-07-14 01:47:13,335 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5003ms
2014-07-14 01:47:13,335 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5277ms
2014-07-14 01:47:13,335 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5238ms
2014-07-14 01:47:13,335 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5148ms
2014-07-14 01:47:13,336 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5183ms
2014-07-14 01:47:13,336 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5117ms
2014-07-14 01:47:13,337 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5074ms
2014-07-14 01:47:13,368 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:47:13,404 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:47:13,444 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:47:13,488 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:47:13,525 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:47:13,558 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:47:15,151 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:47:15,184 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:47:15,214 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:47:15,246 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:47:15,280 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:47:15,311 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:47:15,341 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:47:15,374 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:47:15,410 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:47:15,441 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:47:15,874 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:47:15,884 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:47:15,897 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:47:15,913 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:47:15,913 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:47:15,916 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:47:16,048 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:47:16,098 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:47:16,140 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:47:16,185 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:47:16,216 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:47:16,251 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:47:16,282 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:47:16,760 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:47:16,768 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:47:16,782 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:47:16,810 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:47:16,843 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:47:17,225 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:47:18,332 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10037ms
2014-07-14 01:47:18,334 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10993ms
2014-07-14 01:47:18,334 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11051ms
2014-07-14 01:47:18,334 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10303ms
2014-07-14 01:47:18,334 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10934ms
2014-07-14 01:47:18,335 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10874ms
2014-07-14 01:47:18,335 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10713ms
2014-07-14 01:47:18,335 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10632ms
2014-07-14 01:47:18,335 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-14 01:47:18,335 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10277ms
2014-07-14 01:47:18,336 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10239ms
2014-07-14 01:47:18,336 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10149ms
2014-07-14 01:47:18,336 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10184ms
2014-07-14 01:47:18,337 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10117ms
2014-07-14 01:47:18,337 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10075ms
2014-07-14 01:47:18,368 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:47:18,405 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:47:18,445 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:47:18,489 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:47:18,528 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:47:18,558 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:47:20,152 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:47:20,184 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:47:20,215 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:47:20,247 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:47:20,281 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:47:20,312 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:47:20,341 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:47:20,375 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:47:20,410 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:47:20,441 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:47:20,874 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:47:20,884 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 01:47:20,897 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:47:20,913 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:47:20,914 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:47:20,916 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:47:21,048 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:47:21,099 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:47:21,140 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:47:21,185 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:47:21,217 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:47:21,251 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 01:47:21,283 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:47:21,761 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:47:21,768 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:47:21,783 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:47:21,810 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:47:21,844 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:47:22,225 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:47:22,959 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=18699, memsize=417.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/eb0248ea5a9745e4abd7eb8cc86be87e
2014-07-14 01:47:22,997 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/eb0248ea5a9745e4abd7eb8cc86be87e as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/eb0248ea5a9745e4abd7eb8cc86be87e
2014-07-14 01:47:23,018 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/eb0248ea5a9745e4abd7eb8cc86be87e, entries=1521120, sequenceid=18699, filesize=108.3m
2014-07-14 01:47:23,019 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.2g/1240750560, currentsize=41.2m/43167280 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 19109ms, sequenceid=18699, compaction requested=true
2014-07-14 01:47:23,019 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:47:23,019 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 45 store files, 0 compacting, 45 eligible, 20 blocking
2014-07-14 01:47:23,020 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15797ms
2014-07-14 01:47:23,020 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,020 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 45 files from compaction candidates
2014-07-14 01:47:23,020 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 101475ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:47:23,020 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:47:23,020 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:47:23,020 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:47:23,020 DEBUG [MemStoreFlusher.1] regionserver.HRegion: NOT flushing memstore for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., flushing=true, writesEnabled=true
2014-07-14 01:47:23,021 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16179ms
2014-07-14 01:47:23,021 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,021 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16212ms
2014-07-14 01:47:23,021 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,021 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16240ms
2014-07-14 01:47:23,021 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,025 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16258ms
2014-07-14 01:47:23,025 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,025 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16266ms
2014-07-14 01:47:23,025 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,025 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16743ms
2014-07-14 01:47:23,025 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,029 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16778ms
2014-07-14 01:47:23,029 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,029 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16814ms
2014-07-14 01:47:23,029 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,029 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16845ms
2014-07-14 01:47:23,029 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,036 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16897ms
2014-07-14 01:47:23,036 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,038 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16941ms
2014-07-14 01:47:23,038 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,038 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16992ms
2014-07-14 01:47:23,038 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,038 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17123ms
2014-07-14 01:47:23,038 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,038 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17126ms
2014-07-14 01:47:23,039 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,039 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17127ms
2014-07-14 01:47:23,039 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,039 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17143ms
2014-07-14 01:47:23,039 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,039 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17155ms
2014-07-14 01:47:23,039 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,049 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17176ms
2014-07-14 01:47:23,049 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,049 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12609ms
2014-07-14 01:47:23,049 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,053 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12644ms
2014-07-14 01:47:23,053 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,053 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12679ms
2014-07-14 01:47:23,053 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,053 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12712ms
2014-07-14 01:47:23,053 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,058 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12747ms
2014-07-14 01:47:23,058 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,058 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12778ms
2014-07-14 01:47:23,058 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,058 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12812ms
2014-07-14 01:47:23,058 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,058 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12845ms
2014-07-14 01:47:23,058 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,060 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12877ms
2014-07-14 01:47:23,060 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,060 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12909ms
2014-07-14 01:47:23,060 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,063 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14506ms
2014-07-14 01:47:23,063 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,063 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14538ms
2014-07-14 01:47:23,063 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,063 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14575ms
2014-07-14 01:47:23,063 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,064 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14620ms
2014-07-14 01:47:23,064 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,066 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14661ms
2014-07-14 01:47:23,066 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,067 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14699ms
2014-07-14 01:47:23,067 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,068 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14805ms
2014-07-14 01:47:23,068 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,068 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14849ms
2014-07-14 01:47:23,068 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,072 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14920ms
2014-07-14 01:47:23,072 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,072 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14885ms
2014-07-14 01:47:23,072 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,072 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14975ms
2014-07-14 01:47:23,073 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,073 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15015ms
2014-07-14 01:47:23,074 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,074 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14742ms
2014-07-14 01:47:23,074 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,074 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15371ms
2014-07-14 01:47:23,074 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,074 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15452ms
2014-07-14 01:47:23,074 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,075 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15615ms
2014-07-14 01:47:23,075 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,075 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15675ms
2014-07-14 01:47:23,075 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,075 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15044ms
2014-07-14 01:47:23,075 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,079 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15796ms
2014-07-14 01:47:23,079 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,079 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15738ms
2014-07-14 01:47:23,079 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,080 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14785ms
2014-07-14 01:47:23,080 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:47:23,192 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17475,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327625717,"queuetimems":0,"class":"HRegionServer","responsesize":16061,"method":"Multi"}
2014-07-14 01:47:23,323 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17655,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327625667,"queuetimems":0,"class":"HRegionServer","responsesize":15712,"method":"Multi"}
2014-07-14 01:47:24,532 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:47:24,536 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18769,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327625766,"queuetimems":1,"class":"HRegionServer","responsesize":15789,"method":"Multi"}
2014-07-14 01:47:24,536 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18726,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327625809,"queuetimems":0,"class":"HRegionServer","responsesize":15553,"method":"Multi"}
2014-07-14 01:47:24,573 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 74819 synced till here 74791
2014-07-14 01:47:24,616 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=18727, memsize=411.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/1e354535e5014af9927d369fb2d7b984
2014-07-14 01:47:24,631 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/1e354535e5014af9927d369fb2d7b984 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/1e354535e5014af9927d369fb2d7b984
2014-07-14 01:47:24,643 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/1e354535e5014af9927d369fb2d7b984, entries=1499600, sequenceid=18727, filesize=106.8m
2014-07-14 01:47:24,644 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.2g/1305912480, currentsize=117.7m/123381760 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 21106ms, sequenceid=18727, compaction requested=true
2014-07-14 01:47:24,644 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:47:24,644 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 45 store files, 0 compacting, 45 eligible, 20 blocking
2014-07-14 01:47:24,644 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 45 files from compaction candidates
2014-07-14 01:47:24,644 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:47:24,644 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:47:24,645 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:47:24,803 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18950,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327625853,"queuetimems":0,"class":"HRegionServer","responsesize":15812,"method":"Multi"}
2014-07-14 01:47:24,825 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327625521 with entries=115, filesize=95.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327644533
2014-07-14 01:47:24,825 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327496108
2014-07-14 01:47:24,825 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327531731
2014-07-14 01:47:24,825 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327533444
2014-07-14 01:47:24,825 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327535241
2014-07-14 01:47:24,826 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327536904
2014-07-14 01:47:24,826 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327538366
2014-07-14 01:47:24,826 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327539431
2014-07-14 01:47:24,826 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327541419
2014-07-14 01:47:24,826 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327543074
2014-07-14 01:47:24,826 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327545783
2014-07-14 01:47:24,826 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327548024
2014-07-14 01:47:24,826 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327550030
2014-07-14 01:47:24,826 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327551914
2014-07-14 01:47:24,971 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17748,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327627222,"queuetimems":1,"class":"HRegionServer","responsesize":16184,"method":"Multi"}
2014-07-14 01:47:25,084 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18276,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327626808,"queuetimems":0,"class":"HRegionServer","responsesize":15624,"method":"Multi"}
2014-07-14 01:47:25,506 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:47:25,533 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18691,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327626841,"queuetimems":1,"class":"HRegionServer","responsesize":16061,"method":"Multi"}
2014-07-14 01:47:25,599 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 74897 synced till here 74892
2014-07-14 01:47:25,618 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17595,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327628022,"queuetimems":1,"class":"HRegionServer","responsesize":15812,"method":"Multi"}
2014-07-14 01:47:25,657 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327644533 with entries=78, filesize=66.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327645506
2014-07-14 01:47:26,833 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19132,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327627701,"queuetimems":0,"class":"HRegionServer","responsesize":15685,"method":"Multi"}
2014-07-14 01:47:26,834 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20055,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327626779,"queuetimems":0,"class":"HRegionServer","responsesize":15893,"method":"Multi"}
2014-07-14 01:47:26,834 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18349,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327628485,"queuetimems":0,"class":"HRegionServer","responsesize":16061,"method":"Multi"}
2014-07-14 01:47:26,834 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16495,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327630339,"queuetimems":1,"class":"HRegionServer","responsesize":15913,"method":"Multi"}
2014-07-14 01:47:26,833 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18314,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327628519,"queuetimems":0,"class":"HRegionServer","responsesize":15712,"method":"Multi"}
2014-07-14 01:47:26,835 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18579,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327628255,"queuetimems":0,"class":"HRegionServer","responsesize":15789,"method":"Multi"}
2014-07-14 01:47:26,833 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16622,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327630211,"queuetimems":0,"class":"HRegionServer","responsesize":15674,"method":"Multi"}
2014-07-14 01:47:26,844 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16663,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327630180,"queuetimems":0,"class":"HRegionServer","responsesize":15797,"method":"Multi"}
2014-07-14 01:47:26,844 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20087,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327626757,"queuetimems":1,"class":"HRegionServer","responsesize":16087,"method":"Multi"}
2014-07-14 01:47:26,834 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16686,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327630148,"queuetimems":1,"class":"HRegionServer","responsesize":15778,"method":"Multi"}
2014-07-14 01:47:26,853 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18798,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327628055,"queuetimems":0,"class":"HRegionServer","responsesize":15572,"method":"Multi"}
2014-07-14 01:47:26,834 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20925,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327625909,"queuetimems":0,"class":"HRegionServer","responsesize":15860,"method":"Multi"}
2014-07-14 01:47:26,834 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19437,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327627397,"queuetimems":0,"class":"HRegionServer","responsesize":15556,"method":"Multi"}
2014-07-14 01:47:26,869 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20774,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327626095,"queuetimems":0,"class":"HRegionServer","responsesize":16325,"method":"Multi"}
2014-07-14 01:47:26,870 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18654,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327628215,"queuetimems":0,"class":"HRegionServer","responsesize":15553,"method":"Multi"}
2014-07-14 01:47:26,834 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20554,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327626280,"queuetimems":0,"class":"HRegionServer","responsesize":15797,"method":"Multi"}
2014-07-14 01:47:26,853 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20716,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327626137,"queuetimems":1,"class":"HRegionServer","responsesize":15694,"method":"Multi"}
2014-07-14 01:47:26,844 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16535,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327630308,"queuetimems":0,"class":"HRegionServer","responsesize":16184,"method":"Multi"}
2014-07-14 01:47:27,027 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16752,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327630274,"queuetimems":0,"class":"HRegionServer","responsesize":15772,"method":"Multi"}
2014-07-14 01:47:27,027 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16655,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327630371,"queuetimems":0,"class":"HRegionServer","responsesize":16061,"method":"Multi"}
2014-07-14 01:47:27,027 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16783,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327630243,"queuetimems":0,"class":"HRegionServer","responsesize":15561,"method":"Multi"}
2014-07-14 01:47:27,027 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20845,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327626182,"queuetimems":0,"class":"HRegionServer","responsesize":15616,"method":"Multi"}
2014-07-14 01:47:27,034 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20988,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327626045,"queuetimems":1,"class":"HRegionServer","responsesize":15572,"method":"Multi"}
2014-07-14 01:47:27,034 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18705,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327628329,"queuetimems":0,"class":"HRegionServer","responsesize":16087,"method":"Multi"}
2014-07-14 01:47:27,039 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18748,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327628291,"queuetimems":0,"class":"HRegionServer","responsesize":15624,"method":"Multi"}
2014-07-14 01:47:27,039 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18857,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327628182,"queuetimems":0,"class":"HRegionServer","responsesize":15797,"method":"Multi"}
2014-07-14 01:47:27,039 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19699,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327627340,"queuetimems":0,"class":"HRegionServer","responsesize":15674,"method":"Multi"}
2014-07-14 01:47:27,049 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18901,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327628148,"queuetimems":0,"class":"HRegionServer","responsesize":16325,"method":"Multi"}
2014-07-14 01:47:27,061 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19781,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327627280,"queuetimems":0,"class":"HRegionServer","responsesize":15913,"method":"Multi"}
2014-07-14 01:47:27,061 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16623,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327630438,"queuetimems":0,"class":"HRegionServer","responsesize":15685,"method":"Multi"}
2014-07-14 01:47:27,069 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16663,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327630406,"queuetimems":0,"class":"HRegionServer","responsesize":15556,"method":"Multi"}
2014-07-14 01:47:27,081 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19626,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327627455,"queuetimems":0,"class":"HRegionServer","responsesize":15778,"method":"Multi"}
2014-07-14 01:47:27,287 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:47:27,289 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18734,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327628555,"queuetimems":1,"class":"HRegionServer","responsesize":16053,"method":"Multi"}
2014-07-14 01:47:27,289 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20523,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327626766,"queuetimems":0,"class":"HRegionServer","responsesize":16053,"method":"Multi"}
2014-07-14 01:47:27,289 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19194,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327628095,"queuetimems":0,"class":"HRegionServer","responsesize":15860,"method":"Multi"}
2014-07-14 01:47:27,522 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 75025 synced till here 74975
2014-07-14 01:47:27,566 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19201,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327628365,"queuetimems":0,"class":"HRegionServer","responsesize":15694,"method":"Multi"}
2014-07-14 01:47:27,566 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21352,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327626214,"queuetimems":0,"class":"HRegionServer","responsesize":15797,"method":"Multi"}
2014-07-14 01:47:27,593 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21344,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327626249,"queuetimems":1,"class":"HRegionServer","responsesize":15561,"method":"Multi"}
2014-07-14 01:47:27,598 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19197,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327628400,"queuetimems":0,"class":"HRegionServer","responsesize":15616,"method":"Multi"}
2014-07-14 01:47:27,601 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19983,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327627618,"queuetimems":0,"class":"HRegionServer","responsesize":15772,"method":"Multi"}
2014-07-14 01:47:27,609 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19168,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327628441,"queuetimems":1,"class":"HRegionServer","responsesize":15893,"method":"Multi"}
2014-07-14 01:47:27,840 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327645506 with entries=128, filesize=106.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327647288
2014-07-14 01:47:29,323 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:47:29,368 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 75129 synced till here 75097
2014-07-14 01:47:31,055 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327647288 with entries=104, filesize=88.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327649323
2014-07-14 01:47:31,807 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:47:32,028 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 75235 synced till here 75201
2014-07-14 01:47:32,101 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:47:32,108 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327649323 with entries=106, filesize=91.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327651807
2014-07-14 01:47:32,109 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. has too many store files; delaying flush up to 90000ms
2014-07-14 01:47:32,110 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 45 store files, 0 compacting, 45 eligible, 20 blocking
2014-07-14 01:47:32,111 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 45 files from compaction candidates
2014-07-14 01:47:32,111 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:47:32,111 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:47:32,111 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:47:32,117 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:47:32,656 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003.
2014-07-14 01:47:32,656 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files; delaying flush up to 90000ms
2014-07-14 01:47:32,657 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 45 store files, 0 compacting, 45 eligible, 20 blocking
2014-07-14 01:47:32,657 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 45 files from compaction candidates
2014-07-14 01:47:32,657 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:47:32,657 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:47:32,657 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
2014-07-14 01:47:32,657 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:47:33,447 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:47:33,460 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 75341 synced till here 75312
2014-07-14 01:47:33,850 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327651807 with entries=106, filesize=90.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327653447
2014-07-14 01:47:34,651 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:47:34,964 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327653447 with entries=91, filesize=78.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327654652
2014-07-14 01:47:36,890 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:47:36,918 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 75508 synced till here 75503
2014-07-14 01:47:36,985 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327654652 with entries=76, filesize=65.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327656891
2014-07-14 01:47:38,335 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:47:38,353 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 75584 synced till here 75579
2014-07-14 01:47:38,404 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327656891 with entries=76, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327658335
2014-07-14 01:47:39,701 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:47:39,737 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327658335 with entries=71, filesize=60.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327659701
2014-07-14 01:47:39,737 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 2 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72, b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:47:42,074 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:47:42,093 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 75733 synced till here 75729
2014-07-14 01:47:42,149 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327659701 with entries=78, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327662074
2014-07-14 01:47:42,150 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 2 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72, b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:47:43,241 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:47:43,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 75805 synced till here 75804
2014-07-14 01:47:43,294 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327662074 with entries=72, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327663242
2014-07-14 01:47:43,295 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 2 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72, b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:47:44,784 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:47:44,801 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 75880 synced till here 75879
2014-07-14 01:47:44,823 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327663242 with entries=75, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327664784
2014-07-14 01:47:44,824 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 2 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72, b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:47:46,262 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:47:46,279 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 75957 synced till here 75955
2014-07-14 01:47:46,311 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327664784 with entries=77, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327666262
2014-07-14 01:47:46,312 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 2 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72, b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:47:47,767 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:47:47,794 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327666262 with entries=72, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327667767
2014-07-14 01:47:47,795 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 2 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72, b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:47:49,109 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:47:49,751 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 76127 synced till here 76125
2014-07-14 01:47:49,780 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327667767 with entries=98, filesize=83.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327669110
2014-07-14 01:47:49,780 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 2 regions(s): d5d5cc6db1bf5bd9142b4d4e667bac72, b5c4e1ff5b6b7753b9fecbe9b697bf45
2014-07-14 01:47:50,388 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:47:50,389 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files, but is 1.3g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:47:50,389 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. due to global heap pressure
2014-07-14 01:47:50,389 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 1.3g
2014-07-14 01:47:50,662 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:47:50,662 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files, but is 1.3g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:47:50,662 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. due to global heap pressure
2014-07-14 01:47:50,662 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45., current region memstore size 1.3g
2014-07-14 01:47:50,747 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:47:50,767 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 76202 synced till here 76201
2014-07-14 01:47:50,781 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327669110 with entries=75, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327670747
2014-07-14 01:47:51,337 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:47:51,541 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:47:53,101 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:47:53,121 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 76275 synced till here 76274
2014-07-14 01:47:53,145 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327670747 with entries=73, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327673101
2014-07-14 01:47:53,582 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:53,613 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:53,617 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:53,629 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:54,117 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:54,245 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:54,421 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:54,474 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:54,523 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:54,559 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:54,775 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:54,859 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:54,912 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:54,959 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:55,001 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:55,047 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:55,163 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:55,257 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:55,325 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:55,985 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:56,025 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:56,065 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:56,127 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:56,191 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:56,249 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:56,294 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:57,735 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:57,775 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:57,825 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:57,868 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:57,907 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:57,955 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:57,989 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:58,029 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:58,071 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:58,122 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:58,183 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:58,224 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:58,255 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:58,299 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:58,336 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:58,369 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:47:58,582 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:47:58,614 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:47:58,617 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:47:58,630 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:47:59,118 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:47:59,245 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:47:59,421 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:47:59,475 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:47:59,523 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:47:59,560 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:48:00,496 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5171ms
2014-07-14 01:48:00,496 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5721ms
2014-07-14 01:48:00,497 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5638ms
2014-07-14 01:48:00,497 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5585ms
2014-07-14 01:48:00,497 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5538ms
2014-07-14 01:48:00,498 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5497ms
2014-07-14 01:48:00,498 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5451ms
2014-07-14 01:48:00,498 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5335ms
2014-07-14 01:48:00,499 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5241ms
2014-07-14 01:48:00,509 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:48:00,519 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:48:00,547 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:48:00,576 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:48:00,604 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:48:00,633 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:48:00,670 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:48:00,986 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:48:01,026 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:48:01,065 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:48:01,127 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:48:01,191 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:48:01,249 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:48:01,294 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:48:01,849 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405324583256: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-14 01:48:02,735 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:48:02,775 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:48:02,825 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:48:02,868 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:48:02,907 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:48:02,955 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:48:02,989 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:48:03,029 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:48:03,072 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:48:03,123 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:48:03,183 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:48:03,224 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:48:03,256 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:48:03,299 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:48:03,336 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:48:03,369 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:48:03,583 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:48:03,614 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:48:03,618 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:48:03,630 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:48:04,119 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:48:04,246 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:48:04,421 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:48:04,475 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:48:04,524 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:48:04,560 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:48:05,497 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10172ms
2014-07-14 01:48:05,497 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10638ms
2014-07-14 01:48:05,498 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10722ms
2014-07-14 01:48:05,498 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10586ms
2014-07-14 01:48:05,498 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10539ms
2014-07-14 01:48:05,498 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10497ms
2014-07-14 01:48:05,499 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10451ms
2014-07-14 01:48:05,499 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10242ms
2014-07-14 01:48:05,500 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10336ms
2014-07-14 01:48:05,509 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:48:05,519 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:48:05,547 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:48:05,576 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:48:05,604 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:48:05,634 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-14 01:48:05,671 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-14 01:48:05,986 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:48:06,027 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:48:06,065 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:48:06,128 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:48:06,191 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:48:06,250 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-14 01:48:06,295 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:48:08,011 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10022ms
2014-07-14 01:48:08,012 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1013ms
GC pool 'ParNew' had collection(s): count=1 time=1160ms
2014-07-14 01:48:08,012 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6163ms
2014-07-14 01:48:08,012 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10277ms
2014-07-14 01:48:08,013 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10238ms
2014-07-14 01:48:08,013 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10188ms
2014-07-14 01:48:08,013 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10145ms
2014-07-14 01:48:08,014 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10106ms
2014-07-14 01:48:08,014 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10059ms
2014-07-14 01:48:08,030 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:48:08,072 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:48:08,123 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:48:08,183 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:48:08,224 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:48:08,256 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:48:08,300 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:48:08,336 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:48:08,370 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:48:08,583 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:48:08,614 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:48:08,618 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:48:08,630 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:48:09,119 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:48:09,246 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:48:09,421 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 01:48:09,475 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:48:09,524 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:48:09,560 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:48:10,497 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15172ms
2014-07-14 01:48:10,498 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15586ms
2014-07-14 01:48:10,498 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15723ms
2014-07-14 01:48:10,498 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15639ms
2014-07-14 01:48:10,499 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15540ms
2014-07-14 01:48:10,499 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15452ms
2014-07-14 01:48:10,500 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15498ms
2014-07-14 01:48:10,500 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15243ms
2014-07-14 01:48:10,500 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15337ms
2014-07-14 01:48:10,509 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:48:10,519 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:48:10,547 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:48:10,577 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:48:10,604 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-14 01:48:10,634 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:48:10,671 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-14 01:48:10,987 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:48:11,027 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-14 01:48:11,066 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:48:11,128 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:48:11,192 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:48:11,252 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15004ms
2014-07-14 01:48:11,295 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:48:13,012 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15023ms
2014-07-14 01:48:13,013 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15278ms
2014-07-14 01:48:13,013 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11164ms
2014-07-14 01:48:13,013 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15239ms
2014-07-14 01:48:13,014 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15188ms
2014-07-14 01:48:13,014 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15059ms
2014-07-14 01:48:13,014 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15107ms
2014-07-14 01:48:13,015 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15146ms
2014-07-14 01:48:13,030 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:48:13,072 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:48:13,123 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:48:13,183 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:48:13,224 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-14 01:48:13,256 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:48:13,300 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:48:13,337 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:48:13,370 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-14 01:48:13,583 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:48:13,614 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:48:13,618 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:48:13,630 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:48:14,120 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20003ms
2014-07-14 01:48:14,246 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:48:14,422 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:48:14,475 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:48:14,524 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:48:14,560 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:48:15,799 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15129ms
2014-07-14 01:48:15,800 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20753ms
2014-07-14 01:48:15,800 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20475ms
2014-07-14 01:48:15,800 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20799ms
2014-07-14 01:48:15,800 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20543ms
2014-07-14 01:48:15,801 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20637ms
2014-07-14 01:48:15,801 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15293ms
2014-07-14 01:48:15,801 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15283ms
2014-07-14 01:48:15,801 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15254ms
2014-07-14 01:48:15,801 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15225ms
2014-07-14 01:48:15,801 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15197ms
2014-07-14 01:48:15,802 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15169ms
2014-07-14 01:48:15,802 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20890ms
2014-07-14 01:48:15,802 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21027ms
2014-07-14 01:48:15,802 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20943ms
2014-07-14 01:48:15,802 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20843ms
2014-07-14 01:48:15,987 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:48:16,027 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:48:16,066 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:48:16,128 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:48:16,192 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:48:16,252 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20004ms
2014-07-14 01:48:16,295 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:48:18,013 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20024ms
2014-07-14 01:48:18,013 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16164ms
2014-07-14 01:48:18,013 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20278ms
2014-07-14 01:48:18,014 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20239ms
2014-07-14 01:48:18,014 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20189ms
2014-07-14 01:48:18,014 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20059ms
2014-07-14 01:48:18,014 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20107ms
2014-07-14 01:48:18,015 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20147ms
2014-07-14 01:48:18,030 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:48:18,073 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:48:18,124 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-14 01:48:18,184 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:48:18,225 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:48:18,256 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:48:18,300 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:48:18,337 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:48:18,370 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-14 01:48:18,584 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:48:18,990 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25361ms
2014-07-14 01:48:18,990 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25373ms
2014-07-14 01:48:18,990 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25377ms
2014-07-14 01:48:19,120 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25003ms
2014-07-14 01:48:19,247 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:48:19,422 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:48:19,476 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:48:19,524 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:48:19,560 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:48:20,800 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20130ms
2014-07-14 01:48:20,801 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25799ms
2014-07-14 01:48:20,801 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25476ms
2014-07-14 01:48:20,801 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25754ms
2014-07-14 01:48:20,801 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20293ms
2014-07-14 01:48:20,801 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25638ms
2014-07-14 01:48:20,801 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25544ms
2014-07-14 01:48:20,802 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20226ms
2014-07-14 01:48:20,802 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20255ms
2014-07-14 01:48:20,802 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20284ms
2014-07-14 01:48:20,802 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26027ms
2014-07-14 01:48:20,803 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25890ms
2014-07-14 01:48:20,803 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20170ms
2014-07-14 01:48:20,803 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20199ms
2014-07-14 01:48:20,803 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25844ms
2014-07-14 01:48:20,803 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25944ms
2014-07-14 01:48:20,987 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:48:21,028 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:48:21,066 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-14 01:48:21,129 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:48:21,193 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:48:21,253 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25005ms
2014-07-14 01:48:21,296 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-14 01:48:22,318 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=19070, memsize=730.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/2ed20dd26f5a452cb0c67d45083d51b5
2014-07-14 01:48:22,334 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/2ed20dd26f5a452cb0c67d45083d51b5 as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/2ed20dd26f5a452cb0c67d45083d51b5
2014-07-14 01:48:22,353 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/2ed20dd26f5a452cb0c67d45083d51b5, entries=2658040, sequenceid=19070, filesize=189.3m
2014-07-14 01:48:22,354 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.3g/1397519360, currentsize=50.2m/52667920 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 31964ms, sequenceid=19070, compaction requested=true
2014-07-14 01:48:22,354 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:48:22,354 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 39 store files, 0 compacting, 39 eligible, 20 blocking
2014-07-14 01:48:22,354 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 39 files from compaction candidates
2014-07-14 01:48:22,354 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26060ms
2014-07-14 01:48:22,354 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:48:22,354 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,354 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 103828ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:48:22,355 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26107ms
2014-07-14 01:48:22,355 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,355 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:48:22,355 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26164ms
2014-07-14 01:48:22,355 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,355 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:48:22,355 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72., current region memstore size 50.2m
2014-07-14 01:48:22,357 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26230ms
2014-07-14 01:48:22,357 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,357 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26292ms
2014-07-14 01:48:22,357 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,358 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26333ms
2014-07-14 01:48:22,358 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,358 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26373ms
2014-07-14 01:48:22,358 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,362 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27503ms
2014-07-14 01:48:22,362 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,365 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27406ms
2014-07-14 01:48:22,365 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,365 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21761ms
2014-07-14 01:48:22,365 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,365 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21732ms
2014-07-14 01:48:22,365 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,365 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27453ms
2014-07-14 01:48:22,366 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,366 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27591ms
2014-07-14 01:48:22,366 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,368 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21850ms
2014-07-14 01:48:22,368 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,368 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21821ms
2014-07-14 01:48:22,368 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,368 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21792ms
2014-07-14 01:48:22,368 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,368 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27111ms
2014-07-14 01:48:22,368 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,368 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27205ms
2014-07-14 01:48:22,368 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,370 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21861ms
2014-07-14 01:48:22,370 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,370 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27323ms
2014-07-14 01:48:22,370 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,370 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27045ms
2014-07-14 01:48:22,370 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,370 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27369ms
2014-07-14 01:48:22,370 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,371 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21701ms
2014-07-14 01:48:22,371 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,371 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27812ms
2014-07-14 01:48:22,371 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,371 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27848ms
2014-07-14 01:48:22,371 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,372 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27898ms
2014-07-14 01:48:22,373 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,373 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27952ms
2014-07-14 01:48:22,373 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,374 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28129ms
2014-07-14 01:48:22,374 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,376 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28259ms
2014-07-14 01:48:22,376 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,377 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28764ms
2014-07-14 01:48:22,377 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,380 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28763ms
2014-07-14 01:48:22,380 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,383 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28754ms
2014-07-14 01:48:22,383 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,386 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28804ms
2014-07-14 01:48:22,387 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,387 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24018ms
2014-07-14 01:48:22,388 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,388 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24052ms
2014-07-14 01:48:22,388 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,389 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24089ms
2014-07-14 01:48:22,389 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,390 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24135ms
2014-07-14 01:48:22,390 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,390 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24166ms
2014-07-14 01:48:22,390 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,390 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24208ms
2014-07-14 01:48:22,390 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,391 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24268ms
2014-07-14 01:48:22,391 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,394 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:48:22,397 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24326ms
2014-07-14 01:48:22,397 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,397 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24368ms
2014-07-14 01:48:22,397 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,397 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24529ms
2014-07-14 01:48:22,397 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,397 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24490ms
2014-07-14 01:48:22,398 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,398 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24443ms
2014-07-14 01:48:22,401 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,401 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24576ms
2014-07-14 01:48:22,401 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,401 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24627ms
2014-07-14 01:48:22,401 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,403 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24668ms
2014-07-14 01:48:22,403 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,403 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20554ms
2014-07-14 01:48:22,403 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,403 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24414ms
2014-07-14 01:48:22,403 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405324583256
2014-07-14 01:48:22,447 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29113,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327673333,"queuetimems":0,"class":"HRegionServer","responsesize":15953,"method":"Multi"}
2014-07-14 01:48:22,465 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29088,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327673376,"queuetimems":0,"class":"HRegionServer","responsesize":15616,"method":"Multi"}
2014-07-14 01:48:23,534 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30091,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327673443,"queuetimems":1,"class":"HRegionServer","responsesize":15779,"method":"Multi"}
2014-07-14 01:48:23,909 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:48:24,033 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 76392 synced till here 76364
2014-07-14 01:48:24,294 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327673101 with entries=117, filesize=94.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327703910
2014-07-14 01:48:24,298 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=19080, memsize=737.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/888ae84297c04aa1914c9a050b7accd3
2014-07-14 01:48:24,321 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/.tmp/888ae84297c04aa1914c9a050b7accd3 as hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/888ae84297c04aa1914c9a050b7accd3
2014-07-14 01:48:24,334 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5c4e1ff5b6b7753b9fecbe9b697bf45/family/888ae84297c04aa1914c9a050b7accd3, entries=2683850, sequenceid=19080, filesize=191.2m
2014-07-14 01:48:24,336 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.3g/1391937520, currentsize=50.1m/52501280 for region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. in 33674ms, sequenceid=19080, compaction requested=true
2014-07-14 01:48:24,336 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 40 store files, 0 compacting, 40 eligible, 20 blocking
2014-07-14 01:48:24,336 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 40 files from compaction candidates
2014-07-14 01:48:24,336 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:48:24,337 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:48:24,337 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:48:24,339 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:48:24,785 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28498,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327676287,"queuetimems":0,"class":"HRegionServer","responsesize":15616,"method":"Multi"}
2014-07-14 01:48:24,789 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28670,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327676119,"queuetimems":0,"class":"HRegionServer","responsesize":15465,"method":"Multi"}
2014-07-14 01:48:25,333 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=19104, memsize=50.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/6473e3d386d040a5b26e84ae8831cefd
2014-07-14 01:48:25,348 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/.tmp/6473e3d386d040a5b26e84ae8831cefd as hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/6473e3d386d040a5b26e84ae8831cefd
2014-07-14 01:48:25,360 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d5d5cc6db1bf5bd9142b4d4e667bac72/family/6473e3d386d040a5b26e84ae8831cefd, entries=182880, sequenceid=19104, filesize=13.0m
2014-07-14 01:48:25,360 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~50.2m/52667920, currentsize=7.8m/8144080 for region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. in 3005ms, sequenceid=19104, compaction requested=true
2014-07-14 01:48:25,361 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 40 store files, 0 compacting, 40 eligible, 20 blocking
2014-07-14 01:48:25,361 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 40 files from compaction candidates
2014-07-14 01:48:25,361 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:48:25,361 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:48:25,361 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:48:25,361 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:48:25,628 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:48:25,628 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29605,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327676023,"queuetimems":1,"class":"HRegionServer","responsesize":15790,"method":"Multi"}
2014-07-14 01:48:25,628 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30467,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327675161,"queuetimems":0,"class":"HRegionServer","responsesize":15666,"method":"Multi"}
2014-07-14 01:48:25,628 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31209,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327674419,"queuetimems":0,"class":"HRegionServer","responsesize":15722,"method":"Multi"}
2014-07-14 01:48:25,652 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 76500 synced till here 76485
2014-07-14 01:48:25,794 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29732,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327676062,"queuetimems":0,"class":"HRegionServer","responsesize":15552,"method":"Multi"}
2014-07-14 01:48:25,794 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25287,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327680507,"queuetimems":0,"class":"HRegionServer","responsesize":15835,"method":"Multi"}
2014-07-14 01:48:25,794 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28063,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327677731,"queuetimems":1,"class":"HRegionServer","responsesize":15741,"method":"Multi"}
2014-07-14 01:48:25,794 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30837,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327674957,"queuetimems":0,"class":"HRegionServer","responsesize":15484,"method":"Multi"}
2014-07-14 01:48:25,794 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31021,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327674773,"queuetimems":0,"class":"HRegionServer","responsesize":15741,"method":"Multi"}
2014-07-14 01:48:25,794 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30795,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327674999,"queuetimems":0,"class":"HRegionServer","responsesize":16020,"method":"Multi"}
2014-07-14 01:48:25,796 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29812,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327675983,"queuetimems":1,"class":"HRegionServer","responsesize":15953,"method":"Multi"}
2014-07-14 01:48:25,796 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30473,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327675323,"queuetimems":0,"class":"HRegionServer","responsesize":15785,"method":"Multi"}
2014-07-14 01:48:25,795 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30749,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327675046,"queuetimems":1,"class":"HRegionServer","responsesize":15694,"method":"Multi"}
2014-07-14 01:48:25,795 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25126,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327680669,"queuetimems":1,"class":"HRegionServer","responsesize":15552,"method":"Multi"}
2014-07-14 01:48:25,795 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31274,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327674521,"queuetimems":0,"class":"HRegionServer","responsesize":15843,"method":"Multi"}
2014-07-14 01:48:25,795 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31236,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327674558,"queuetimems":0,"class":"HRegionServer","responsesize":15777,"method":"Multi"}
2014-07-14 01:48:25,795 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32173,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327673622,"queuetimems":0,"class":"HRegionServer","responsesize":15988,"method":"Multi"}
2014-07-14 01:48:25,795 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29551,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327676243,"queuetimems":0,"class":"HRegionServer","responsesize":15779,"method":"Multi"}
2014-07-14 01:48:25,795 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30884,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327674910,"queuetimems":0,"class":"HRegionServer","responsesize":15708,"method":"Multi"}
2014-07-14 01:48:25,794 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25163,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327680631,"queuetimems":0,"class":"HRegionServer","responsesize":15790,"method":"Multi"}
2014-07-14 01:48:25,794 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28024,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327677770,"queuetimems":0,"class":"HRegionServer","responsesize":15788,"method":"Multi"}
2014-07-14 01:48:25,805 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27773,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327678024,"queuetimems":1,"class":"HRegionServer","responsesize":15708,"method":"Multi"}
2014-07-14 01:48:25,804 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31553,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327674244,"queuetimems":1,"class":"HRegionServer","responsesize":15866,"method":"Multi"}
2014-07-14 01:48:25,804 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30940,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327674858,"queuetimems":1,"class":"HRegionServer","responsesize":15503,"method":"Multi"}
2014-07-14 01:48:25,804 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30542,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327675255,"queuetimems":0,"class":"HRegionServer","responsesize":15788,"method":"Multi"}
2014-07-14 01:48:25,804 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31324,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327674473,"queuetimems":0,"class":"HRegionServer","responsesize":15577,"method":"Multi"}
2014-07-14 01:48:25,801 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31682,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327674115,"queuetimems":0,"class":"HRegionServer","responsesize":15563,"method":"Multi"}
2014-07-14 01:48:25,800 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27813,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327677986,"queuetimems":0,"class":"HRegionServer","responsesize":15777,"method":"Multi"}
2014-07-14 01:48:25,800 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29610,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327676187,"queuetimems":0,"class":"HRegionServer","responsesize":15835,"method":"Multi"}
2014-07-14 01:48:25,818 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327703910 with entries=108, filesize=92.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327705628
2014-07-14 01:48:25,818 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327553754
2014-07-14 01:48:25,818 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327555431
2014-07-14 01:48:25,818 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327589202
2014-07-14 01:48:25,818 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327591031
2014-07-14 01:48:25,818 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327593065
2014-07-14 01:48:25,818 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327594658
2014-07-14 01:48:25,819 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327596417
2014-07-14 01:48:25,819 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327597877
2014-07-14 01:48:25,819 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327598998
2014-07-14 01:48:25,819 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327602479
2014-07-14 01:48:25,819 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327605861
2014-07-14 01:48:25,819 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327606873
2014-07-14 01:48:25,819 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327608417
2014-07-14 01:48:25,819 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327609715
2014-07-14 01:48:25,819 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327611468
2014-07-14 01:48:25,819 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327613275
2014-07-14 01:48:25,819 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327614229
2014-07-14 01:48:25,819 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327615337
2014-07-14 01:48:25,819 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327617570
2014-07-14 01:48:25,819 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327619831
2014-07-14 01:48:25,819 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327621346
2014-07-14 01:48:26,120 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28262,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327677857,"queuetimems":0,"class":"HRegionServer","responsesize":15843,"method":"Multi"}
2014-07-14 01:48:26,120 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24274,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327681845,"queuetimems":1,"class":"HRegionServer","responsesize":15741,"method":"Multi"}
2014-07-14 01:48:26,120 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28002,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327678117,"queuetimems":0,"class":"HRegionServer","responsesize":16020,"method":"Multi"}
2014-07-14 01:48:26,120 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28051,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327678068,"queuetimems":0,"class":"HRegionServer","responsesize":15866,"method":"Multi"}
2014-07-14 01:48:26,121 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25604,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327680517,"queuetimems":0,"class":"HRegionServer","responsesize":15779,"method":"Multi"}
2014-07-14 01:48:26,122 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28220,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327677901,"queuetimems":1,"class":"HRegionServer","responsesize":15694,"method":"Multi"}
2014-07-14 01:48:26,133 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25531,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327680602,"queuetimems":0,"class":"HRegionServer","responsesize":15465,"method":"Multi"}
2014-07-14 01:48:26,139 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25593,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327680546,"queuetimems":1,"class":"HRegionServer","responsesize":15953,"method":"Multi"}
2014-07-14 01:48:26,140 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25565,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327680574,"queuetimems":0,"class":"HRegionServer","responsesize":15616,"method":"Multi"}
2014-07-14 01:48:26,140 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27808,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327678332,"queuetimems":0,"class":"HRegionServer","responsesize":15503,"method":"Multi"}
2014-07-14 01:48:26,149 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27974,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327678175,"queuetimems":1,"class":"HRegionServer","responsesize":15577,"method":"Multi"}
2014-07-14 01:48:26,157 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27942,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327678215,"queuetimems":0,"class":"HRegionServer","responsesize":15563,"method":"Multi"}
2014-07-14 01:48:26,588 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:48:26,592 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28226,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327678365,"queuetimems":0,"class":"HRegionServer","responsesize":15722,"method":"Multi"}
2014-07-14 01:48:26,592 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28771,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327677821,"queuetimems":0,"class":"HRegionServer","responsesize":15484,"method":"Multi"}
2014-07-14 01:48:26,594 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28343,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327678251,"queuetimems":0,"class":"HRegionServer","responsesize":15785,"method":"Multi"}
2014-07-14 01:48:26,597 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28302,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327678295,"queuetimems":0,"class":"HRegionServer","responsesize":15988,"method":"Multi"}
2014-07-14 01:48:27,267 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 76602 synced till here 76578
2014-07-14 01:48:27,629 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327705628 with entries=102, filesize=85.6m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327706589
2014-07-14 01:48:28,042 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30091,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:47152","starttimems":1405327677950,"queuetimems":0,"class":"HRegionServer","responsesize":15666,"method":"Multi"}
2014-07-14 01:48:28,692 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:48:29,738 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 76720 synced till here 76715
2014-07-14 01:48:29,826 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327706589 with entries=118, filesize=99.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327708693
2014-07-14 01:48:30,728 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:48:30,843 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 76834 synced till here 76822
2014-07-14 01:48:31,563 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327708693 with entries=114, filesize=97.7m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327710728
2014-07-14 01:48:32,013 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45.
2014-07-14 01:48:32,013 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. has too many store files; delaying flush up to 90000ms
2014-07-14 01:48:32,014 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:48:32,014 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 40 store files, 0 compacting, 40 eligible, 20 blocking
2014-07-14 01:48:32,014 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 40 files from compaction candidates
2014-07-14 01:48:32,014 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:48:32,014 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:48:32,014 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user3,1405324961957.b5c4e1ff5b6b7753b9fecbe9b697bf45. because compaction request was cancelled
2014-07-14 01:48:32,263 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:48:32,290 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 76941 synced till here 76914
2014-07-14 01:48:32,500 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327710728 with entries=107, filesize=84.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327712263
2014-07-14 01:48:33,959 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:48:34,007 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 77024 synced till here 77014
2014-07-14 01:48:34,111 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327712263 with entries=83, filesize=70.1m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327713959
2014-07-14 01:48:34,377 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72.
2014-07-14 01:48:34,377 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. has too many store files; delaying flush up to 90000ms
2014-07-14 01:48:34,378 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:48:34,378 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 40 store files, 0 compacting, 40 eligible, 20 blocking
2014-07-14 01:48:34,378 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 40 files from compaction candidates
2014-07-14 01:48:34,378 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:48:34,378 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:48:34,378 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user7,1405324961957.d5d5cc6db1bf5bd9142b4d4e667bac72. because compaction request was cancelled
2014-07-14 01:48:38,668 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:48:38,693 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327713959 with entries=77, filesize=61.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327718669
2014-07-14 01:48:40,829 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:48:40,849 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 77179 synced till here 77174
2014-07-14 01:48:40,916 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327718669 with entries=78, filesize=66.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327720830
2014-07-14 01:48:42,728 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:48:42,744 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 77254 synced till here 77252
2014-07-14 01:48:42,769 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327720830 with entries=75, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327722730
2014-07-14 01:48:44,189 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:48:44,518 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327722730 with entries=97, filesize=80.9m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327724190
2014-07-14 01:48:46,024 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:48:46,049 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 77429 synced till here 77427
2014-07-14 01:48:46,181 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327724190 with entries=78, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327726024
2014-07-14 01:48:50,272 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:48:50,305 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327726024 with entries=76, filesize=61.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327730272
2014-07-14 01:48:50,306 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 2 regions(s): 6e2bf74487f61672cd8bc06d8b34f003, 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:48:52,569 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:48:52,587 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 77579 synced till here 77577
2014-07-14 01:48:52,684 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327730272 with entries=74, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327732569
2014-07-14 01:48:52,685 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 2 regions(s): 6e2bf74487f61672cd8bc06d8b34f003, 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:48:54,907 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:48:55,437 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 77678 synced till here 77677
2014-07-14 01:48:55,455 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327732569 with entries=99, filesize=82.8m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327734971
2014-07-14 01:48:55,456 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 2 regions(s): 6e2bf74487f61672cd8bc06d8b34f003, 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:48:57,267 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:48:57,279 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 77752 synced till here 77750
2014-07-14 01:48:57,299 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327734971 with entries=74, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327737268
2014-07-14 01:48:57,301 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 2 regions(s): 6e2bf74487f61672cd8bc06d8b34f003, 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:48:58,835 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:48:58,861 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327737268 with entries=74, filesize=61.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327738835
2014-07-14 01:48:58,862 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 2 regions(s): 6e2bf74487f61672cd8bc06d8b34f003, 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:49:01,065 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:49:01,092 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327738835 with entries=74, filesize=61.3m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327741065
2014-07-14 01:49:01,093 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 2 regions(s): 6e2bf74487f61672cd8bc06d8b34f003, 92dfa1977cf31f19d29822ea57a55422
2014-07-14 01:49:02,146 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90045ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422.
2014-07-14 01:49:02,147 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422., current region memstore size 1.2g
2014-07-14 01:49:03,399 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:49:03,584 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-14 01:49:03,624 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327741065 with entries=71, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1405324583256/slave1%2C60020%2C1405324583256.1405327743585
2014-07-14 01:49:03,625 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 6e2bf74487f61672cd8bc06d8b34f003
2014-07-14 01:49:03,716 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-14 01:49:03,717 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. has too many store files, but is 1.2g vs best flushable region's 0.0. Choosing the bigger.
2014-07-14 01:49:03,717 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. due to global heap pressure
2014-07-14 01:49:03,717 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003., current region memstore size 1.2g
2014-07-14 01:49:04,574 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-14 01:49:26,272 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=19562, memsize=674.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/9f8d2d69b020450ca93c9fa3e490d5f0
2014-07-14 01:49:26,283 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/.tmp/9f8d2d69b020450ca93c9fa3e490d5f0 as hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/9f8d2d69b020450ca93c9fa3e490d5f0
2014-07-14 01:49:26,291 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/92dfa1977cf31f19d29822ea57a55422/family/9f8d2d69b020450ca93c9fa3e490d5f0, entries=2454310, sequenceid=19562, filesize=174.8m
2014-07-14 01:49:26,292 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.2g/1326406400, currentsize=42.1m/44175360 for region usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. in 24145ms, sequenceid=19562, compaction requested=true
2014-07-14 01:49:26,293 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:49:26,293 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 46 store files, 0 compacting, 46 eligible, 20 blocking
2014-07-14 01:49:26,293 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 46 files from compaction candidates
2014-07-14 01:49:26,293 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:49:26,293 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:49:26,293 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user1,1405324961957.92dfa1977cf31f19d29822ea57a55422. because compaction request was cancelled
2014-07-14 01:49:28,126 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=19529, memsize=685.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/eeb57ee83ccb42fa95cd3475a943011f
2014-07-14 01:49:28,136 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/.tmp/eeb57ee83ccb42fa95cd3475a943011f as hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/eeb57ee83ccb42fa95cd3475a943011f
2014-07-14 01:49:28,143 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6e2bf74487f61672cd8bc06d8b34f003/family/eeb57ee83ccb42fa95cd3475a943011f, entries=2495590, sequenceid=19529, filesize=177.7m
2014-07-14 01:49:28,144 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.2g/1338344400, currentsize=26.4m/27707920 for region usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. in 24427ms, sequenceid=19529, compaction requested=true
2014-07-14 01:49:28,144 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-14 01:49:28,144 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Selecting compaction from 46 store files, 0 compacting, 46 eligible, 20 blocking
2014-07-14 01:49:28,145 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 46 files from compaction candidates
2014-07-14 01:49:28,145 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-14 01:49:28,145 DEBUG [regionserver60020-smallCompactions-1405324622848] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-14 01:49:28,145 DEBUG [regionserver60020-smallCompactions-1405324622848] regionserver.CompactSplitThread: Not compacting usertable,user2,1405324961957.6e2bf74487f61672cd8bc06d8b34f003. because compaction request was cancelled
